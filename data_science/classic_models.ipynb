{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25ea4539",
   "metadata": {},
   "source": [
    "# Classic models\n",
    "\n",
    "There is a set of approaches for building algorithms that learn patterns from data, which were spreaded before the deep learning networks. This page considers these approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10322186",
   "metadata": {},
   "source": [
    "## Gradient boosting\n",
    "\n",
    "Gradient boosting is an ensemble approach to fitting machine learning models. Each subsequent weak learner (single model which composes the final ensemble) corrects the errors of the previous one.\n",
    "\n",
    "Consider **general boosting algorithm**:\n",
    "\n",
    "Lets denote:\n",
    "\n",
    "- $K$: nubmer of weak learners in the model.\n",
    "- $f_k(x_i), k = \\overline{1, K}$: $k$-th weak laerner.\n",
    "- $\\hat{y}_i^{(k)}, k = \\overline{1, K}$: predict of the $k$-th weak learner for $i$-th observation.\n",
    "\n",
    "$$\n",
    "\\hat{y}_i^{(0)} = 0 \\\\\n",
    "\\hat{y}_i^{(1)} = f_1(x_i) = \\hat{y}_i^{(0)} + f_1(x_i) \\\\\n",
    "\\hat{y}_i^{(2)} = f_1(x_i) + f_2(x_i)  = \\hat{y}_i^{(1)} + f_2(x_i)\\\\\n",
    "\\ldots \\\\\n",
    "\\hat{y}_i^{(K)} = \\hat{y}_i^{(K-1)} + f_{K}(x_i)  \n",
    "$$\n",
    "\n",
    "The final learner is:\n",
    "\n",
    "$$\\hat{y}_i^{(K)} = \\hat{y}_i^{(K-1)} + f_{K}(x_i) = \\sum_{k=0}^{K} f_i(x_i)$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
