{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "This section focuses on ways to measure something numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy\n",
    "\n",
    "The popularity of this metric stems from the fact that it is differentiable, making it suitable to be used as a loss function when fitting the parameters of machine learning models.\n",
    "\n",
    "Check [Cross-Entropy section at MLGlossary](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy for $o$-th observation can be written using following formula:\n",
    "\n",
    "$$-\\sum_{c=1}^M y_{o, c} log(p_{o,c})\n",
    "\\\\\n",
    "y_{o,c} = \n",
    "\\begin{cases}\n",
    "1, & \\text{if the } o\\text{-th observation belongs to class } c, \\\\\n",
    "0, & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "- $M$: number of possible classes.\n",
    "- $o$: index of the observation.\n",
    "- $c$: index of the class.\n",
    "- $p_{o,c}$: predicted probability that object $o$ belongs to class $c$; it must satisfy all probability properties, specifically $\\sum_{c=1}^M p_{o,c} = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Popular particular case is cross entropy for binary classification:\n",
    "\n",
    "$$-(y_o log[p_o] + [1-y_o] log[1-p_o])$$\n",
    "$$\n",
    "y_o=\\begin{cases}\n",
    "1, & \\text{if the } o\\text{-th observation has a learnt trait}, \\\\\n",
    "0, & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- $p_o$: probability that trait under consideration manifests itself in an $o$-th object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider example estimating performance of the predicted probabilites to the array where each object belongs to one of three classes - $\\{1, 2, 2, 3, 3, 1, 3\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has to be transformed into an array like $y_{o, c}$. The following cell shows the corresponding Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.array([1, 2, 2, 3, 3, 1, 3])\n",
    "y = np.concatenate(\n",
    "    [\n",
    "        (array==i).astype(int)[None, :].T\n",
    "        for i in np.sort(np.unique(array))\n",
    "    ], \n",
    "    axis=1\n",
    ")\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we have three algorithms that return probabilities $p_{o,c}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.833, 0.327, 0.538],\n",
       "       [0.412, 0.762, 0.31 ],\n",
       "       [0.44 , 0.711, 0.361],\n",
       "       [0.273, 0.245, 0.82 ],\n",
       "       [0.252, 0.431, 0.723],\n",
       "       [0.745, 0.266, 0.426],\n",
       "       [0.227, 0.366, 0.502]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.506, 0.392, 0.347],\n",
       "       [0.505, 0.566, 0.364],\n",
       "       [0.388, 0.585, 0.446],\n",
       "       [0.378, 0.499, 0.671],\n",
       "       [0.647, 0.249, 0.839],\n",
       "       [0.692, 0.32 , 0.599],\n",
       "       [0.574, 0.214, 0.713]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.493, 0.47 , 0.585],\n",
       "       [0.571, 0.522, 0.529],\n",
       "       [0.453, 0.524, 0.426],\n",
       "       [0.469, 0.465, 0.592],\n",
       "       [0.514, 0.527, 0.583],\n",
       "       [0.305, 0.368, 0.624],\n",
       "       [0.747, 0.638, 0.486]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "gen_probs = lambda m: np.clip(\n",
    "    np.random.normal(loc=m, scale=0.1 ,size=y.shape), \n",
    "    a_min=0, \n",
    "    a_max=1\n",
    ")\n",
    "best_probs = np.round(np.where(y==1, gen_probs(0.7), gen_probs(0.3)), 3)\n",
    "good_probs = np.round(np.where(y==1, gen_probs(0.6), gen_probs(0.4)), 3)\n",
    "bad_probs = np.round(np.where(y==1, gen_probs(0.5), gen_probs(0.5)), 3)\n",
    "\n",
    "for v in [best_probs, good_probs, bad_probs]:\n",
    "    display(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the \"algorithms\" are generated in such a way that there is a decrease in quality. The first and second \"algorithms\" have almost perfect accuracy, but the predictions of the first \"algorithm\" are more confident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the components of $y_{o, c} \\log(p_{o,c})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18272164, 0.        , 0.        ],\n",
       "       [0.        , 0.27180872, 0.        ],\n",
       "       [0.        , 0.34108285, 0.        ],\n",
       "       [0.        , 0.        , 0.19845094],\n",
       "       [0.        , 0.        , 0.32434606],\n",
       "       [0.29437106, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.68915516]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.68121861, 0.        , 0.        ],\n",
       "       [0.        , 0.5691612 , 0.        ],\n",
       "       [0.        , 0.53614343, 0.        ],\n",
       "       [0.        , 0.        , 0.39898614],\n",
       "       [0.        , 0.        , 0.17554457],\n",
       "       [0.36816932, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.33827386]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.7072461 , 0.        , 0.        ],\n",
       "       [0.        , 0.65008769, 0.        ],\n",
       "       [0.        , 0.64626359, 0.        ],\n",
       "       [0.        , 0.        , 0.52424864],\n",
       "       [0.        , 0.        , 0.53956809],\n",
       "       [1.1874435 , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.72154666]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for p in [best_probs, good_probs, bad_probs]:\n",
    "    display(-np.log(p)*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting that only the predictions for $y_{o,c}$ play a role; more confident predictions generate less cross-entropy for that observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just to be sure, let's compute the average cross-entropy for the entire sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3288480606756968"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.43821387695451447"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7109148978408835"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for p in [best_probs, good_probs, bad_probs]:\n",
    "    display(np.sum(-np.log(p)*y, axis=1, keepdims=True).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, higher-quality algorithms received a lower score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
