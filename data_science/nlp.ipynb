{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "\n",
    "Natural language processing is a field focused on algorithms for processing ordinary human text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "\n",
    "To work with text, you need to transform it into numbers - an approach more aligned with how computers process information. There are various techniques for achieving this.\n",
    "\n",
    "The following table describes typical terms used in the field of \"wrangling and preprocessing\".\n",
    "\n",
    "| Term                | Meaning                                                   | \n",
    "| ------------------- | --------------------------------------------------------- |\n",
    "| Conversion          | Transform text into a standartized format.                |\n",
    "| Sanitization        | Remove noise, unnecessary characters.                     |\n",
    "| Tokenization        | Split text into words or phrases                          |\n",
    "| Stemming            | Reduce words to their root form                           |\n",
    "| Lemmatization       | Normalize words based on their dictionary meaning.        |\n",
    "| Building embeddings | Transforming tokens into the vectors.                     |\n",
    "\n",
    "Check more details on this in the [Pre-Processing](nlp/preprocessing.ipynb) page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language understanding\n",
    "\n",
    "Is a group of NLP tasks aimed at understanding the structure and syntax of the language. Following table counts and describes typical tasks related to language understanding.\n",
    "\n",
    "| Task                 | Description                                        |\n",
    "| -------------------- | -------------------------------------------------- |\n",
    "| Parts of the speech  | Identify nouns, verbs, adjectives, etc.            |\n",
    "| Chunking             | Group related words together                       |\n",
    "| Dependency parsing   | Analyse grammatical relationship in a sentence     |\n",
    "| Constituency parsing | Break down a sentence into hierarchical sub-units  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and Functionality\n",
    "\n",
    "NLP has numerous applications in business. The most typical are listed in the following table:\n",
    "\n",
    "| Functionality                   | Description                                                    |\n",
    "|--------------------------------|----------------------------------------------------------------|\n",
    "| **Named Entity Recognition (NER)** | Identify proper names (e.g., people, places).                  |\n",
    "| **N-gram Identification**      | Analyze word sequences to predict text.                        |\n",
    "| **Sentiment Analysis**         | Detect emotions and opinions in text.                          |\n",
    "| **Information Extraction**     | Identify key information from unstructured data.               |\n",
    "| **Information Retrieval**      | Find relevant documents or data.                               |\n",
    "| **Questions and Answering**    | Process user queries for precise answers.                      |\n",
    "| **Topic Modeling**             | Identify key themes in text data.                              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-gen metrics\n",
    "\n",
    "It's a complex task to estimate the quality of text generation. Generally, there are no ground truth subsets with which we can compare generated texts in such a task. The existing approaches allow us to estimate:\n",
    "\n",
    "- **Diversity**: Ensures that the generated texts are not just the most popular combinations of tokens. A common way to measure of unique n-grams is to calculate the ratio of unique n-grams to the total number of n-grams:\n",
    "\n",
    "    $$\\frac{|\\mathrm{unique\\ n\\!-\\!grams}|}{|\\mathrm{all\\ n\\!-\\!grams}|}$$\n",
    "\n",
    "- **Memorization**: The proportion of generated n-grams that match n-grams in the training corpus.\n",
    "- **Perplexity**: It is the probability that the model will generate the text from a given text corpus. The general formula for compuging perplexity is as follows:\n",
    "\n",
    "    $$PPL(x) = p(x_1, x_2, \\ldots, x_m)^{-1/m}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text CNN\n",
    "\n",
    "Text CNN is a method for applying convolutional architecture concepts to NLP.\n",
    "\n",
    "Check the [Text CNN](nlp/text_cnn.ipynb) page for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs\n",
    "\n",
    "LLMs are models designed primarily to predict text. The most advanced LLMs can simulate a wide range of linguistic behaviors. With the right configuration, they can be applied to many problems that are difficult to solve with traditional programming. In this section on LLMs, we will cover:\n",
    "\n",
    "* **How LLMs work**\n",
    "* **Using LLMs**\n",
    "\n",
    "  * **Prompt engineering**: techniques for guiding LLMs to achieve specific goals.\n",
    "  * **Agent systems**: enabling LLMs to use tools and interact with other systems.\n",
    "\n",
    "Check the [LLMs](nlp/LLMs.ipynb) page."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
