{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "\n",
    "Natural language processing is a field focused on algorithms for processing ordinary human text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "\n",
    "To work with text, you need to transform it into numbers - an approach more aligned with how computers process information. There are various techniques for achieving this.\n",
    "\n",
    "The following table describes typical terms used in the field of \"wrangling and preprocessing\".\n",
    "\n",
    "| Term                | Meaning                                                   | \n",
    "| ------------------- | --------------------------------------------------------- |\n",
    "| Conversion          | Transform text into a standartized format.                |\n",
    "| Sanitization        | Remove noise, unnecessary characters.                     |\n",
    "| Tokenization        | Split text into words or phrases                          |\n",
    "| Stemming            | Reduce words to their root form                           |\n",
    "| Lemmatization       | Normalize words based on their dictionary meaning.        |\n",
    "| Building embeddings | Transforming tokens into the vectors.                     |\n",
    "\n",
    "Check more details on this in the [Pre-Processing](nlp/preprocessing.ipynb) page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language understanding\n",
    "\n",
    "Is a group of NLP tasks aimed at understanding the structure and syntax of the language. Following table counts and describes typical tasks related to language understanding.\n",
    "\n",
    "| Task                 | Description                                        |\n",
    "| -------------------- | -------------------------------------------------- |\n",
    "| Parts of the speech  | Identify nouns, verbs, adjectives, etc.            |\n",
    "| Chunking             | Group related words together                       |\n",
    "| Dependency parsing   | Analyse grammatical relationship in a sentence     |\n",
    "| Constituency parsing | Break down a sentence into hierarchical sub-units  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and Functionality\n",
    "\n",
    "NLP has numerous applications in business. The most typical are listed in the following table:\n",
    "\n",
    "| Functionality                   | Description                                                    |\n",
    "|--------------------------------|----------------------------------------------------------------|\n",
    "| **Named Entity Recognition (NER)** | Identify proper names (e.g., people, places).                  |\n",
    "| **N-gram Identification**      | Analyze word sequences to predict text.                        |\n",
    "| **Sentiment Analysis**         | Detect emotions and opinions in text.                          |\n",
    "| **Information Extraction**     | Identify key information from unstructured data.               |\n",
    "| **Information Retrieval**      | Find relevant documents or data.                               |\n",
    "| **Questions and Answering**    | Process user queries for precise answers.                      |\n",
    "| **Topic Modeling**             | Identify key themes in text data.                              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-gen metrics\n",
    "\n",
    "It's a complex task to estimate the quality of text generation. Generally, there are no ground truth subsets with which we can compare generated texts in such a task. The existing approaches allow us to estimate:\n",
    "\n",
    "- **Diversity**: Ensures that the generated texts are not just the most popular combinations of tokens. A common way to measure of unique n-grams is to calculate the ratio of unique n-grams to the total number of n-grams:\n",
    "\n",
    "    $$\\frac{|\\mathrm{unique\\ n\\!-\\!grams}|}{|\\mathrm{all\\ n\\!-\\!grams}|}$$\n",
    "\n",
    "- **Memorization**: The proportion of generated n-grams that match n-grams in the training corpus.\n",
    "- **Perplexity**: It is the probability that the model will generate the text from a given text corpus. The general formula for compuging perplexity is as follows:\n",
    "\n",
    "    $$PPL(x) = p(x_1, x_2, \\ldots, x_m)^{-1/m}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text CNN\n",
    "\n",
    "Text CNN is a method for applying convolutional architecture concepts to NLP.\n",
    "\n",
    "Check the [Text CNN](nlp/text_cnn.ipynb) page for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt engineering\n",
    "\n",
    "Prompt engineering is a set of approaches used to configure a text generation model to produce the exact results you're interested in.\n",
    "\n",
    "Check the [Prompt Engineering](https://www.gptaiflow.com/assets/files/2025-01-18-pdf-1-TechAI-Goolge-whitepaper_Prompt%20Engineering_v4-af36dcc7a49bb7269a58b1c9b89a8ae1.pdf) guide provided by google.\n",
    "\n",
    "Generally there are two concepts you need to make the model to produce relevant output:\n",
    "\n",
    "- **Configure the model** by changing parameters. Models implemented by different organizatinos have different configuration options but most have: output length and sampling controls.\n",
    "- **Bulding a promt**. for this there are following techniques:\n",
    "    - General prompting / zero shot.\n",
    "    - One & few shot prompting.\n",
    "    - System, contextual and role prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token sampling\n",
    "\n",
    "At each step of the generation process, the model generates the next token by classifying it based on previous tokens. Thus, at some point, the model's predictions resemble the probabilities of the next token:\n",
    "\n",
    "$\\left(p_1, p_2, \\ldots, p_n\\right), \\sum_{i=1}^n p_i =1$\n",
    "\n",
    "Where $n$ is a vocabulary of the model.\n",
    "\n",
    "**The temperature** regulates the randomness of the selected tokens. A value of 0 results in deterministic model outputs. The higher the value,a the more creative the model's output will be.\n",
    "\n",
    "**Top-k** specifies the model that selects the next token from among the $k$ tokens with the highest probability.\n",
    "\n",
    "**Top-p** it considers the smallest possible set of tokens whose cumulative probability exceeds a predefined threshold, $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting techniques\n",
    "\n",
    "There are different approaches associated to providing model information about the structure of required output:\n",
    "\n",
    "- **Zero shot**: general prompting technique, just query to the model without providing any additional information.\n",
    "- **One shot & Zero shot**: to explain the model the structure of the output you expect from it.\n",
    "\n",
    "There are following options when specifying the general patterns of the model behaviour:\n",
    "\n",
    "- **System prompting** sets the overall context and purpose for the language model.\n",
    "- **Contextual prompting** provides specific details or background information relevant to the current convecrsation task.\n",
    "- **Role prompting** assigning the a specific character or identity for the language model\n",
    "\n",
    "**Step-back prompting**: Two prompts are provided: a spefic prompt and a more general prompt. The more general prompt ussually asks about typical approaches to the issue described in the specific prompt. The second prompt includes the model's answer to the general prompt as context and the specific prompt as the taks.\n",
    "\n",
    "**Chain of Thought (CoT)**: A technique in which the model is asked to solve a task step by step. In the most basic implementation, the prompt literally asks the model to solve the problem \"step by step\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n",
    "**AI agents are** programs where AI controls workflow.\n",
    "\n",
    "Agent can complete some **actions** using some **tools**.\n",
    "\n",
    "**Tools**:  provide the agent with the ability to execute actions a text-generation model cannot perform natively, such as making coffe or generating images.\n",
    "\n",
    "There are different ways in which AI outputs can influence the workflow. These approaches are listed in the following table:\n",
    "\n",
    "| Name             | Description                                                     | Example code                                       |\n",
    "|------------------|-----------------------------------------------------------------|----------------------------------------------------|\n",
    "| Router           | LLM output controls an if/else switch                           | `if llm_description(): path_a() else: path_b()`    |\n",
    "| Tool call        | LLM output controls function execution                          | `run_function(llm_chosen_tool, llm_chosen_args)`   |\n",
    "| Multi-step Agent | LLM output controls interation and program continuation         | `while llm_should_continue(): execute_next_step()` |\n",
    "| Multi-Agent      | One agentig workflow can start another workflow                 | `if llm_trigger(): execute_agent()`                |\n",
    "| Code Agents      | LLM acts in code, can define its own tools / start other agents | `def custom_tool()`                                |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
