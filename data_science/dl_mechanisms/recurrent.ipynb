{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent\n",
    "\n",
    "This page explains the concept of a recurrent layer.\n",
    "\n",
    "The key idea is to create a mechanism where each input affects the processing and outcome of subsequent inputs.\n",
    "\n",
    "![](recurrent_files/recurrent_schema.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An RNN is essentially a single layer. At each step, it uses $h_{t-1}$, a special state vector from the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strictly speaking, the deduction is as follows:  \n",
    "\n",
    "$$h_t = f(x_t W^T_1 + b_1 + h_{t-1} W^T_2 + b_2)$$\n",
    "\n",
    "Where:  \n",
    "- $x_t$: input at the $t$-th step.  \n",
    "- $h_t$: vector that describes hidden state at the $t$-th step.  \n",
    "- $W_1$: weights associated with the input.  \n",
    "- $W_2$: weights associated with the state.  \n",
    "- $b_1$: bias associated with the input.  \n",
    "- $b_2$: bias associated with the state.  \n",
    "- $f$: activation function, typically a hyperbolic tangent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really popular wa to visualise RNN transformation is following:\n",
    "\n",
    "\n",
    "![](recurrent_files/Basic%20RNN%20schema.svg)\n",
    "\n",
    "It is assumed that to the hidden state $h_t$ can be applied linear transformation, $O$, that brings it to the expected dimensionality if this element is to be used in the subsequent steps.\n",
    "\n",
    "It's important to understand this approach to visualization because as it is widely used to describe the RNN's modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realization on python\n",
    "\n",
    "In this section, we will step by step implement the computations performed by a recurrent layer and compare them with `torch.nn.RNN` as the reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines the parameters of the recurrent procedure that we will use as an example.\n",
    "\n",
    "We have:  \n",
    "\n",
    "- A sequence of $15$ elements: $\\left\\{ x_1, x_2, \\dots, x_{10} \\right\\}$.  \n",
    "- Each element is a vector of $5$ elements: $x_t \\in \\mathbb{R}^5$.  \n",
    "- We are working with a sample containing $10$ sequences.  \n",
    "- The state vector is a $3$-element vector: $h_t \\in \\mathbb{R}^3$.  \n",
    "- The activation function $f(x)$ is the hyperbolic tangent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "samples_size = 15\n",
    "element_size = 5\n",
    "sequence_size = 10\n",
    "state_size = 3\n",
    "activation = torch.nn.Tanh()\n",
    "\n",
    "input_data = torch.rand(samples_size, sequence_size, element_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For given inputs:\n",
    "\n",
    "- Input weights $W_1$ should be a $3 \\times 5$ matrix.  \n",
    "- Input bias $b_1$ should be a vector with $3$ elements.  \n",
    "- State weights $W_2$ should be a $3 \\times 3$ matrix.  \n",
    "- State bias $b_2$ should be a vector with $3$ elements.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_1 = torch.rand(state_size, element_size)\n",
    "b_1 = torch.rand(state_size)\n",
    "W_2 = torch.rand(state_size, state_size)\n",
    "b_2 = torch.rand(state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realisation of $x_1 W^T_1 + b_1 + h_{0} W^T_2 + b_2$, where initial state initialised as $h_{0}=(0)_{10}$, will take form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0187, 0.5179, 2.1283],\n",
       "        [2.3602, 1.7332, 3.0126],\n",
       "        [1.7238, 1.3923, 2.8326],\n",
       "        [1.6351, 1.1317, 2.6059],\n",
       "        [1.5822, 1.4294, 3.1752],\n",
       "        [1.5521, 0.8518, 2.3209],\n",
       "        [2.0734, 1.5617, 2.4171],\n",
       "        [2.1795, 1.8786, 3.2781],\n",
       "        [1.9290, 1.4330, 3.1887],\n",
       "        [1.8527, 1.4238, 3.0310],\n",
       "        [1.9417, 1.5560, 2.9881],\n",
       "        [1.5617, 0.9667, 2.4146],\n",
       "        [2.4599, 2.0704, 3.4121],\n",
       "        [2.0325, 1.6429, 3.2050],\n",
       "        [1.4736, 0.9799, 2.1185]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = torch.zeros(samples_size, state_size)\n",
    "(input_data[:, 0, :] @ W_1.T) + b_1 + (state @ W_2.T) + b_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the result we got $h_1$ for each sample out of $15$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of the full recurrent procedure for all $15$ elements of the sequence is provided in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [state]\n",
    "\n",
    "for i in range(input_data.shape[1]):\n",
    "    res = activation( \n",
    "        (input_data[:, i, :] @ W_1.T) + b_1\n",
    "        + (states[-1] @ W_2.T) + b_2\n",
    "    )\n",
    "    states.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As result we have $11$ (including initial) states. Which is matrix of $15$ rows - each row for corresponding observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9638, 0.9522, 0.9989],\n",
       "        [0.9718, 0.9704, 0.9994],\n",
       "        [0.9982, 0.9973, 0.9999],\n",
       "        [0.9974, 0.9940, 0.9999],\n",
       "        [0.9912, 0.9944, 0.9998],\n",
       "        [0.9909, 0.9858, 0.9996],\n",
       "        [0.9976, 0.9981, 0.9999],\n",
       "        [0.9956, 0.9934, 0.9998],\n",
       "        [0.9956, 0.9870, 0.9998],\n",
       "        [0.9964, 0.9967, 0.9999],\n",
       "        [0.9839, 0.9770, 0.9994],\n",
       "        [0.9890, 0.9855, 0.9996],\n",
       "        [0.9906, 0.9791, 0.9996],\n",
       "        [0.9985, 0.9980, 0.9999],\n",
       "        [0.9965, 0.9937, 0.9997]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we take all first rows as states of the first sample, all second rows as states of the second sample and so on - which can be realized as stacking on the columns (-2) dimension. Torch also separately represents the last states for all samples, so it is a second output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ans = (\n",
    "    torch.stack(states[1:], dim=-2), \n",
    "    states[-1][None]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will do the same using the ready-made `torch.nn.RNN` class. Before computing, we need to set the weights of the instance to match those used in the custom procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = torch.nn.RNN(element_size, state_size, batch_first=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Copying the parameters that were used earlier:\n",
    "    rnn.weight_ih_l0.copy_(W_1)\n",
    "    rnn.bias_ih_l0.copy_(b_1)\n",
    "    rnn.weight_hh_l0.copy_(W_2)\n",
    "    rnn.bias_hh_l0.copy_(b_2)\n",
    "\n",
    "    torch_ans = rnn(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell verifies that both outputs are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.testing.assert_close(torch_ans[0], my_ans[0])\n",
    "torch.testing.assert_close(torch_ans[1], my_ans[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
