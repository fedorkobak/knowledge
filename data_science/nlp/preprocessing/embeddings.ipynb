{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00ba84a0",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "Embedding is the representation of something as a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b7a0b",
   "metadata": {},
   "source": [
    "## Dence vs Sparce\n",
    "\n",
    "There are two types of embeddings: **dence** and **sparce**.\n",
    "\n",
    "**Sparce** embeddings usually result in a high-dimensional vector close to the vocabulary size (30 000 is a typical size). Each position of in the encoding vector corresponds to a specific token in the vocabulary, which makes the interpreting the results easier.\n",
    "\n",
    "**Dence** embeddings typically have fewer dimentions (384, 768, or 1024 elements), and the position of an element is not directly related to a specific token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0306d887",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Word2Vec (W2V) is an approach to building word embeddings based on the context. Words with similar contexts will have similar embeddings.\n",
    "\n",
    "To each word corresponds two vectors:\n",
    "\n",
    "- $u_i \\in \\mathbb{R}^n$: center vector.\n",
    "- $\\nu_i \\in \\mathbb{R}^n$: context vector.\n",
    "\n",
    "Now, let's consider words, $i$ and $j$. The probability of encounting word $i$ in the context of word $j$ we'll define as following:\n",
    "\n",
    "$$p_{ij} = \\sigma(u_i^T \\nu_j)$$\n",
    "\n",
    "- $\\sigma$: sigmoid function.\n",
    "\n",
    "The optimization algorithm looks for $u_i$ and $\\nu_i$ that maximize $p_{ij}$ when word $i$ contains word $j$ in its context and minimize it when it does not."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
