{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "902eb5b1",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Tokenization is the process of identyfying the segments that would be considred by the text processing algorithms as an atomic units.\n",
    "\n",
    "**Vocabulary**: The set of tokens used by the tokenization approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f098d0c9",
   "metadata": {},
   "source": [
    "## Wordpiece\n",
    "\n",
    "This approach takes all possible symbols that is awailable in the example texts courpuse - at the first step it is a vocabulary of the model. At the each following step it adds to the vocabulary pair of tokens from the current vocabulary that have the highest score:\n",
    "\n",
    "$$\\frac{N(i, j)}{N(i)N(j)}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $N(i, j)$: frequency of the pair formed from $i$-th and $j$-th tokens.\n",
    "- $N(i)$: frequenccy of the $i$-th token.\n",
    "\n",
    "The idea behind this formula is as follows: If some tokens appear together very frequently but not often separately, then a pair should be added. Accordingly, if the nominator $N(i,j)$, is high if tokens often appear together often, but denominator, $N(i)N(j)$, will take relatively low values, if separately do not appear together often."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446fea3f",
   "metadata": {},
   "source": [
    "## Tokenization playground\n",
    "\n",
    "Try the tokenization playgroud launched by Hugging Face. It's a service that allows you to experiment with various tokenizers used in modern machine learning models.\n",
    "\n",
    "<iframe src=\"https://agents-course-the-tokenizer-playground.static.hf.space\" frameborder=\"0\" width=\"850\" height=\"450\"></iframe>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
