{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKNGHzK1-R4o"
   },
   "source": [
    "# Recurrent translations\n",
    "\n",
    "This notebook considers the solution of the translation task using recurrent layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T12:34:10.636157Z",
     "iopub.status.busy": "2024-12-08T12:34:10.635876Z",
     "iopub.status.idle": "2024-12-08T12:34:15.314008Z",
     "shell.execute_reply": "2024-12-08T12:34:15.313261Z",
     "shell.execute_reply.started": "2024-12-08T12:34:10.636089Z"
    },
    "id": "hQcqiP4P-R4t",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9SYzcLJ-R4u"
   },
   "source": [
    "## Data\n",
    "\n",
    "As an example, the English->Russian data set `tatoeda` is considered. [Tatoeda](https://tatoeba.org/en/) - collections of sentences and their translations. In particular, it's [hugging face implementation](https://huggingface.co/datasets/Helsinki-NLP/tatoeba) was used. The following cell loads data, transforms it into a more convenient format and displays some sentences from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-08T12:34:15.316645Z",
     "iopub.status.busy": "2024-12-08T12:34:15.315684Z",
     "iopub.status.idle": "2024-12-08T12:34:43.809260Z",
     "shell.execute_reply": "2024-12-08T12:34:43.808348Z",
     "shell.execute_reply.started": "2024-12-08T12:34:15.316601Z"
    },
    "id": "-gBA7tiJ-R4w",
    "outputId": "3f6cb108-b834-4513-f2c7-fbc8c8f3e2af",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"tatoeba\", lang1=\"en\", lang2=\"ru\")\n",
    "dataset = [\n",
    "    (translation[\"en\"], translation[\"ru\"])\n",
    "    for translation in dataset[\"train\"][\"translation\"]\n",
    "]\n",
    "dataset[200:205]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell prepares the following:  \n",
    "- Service tokens.\n",
    "- Vocabulary. \n",
    "- Tokenization transformation.  \n",
    "- Mappings between tokens and indices (both directions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T12:34:43.810543Z",
     "iopub.status.busy": "2024-12-08T12:34:43.810266Z",
     "iopub.status.idle": "2024-12-08T12:34:45.165901Z",
     "shell.execute_reply": "2024-12-08T12:34:45.164954Z",
     "shell.execute_reply.started": "2024-12-08T12:34:43.810517Z"
    },
    "id": "4LR2EYTD-R4x",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Special tokens\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "# End of sentence\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "# Start of sentence\n",
    "SOS_TOKEN = \"<SOS>\"\n",
    "# Unknown\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "\n",
    "def tokenize(sentence: str) -> list[str]:\n",
    "    '''Tokinelattion of the given stirng  by words'''\n",
    "    return sentence.lower().split()\n",
    "\n",
    "EN_VOCAB = {PAD_TOKEN, EOS_TOKEN, SOS_TOKEN, UNK_TOKEN}\n",
    "RU_VOCAB = {PAD_TOKEN, EOS_TOKEN, SOS_TOKEN, UNK_TOKEN}\n",
    "for en, ru in dataset:\n",
    "    EN_VOCAB.update(tokenize(en))\n",
    "    RU_VOCAB.update(tokenize(ru))\n",
    "\n",
    "def create_mappings(vocab: set) -> tuple[dict[str, int], dict[int, str]]:\n",
    "    '''\n",
    "    Create mappings.\n",
    "    '''\n",
    "    word2int = {word: i for i, word in enumerate(vocab)}\n",
    "    int2word = {i: word for word, i in word2int.items()}\n",
    "    return word2int, int2word\n",
    "\n",
    "EN_WORD2INT, EN_INT2WORD = create_mappings(EN_VOCAB)\n",
    "RU_WORD2INT, RU_INT2WORD = create_mappings(RU_VOCAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `torch.utils.data.Dataset` is set up to iterate over sentence pairs, already transformed into tensors containing token indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-08T12:34:45.168041Z",
     "iopub.status.busy": "2024-12-08T12:34:45.167766Z",
     "iopub.status.idle": "2024-12-08T12:34:45.361844Z",
     "shell.execute_reply": "2024-12-08T12:34:45.361017Z",
     "shell.execute_reply.started": "2024-12-08T12:34:45.168016Z"
    },
    "id": "151t2z9S-R4y",
    "outputId": "01fd9e6d-1e0e-4776-bc93-b98d84b3798d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tensor_tokenize(\n",
    "    sentence: str,\n",
    "    word2int: dict[str, int],\n",
    "    device: torch.device = DEVICE\n",
    ") -> torch.Tensor:\n",
    "    '''Transform sentence into tensor with indeces of tokens.'''\n",
    "    return torch.tensor(\n",
    "        [\n",
    "            word2int.get(word, word2int[UNK_TOKEN])\n",
    "            for word in tokenize(sentence)\n",
    "        ]\n",
    "        + [word2int[EOS_TOKEN]],\n",
    "        dtype=torch.long,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "\n",
    "class TranslationDataset(torch.utils.data.Dataset):\n",
    "    '''A data set iterating over input->translation pairs.'''\n",
    "    def __init__(\n",
    "        self,\n",
    "        pairs: list[tuple[str, str]],\n",
    "        en_word2int: dict[str, int],\n",
    "        ru_word2int: dict[str, int]\n",
    "    ):\n",
    "        self.pairs = pairs\n",
    "        self.en_word2int = en_word2int\n",
    "        self.ru_word2int = ru_word2int\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eng, rus = self.pairs[idx]\n",
    "        eng_tensor = tensor_tokenize(sentence=eng, word2int=self.en_word2int)\n",
    "        rus_tensor = tensor_tokenize(sentence=rus, word2int=self.ru_word2int)\n",
    "        return eng_tensor, rus_tensor\n",
    "\n",
    "translation_dataset = TranslationDataset(\n",
    "    pairs=dataset,\n",
    "    en_word2int=EN_WORD2INT,\n",
    "    ru_word2int=RU_WORD2INT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of using the created dataset - it simply returns a pair of tensors containing the indices of English and Russian tokens, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([35081, 45487, 32934, 30542,  3478, 14224, 39060, 16212, 16250, 48871,\n",
       "         44435, 48467,  1121, 19334, 49510]),\n",
       " tensor([ 54680,  39000,  37535,  60821, 120953,  29419,  54389,  24444,  19745,\n",
       "          28568,  90264,  60302]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(translation_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, a dataloader constructs minibatches of pairs and pads shorter sentences so that data across a set of sentences can be represented as a single tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-08T12:34:45.363010Z",
     "iopub.status.busy": "2024-12-08T12:34:45.362778Z",
     "iopub.status.idle": "2024-12-08T12:34:45.419813Z",
     "shell.execute_reply": "2024-12-08T12:34:45.418942Z",
     "shell.execute_reply.started": "2024-12-08T12:34:45.362987Z"
    },
    "id": "JFtSwmsE-R40",
    "outputId": "c059a5bf-4754-4f66-d40c-d93d400501a6",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[35081, 45487, 32934, 30542,  3478, 14224, 39060, 16212, 16250, 48871,\n",
       "          44435, 48467,  1121, 19334, 49510],\n",
       "         [24420,  6220, 31775, 49510, 11129, 11129, 11129, 11129, 11129, 11129,\n",
       "          11129, 11129, 11129, 11129, 11129]]),\n",
       " tensor([[ 54680,  39000,  37535,  60821, 120953,  29419,  54389,  24444,  19745,\n",
       "           28568,  90264,  60302,  80629,  80629,  80629,  80629],\n",
       "         [122624,  64426,  45452,  60302,  80629,  80629,  80629,  80629,  80629,\n",
       "           80629,  80629,  80629,  80629,  80629,  80629,  80629]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate_fn(batch: list[torch.Tensor, torch.Tensor]):\n",
    "    '''\n",
    "    Transforms a list of tokinized sentences into the torch tensor. Should be\n",
    "    used as `collate_fn` argument of the dataloader. The main purpose is to\n",
    "    pad all sentences to have the same length.\n",
    "    '''\n",
    "    eng_batch, rus_batch = zip(*batch)\n",
    "    eng_batch_padded = pad_sequence(\n",
    "        eng_batch, batch_first=True, padding_value=EN_WORD2INT[PAD_TOKEN]\n",
    "    )\n",
    "    rus_batch_padded = pad_sequence(\n",
    "        rus_batch, batch_first=True, padding_value=RU_WORD2INT[PAD_TOKEN]\n",
    "    )\n",
    "    return eng_batch_padded, rus_batch_padded\n",
    "\n",
    "batch_size = 64\n",
    "translation_dataloader = torch.utils.data.DataLoader(\n",
    "    translation_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "batch = next(iter(translation_dataloader))\n",
    "(batch[0][:2], batch[1][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4qn2g0Wa-R41"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-08T12:34:45.421066Z",
     "iopub.status.busy": "2024-12-08T12:34:45.420807Z",
     "iopub.status.idle": "2024-12-08T12:34:45.433082Z",
     "shell.execute_reply": "2024-12-08T12:34:45.432452Z",
     "shell.execute_reply.started": "2024-12-08T12:34:45.421041Z"
    },
    "id": "oBP2ezRT-R42",
    "outputId": "a0c02ab0-cf8a-4c57-d4a3-5c6cfe789ce5",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 15])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = next(iter(translation_dataloader))[0]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T12:34:45.434556Z",
     "iopub.status.busy": "2024-12-08T12:34:45.434198Z",
     "iopub.status.idle": "2024-12-08T12:34:45.440984Z",
     "shell.execute_reply": "2024-12-08T12:34:45.440246Z",
     "shell.execute_reply.started": "2024-12-08T12:34:45.434519Z"
    },
    "id": "71y9rIeN-R44",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embed_size: int,\n",
    "        hidden_size: int,\n",
    "        num_layers: int = 1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=embed_size,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        # Concatenating results of all layers and all directions to one long\n",
    "        # sequence\n",
    "        hidden = torch.cat(\n",
    "            [hidden[i, :, :] for i in range(len(hidden))], dim=1\n",
    "        ).unsqueeze(0)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-08T12:34:45.442690Z",
     "iopub.status.busy": "2024-12-08T12:34:45.442070Z",
     "iopub.status.idle": "2024-12-08T12:34:45.725839Z",
     "shell.execute_reply": "2024-12-08T12:34:45.724991Z",
     "shell.execute_reply.started": "2024-12-08T12:34:45.442654Z"
    },
    "id": "-P3eNK66-R44",
    "outputId": "cda00ca2-c604-443c-ecbe-a5f0ac47048e",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 80])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_size = 10\n",
    "hidden_size = 10\n",
    "num_layers = 4\n",
    "\n",
    "encoder = Encoder(\n",
    "    vocab_size=len(EN_VOCAB),\n",
    "    embed_size=embed_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers\n",
    ").to(DEVICE)\n",
    "encoder(x)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T12:34:45.727172Z",
     "iopub.status.busy": "2024-12-08T12:34:45.726861Z",
     "iopub.status.idle": "2024-12-08T12:34:45.733027Z",
     "shell.execute_reply": "2024-12-08T12:34:45.732144Z",
     "shell.execute_reply.started": "2024-12-08T12:34:45.727139Z"
    },
    "id": "Pv4nzYAf-R45",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self: int,\n",
    "        vocab_size: int,\n",
    "        embed_size: int,\n",
    "        hidden_size: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=embed_size,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(in_features=hidden_size, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        out = self.embedding(x)\n",
    "        out, hidden = self.rnn(out, hidden)\n",
    "        out = self.fc(out).reshape(out.size(0), -1)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T12:34:45.736232Z",
     "iopub.status.busy": "2024-12-08T12:34:45.735910Z",
     "iopub.status.idle": "2024-12-08T12:34:45.869033Z",
     "shell.execute_reply": "2024-12-08T12:34:45.868121Z",
     "shell.execute_reply.started": "2024-12-08T12:34:45.736186Z"
    },
    "id": "3TXH5ccs-R45",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(\n",
    "    vocab_size=len(RU_VOCAB),\n",
    "    embed_size=10,\n",
    "    hidden_size=hidden_size*num_layers*2\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T12:34:45.870688Z",
     "iopub.status.busy": "2024-12-08T12:34:45.870334Z",
     "iopub.status.idle": "2024-12-08T12:34:45.878319Z",
     "shell.execute_reply": "2024-12-08T12:34:45.877490Z",
     "shell.execute_reply.started": "2024-12-08T12:34:45.870650Z"
    },
    "id": "mjsFohNI-R45",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def translate(\n",
    "    encoder: Encoder,\n",
    "    decoder: Decoder,\n",
    "    sentence: str,\n",
    "    en_word2int: dict[str: int],\n",
    "    ru_int2word: dict[int: str],\n",
    "    ru_word2int: dict[str: int],\n",
    "    max_length: int = 15,\n",
    "    device: torch.device = DEVICE\n",
    "):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        input_tensor = tensor_tokenize(sentence=sentence, word2int=en_word2int)\n",
    "        input_tensor = input_tensor.view(1, -1).to(device)\n",
    "\n",
    "        # Pass input sentence through encoder\n",
    "        _, encoder_hidden = encoder(input_tensor)\n",
    "        # Intialise hidden state of decoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        last_word = torch.tensor([[en_word2int[SOS_TOKEN]]]).to(device)\n",
    "        for _ in range(max_length):\n",
    "            # Pass last predicted token through decoder\n",
    "            logits, decoder_hidden = decoder(last_word, decoder_hidden)\n",
    "            # Selecting the most probable token\n",
    "            next_token = logits.argmax(dim=1)\n",
    "            last_word = next_token.unsqueeze(0).to(device)\n",
    "            if next_token.item() == ru_word2int[EOS_TOKEN]:\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(ru_int2word.get(next_token.item()))\n",
    "\n",
    "    # return predicted words as a string\n",
    "    return \" \".join(decoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "execution": {
     "iopub.execute_input": "2024-12-08T12:34:45.880393Z",
     "iopub.status.busy": "2024-12-08T12:34:45.879781Z",
     "iopub.status.idle": "2024-12-08T12:34:45.957742Z",
     "shell.execute_reply": "2024-12-08T12:34:45.956950Z",
     "shell.execute_reply.started": "2024-12-08T12:34:45.880354Z"
    },
    "id": "7jchNZcb-R46",
    "outputId": "835916b7-45d5-40a2-b913-62edca58b7ee",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'экватор детьми-близнецами бифштекс? посещаете горячая? крыса отсутствии иностранца. принципа, умрёте? пловцы. спорно. джастин драться, иностранца. принципа, умрёте? пловцы. спорно. джастин'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    sentence=\"hello world\",\n",
    "    en_word2int=EN_WORD2INT,\n",
    "    ru_int2word=RU_INT2WORD,\n",
    "    ru_word2int=RU_WORD2INT,\n",
    "    max_length=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zChwPAzb-R46"
   },
   "source": [
    "## Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T12:34:45.959456Z",
     "iopub.status.busy": "2024-12-08T12:34:45.958852Z",
     "iopub.status.idle": "2024-12-08T12:34:55.148410Z",
     "shell.execute_reply": "2024-12-08T12:34:55.147721Z",
     "shell.execute_reply.started": "2024-12-08T12:34:45.959413Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i, v in enumerate(translation_dataloader):\n",
    "    if i == 1965:\n",
    "        break\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=EN_WORD2INT[PAD_TOKEN])\n",
    "\n",
    "input_tensor, target_tensor = v\n",
    "target_length = target_tensor.size(1)\n",
    "_, encoder_hidden = encoder(input_tensor)\n",
    "decoder_input = torch.full(\n",
    "    (batch_size, 1), EN_WORD2INT[SOS_TOKEN], dtype=torch.long\n",
    ").to(DEVICE)\n",
    "decoder_hidden = encoder_hidden\n",
    "loss = torch.tensor(0.0, device=DEVICE, requires_grad=True)\n",
    "for di in range(target_length):\n",
    "    logits, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "\n",
    "    loss = loss + loss_fn(logits, target_tensor[:, di])\n",
    "\n",
    "    decoder_input = target_tensor[:, di].reshape(batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zgXQXwxH-R46",
    "outputId": "5d9164e8-0e3a-4010-f5d3-c86810391170",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index=EN_WORD2INT[PAD_TOKEN])\n",
    "\n",
    "encoder_optimizer = optim.AdamW(encoder.parameters())\n",
    "decoder_optimizer = optim.AdamW(decoder.parameters())\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    iterator = tqdm(enumerate(translation_dataloader))\n",
    "    for i, (input_tensor, target_tensor) in iterator:\n",
    "        input_tensor, target_tensor = (\n",
    "            input_tensor.to(DEVICE),\n",
    "            target_tensor.to(DEVICE)\n",
    "        )\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        target_length = target_tensor.size(1)\n",
    "\n",
    "        _, encoder_hidden = encoder(input_tensor)\n",
    "\n",
    "        decoder_input = torch.full(\n",
    "            (batch_size, 1), EN_WORD2INT[SOS_TOKEN], dtype=torch.long\n",
    "        ).to(DEVICE)\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        loss = torch.tensor(0.0, device=DEVICE, requires_grad=True)\n",
    "        for di in range(target_length):\n",
    "            logits, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "\n",
    "            loss = loss + loss_fn(logits, target_tensor[:, di])\n",
    "\n",
    "            decoder_input = target_tensor[:, di].reshape(batch_size, 1)\n",
    "\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch}, \",\n",
    "                f\"Batch {i}, \",\n",
    "                f\"Loss: {loss.item() / target_length:.4f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T12:44:01.097644Z",
     "iopub.status.busy": "2024-12-08T12:44:01.097312Z",
     "iopub.status.idle": "2024-12-08T12:44:01.107873Z",
     "shell.execute_reply": "2024-12-08T12:44:01.106984Z",
     "shell.execute_reply.started": "2024-12-08T12:44:01.097613Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'как это не так.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    sentence=\"How does it works?\",\n",
    "    en_word2int=EN_WORD2INT,\n",
    "    ru_int2word=RU_INT2WORD,\n",
    "    ru_word2int=RU_WORD2INT,\n",
    "    max_length=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T12:44:45.695828Z",
     "iopub.status.busy": "2024-12-08T12:44:45.694804Z",
     "iopub.status.idle": "2024-12-08T12:44:45.706825Z",
     "shell.execute_reply": "2024-12-08T12:44:45.706087Z",
     "shell.execute_reply.started": "2024-12-08T12:44:45.695792Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'давайте поговорим и не будем быть в бостоне.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    sentence=\"Let's try something.\",\n",
    "    en_word2int=EN_WORD2INT,\n",
    "    ru_int2word=RU_INT2WORD,\n",
    "    ru_word2int=RU_WORD2INT,\n",
    "    max_length=20\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5101699,
     "sourceId": 8539972,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
