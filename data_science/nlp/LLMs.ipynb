{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69260b7e",
   "metadata": {},
   "source": [
    "# LLMs\n",
    "\n",
    "\n",
    "LLMs are models designed primarily to predict text. The most advanced LLMs can simulate a wide range of linguistic behaviors. With the right configuration, they can be applied to many problems that are difficult to solve with traditional programming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69b8ebc",
   "metadata": {},
   "source": [
    "## Chat templates\n",
    "\n",
    "Since LLMs are designed only to predict the next token based on a range of previous tokens, they are not able to behive in a chat-like pattern by default. To achieve this, models are fine-tuned to follow the chat templates. A model that has passed the so-called supervised fine-tuning, which is intended to train it to follow the \"query-response\" pattern, usually has the `instruct` prefix or postfix somewhere in its name/identifier.\n",
    "\n",
    "**Chat template** is a set of rules for structuring model inputs, separating messages, and specifying the role of the speaker for each message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e10b2d",
   "metadata": {},
   "source": [
    "Special tokens ussually define the beginning and ending of different messages. Role is specified using some defined syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1396d0",
   "metadata": {},
   "source": [
    "The chat-templates for a popular LLMs are:\n",
    "\n",
    "* **OpenAI ChatML**\n",
    "\n",
    "  * Common delimiters: `<|im_start|>` precedes each message; `<|im_end|>` closes them.\n",
    "  * Roles are explicitly labeled (e.g., `<|system|>`, `<|user|>`, `<|assistant|>`).\n",
    "\n",
    "* **LLaMA-2 “INST” Format**\n",
    "\n",
    "  * Prompts often use `[INST] ... [/INST]` markers, with optional system-specific wrappers like `<<SYS>> ... <</SYS>>`. These markers aren't necessarily single tokens but are recognized by tokenizer logic.\n",
    "* **LLaMA-3 / SentencePiece Chat Templates**\n",
    "\n",
    "  * Conversations begin with `<|begin_of_text|>`.\n",
    "  * Role headers are wrapped using `<|start_header_id|>role<|end_header_id|>`.\n",
    "  * Each message ends with a special end-of-turn token: `<|eot_id|>`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619d88af",
   "metadata": {},
   "source": [
    "Hugging Face provides a playground where you can see how the chat templates will be applied for the different models available on the platform:\n",
    "\n",
    "<iframe src=\"https://jofthomas-chat-template-viewer.hf.space\" frameborder=\"0\" width=\"850\" height=\"450\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d572dea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The tokenizers that come with the models in the `transformers` package apply chat templates to the sets of messages using the `apply_chat_template` method. The following cell shows the result applying the chat-format to the SmolLM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13ebf7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an AI assistant with access to various tools.<|im_end|>\n",
      "<|im_start|>user\n",
      "Hi !<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi human, what can help you with ?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an AI assistant with access to various tools.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi !\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi human, what can help you with ?\"},\n",
    "]\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B-Instruct\")\n",
    "rendered_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(rendered_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e817b26b",
   "metadata": {},
   "source": [
    "## Prompt engineering\n",
    "\n",
    "Prompt engineering is a set of approaches used to configure a text generation model to produce the exact results you're interested in.\n",
    "\n",
    "Check the [Prompt Engineering](https://www.gptaiflow.com/assets/files/2025-01-18-pdf-1-TechAI-Goolge-whitepaper_Prompt%20Engineering_v4-af36dcc7a49bb7269a58b1c9b89a8ae1.pdf) guide provided by google.\n",
    "\n",
    "Generally there are two concepts you need to make the model to produce relevant output:\n",
    "\n",
    "- **Configure the model** by changing parameters. Models implemented by different organizatinos have different configuration options but most have: output length and sampling controls.\n",
    "- **Bulding a promt**. for this there are following techniques:\n",
    "    - General prompting / zero shot.\n",
    "    - One & few shot prompting.\n",
    "    - System, contextual and role prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bb3df2",
   "metadata": {},
   "source": [
    "### Token sampling\n",
    "\n",
    "At each step of the generation process, the model generates the next token by classifying it based on previous tokens. Thus, at some point, the model's predictions resemble the probabilities of the next token:\n",
    "\n",
    "$\\left(p_1, p_2, \\ldots, p_n\\right), \\sum_{i=1}^n p_i =1$\n",
    "\n",
    "Where $n$ is a vocabulary of the model.\n",
    "\n",
    "**The temperature** regulates the randomness of the selected tokens. A value of 0 results in deterministic model outputs. The higher the value,a the more creative the model's output will be.\n",
    "\n",
    "**Top-k** specifies the model that selects the next token from among the $k$ tokens with the highest probability.\n",
    "\n",
    "**Top-p** it considers the smallest possible set of tokens whose cumulative probability exceeds a predefined threshold, $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675222e8",
   "metadata": {},
   "source": [
    "### Prompting techniques\n",
    "\n",
    "There are different approaches associated to providing model information about the structure of required output:\n",
    "\n",
    "- **Zero shot**: general prompting technique, just query to the model without providing any additional information.\n",
    "- **One shot & Zero shot**: to explain the model the structure of the output you expect from it.\n",
    "\n",
    "There are following options when specifying the general patterns of the model behaviour:\n",
    "\n",
    "- **System prompting** sets the overall context and purpose for the language model.\n",
    "- **Contextual prompting** provides specific details or background information relevant to the current convecrsation task.\n",
    "- **Role prompting** assigning the a specific character or identity for the language model\n",
    "\n",
    "**Step-back prompting**: Two prompts are provided: a spefic prompt and a more general prompt. The more general prompt ussually asks about typical approaches to the issue described in the specific prompt. The second prompt includes the model's answer to the general prompt as context and the specific prompt as the taks.\n",
    "\n",
    "**Chain of Thought (CoT)**: A technique in which the model is asked to solve a task step by step. In the most basic implementation, the prompt literally asks the model to solve the problem \"step by step\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ee1bf0",
   "metadata": {},
   "source": [
    "## RAG\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is an approach that provides LLM with context associated with the specific information. The general idea is to create a knowledge base in the form of vector database, where encoded as embeddings documents corresponding to the information to be added to the model context. When the system needs information, it searches for embeddings with corresponding properties decodes them, and add them as context to the machine learning model.\n",
    "\n",
    "There are sevaral topics related to RAG systems that need to be discussed:\n",
    "\n",
    "- **Chunking**: The process of separating documents from the knowledge base into the chunks that can be used to prepare the embeddings.\n",
    "- **Retrieval**: There are a set of approaches and tools to collect the relevant information contained in the chunks.\n",
    "- **Qality estimation**: As there are few compoments in the RAG system that's why the estimation of the system is a complex process.\n",
    "\n",
    "Check more in the corresponding [RAG](LLMs/rag.ipynb) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14876609",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n",
    "**AI agents are** programs where AI controls workflow.\n",
    "\n",
    "There is some typical terminology in the field of agentic frameworks:\n",
    "\n",
    "- **Tools**:  provide the agent with the ability to execute actions a text-generation model cannot perform natively, such as making coffe or generating images.\n",
    "- **Actions** are the concrete steps an AI agent takes to interact with its environment.\n",
    "- **Observations**: The ouputs of the tools tha are used as a context for the model.\n",
    "\n",
    "There are different ways in which AI outputs can influence the workflow. These approaches are listed in the following table:\n",
    "\n",
    "| Name             | Description                                                     | Example code                                       |\n",
    "|------------------|-----------------------------------------------------------------|----------------------------------------------------|\n",
    "| Router           | LLM output controls an if/else switch                           | `if llm_description(): path_a() else: path_b()`    |\n",
    "| Tool call        | LLM output controls function execution                          | `run_function(llm_chosen_tool, llm_chosen_args)`   |\n",
    "| Multi-step Agent | LLM output controls interation and program continuation         | `while llm_should_continue(): execute_next_step()` |\n",
    "| Multi-Agent      | One agentig workflow can start another workflow                 | `if llm_trigger(): execute_agent()`                |\n",
    "| Code Agents      | LLM acts in code, can define its own tools / start other agents | `def custom_tool()`                                |\n",
    "\n",
    "Check more details in the [Agents](LLMs/agents.ipynb) page."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowledge (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
