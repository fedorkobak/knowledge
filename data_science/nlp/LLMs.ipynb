{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69260b7e",
   "metadata": {},
   "source": [
    "# LLMs\n",
    "\n",
    "\n",
    "LLMs are models designed primarily to predict text. The most advanced LLMs can simulate a wide range of linguistic behaviors. With the right configuration, they can be applied to many problems that are difficult to solve with traditional programming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e817b26b",
   "metadata": {},
   "source": [
    "## Prompt engineering\n",
    "\n",
    "Prompt engineering is a set of approaches used to configure a text generation model to produce the exact results you're interested in.\n",
    "\n",
    "Check the [Prompt Engineering](https://www.gptaiflow.com/assets/files/2025-01-18-pdf-1-TechAI-Goolge-whitepaper_Prompt%20Engineering_v4-af36dcc7a49bb7269a58b1c9b89a8ae1.pdf) guide provided by google.\n",
    "\n",
    "Generally there are two concepts you need to make the model to produce relevant output:\n",
    "\n",
    "- **Configure the model** by changing parameters. Models implemented by different organizatinos have different configuration options but most have: output length and sampling controls.\n",
    "- **Bulding a promt**. for this there are following techniques:\n",
    "    - General prompting / zero shot.\n",
    "    - One & few shot prompting.\n",
    "    - System, contextual and role prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bb3df2",
   "metadata": {},
   "source": [
    "### Token sampling\n",
    "\n",
    "At each step of the generation process, the model generates the next token by classifying it based on previous tokens. Thus, at some point, the model's predictions resemble the probabilities of the next token:\n",
    "\n",
    "$\\left(p_1, p_2, \\ldots, p_n\\right), \\sum_{i=1}^n p_i =1$\n",
    "\n",
    "Where $n$ is a vocabulary of the model.\n",
    "\n",
    "**The temperature** regulates the randomness of the selected tokens. A value of 0 results in deterministic model outputs. The higher the value,a the more creative the model's output will be.\n",
    "\n",
    "**Top-k** specifies the model that selects the next token from among the $k$ tokens with the highest probability.\n",
    "\n",
    "**Top-p** it considers the smallest possible set of tokens whose cumulative probability exceeds a predefined threshold, $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675222e8",
   "metadata": {},
   "source": [
    "### Prompting techniques\n",
    "\n",
    "There are different approaches associated to providing model information about the structure of required output:\n",
    "\n",
    "- **Zero shot**: general prompting technique, just query to the model without providing any additional information.\n",
    "- **One shot & Zero shot**: to explain the model the structure of the output you expect from it.\n",
    "\n",
    "There are following options when specifying the general patterns of the model behaviour:\n",
    "\n",
    "- **System prompting** sets the overall context and purpose for the language model.\n",
    "- **Contextual prompting** provides specific details or background information relevant to the current convecrsation task.\n",
    "- **Role prompting** assigning the a specific character or identity for the language model\n",
    "\n",
    "**Step-back prompting**: Two prompts are provided: a spefic prompt and a more general prompt. The more general prompt ussually asks about typical approaches to the issue described in the specific prompt. The second prompt includes the model's answer to the general prompt as context and the specific prompt as the taks.\n",
    "\n",
    "**Chain of Thought (CoT)**: A technique in which the model is asked to solve a task step by step. In the most basic implementation, the prompt literally asks the model to solve the problem \"step by step\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14876609",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n",
    "**AI agents are** programs where AI controls workflow.\n",
    "\n",
    "Agent can complete some **actions** using some **tools**.\n",
    "\n",
    "**Tools**:  provide the agent with the ability to execute actions a text-generation model cannot perform natively, such as making coffe or generating images.\n",
    "\n",
    "There are different ways in which AI outputs can influence the workflow. These approaches are listed in the following table:\n",
    "\n",
    "| Name             | Description                                                     | Example code                                       |\n",
    "|------------------|-----------------------------------------------------------------|----------------------------------------------------|\n",
    "| Router           | LLM output controls an if/else switch                           | `if llm_description(): path_a() else: path_b()`    |\n",
    "| Tool call        | LLM output controls function execution                          | `run_function(llm_chosen_tool, llm_chosen_args)`   |\n",
    "| Multi-step Agent | LLM output controls interation and program continuation         | `while llm_should_continue(): execute_next_step()` |\n",
    "| Multi-Agent      | One agentig workflow can start another workflow                 | `if llm_trigger(): execute_agent()`                |\n",
    "| Code Agents      | LLM acts in code, can define its own tools / start other agents | `def custom_tool()`                                |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
