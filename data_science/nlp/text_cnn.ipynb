{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text CNN\n",
    "\n",
    "Text CNN is a method for applying convolutional architecture to text tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from gensim import downloader\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections.abc import Collection\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"talk.politics.guns\", \"rec.motorcycles\"]\n",
    "train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test = fetch_20newsgroups(subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = downloader.load(\"word2vec-google-news-300\")\n",
    "\n",
    "def seq_to_emb(\n",
    "    sentences: Collection[str], \n",
    "    wv: KeyedVectors, \n",
    "    tokens_num: int, \n",
    "    pad_token: str = \"</s>\"\n",
    ") -> np.ndarray:\n",
    "    '''\n",
    "    Convert a set of sentences into embeddings. Each set of symbols separated by\n",
    "    a space will be recognized as a separate token.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences: Collection[str]\n",
    "        A collection of sentences that require transformation into embeddings.\n",
    "    wv: KeyedVectors\n",
    "        This needs to be used to transform tokens into embeddings.\n",
    "    tokens_num: int\n",
    "        The number of tokens to take from each sample. Extra tokens will be \n",
    "        dropped, and if there are not enough tokens, padding will be added.\n",
    "    pad_token: str = \"</s.>\"\n",
    "        The token that will be used for padding if there are not enough tokens \n",
    "        in a sample.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out: np.ndarray\n",
    "        Of size (<samples> ,<embedding size>, <tokens_num>).\n",
    "    '''\n",
    "\n",
    "    rv = []\n",
    "    pad_vector = wv.get_vector(pad_token)\n",
    "\n",
    "    for sentence in sentences:\n",
    "\n",
    "        sentence_embeddings = []\n",
    "        got_emb = 0\n",
    "        for one_token in sentence.split():\n",
    "            if wv.has_index_for(one_token):\n",
    "                sentence_embeddings.append(wv.get_vector(one_token))\n",
    "                got_emb += 1\n",
    "                # Taking embedings only for some of the words\n",
    "                if got_emb >= tokens_num: break\n",
    "\n",
    "        sentence_embeddings = np.stack(sentence_embeddings, axis=1)\n",
    "        pad_array = np.tile(\n",
    "            pad_vector[:, None], \n",
    "            reps=(1, tokens_num - sentence_embeddings.shape[1])\n",
    "        )\n",
    "        sentence_embeddings = np.hstack([sentence_embeddings, pad_array])\n",
    "        rv.append(sentence_embeddings)\n",
    "\n",
    "    return np.stack(rv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_num = 100\n",
    "\n",
    "X_train = torch.tensor(seq_to_emb(\n",
    "    sentences=train[\"data\"],\n",
    "    wv=wv,\n",
    "    tokens_num=tokens_num\n",
    "))\n",
    "X_test = torch.tensor(seq_to_emb(\n",
    "    sentences=test[\"data\"],\n",
    "    wv=wv,\n",
    "    tokens_num=tokens_num\n",
    "))\n",
    "\n",
    "y_train = torch.tensor(train[\"target\"], dtype=torch.float)\n",
    "y_test = torch.tensor(test[\"target\"], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        kernel_sizes: list[int], \n",
    "        in_channels: int, \n",
    "        out_channels: int\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_transforms = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=ks\n",
    "                ),\n",
    "                nn.AdaptiveAvgPool1d(output_size=1),\n",
    "                nn.Flatten()\n",
    "            )\n",
    "            for ks in kernel_sizes\n",
    "        ])\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                in_features=len(kernel_sizes)*out_channels,\n",
    "                out_features=1\n",
    "            ),\n",
    "            nn.Flatten(start_dim=0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, X: torch.Tensor):\n",
    "        return self.head(torch.cat([ct(X) for ct in self.conv_transforms], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:04<00:00,  4.86it/s]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(10)\n",
    "text_cnn = TextCNN([2,3], 300, 10)\n",
    "optimizer = torch.optim.Adam(text_cnn.parameters(), lr=1e-3)\n",
    "for i in tqdm(range(20)):\n",
    "    optimizer.zero_grad()\n",
    "    predict = text_cnn(X_train)\n",
    "    loss_value = nn.functional.binary_cross_entropy(\n",
    "        input=predict, target=y_train)\n",
    "    loss_value.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8635)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cnn.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_pred = text_cnn(X_test)\n",
    "\n",
    "((test_pred > 0.5).to(dtype=torch.float) == y_test).to(dtype=torch.float).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
