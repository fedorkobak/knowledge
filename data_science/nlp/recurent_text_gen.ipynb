{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurent text gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from random import sample\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "path = kagglehub.dataset_download(\"Cornell-University/arxiv/versions/205\")\n",
    "path = Path(path)/\"arxiv-metadata-oai-snapshot.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"recurent_layer_files\")\n",
    "data_path.mkdir(exist_ok=True)\n",
    "data_path = data_path/\"arxiv_small.json\"\n",
    "\n",
    "if not data_path.exists():\n",
    "    \n",
    "    lines = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for i, one_line in enumerate(tqdm.tqdm(f.readlines())):\n",
    "            if i % 10 == 0:\n",
    "                lines.append(one_line)\n",
    "\n",
    "    with open(data_path, mode=\"w\") as f:\n",
    "        f.writelines(lines)\n",
    "\n",
    "data = pd.read_json(data_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Calculation of prompt diphoton production cross sections at Tevatron and   LHC energies ;   A fully differential calculation in perturbative quantum chromodynamics is presented for the production of massive photon pairs at hadron colliders. All next-to-leading order perturbative contributions from quark-antiquark, gluon-(anti)quark, and gluon-gluon subprocesses are included, as well as all-orders resummation of initial-state gluon radiation valid at next-to-next-to-leading logarithmic accuracy. The region o\\n',\n",
       " ' Computing genus 2 Hilbert-Siegel modular forms over $\\\\Q(\\\\sqrt{5})$ via   the Jacquet-Langlands correspondence ;   In this paper we present an algorithm for computing Hecke eigensystems of Hilbert-Siegel cusp forms over real quadratic fields of narrow class number one. We give some illustrative examples using the quadratic field $\\\\Q(\\\\sqrt{5})$. In those examples, we identify Hilbert-Siegel eigenforms that are possible lifts from Hilbert eigenforms. \\n',\n",
       " ' Molecular Synchronization Waves in Arrays of Allosterically Regulated   Enzymes ;   Spatiotemporal pattern formation in a product-activated enzymic reaction at high enzyme concentrations is investigated. Stochastic simulations show that catalytic turnover cycles of individual enzymes can become coherent and that complex wave patterns of molecular synchronization can develop. The analysis based on the mean-field approximation indicates that the observed patterns result from the presence of Hopf and wave bifu\\n']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOS, EOS = \" \", \"\\n\"\n",
    "lines = (\n",
    "    data\n",
    "    .apply(lambda row: (row[\"title\"] + \" ; \" + row[\"abstract\"])[:512], axis=1)\n",
    "    .apply(lambda line: BOS + line.replace(EOS, \" \") + EOS)\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "lines[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n !\"#$%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\\x7f\\x80\\x99Ã¢'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENS = {one_char for one_line in lines for one_char in one_line}\n",
    "\n",
    "TOKENS = sorted(TOKENS)\n",
    "\"\".join(TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, 66, 67, 68,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 66, 67, 66, 68, 66, 67, 66,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 1, 66, 67, 68, 18, 19, 20, 21, 22, 23, 24, 25, 26, 17,  0]])\n"
     ]
    }
   ],
   "source": [
    "token_to_id = {x: i for i, x in enumerate(TOKENS)}\n",
    "\n",
    "def to_tensor(\n",
    "    lines: list[str],\n",
    "    max_len: int | None = None,\n",
    "    pad: str = token_to_id[EOS],\n",
    "    dtype=torch.int64,\n",
    "):\n",
    "    max_len = max_len or max(map(len, lines))\n",
    "    lines_ix = torch.full([len(lines), max_len], pad, dtype=dtype)\n",
    "    for i in range(len(lines)):\n",
    "        line_ix = [token_to_id[x] for x in lines[i][:max_len]]\n",
    "        lines_ix[i, : len(line_ix)] = torch.tensor(line_ix)\n",
    "    return lines_ix\n",
    "\n",
    "\n",
    "print(to_tensor([\" abc\\n\", \" abacaba\\n\", \" abc1234567890\\n\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = to_tensor([\"sentence 1 hi\", \"sentence 2 wow\"])\n",
    "emb_size = 16\n",
    "hidden_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 14, 16])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = nn.Embedding(num_embeddings=len(TOKENS), embedding_dim=emb_size)\n",
    "embeded = emb(inp)\n",
    "embeded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 14, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = nn.RNN(input_size=emb_size, hidden_size=hidden_size)\n",
    "states = rnn(embeded)[0]\n",
    "states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 14, 100])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(in_features=hidden_size, out_features=len(TOKENS))\n",
    "linear(states).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 14, 100])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_tokens: int, \n",
    "        emb_size: int = 16, \n",
    "        hid_size: int = 256\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings=n_tokens, embedding_dim=emb_size)\n",
    "        self.rnn = nn.RNN(emb_size, hid_size, batch_first=True)\n",
    "        self.linear = nn.Linear(in_features=hid_size, out_features=n_tokens)\n",
    "\n",
    "    def forward(self, input_ix):\n",
    "        rv: torch.Tensor = self.emb(input_ix)\n",
    "        rv: torch.Tensor = self.rnn(rv)[0]\n",
    "        rv: torch.Tensor = self.linear(rv)\n",
    "        return rv\n",
    "    \n",
    "model = RNNLanguageModel(len(TOKENS))\n",
    "model(inp).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "The loss formula is as follows:\n",
    "\n",
    "$$\n",
    "L = - \\cfrac{1}{N} \\sum_{i=1}^N \\ln p(x_t^{(i)} | x_{t-1}^{(i)}, \\dots, x_1^{(i)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll pass to our network some sequence of tokens, so if we want to predict $t$-th token we have to pass all previous $t-1$ tokens. The result would be probabilities for each token to be $t$-th.\n",
    "\n",
    "\n",
    "$p(x_t^{(i)} | x_{t-1}^{(i)}, \\dots, x_1^{(i)})$: is a probability that $t$-th token follows previou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = to_tensor([\"Some long input to the model\", \"short\"])\n",
    "model = RNNLanguageModel(len(TOKENS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 27, 100])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(inp[:, :-1])\n",
    "probas = torch.softmax(logits, 2)\n",
    "probas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_answers = inp[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0104, 0.0104, 0.0092, 0.0103, 0.0098, 0.0085, 0.0112, 0.0130, 0.0109,\n",
       "         0.0105, 0.0100, 0.0093, 0.0089, 0.0112, 0.0085, 0.0095, 0.0096, 0.0094,\n",
       "         0.0086, 0.0093, 0.0091, 0.0089, 0.0117, 0.0089, 0.0090, 0.0103, 0.0091],\n",
       "        [0.0098, 0.0105, 0.0104, 0.0085, 0.0098, 0.0108, 0.0105, 0.0106, 0.0103,\n",
       "         0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104,\n",
       "         0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104, 0.0104]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_probas = torch.gather(probas, 2, reference_answers[..., None]).squeeze(2)\n",
    "real_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eos_ix = token_to_id[EOS]\n",
    "mask = torch.nn.functional.pad(\n",
    "    torch.cumsum(inp == eos_ix, dim=-1)[..., :-1] < 1,\n",
    "    pad=(1, 0, 0, 0),\n",
    "    value=True\n",
    ")\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(74.0398, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.sum(torch.log(real_probas) * mask[:, 1:]) / inp.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final realisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(74.0398, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_mask(input_ix, eos_ix=token_to_id[EOS]) -> torch.Tensor:\n",
    "    return torch.nn.functional.pad(\n",
    "        torch.cumsum(input_ix == eos_ix, dim=-1)[..., :-1] < 1,\n",
    "        pad=(1, 0, 0, 0),\n",
    "        value=True,\n",
    "    )\n",
    "\n",
    "def compute_loss(\n",
    "    model: nn.Module, \n",
    "    input_ix: torch.Tensor, \n",
    "    device: str = \"cpu\"\n",
    ") -> torch.Tensor:\n",
    "    input_ix = torch.as_tensor(input_ix, dtype=torch.int64)\n",
    "    input_ix = input_ix.to(device)\n",
    "\n",
    "    logits = model(input_ix[:, :-1])\n",
    "    reference_answers = input_ix[:, 1:]\n",
    "    rv = torch.softmax(logits, dim=-1)\n",
    "    rv = torch.gather(rv, 2, reference_answers[:, :, None]).squeeze(2)\n",
    "    rv = torch.log(rv)\n",
    "\n",
    "    rv = rv * compute_mask(input_ix)[:, 1:]\n",
    "    return -torch.sum(rv) / input_ix.shape[0]\n",
    "\n",
    "\n",
    "compute_loss(\n",
    "    model=model,\n",
    "    input_ix=inp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = \"hello\"\n",
    "p = torch.softmax(\n",
    "    model(to_tensor([prefix]))[0, -1], dim=-1\n",
    ").detach().numpy()\n",
    "TOKENS[np.argmax(p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model: RNNLanguageModel,\n",
    "    prefix: str = BOS,\n",
    "    max_len: int = 100,\n",
    "    device: torch.device = torch.device(\"cpu\")\n",
    "):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(max_len):\n",
    "            logits = model(to_tensor([prefix]).to(device))[0, -1]\n",
    "            probs = torch.softmax(logits, dim=-1).detach().numpy()\n",
    "            next_token = TOKENS[np.argmax(probs)]\n",
    "            prefix += next_token\n",
    "            if next_token == EOS:\n",
    "                break\n",
    "    \n",
    "    return prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test linevRg$IERi3\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model, prefix=\"test line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Training loop really typical, except for `nn.utils.clip_grad_norm_` - this is needed due to peculiarities of recurrent layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 3000/3000 [11:22<00:00,  4.39it/s]\n"
     ]
    }
   ],
   "source": [
    "def train_loop(model: nn.Module, train_lines: list[str], device: str = \"cpu\"):\n",
    "    clip_norm = 1e5\n",
    "    batch_size = 64\n",
    "    opt = torch.optim.Adam(model.parameters())\n",
    "    train_history = []\n",
    "    model.to(device)\n",
    "    for i in tqdm.trange(len(train_history), 3000):\n",
    "        batch = to_tensor(sample(train_lines, batch_size)).to(device)\n",
    "        loss_i = compute_loss(model, batch, device=device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss_i.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "        opt.step()\n",
    "\n",
    "        train_history.append((i, float(loss_i)))\n",
    "\n",
    "        if (i + 1) % 50 == 0:\n",
    "            for _ in range(3):\n",
    "                example = generate(model, device=device)\n",
    "\n",
    "\n",
    "torch.manual_seed(10)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_loop(model, lines, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How this works with the states of the control to the problem of the problem of the problem of the problem of the p'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "generate(model=model, prefix=\"How this works\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
