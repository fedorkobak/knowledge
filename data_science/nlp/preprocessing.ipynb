{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "\n",
    "This notebook provides an overview of common techniques for processing text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenezation\n",
    "\n",
    "Tokenezation is the process of breaking a stream of textual data into words, terms, sentences, symbols, or some other meaningful elements called tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We will explore differences among tokenizers using the sentence provided in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = \"I'm loving NLP—it's amazing!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple **whitespace** tokenization allows text to be split by spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm\", 'loving', \"NLP—it's\", 'amazing!']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Symbol** tokenization, of course, treats each symbol as a separate token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'\", 'm', ' ', 'l', 'o', 'v', 'i', 'n', 'g', ' ', 'N', 'L', 'P', '—', 'i', 't', \"'\", 's', ' ', 'a', 'm', 'a', 'z', 'i', 'n', 'g', '!']\n"
     ]
    }
   ],
   "source": [
    "print(list(inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Wold** tokenizer separates text into wolds. It differs from the whitespace tokenizer in that it assumes that different tokens can be separated by more than just whitespace. The following cell shows the transformation of the example sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"'m\", 'loving', 'NLP—it', \"'s\", 'amazing', '!']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.tokenize.word_tokenize(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "There are many words in text that are not considered to have much meaning - they are commonly called stop words and are not considered when processing text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell shows the stop word list according to the `nltk` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Lemmatization is the process of applying words from the text to their base form. It helps reduce the size of the dictionary by removing not-so-important variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell shows the transformation process for different forms of the word \"invest\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invest -> invest\n",
      "invests -> invest\n",
      "invested -> invest\n",
      "investing -> invest\n",
      "investment -> invest\n",
      "investments -> invest\n",
      "investor -> investor\n",
      "investors -> investor\n",
      "investiture -> investitur\n",
      "investedness -> invested\n"
     ]
    }
   ],
   "source": [
    "input = [\n",
    "    \"invest\",\n",
    "    \"invests\",\n",
    "    \"invested\",\n",
    "    \"investing\",\n",
    "    \"investment\",\n",
    "    \"investments\",\n",
    "    \"investor\",\n",
    "    \"investors\",\n",
    "    \"investiture\",\n",
    "    \"investedness\"\n",
    "]\n",
    "\n",
    "porter_stemmer = nltk.stem.PorterStemmer()\n",
    "output = [porter_stemmer.stem(w) for w in input]\n",
    "for inp, out in zip(input, output):\n",
    "    print(f\"{inp} -> {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemming\n",
    "\n",
    "There are a lot of misunderstanding between terms *stemming* and *lemming*. I haven't found authorative opinion yet but, there is descirption that corresponds to the behaviour of the tools:\n",
    "\n",
    "- **Stemming** is a rule-based process that removes prefixes or suffixes from a word to reduce it to a base form. However, stemming does not guarantee that the resulting word will be a valid or meaningful word in the language. It may result in non-existent or strange forms.\n",
    "\n",
    "- **Lemmatization**, on the other hand, involves reducing a word to its lemma, or canonical form, based on its dictionary definition. The resulting word is always a valid word in the language, and lemmatization often takes into account the part of speech and other grammatical information.\n",
    "\n",
    "**Long story short:** Stemming might produce words that don't exist, while lemmatization guarantees a real word as the result.\n",
    "\n",
    "These things are usually considered as separate processes, but from my perspective *lematization* is the specification of the stem, so in this site they are considered as such."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The next cell shows a set of words that have been transformed into a non-existent word by stemming and outputting the limming for the same example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>porter stemming</th>\n",
       "      <th>lemming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>geese</td>\n",
       "      <td>gees</td>\n",
       "      <td>goose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>happily</td>\n",
       "      <td>happili</td>\n",
       "      <td>happily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>generously</td>\n",
       "      <td>gener</td>\n",
       "      <td>generously</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>studied</td>\n",
       "      <td>studi</td>\n",
       "      <td>studied</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        input porter stemming     lemming\n",
       "0       geese            gees       goose\n",
       "1     happily         happili     happily\n",
       "2  generously           gener  generously\n",
       "3     studied           studi     studied"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter_stemmer = nltk.stem.PorterStemmer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "input = [\n",
    "    \"geese\",\n",
    "    \"happily\",\n",
    "    \"generously\",\n",
    "    \"studied\"\n",
    "]\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"input\": input,\n",
    "    \"porter stemming\": [porter_stemmer.stem(w) for w in input],\n",
    "    \"lemming\": [lemmatizer.lemmatize(w) for w in input]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building embeddings\n",
    "\n",
    "To use tokenized text in any type of algorithm, particularly in machine learning algorithms, you need to transform it into numerical data - generally, a vector formed according to some rules. Building embeddings is not an approach exclusive to the NLP domain; here, we consider exactly where embeddings are used in NLP.\n",
    "\n",
    "The following table contains the typical wide-known approaches:\n",
    "\n",
    "| Approach          | Description                                                                 | Output Format          | Pros                                                  | Cons                                                       |\n",
    "|-------------------|-----------------------------------------------------------------------------|------------------------|-------------------------------------------------------|------------------------------------------------------------|\n",
    "| Bag of Words (BoW)| Counts word occurrences in a document.                                      | Sparse vector          | Simple, fast, interpretable                           | Ignores word order and semantics                           |\n",
    "| TF-IDF            | Weighted word counts based on term frequency and inverse document frequency.| Sparse vector          | Reduces impact of common words, more informative than BoW | Still ignores word order and semantics                   |\n",
    "| Word2Vec          | Learns word embeddings using context (CBOW or Skip-gram).                   | Dense vector (per word)| Captures semantic similarity                          | Needs large corpus and training time                       |\n",
    "| GloVe             | Embeddings based on global word co-occurrence statistics.                   | Dense vector (per word)| Combines global and local information                 | Pretrained; less adaptable to specific domains             |\n",
    "| FastText          | Extends Word2Vec using subword (character n-gram) information.              | Dense vector (per word)| Handles out-of-vocabulary words well                  | Larger model size                                          |\n",
    "| BERT embeddings   | Contextualized embeddings using Transformer encoder.                        | Dense vector (contextual)| Captures context, syntax, semantics               | Computationally expensive                                  |\n",
    "| Sentence Transformers| Generates sentence-level embeddings using pretrained transformers.       | Dense vector (sentence)| Effective for sentence similarity and retrieval       | Heavier model; fine-tuning may be needed                   |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
