{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model properties\n",
    "\n",
    "Fitting models is ineffective without evaluating their performance. Estimating performance by merely observing results can lead to misunderstandings. This page describes methods that provide detailed insights into your model, offering valuable information for its improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "\n",
    "Is a way to learn properties of your classification model.\n",
    "\n",
    "Suppose we have formed some classifier. We have the following groups of observations.\n",
    "\n",
    "- *True positive*: observations that were positive in the sample and we correctly predicted them as positive. We will denote their number as $TP$;\n",
    "- *True negative*: observations that were negative in the sample and we correcrly predicted then as negative. We will denote their number as $TN$;\n",
    "- *False positve*: observations that were negative in the sample, but which we then mistakenly predicted to be positive. We will denote their number as $FP$;\n",
    "- *False negative*: observations that were positive in the sample, but wich we then mistakenly predicted to be negative. We will denote their number as $FN$.\n",
    "\n",
    "So, if you put the actual value on the rows and the predicted value on the columns, you will get a confusion matrix.\n",
    "\n",
    "\n",
    "| | Predicted $N$ | Predicted $P$ |\n",
    "|:---|:---:|:---:|\n",
    "| Actual $N$ |$TN$|$FP$ |\n",
    "| Actual $P$ |$FN$|$TP$|\n",
    "\n",
    "Find out more in the [special page](model_properties/confusion_matrix.ipynb)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
