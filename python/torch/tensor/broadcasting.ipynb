{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc26b3d2-cf93-4cc5-8f1f-d8542481a923",
   "metadata": {},
   "source": [
    "# Broadcasting\n",
    "\n",
    "Broadcasting is a mechanism that allows tensors of different shapes to be used together in element-wise operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53cdf667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed706e6",
   "metadata": {},
   "source": [
    "## Broadcasting rules\n",
    "\n",
    "To understand broadcasing consider its rules:\n",
    "\n",
    "- **Rule 1 (dimensioning)**: If tensors have a different number of dimensions, the shape of the smaller tensor is padded with ones on the left side until both tensors have the same number of dimensions.\n",
    "\n",
    "- **Rule 2 (compatability)**: Two dimensions are compatible when: They are equal, or one of them is 1\n",
    "\n",
    "- **Rule 3 (broadcasting)**: If the dimensions are compatible, the tensors are broadcasted to the shape of the larger tensor. If not compatible, an error is raised.\n",
    "\n",
    "Look at each rule separately with examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f51579f",
   "metadata": {},
   "source": [
    "### Dimensioning\n",
    "\n",
    "To apply element-wise operations, tensors must have the same number of dimensions. By default, if two tensors have different dimensionalities, the tensor with fewer dimensions will be expanded to match the higher-dimensional tensor. More formally, given two tensors $A$ and $B$ with dimensionalities $(d_1, d_2, ..., d_n)$ and $(d'_1, d'_2, ... , d'_m)$ respectively, where $m < n$, PyTorch will transform the dimensionality of $B$ to $(d''1, d''2, ..., d''{n-m}, d''{n-m+1}, ..., d''_n)$ where:\n",
    "\n",
    "$$\n",
    "d''_i = \\begin{cases}\n",
    "1, & \\text{if } i \\in \\{1, 2, \\ldots, n-m\\}, \\\\\n",
    "d'_j, & \\text{if } i \\in \\{n-m+1, \\ldots, n\\} \\text{ and } j = i - n + m,\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Consider an example where we have tensors $A$ and $B$ with dimensionalities: $(1, 2, 3, 2, 3)$ and $(3, 2, 3)$ respectively.\n",
    "\n",
    "By default, PyTorch will transform $B$ to have a dimensionality of $(1, 1, 3, 2, 3)$. It adds ones to the beginning of the dimensionality until it reaches the same length as $A$ (5 dimensions), and then uses the original dimensions of $B$.\n",
    "\n",
    "When a tensor is transformed to match another tensor's dimensionality, it's effectively being wrapped in a series of additional dimensions, each of size 1. This creates a \"shell\" around the original tensor, expanding it to have the same number of dimensions as the target tensor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4531ccf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's consider a code example that demonstrates this dimensionality transformation. We'll add an element to a tensor and observe how PyTorch handles the broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "88d1d3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(4).reshape(2,2)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414dcbd5",
   "metadata": {},
   "source": [
    "As a second tensor, we'll use a one-dimensional tensor `b`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f08c347b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 6])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor([5, 6])\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a619dd",
   "metadata": {},
   "source": [
    "There could be two ways to add two elements to the tensor `a`. By rows and by columns the following cell shows both options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6744eb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding\n",
      "tensor([[1],\n",
      "        [2]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Adding\")\n",
    "print(b[:, None])\n",
    "a + b[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "574b3f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding tensor([[1, 2]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 3],\n",
       "        [3, 5]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Adding\", b[None])\n",
    "a + b[None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5272eb3f",
   "metadata": {},
   "source": [
    "Before we specify the exact method for adding the missing dimensionality, let's examine how PyTorch chooses the default approach. This will be demonstrated in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e92f9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 3],\n",
       "        [3, 5]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071e1014",
   "metadata": {},
   "source": [
    "By default, PyTorch added the extra dimension by rows. This behavior is because wrapping a one-dimensional tensor in an outer dimension results in a one-row matrix.  The two operands then share the column dimension, enabling element-wise operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f242c67-d526-4cba-810d-73d54e779e6e",
   "metadata": {},
   "source": [
    "## By row\n",
    "\n",
    "Suppose you need to add a zero tensor to an array containing two numbers. So in the following cells the tensors `a` and `b` are created, which we will use for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d02debe-b8d3-4ebe-b89c-782dbe8b806d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros((2, 3))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6974236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor([2, 3])\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc68a4cf",
   "metadata": {},
   "source": [
    "So suppose we want to add the first element of `b` to the first row of `a` and the second element of `b` to the second row of `a`. Obviously just using operator `+` won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7d72127",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99627d17-15c9-4156-9c07-4a53c39533f4",
   "metadata": {},
   "source": [
    "The added tensor must be converted so that $n_1=m_1$. \n",
    "\n",
    "Where :\n",
    "- $n_i$ - zeros tensor $i-th$ dimention size;\n",
    "- $m_i$ - addimg tensor $i-th$ dimetion size.\n",
    "\n",
    "So in the following example we will use `unsqueeze` for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e870a0-974e-4671-8717-64ab25711931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2],\n",
       "        [3]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_unsqueezed = b.unsqueeze(1)\n",
    "b_unsqueezed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9d06f-273a-46c9-a0a0-bee6302ec83d",
   "metadata": {},
   "source": [
    "And finally result of sum. Essentially we just added the corresponding values of array `b` to each element in the row of array `a`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26c31cf3-cf41-4822-b798-7e89ab306507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2.],\n",
       "        [3., 3., 3.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b_unsqueezed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8446ab55-efd8-4b81-936b-eb48c609e35b",
   "metadata": {},
   "source": [
    "## By layer\n",
    "\n",
    "This shows how to add a tensor of any dimension to another tensor along the selected axis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d011513",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell creates a tensor with the dimensionality `(3, 2, 3, 3)`. We'll interpret this as 3 cubes with 2 layers, 3 rows and 3 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9380895d-1b6d-4f2e-b9ab-4e31ec9fa671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros((3, 2, 3, 3))\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2035d917",
   "metadata": {},
   "source": [
    "Suppose we need to add same number to the corresponding layer of each cube. Let it be 2 for first layer and 3 for second layer. So we have to create tensor with dimentions `(2,1,1)` - which you can understand as two elements in different layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a54f8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2]],\n",
      "\n",
      "        [[3]]])\n",
      "torch.Size([2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "b = torch.tensor([[[2]], [[3]]])\n",
    "print(b)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c094d79-6882-4404-9f96-2607f34bcd28",
   "metadata": {},
   "source": [
    "Now perform an addition operation on the corresponding layer of each cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "307350e1-1cbd-40e7-a9fe-12a41df02f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[2., 2., 2.],\n",
       "          [2., 2., 2.],\n",
       "          [2., 2., 2.]],\n",
       "\n",
       "         [[3., 3., 3.],\n",
       "          [3., 3., 3.],\n",
       "          [3., 3., 3.]]],\n",
       "\n",
       "\n",
       "        [[[2., 2., 2.],\n",
       "          [2., 2., 2.],\n",
       "          [2., 2., 2.]],\n",
       "\n",
       "         [[3., 3., 3.],\n",
       "          [3., 3., 3.],\n",
       "          [3., 3., 3.]]],\n",
       "\n",
       "\n",
       "        [[[2., 2., 2.],\n",
       "          [2., 2., 2.],\n",
       "          [2., 2., 2.]],\n",
       "\n",
       "         [[3., 3., 3.],\n",
       "          [3., 3., 3.],\n",
       "          [3., 3., 3.]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feba8f24",
   "metadata": {},
   "source": [
    "## Practical example\n",
    "\n",
    "Normalizing data can be achieved efficiently and easily using broadcasting techniques. Imagine we have four objects, each represented by a 3x3 matrix.  This gives us a three-dimensional tensor containing these matrices. However, each object has a different base value.  The following code snippet generates and prints such a tensor: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a09f87f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  12.8460,   11.6038,    8.2267],\n",
       "         [  13.3707,   13.0595,    9.3532],\n",
       "         [  10.6257,   10.8800,    5.5372]],\n",
       "\n",
       "        [[ 102.3054,   98.2987,   89.9491],\n",
       "         [ 103.9013,   98.5896,   98.1971],\n",
       "         [ 106.4493,   98.1544,   96.3743]],\n",
       "\n",
       "        [[ 501.0322,  499.2918,  504.5105],\n",
       "         [ 496.8454,  498.7290,  499.3281],\n",
       "         [ 499.4231,  504.0831,  499.4671]],\n",
       "\n",
       "        [[ 997.9377, 1001.2782, 1005.9980],\n",
       "         [ 999.5461, 1001.3517,  997.5663],\n",
       "         [1003.1572,  999.6476, 1001.6178]]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = torch.stack([\n",
    "    torch.normal(mean=mean, std=3., size=(3, 3))\n",
    "    for mean in [10, 100, 500, 1000]\n",
    "])\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fa7d95",
   "metadata": {},
   "source": [
    "Let's say we want to standardize all of these objects, meaning we want to bring them to zero mean and unit variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4af36e3",
   "metadata": {},
   "source": [
    "First, we need to calculate the mean and standard deviation for each object we're considering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "579b54b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  10.6114]],\n",
      "\n",
      "        [[  99.1355]],\n",
      "\n",
      "        [[ 500.3011]],\n",
      "\n",
      "        [[1000.9001]]])\n",
      "tensor([[[2.5660]],\n",
      "\n",
      "        [[4.7580]],\n",
      "\n",
      "        [[2.5100]],\n",
      "\n",
      "        [[2.6277]]])\n"
     ]
    }
   ],
   "source": [
    "mean = value.mean(axis=[1,2], keepdim=True)\n",
    "print(mean)\n",
    "std = value.std(axis=[1,2], keepdim=True)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f81577",
   "metadata": {},
   "source": [
    "The result is two vectors, each containing a value for every object in the input array. These vectors have the same dimensionality as the objects themselves - the `keepdim=True` argument ensures this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d404957",
   "metadata": {},
   "source": [
    "Now, by subtracting the means from the original arrays and dividing the result by the standard deviations, we've effectively scaled all the objects to the same scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6e824fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8708,  0.3867, -0.9293],\n",
       "         [ 1.0753,  0.9540, -0.4904],\n",
       "         [ 0.0056,  0.1047, -1.9775]],\n",
       "\n",
       "        [[ 0.6662, -0.1759, -1.9307],\n",
       "         [ 1.0016, -0.1147, -0.1972],\n",
       "         [ 1.5372, -0.2062, -0.5803]],\n",
       "\n",
       "        [[ 0.2912, -0.4021,  1.6770],\n",
       "         [-1.3768, -0.6263, -0.3877],\n",
       "         [-0.3498,  1.5068, -0.3323]],\n",
       "\n",
       "        [[-1.1274,  0.1439,  1.9401],\n",
       "         [-0.5153,  0.1719, -1.2687],\n",
       "         [ 0.8590, -0.4767,  0.2731]]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(value - mean)/std"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
