{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e78e44c-98cc-4059-8ae6-72fb058cabf4",
   "metadata": {},
   "source": [
    "# Managing network\n",
    "\n",
    "Is a class that allows you to define complex neural networks in Torch. Simply use this class as a descendant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cb58b03-6c03-4bf4-bed2-be16d2fc2287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89b3b93",
   "metadata": {},
   "source": [
    "## Sequential\n",
    "\n",
    "You can use `torch.nn.Sequential` to combine multiple network layers into a sequential chain. Find out more in the [specific page](managing_network/sequential.ipynb).\n",
    "\n",
    "---\n",
    "\n",
    "The following cell demonstrates a basic example where a linear transformation is applied to the input, followed by a ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329df276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.8781],\n",
       "        [0.4362, 0.0000, 0.7350],\n",
       "        [0.0000, 0.0000, 1.1225]], grad_fn=<ReluBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "size = 3\n",
    "\n",
    "sequential = torch.nn.Sequential(\n",
    "    torch.nn.Linear(size, size, bias=False),\n",
    "    torch.nn.ReLU()\n",
    ")\n",
    "\n",
    "X = torch.randn([3, 3])\n",
    "sequential(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b5b73f",
   "metadata": {},
   "source": [
    "## Separate class\n",
    "\n",
    "You can define a neural network as a separate class, which allows you to add custom logic for initialization or network-specific procedures. To create a network class, follow these rules:\n",
    "\n",
    "- **Inherit from `torch.nn.Module`:** This establishes your class as a PyTorch module, providing access to its functionality.\n",
    "- **Call `super().__init__()` in the constructor:** This initializes the base `nn.Module` class, ensuring proper setup.\n",
    "- **Define a `forward` method:** This method implements the computational procedure of your network. It defines how input data flows through your layers to produce output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b28f6b2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines a set of Linear layers whose size is determined during class creation. The forward method standardizes the data before applying the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dc0fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleNetwork(torch.nn.Module):\n",
    "    def __init__(self, layers_number: int, neurons: int):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = torch.nn.Sequential(*[\n",
    "            torch.nn.Linear(neurons, neurons)\n",
    "            for i in range(layers_number)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, X: torch.Tensor):\n",
    "        X = (X - X.mean(axis=0, keepdim=True))/X.std(axis=0, keepdim=True)\n",
    "        return self.network(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6f24bf",
   "metadata": {},
   "source": [
    "Let's check if the network we've defined works as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e32c819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2482,  0.0882,  0.4507],\n",
       "        [-0.2465,  0.0897,  0.4466],\n",
       "        [-0.2531,  0.0827,  0.4587],\n",
       "        [-0.2463,  0.0899,  0.4459],\n",
       "        [-0.2461,  0.0892,  0.4429]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ExampleNetwork(layers_number=10, neurons=3)(X = torch.randn([5, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b0f9df-64a9-4613-98da-670e17eb8244",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "To be able to optimize network properly you need tools that allows to access paraterers and manage them. As this section we consider typical methods that help to manage model parameters in torch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f17636",
   "metadata": {},
   "source": [
    "### Access parameters\n",
    "\n",
    "To access model parameters, use the `torch.nn.Module.parameters` method. This method returns a generator that iterates over the parameters of all layers in the network.\n",
    "\n",
    "Check the official [documentation on the `parameters` method](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d7afce-4e96-4d41-b8c6-f5e7a328741d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In the following cell we have an empty `nn.Module` - so when we try to unpack it generator to list we have just an empty list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad8022cb-2f1d-4a23-a29d-540946fa9cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmptyNetwork(nn.Module):\n",
    "    pass\n",
    "empty_network = EmptyNetwork()\n",
    "[i for i in empty_network.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f585fdc-cd8f-41f6-8075-57899c5a6d49",
   "metadata": {},
   "source": [
    "This cell implements such a descendant of the `nn.Module`, taking some parameters from its files. To be more specific, there are two fully connected layers defined here. So we end up with four tensors, two matrices for fully connected layers and their biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bad4179-1ca1-4e10-a24a-3510f80c57f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4889, -0.2448, -0.1750],\n",
      "        [ 0.0770, -0.0333,  0.2421],\n",
      "        [-0.0755, -0.2302, -0.4851]])\n",
      "tensor([ 0.1668, -0.5771,  0.4508])\n",
      "tensor([[ 0.1828, -0.3526, -0.3598,  0.4468, -0.2286],\n",
      "        [-0.0492,  0.3426,  0.2613,  0.2133, -0.2792],\n",
      "        [-0.2052,  0.2514,  0.0616,  0.4382, -0.2944],\n",
      "        [-0.2796, -0.0471, -0.4185,  0.4359,  0.2697],\n",
      "        [ 0.3577,  0.4372, -0.0179,  0.1575,  0.2003]])\n",
      "tensor([-0.4275,  0.0130,  0.4131, -0.2934,  0.3826])\n"
     ]
    }
   ],
   "source": [
    "class ParametersNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.foo = nn.Linear(3, 3)\n",
    "        self.bar = nn.Linear(5, 5)\n",
    "\n",
    "network = ParametersNetwork()\n",
    "for i in network.parameters():\n",
    "    print(i.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319f1b6e",
   "metadata": {},
   "source": [
    "### Requires grad\n",
    "\n",
    "You can manipulate the set of parameters that will compute gradients in a neural network. You can do this directly by accessing the parameters and setting their `requires_grad` attribute to `False`. However, there is a `requires_grad_()` method that allows you to set the gradient property for all weights of an `nn.Module`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97e0d1e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines a network and prints the `requires_grad` attribute of its weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b8725953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight True\n",
      "0.bias True\n",
      "2.weight True\n",
      "2.bias True\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(10)\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=10, out_features=10),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=10, out_features=1),\n",
    ")\n",
    "\n",
    "for name, parameters in model.named_parameters():\n",
    "    print(name, parameters.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebb55d2",
   "metadata": {},
   "source": [
    "By default, all parameters require gradients. The following cell applies `requires_grad_(False)` to the entire network and then sets `requires_grad(True)` for just one of the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "35a8fb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight False\n",
      "0.bias False\n",
      "2.weight True\n",
      "2.bias True\n"
     ]
    }
   ],
   "source": [
    "model.requires_grad_(False)\n",
    "model[2].requires_grad_(True)\n",
    "\n",
    "for name, parameters in model.named_parameters():\n",
    "    print(name, parameters.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0718f659",
   "metadata": {},
   "source": [
    "As a result, only the corresponding parameters will require gradients. During the optimization process, only those parameters that require gradients will be updated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
