{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gan\n",
    "\n",
    "This notebook covers the process of building a Generative Adversarial Network (GAN) and it's application to the `MNIST` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "print(\"using device\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As example `MNIST` dataset will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET = MNIST(\n",
    "    Path(\"mnist_files\"),\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=T.ToTensor()\n",
    ")\n",
    "DATA_LOADER = DataLoader(TRAIN_DATASET, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator's feature map\n",
    "ngf = 64\n",
    "# Input channels\n",
    "nc = 1\n",
    "# Size of the vector from which generator will create a picture\n",
    "nz = 100\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # (nz) x 1 x 1\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=nz, \n",
    "                out_channels=ngf * 2, \n",
    "                kernel_size=7,\n",
    "                stride=1, \n",
    "                padding=0, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (ngf*2) x 7 x 7\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (ngf) x 14 x 14\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "            # (nc) x 28 x 28\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "# Discriminator's feature map\n",
    "ndf = 64\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # (nc) x 28 x 28\n",
    "            nn.Conv2d(\n",
    "                in_channels=nc, \n",
    "                out_channels=ndf, \n",
    "                kernel_size=4, \n",
    "                stride=2, \n",
    "                padding=1, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "\n",
    "            # (ndf) x 14 x 14\n",
    "            nn.Conv2d(\n",
    "                in_channels=ndf, \n",
    "                out_channels=ndf * 2, \n",
    "                kernel_size=4, \n",
    "                stride=2, \n",
    "                padding=1, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "\n",
    "            # (ndf*2) x 7 x 7\n",
    "            nn.Conv2d(\n",
    "                in_channels=ndf * 2, \n",
    "                out_channels=1, \n",
    "                kernel_size=7, \n",
    "                stride=1, \n",
    "                padding=0, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator().to(DEVICE)\n",
    "netG = netG.apply(weights_init)\n",
    "\n",
    "netD = Discriminator().to(DEVICE)\n",
    "netD = netD.apply(weights_init)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "lr = 0.001\n",
    "beta1 = 0.5\n",
    "\n",
    "# We'll draw images from the same input to compare results.\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=DEVICE)\n",
    "\n",
    "# Labels for real and fake images\n",
    "real_label, fake_label = 1., 0.\n",
    "\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(DATA_LOADER, 0):\n",
    "        real_batch = data[0].to(DEVICE)\n",
    "        batch_size = real_batch.size(0)\n",
    "        \n",
    "\n",
    "        # Maximazing over discriminator log(D(x)) + log(1 - D(G(z)))\n",
    "        # Step on the real image\n",
    "        netD.zero_grad()\n",
    "        label = torch.full((batch_size,), real_label, dtype=torch.float, device=DEVICE)\n",
    "        output = netD(real_batch).view(-1)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # Step on the fake image\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=DEVICE)\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        # Maximizing for generator log(D(G(z)))\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)\n",
    "        output = netD(fake).view(-1)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(\n",
    "                \"[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    num_epochs,\n",
    "                    i,\n",
    "                    len(DATA_LOADER),\n",
    "                    errD.item(),\n",
    "                    errG.item(),\n",
    "                    D_x,\n",
    "                    D_G_z1,\n",
    "                    D_G_z2,\n",
    "                )\n",
    "            )\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        if (iters % 500 == 0) or (\n",
    "            (epoch == num_epochs - 1) and (i == len(DATA_LOADER) - 1)\n",
    "        ):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "noize = torch.randn(batch_size, nz, 1, 1, device=DEVICE)\n",
    "plt.imshow(netG(noize).squeeze().detach().numpy())\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional\n",
    "\n",
    "Here, we consider a modification of the previous model. The model shown in this section will draw a picture of the given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator's feature map\n",
    "ngf = 64\n",
    "# Input channels\n",
    "nc = 1\n",
    "# Size of the vector from which generator will create a picture\n",
    "nz = 100\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=10,\n",
    "            embedding_dim=10\n",
    "        )\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # (nz) x 1 x 1\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=nz+10,\n",
    "                out_channels=ngf * 2, \n",
    "                kernel_size=7,\n",
    "                stride=1, \n",
    "                padding=0, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (ngf*2) x 7 x 7\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (ngf) x 14 x 14\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "            # (nc) x 28 x 28\n",
    "        )\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        ohe_labels = self.embedding(labels)[:, :, None, None]\n",
    "        input = torch.cat([input, ohe_labels], dim=1)\n",
    "        return self.main(input)\n",
    "\n",
    "# Discriminator's feature map\n",
    "ndf = 64\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(\n",
    "            num_embeddings=10, \n",
    "            embedding_dim=28 * 28\n",
    "        )\n",
    "        self.main = nn.Sequential(\n",
    "            # (nc) x 28 x 28\n",
    "            nn.Conv2d(\n",
    "                in_channels=nc+1, \n",
    "                out_channels=ndf, \n",
    "                kernel_size=4, \n",
    "                stride=2, \n",
    "                padding=1, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "\n",
    "            # (ndf) x 14 x 14\n",
    "            nn.Conv2d(\n",
    "                in_channels=ndf, \n",
    "                out_channels=ndf * 2, \n",
    "                kernel_size=4, \n",
    "                stride=2, \n",
    "                padding=1, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "\n",
    "            # (ndf*2) x 7 x 7\n",
    "            nn.Conv2d(\n",
    "                in_channels=ndf * 2, \n",
    "                out_channels=1,\n",
    "                kernel_size=7, \n",
    "                stride=1, \n",
    "                padding=0, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        labels = self.embedding(labels).reshape_as(input)\n",
    "        input = torch.cat([input, labels], dim=1)\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator().to(DEVICE)\n",
    "netG = netG.apply(weights_init)\n",
    "\n",
    "netD = Discriminator().to(DEVICE)\n",
    "netD = netD.apply(weights_init)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "lr = 0.001\n",
    "beta1 = 0.5\n",
    "\n",
    "# We'll draw images from the same input to compare results.\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=DEVICE)\n",
    "\n",
    "# Labels for real and fake images\n",
    "real_label, fake_label = 1., 0.\n",
    "\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (X, y) in enumerate(DATA_LOADER, 0):\n",
    "        real_batch = X.to(DEVICE)\n",
    "        batch_size = real_batch.size(0)\n",
    "        \n",
    "\n",
    "        # Maximazing over discriminator log(D(x)) + log(1 - D(G(z)))\n",
    "        # Step on the real image\n",
    "        netD.zero_grad()\n",
    "        label = torch.full((batch_size,), real_label, dtype=torch.float, device=DEVICE)\n",
    "        output = netD(real_batch, y).view(-1)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # Step on the fake image\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=DEVICE)\n",
    "        fake = netG(noise, y)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach(), y).view(-1)\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        # Maximizing for generator log(D(G(z)))\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)\n",
    "        output = netD(fake, y).view(-1)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(\n",
    "                \"[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    num_epochs,\n",
    "                    i,\n",
    "                    len(DATA_LOADER),\n",
    "                    errD.item(),\n",
    "                    errG.item(),\n",
    "                    D_x,\n",
    "                    D_G_z1,\n",
    "                    D_G_z2,\n",
    "                )\n",
    "            )\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        if (iters % 500 == 0) or (\n",
    "            (epoch == num_epochs - 1) and (i == len(DATA_LOADER) - 1)\n",
    "        ):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise, y).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows the result of the obtained network. In the `number` variable, you can specify the number to draw, and the result from the model will be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "noise = torch.randn(1, nz, 1, 1, device=DEVICE)\n",
    "number = 0\n",
    "\n",
    "res = netG(\n",
    "    noise, \n",
    "    torch.tensor([number])\n",
    ")\n",
    "plt.imshow(res.squeeze().detach().numpy())\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
