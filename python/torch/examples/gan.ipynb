{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gan\n",
    "\n",
    "This notebook covers the process of building a Generative Adversarial Network (GAN) and it's application to the `MNIST` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "print(\"using device\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As example `MNIST` dataset will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET = MNIST(\n",
    "    Path(\"mnist_files\"),\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=T.ToTensor()\n",
    ")\n",
    "DATA_LOADER = DataLoader(TRAIN_DATASET, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics\n",
    "\n",
    "At the begining consider network that just tries imitate digits that are in the mnist dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "Following cell implements `Generator` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    A class that generates a picture from a set of random noise.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_map_size: int\n",
    "        Feature map's size of the generator.\n",
    "    number_channels: int\n",
    "        Number of input channels.\n",
    "    noize_size: int\n",
    "        Size of the vector that is expected to be transformed to the picture by \n",
    "        the model.\n",
    "    '''\n",
    "    def __init__(\n",
    "        self, \n",
    "        feature_map_size: int, \n",
    "        number_channels: int, \n",
    "        noize_size: int\n",
    "    ) -> None:\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # (nz) x 1 x 1\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=noize_size, \n",
    "                out_channels=feature_map_size * 2, \n",
    "                kernel_size=7,\n",
    "                stride=1, \n",
    "                padding=0, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(feature_map_size * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (feature_map_size*2) x 7 x 7\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=feature_map_size * 2, \n",
    "                out_channels=feature_map_size, \n",
    "                kernel_size=4, \n",
    "                stride=2, \n",
    "                padding=1, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(feature_map_size),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (feature_map_size) x 14 x 14\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=feature_map_size, \n",
    "                out_channels=number_channels, \n",
    "                kernel_size=4, \n",
    "                stride=2, \n",
    "                padding=1, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.Tanh(),\n",
    "            # (nc) x 28 x 28\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        Apply model to given data.\n",
    "\n",
    "        Paramaters\n",
    "        ----------\n",
    "        input: torch.Tensor\n",
    "            Tensor with size (n_samples, self.nz)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out: torch.Tensor\n",
    "            Tensor that represents set of generated pictures.\n",
    "        '''\n",
    "        return self.main(input[:, :, None, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check what options there are to use it: to generator you have to pass vectors with random values. For each random vertor it will return picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = Generator(\n",
    "    feature_map_size=64, \n",
    "    number_channels=1, \n",
    "    noize_size=100\n",
    ")\n",
    "generator(torch.randn(20, 100)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n",
    "The discriminator is a model that tries to determine if a picture was created by the generator or not. The following cell defines the generator that we will use for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Realisation of the discriminator. Class that takes picture and generate scor\n",
    "    which expresses how much the model thinks the picture is generated.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    number_channels: int\n",
    "        Number of channels in input.\n",
    "    feature_map_size: int\n",
    "        Feature map's size.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, number_channels: int, feature_map_size: int):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # (number_channels) x 28 x 28\n",
    "            nn.Conv2d(\n",
    "                in_channels=number_channels, \n",
    "                out_channels=feature_map_size, \n",
    "                kernel_size=4, \n",
    "                stride=2, \n",
    "                padding=1, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "\n",
    "            # (feature_map_size) x 14 x 14\n",
    "            nn.Conv2d(\n",
    "                in_channels=feature_map_size, \n",
    "                out_channels=feature_map_size * 2, \n",
    "                kernel_size=4, \n",
    "                stride=2, \n",
    "                padding=1, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(feature_map_size * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "\n",
    "            # (feature_map_size*2) x 7 x 7\n",
    "            nn.Conv2d(\n",
    "                in_channels=feature_map_size * 2, \n",
    "                out_channels=1, \n",
    "                kernel_size=7, \n",
    "                stride=1, \n",
    "                padding=0, \n",
    "                bias=False\n",
    "            ),\n",
    "            \n",
    "            nn.Sigmoid(),\n",
    "            nn.Flatten(start_dim=0, end_dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider how the discriminator works by passing a sample picture from the training data to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4926], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator = Discriminator(\n",
    "    number_channels=1, \n",
    "    feature_map_size=64\n",
    ")\n",
    "discriminator(TRAIN_DATASET[0][0][None, :, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a score that represents the model's prediction of whether the picture we passed was generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map_size = 64\n",
    "noise_size = 100\n",
    "\n",
    "netG = Generator(\n",
    "    feature_map_size=feature_map_size, \n",
    "    number_channels=1, \n",
    "    noize_size=noise_size\n",
    ").to(DEVICE)\n",
    "netG = netG.apply(weights_init)\n",
    "\n",
    "netD = Discriminator(\n",
    "    feature_map_size=feature_map_size, \n",
    "    number_channels=1\n",
    ").to(DEVICE)\n",
    "netD = netD.apply(weights_init)\n",
    "\n",
    "lr = 0.001\n",
    "beta1 = 0.5\n",
    "\n",
    "# We'll draw images from the same input to compare results.\n",
    "fixed_noise = torch.randn(64, noise_size, device=DEVICE)\n",
    "\n",
    "# Labels for real and fake images\n",
    "real_label, fake_label = 1., 0.\n",
    "\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_step(\n",
    "    pictures: torch.Tensor,\n",
    "    generation: torch.Tensor,\n",
    "    discriminator: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer\n",
    "):\n",
    "    '''\n",
    "    Step of the discriminator. Maximize log(D(x)) + log(1 - D(G(z))) - tries to \n",
    "    to improve the prediction that the real images have 1. scores and the \n",
    "    generated images have have 0. scores.\n",
    "    '''\n",
    "\n",
    "    batch_size = pictures.shape[0]\n",
    "\n",
    "    # Gradient accumulation on real images\n",
    "    discriminator.zero_grad()\n",
    "    label = torch.full((batch_size,), 1., dtype=torch.float, device=DEVICE)\n",
    "    output = discriminator(pictures)\n",
    "    errD_real = binary_cross_entropy(output, label)\n",
    "    errD_real.backward()\n",
    "    D_x = output.mean().item()\n",
    "\n",
    "    # Gradient accumulation on fake images\n",
    "    label.fill_(fake_label)\n",
    "    output = discriminator(generation.detach())\n",
    "    errD_fake = binary_cross_entropy(output, label)\n",
    "    errD_fake.backward()\n",
    "    D_G_z1 = output.mean().item()\n",
    "    errD = errD_real + errD_fake\n",
    "\n",
    "    # Step based on accumulated gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    return D_x, D_G_z1, errD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][0/938]\tLoss_D: 2.8116\tLoss_G: 2.6194\tD(x): 0.1223\tD(G(z)): 0.2820 / 0.0932\n",
      "[0/1][50/938]\tLoss_D: 1.2805\tLoss_G: 10.2501\tD(x): 0.9953\tD(G(z)): 0.6238 / 0.0001\n",
      "[0/1][100/938]\tLoss_D: 0.8899\tLoss_G: 2.3883\tD(x): 0.4925\tD(G(z)): 0.0013 / 0.1565\n",
      "[0/1][150/938]\tLoss_D: 0.1096\tLoss_G: 4.6373\tD(x): 0.9740\tD(G(z)): 0.0739 / 0.0140\n",
      "[0/1][200/938]\tLoss_D: 0.0962\tLoss_G: 4.5319\tD(x): 0.9323\tD(G(z)): 0.0237 / 0.0171\n",
      "[0/1][250/938]\tLoss_D: 0.2454\tLoss_G: 4.7991\tD(x): 0.9871\tD(G(z)): 0.1906 / 0.0165\n",
      "[0/1][300/938]\tLoss_D: 1.2189\tLoss_G: 2.4034\tD(x): 0.3960\tD(G(z)): 0.0036 / 0.1661\n",
      "[0/1][350/938]\tLoss_D: 0.3990\tLoss_G: 2.2109\tD(x): 0.8651\tD(G(z)): 0.2049 / 0.1399\n",
      "[0/1][400/938]\tLoss_D: 0.1726\tLoss_G: 5.5196\tD(x): 0.8615\tD(G(z)): 0.0110 / 0.0110\n",
      "[0/1][450/938]\tLoss_D: 0.2412\tLoss_G: 7.0861\tD(x): 0.8026\tD(G(z)): 0.0012 / 0.0034\n",
      "[0/1][500/938]\tLoss_D: 0.5960\tLoss_G: 4.7837\tD(x): 0.9621\tD(G(z)): 0.3752 / 0.0128\n",
      "[0/1][550/938]\tLoss_D: 0.0552\tLoss_G: 4.2920\tD(x): 0.9689\tD(G(z)): 0.0222 / 0.0186\n",
      "[0/1][600/938]\tLoss_D: 0.3489\tLoss_G: 2.7729\tD(x): 0.7739\tD(G(z)): 0.0454 / 0.0939\n",
      "[0/1][650/938]\tLoss_D: 0.0900\tLoss_G: 4.2776\tD(x): 0.9614\tD(G(z)): 0.0448 / 0.0234\n",
      "[0/1][700/938]\tLoss_D: 0.3701\tLoss_G: 3.9685\tD(x): 0.9633\tD(G(z)): 0.2412 / 0.0293\n",
      "[0/1][750/938]\tLoss_D: 0.7662\tLoss_G: 1.1196\tD(x): 0.5873\tD(G(z)): 0.0789 / 0.4315\n",
      "[0/1][800/938]\tLoss_D: 0.8720\tLoss_G: 0.5696\tD(x): 0.5600\tD(G(z)): 0.0481 / 0.6255\n",
      "[0/1][850/938]\tLoss_D: 0.4283\tLoss_G: 3.9890\tD(x): 0.9244\tD(G(z)): 0.2428 / 0.0242\n",
      "[0/1][900/938]\tLoss_D: 0.2087\tLoss_G: 3.1107\tD(x): 0.9133\tD(G(z)): 0.1002 / 0.0599\n"
     ]
    }
   ],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (pictures, _) in enumerate(DATA_LOADER, 0):\n",
    "        pictures = pictures.to(DEVICE)\n",
    "        batch_size = pictures.size(0)\n",
    "        \n",
    "        # Getting generated (\"fake\") picture that tries to trick discriminator\n",
    "        noise = torch.randn(batch_size, noise_size, device=DEVICE)\n",
    "        fake = netG(noise)\n",
    "\n",
    "        D_x, D_G_z1, errD = discriminator_step(\n",
    "            pictures=pictures,\n",
    "            generation=fake,\n",
    "            discriminator=netD,\n",
    "            optimizer=optimizerD\n",
    "        )\n",
    "\n",
    "        # Maximizing for generator log(D(G(z)))\n",
    "        netG.zero_grad()\n",
    "        label = torch.full((batch_size,), real_label, dtype=torch.float, device=DEVICE)\n",
    "        output = netD(fake)\n",
    "        errG = binary_cross_entropy(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(\n",
    "                \"[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    num_epochs,\n",
    "                    i,\n",
    "                    len(DATA_LOADER),\n",
    "                    errD.item(),\n",
    "                    errG.item(),\n",
    "                    D_x,\n",
    "                    D_G_z1,\n",
    "                    D_G_z2,\n",
    "                )\n",
    "            )\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        if (iters % 500 == 0) or (\n",
    "            (epoch == num_epochs - 1) and (i == len(DATA_LOADER) - 1)\n",
    "        ):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "noize = torch.randn(1, noise_size, device=DEVICE)\n",
    "plt.imshow(netG(noize).squeeze().detach().numpy())\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional\n",
    "\n",
    "Here, we consider a modification of the previous model. The model shown in this section will draw a picture of the given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator's feature map\n",
    "ngf = 64\n",
    "# Input channels\n",
    "nc = 1\n",
    "# Size of the vector from which generator will create a picture\n",
    "nz = 100\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=10,\n",
    "            embedding_dim=10\n",
    "        )\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # (nz) x 1 x 1\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=nz+10,\n",
    "                out_channels=ngf * 2, \n",
    "                kernel_size=7,\n",
    "                stride=1, \n",
    "                padding=0, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (ngf*2) x 7 x 7\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (ngf) x 14 x 14\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "            # (nc) x 28 x 28\n",
    "        )\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        ohe_labels = self.embedding(labels)[:, :, None, None]\n",
    "        input = torch.cat([input, ohe_labels], dim=1)\n",
    "        return self.main(input)\n",
    "\n",
    "# Discriminator's feature map\n",
    "ndf = 64\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(\n",
    "            num_embeddings=10, \n",
    "            embedding_dim=28 * 28\n",
    "        )\n",
    "        self.main = nn.Sequential(\n",
    "            # (nc) x 28 x 28\n",
    "            nn.Conv2d(\n",
    "                in_channels=nc+1, \n",
    "                out_channels=ndf, \n",
    "                kernel_size=4, \n",
    "                stride=2, \n",
    "                padding=1, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "\n",
    "            # (ndf) x 14 x 14\n",
    "            nn.Conv2d(\n",
    "                in_channels=ndf, \n",
    "                out_channels=ndf * 2, \n",
    "                kernel_size=4, \n",
    "                stride=2, \n",
    "                padding=1, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "\n",
    "            # (ndf*2) x 7 x 7\n",
    "            nn.Conv2d(\n",
    "                in_channels=ndf * 2, \n",
    "                out_channels=1,\n",
    "                kernel_size=7, \n",
    "                stride=1, \n",
    "                padding=0, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        labels = self.embedding(labels).reshape_as(input)\n",
    "        input = torch.cat([input, labels], dim=1)\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator().to(DEVICE)\n",
    "netG = netG.apply(weights_init)\n",
    "\n",
    "netD = Discriminator().to(DEVICE)\n",
    "netD = netD.apply(weights_init)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "lr = 0.001\n",
    "beta1 = 0.5\n",
    "\n",
    "# We'll draw images from the same input to compare results.\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=DEVICE)\n",
    "\n",
    "# Labels for real and fake images\n",
    "real_label, fake_label = 1., 0.\n",
    "\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (X, y) in enumerate(DATA_LOADER, 0):\n",
    "        real_batch = X.to(DEVICE)\n",
    "        batch_size = real_batch.size(0)\n",
    "        \n",
    "\n",
    "        # Maximazing over discriminator log(D(x)) + log(1 - D(G(z)))\n",
    "        # Step on the real image\n",
    "        netD.zero_grad()\n",
    "        label = torch.full((batch_size,), real_label, dtype=torch.float, device=DEVICE)\n",
    "        output = netD(real_batch, y).view(-1)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # Step on the fake image\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=DEVICE)\n",
    "        fake = netG(noise, y)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach(), y).view(-1)\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        # Maximizing for generator log(D(G(z)))\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)\n",
    "        output = netD(fake, y).view(-1)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(\n",
    "                \"[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    num_epochs,\n",
    "                    i,\n",
    "                    len(DATA_LOADER),\n",
    "                    errD.item(),\n",
    "                    errG.item(),\n",
    "                    D_x,\n",
    "                    D_G_z1,\n",
    "                    D_G_z2,\n",
    "                )\n",
    "            )\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        if (iters % 500 == 0) or (\n",
    "            (epoch == num_epochs - 1) and (i == len(DATA_LOADER) - 1)\n",
    "        ):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise, y).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows the result of the obtained network. In the `number` variable, you can specify the number to draw, and the result from the model will be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "noise = torch.randn(1, nz, 1, 1, device=DEVICE)\n",
    "number = 0\n",
    "\n",
    "res = netG(\n",
    "    noise, \n",
    "    torch.tensor([number])\n",
    ")\n",
    "plt.imshow(res.squeeze().detach().numpy())\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
