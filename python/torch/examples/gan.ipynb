{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gan\n",
    "\n",
    "This notebook covers the process of building a Generative Adversarial Network (GAN) and it's application to the `MNIST` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "print(\"using device\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As example `MNIST` dataset will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET = MNIST(\n",
    "    Path(\"mnist_files\"),\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=T.ToTensor()\n",
    ")\n",
    "DATA_LOADER = DataLoader(TRAIN_DATASET, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics\n",
    "\n",
    "At the begining consider network that just tries imitate digits that are in the mnist dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "Following cell implements `Generator` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    A class that generates a picture from a set of random noise.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_map_size: int\n",
    "        Feature map's size of the generator.\n",
    "    number_channels: int\n",
    "        Number of input channels.\n",
    "    noize_size: int\n",
    "        Size of the vector that is expected to be transformed to the picture by \n",
    "        the model.\n",
    "    '''\n",
    "    def __init__(\n",
    "        self, \n",
    "        feature_map_size: int, \n",
    "        number_channels: int, \n",
    "        noize_size: int\n",
    "    ) -> None:\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # (nz) x 1 x 1\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=noize_size, \n",
    "                out_channels=feature_map_size * 2, \n",
    "                kernel_size=7,\n",
    "                stride=1, \n",
    "                padding=0, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(feature_map_size * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (feature_map_size*2) x 7 x 7\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=feature_map_size * 2, \n",
    "                out_channels=feature_map_size, \n",
    "                kernel_size=4, \n",
    "                stride=2, \n",
    "                padding=1, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(feature_map_size),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (feature_map_size) x 14 x 14\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=feature_map_size, \n",
    "                out_channels=number_channels, \n",
    "                kernel_size=4, \n",
    "                stride=2, \n",
    "                padding=1, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.Tanh(),\n",
    "            # (nc) x 28 x 28\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        Apply model to given data.\n",
    "\n",
    "        Paramaters\n",
    "        ----------\n",
    "        input: torch.Tensor\n",
    "            Tensor with size (n_samples, self.nz)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out: torch.Tensor\n",
    "            Tensor that represents set of generated pictures.\n",
    "        '''\n",
    "        return self.main(input[:, :, None, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check what options there are to use it: to generator you have to pass 10 vectors with random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = Generator(\n",
    "    feature_map_size=64, \n",
    "    number_channels=1, \n",
    "    noize_size=100\n",
    ")\n",
    "generator(torch.randn(20, 100)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n",
    "The discriminator is a model that tries to determine if a picture was generated by the generator or not. The following cell defines the generator that we will use for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Realisation of the discriminator. Class that takes picture and generate scor\n",
    "    which expresses how much the model thinks the picture is generated.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    number_channels: int\n",
    "        Number of channels in input.\n",
    "    feature_map_size: int\n",
    "        Feature map's size.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, number_channels: int, feature_map_size: int):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # (number_channels) x 28 x 28\n",
    "            nn.Conv2d(\n",
    "                in_channels=number_channels, \n",
    "                out_channels=feature_map_size, \n",
    "                kernel_size=4, \n",
    "                stride=2, \n",
    "                padding=1, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "\n",
    "            # (feature_map_size) x 14 x 14\n",
    "            nn.Conv2d(\n",
    "                in_channels=feature_map_size, \n",
    "                out_channels=feature_map_size * 2, \n",
    "                kernel_size=4, \n",
    "                stride=2, \n",
    "                padding=1, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(feature_map_size * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "\n",
    "            # (feature_map_size*2) x 7 x 7\n",
    "            nn.Conv2d(\n",
    "                in_channels=feature_map_size * 2, \n",
    "                out_channels=1, \n",
    "                kernel_size=7, \n",
    "                stride=1, \n",
    "                padding=0, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider how the discriminator works by passing a sample picture from the training data to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.5263]]]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator = Discriminator(\n",
    "    number_channels=1, \n",
    "    feature_map_size=64\n",
    ")\n",
    "discriminator(TRAIN_DATASET[0][0][None, :, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a score that represents the model's prediction of whether the picture we passed was generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_map_size = 64\n",
    "NZ = 100\n",
    "\n",
    "netG = Generator(\n",
    "    feature_map_size=feature_map_size, \n",
    "    number_channels=1, \n",
    "    noize_size=NZ\n",
    ").to(DEVICE)\n",
    "netG = netG.apply(weights_init)\n",
    "\n",
    "netD = Discriminator(\n",
    "    feature_map_size=feature_map_size, \n",
    "    number_channels=1\n",
    ").to(DEVICE)\n",
    "netD = netD.apply(weights_init)\n",
    "\n",
    "# Binary cross entropy loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "lr = 0.001\n",
    "beta1 = 0.5\n",
    "\n",
    "# We'll draw images from the same input to compare results.\n",
    "fixed_noise = torch.randn(64, NZ, device=DEVICE)\n",
    "\n",
    "# Labels for real and fake images\n",
    "real_label, fake_label = 1., 0.\n",
    "\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][0/938]\tLoss_D: 0.2062\tLoss_G: 3.7812\tD(x): 0.9794\tD(G(z)): 0.1636 / 0.0289\n",
      "[0/1][50/938]\tLoss_D: 0.1083\tLoss_G: 4.3305\tD(x): 0.9642\tD(G(z)): 0.0613 / 0.0295\n",
      "[0/1][100/938]\tLoss_D: 0.9515\tLoss_G: 0.9862\tD(x): 0.5696\tD(G(z)): 0.2470 / 0.4016\n",
      "[0/1][150/938]\tLoss_D: 0.4783\tLoss_G: 2.2991\tD(x): 0.8499\tD(G(z)): 0.2488 / 0.1320\n",
      "[0/1][200/938]\tLoss_D: 0.1481\tLoss_G: 3.2683\tD(x): 0.9188\tD(G(z)): 0.0582 / 0.0534\n",
      "[0/1][250/938]\tLoss_D: 0.2824\tLoss_G: 3.4302\tD(x): 0.8028\tD(G(z)): 0.0348 / 0.0490\n",
      "[0/1][300/938]\tLoss_D: 0.4252\tLoss_G: 1.1380\tD(x): 0.7548\tD(G(z)): 0.0913 / 0.3991\n",
      "[0/1][350/938]\tLoss_D: 0.2799\tLoss_G: 2.7492\tD(x): 0.9295\tD(G(z)): 0.1703 / 0.1004\n",
      "[0/1][400/938]\tLoss_D: 1.0487\tLoss_G: 4.4676\tD(x): 0.4539\tD(G(z)): 0.0084 / 0.0359\n",
      "[0/1][450/938]\tLoss_D: 0.4071\tLoss_G: 2.6038\tD(x): 0.9535\tD(G(z)): 0.2643 / 0.1066\n",
      "[0/1][500/938]\tLoss_D: 0.1750\tLoss_G: 2.1053\tD(x): 0.9015\tD(G(z)): 0.0585 / 0.1730\n",
      "[0/1][550/938]\tLoss_D: 0.4777\tLoss_G: 4.5121\tD(x): 0.9820\tD(G(z)): 0.3252 / 0.0151\n",
      "[0/1][600/938]\tLoss_D: 0.2070\tLoss_G: 2.7031\tD(x): 0.8881\tD(G(z)): 0.0667 / 0.0931\n",
      "[0/1][650/938]\tLoss_D: 0.2278\tLoss_G: 3.5528\tD(x): 0.9409\tD(G(z)): 0.1319 / 0.0419\n",
      "[0/1][700/938]\tLoss_D: 0.2550\tLoss_G: 2.9568\tD(x): 0.8592\tD(G(z)): 0.0825 / 0.0744\n",
      "[0/1][750/938]\tLoss_D: 0.9476\tLoss_G: 1.1937\tD(x): 0.4943\tD(G(z)): 0.0453 / 0.4000\n",
      "[0/1][800/938]\tLoss_D: 0.4483\tLoss_G: 2.5015\tD(x): 0.7957\tD(G(z)): 0.1530 / 0.1234\n",
      "[0/1][850/938]\tLoss_D: 0.3436\tLoss_G: 2.3027\tD(x): 0.8057\tD(G(z)): 0.1011 / 0.1457\n",
      "[0/1][900/938]\tLoss_D: 0.2300\tLoss_G: 3.0957\tD(x): 0.9052\tD(G(z)): 0.1099 / 0.0595\n"
     ]
    }
   ],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(DATA_LOADER, 0):\n",
    "        real_batch = data[0].to(DEVICE)\n",
    "        batch_size = real_batch.size(0)\n",
    "        \n",
    "\n",
    "        # Maximazing over discriminator log(D(x)) + log(1 - D(G(z)))\n",
    "        # Step on the real image\n",
    "        netD.zero_grad()\n",
    "        label = torch.full((batch_size,), real_label, dtype=torch.float, device=DEVICE)\n",
    "        output = netD(real_batch).view(-1)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # Step on the fake image\n",
    "        noise = torch.randn(batch_size, NZ, device=DEVICE)\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        # Maximizing for generator log(D(G(z)))\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)\n",
    "        output = netD(fake).view(-1)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(\n",
    "                \"[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    num_epochs,\n",
    "                    i,\n",
    "                    len(DATA_LOADER),\n",
    "                    errD.item(),\n",
    "                    errG.item(),\n",
    "                    D_x,\n",
    "                    D_G_z1,\n",
    "                    D_G_z2,\n",
    "                )\n",
    "            )\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        if (iters % 500 == 0) or (\n",
    "            (epoch == num_epochs - 1) and (i == len(DATA_LOADER) - 1)\n",
    "        ):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "noize = torch.randn(1, NZ, device=DEVICE)\n",
    "plt.imshow(netG(noize).squeeze().detach().numpy())\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional\n",
    "\n",
    "Here, we consider a modification of the previous model. The model shown in this section will draw a picture of the given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator's feature map\n",
    "ngf = 64\n",
    "# Input channels\n",
    "nc = 1\n",
    "# Size of the vector from which generator will create a picture\n",
    "nz = 100\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=10,\n",
    "            embedding_dim=10\n",
    "        )\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # (nz) x 1 x 1\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=nz+10,\n",
    "                out_channels=ngf * 2, \n",
    "                kernel_size=7,\n",
    "                stride=1, \n",
    "                padding=0, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (ngf*2) x 7 x 7\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (ngf) x 14 x 14\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "            # (nc) x 28 x 28\n",
    "        )\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        ohe_labels = self.embedding(labels)[:, :, None, None]\n",
    "        input = torch.cat([input, ohe_labels], dim=1)\n",
    "        return self.main(input)\n",
    "\n",
    "# Discriminator's feature map\n",
    "ndf = 64\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(\n",
    "            num_embeddings=10, \n",
    "            embedding_dim=28 * 28\n",
    "        )\n",
    "        self.main = nn.Sequential(\n",
    "            # (nc) x 28 x 28\n",
    "            nn.Conv2d(\n",
    "                in_channels=nc+1, \n",
    "                out_channels=ndf, \n",
    "                kernel_size=4, \n",
    "                stride=2, \n",
    "                padding=1, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "\n",
    "            # (ndf) x 14 x 14\n",
    "            nn.Conv2d(\n",
    "                in_channels=ndf, \n",
    "                out_channels=ndf * 2, \n",
    "                kernel_size=4, \n",
    "                stride=2, \n",
    "                padding=1, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "\n",
    "            # (ndf*2) x 7 x 7\n",
    "            nn.Conv2d(\n",
    "                in_channels=ndf * 2, \n",
    "                out_channels=1,\n",
    "                kernel_size=7, \n",
    "                stride=1, \n",
    "                padding=0, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        labels = self.embedding(labels).reshape_as(input)\n",
    "        input = torch.cat([input, labels], dim=1)\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator().to(DEVICE)\n",
    "netG = netG.apply(weights_init)\n",
    "\n",
    "netD = Discriminator().to(DEVICE)\n",
    "netD = netD.apply(weights_init)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "lr = 0.001\n",
    "beta1 = 0.5\n",
    "\n",
    "# We'll draw images from the same input to compare results.\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=DEVICE)\n",
    "\n",
    "# Labels for real and fake images\n",
    "real_label, fake_label = 1., 0.\n",
    "\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (X, y) in enumerate(DATA_LOADER, 0):\n",
    "        real_batch = X.to(DEVICE)\n",
    "        batch_size = real_batch.size(0)\n",
    "        \n",
    "\n",
    "        # Maximazing over discriminator log(D(x)) + log(1 - D(G(z)))\n",
    "        # Step on the real image\n",
    "        netD.zero_grad()\n",
    "        label = torch.full((batch_size,), real_label, dtype=torch.float, device=DEVICE)\n",
    "        output = netD(real_batch, y).view(-1)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # Step on the fake image\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=DEVICE)\n",
    "        fake = netG(noise, y)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach(), y).view(-1)\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        # Maximizing for generator log(D(G(z)))\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)\n",
    "        output = netD(fake, y).view(-1)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(\n",
    "                \"[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    num_epochs,\n",
    "                    i,\n",
    "                    len(DATA_LOADER),\n",
    "                    errD.item(),\n",
    "                    errG.item(),\n",
    "                    D_x,\n",
    "                    D_G_z1,\n",
    "                    D_G_z2,\n",
    "                )\n",
    "            )\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        if (iters % 500 == 0) or (\n",
    "            (epoch == num_epochs - 1) and (i == len(DATA_LOADER) - 1)\n",
    "        ):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise, y).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows the result of the obtained network. In the `number` variable, you can specify the number to draw, and the result from the model will be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "noise = torch.randn(1, nz, 1, 1, device=DEVICE)\n",
    "number = 0\n",
    "\n",
    "res = netG(\n",
    "    noise, \n",
    "    torch.tensor([number])\n",
    ")\n",
    "plt.imshow(res.squeeze().detach().numpy())\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
