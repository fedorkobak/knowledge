{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gan\n",
    "\n",
    "This notebook covers the process of building a Generative Adversarial Network (GAN) and it's application to the `MNIST` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "print(\"using device\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As example `MNIST` dataset will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET = MNIST(\n",
    "    Path(\"mnist_files\"),\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=T.ToTensor()\n",
    ")\n",
    "DATA_LOADER = DataLoader(TRAIN_DATASET, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics\n",
    "\n",
    "At the begining consider network that just tries imitate digits that are in the mnist dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "Following cell implements `Generator` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    A class that generates a picture from a set of random noise.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_map_size: int\n",
    "        Feature map's size of the generator.\n",
    "    number_channels: int\n",
    "        Number of input channels.\n",
    "    noize_size: int\n",
    "        Size of the vector that is expected to be transformed to the picture by \n",
    "        the model.\n",
    "    '''\n",
    "    def __init__(\n",
    "        self, \n",
    "        feature_map_size: int, \n",
    "        number_channels: int, \n",
    "        noize_size: int\n",
    "    ) -> None:\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # (nz) x 1 x 1\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=noize_size, \n",
    "                out_channels=feature_map_size * 2, \n",
    "                kernel_size=7,\n",
    "                stride=1, \n",
    "                padding=0, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(feature_map_size * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (feature_map_size*2) x 7 x 7\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=feature_map_size * 2, \n",
    "                out_channels=feature_map_size, \n",
    "                kernel_size=4, \n",
    "                stride=2, \n",
    "                padding=1, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(feature_map_size),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (feature_map_size) x 14 x 14\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=feature_map_size, \n",
    "                out_channels=number_channels, \n",
    "                kernel_size=4, \n",
    "                stride=2, \n",
    "                padding=1, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.Tanh(),\n",
    "            # (nc) x 28 x 28\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        Apply model to given data.\n",
    "\n",
    "        Paramaters\n",
    "        ----------\n",
    "        input: torch.Tensor\n",
    "            Tensor with size (n_samples, self.nz)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out: torch.Tensor\n",
    "            Tensor that represents set of generated pictures.\n",
    "        '''\n",
    "        return self.main(input[:, :, None, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check what options there are to use it: to generator you have to pass vectors with random values. For each random vertor it will return picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = Generator(\n",
    "    feature_map_size=64, \n",
    "    number_channels=1, \n",
    "    noize_size=100\n",
    ")\n",
    "generator(torch.randn(20, 100)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n",
    "The discriminator is a model that tries to determine if a picture was created by the generator or not. The following cell defines the generator that we will use for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Realisation of the discriminator. Class that takes picture and generate scor\n",
    "    which expresses how much the model thinks the picture is generated.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    number_channels: int\n",
    "        Number of channels in input.\n",
    "    feature_map_size: int\n",
    "        Feature map's size.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, number_channels: int, feature_map_size: int):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # (number_channels) x 28 x 28\n",
    "            nn.Conv2d(\n",
    "                in_channels=number_channels, \n",
    "                out_channels=feature_map_size, \n",
    "                kernel_size=4, \n",
    "                stride=2, \n",
    "                padding=1, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "\n",
    "            # (feature_map_size) x 14 x 14\n",
    "            nn.Conv2d(\n",
    "                in_channels=feature_map_size, \n",
    "                out_channels=feature_map_size * 2, \n",
    "                kernel_size=4, \n",
    "                stride=2, \n",
    "                padding=1, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(feature_map_size * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "\n",
    "            # (feature_map_size*2) x 7 x 7\n",
    "            nn.Conv2d(\n",
    "                in_channels=feature_map_size * 2, \n",
    "                out_channels=1, \n",
    "                kernel_size=7, \n",
    "                stride=1, \n",
    "                padding=0, \n",
    "                bias=False\n",
    "            ),\n",
    "            \n",
    "            nn.Sigmoid(),\n",
    "            nn.Flatten(start_dim=0, end_dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider how the discriminator works by passing a sample picture from the training data to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4717], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator = Discriminator(\n",
    "    number_channels=1, \n",
    "    feature_map_size=64\n",
    ")\n",
    "discriminator(TRAIN_DATASET[0][0][None, :, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a score that represents the model's prediction of whether the picture we passed was generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to train the discriminator to determine if its input was original or generated by the generator. The following cell implements the optimization step with two core elements:\n",
    "\n",
    "- Gradient accumulation to increase predicted scores for real images.\n",
    "- Gradient accumulation to decrease predicted scores for generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_step(\n",
    "    pictures: torch.Tensor,\n",
    "    generation: torch.Tensor,\n",
    "    discriminator: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer\n",
    ") -> tuple[float, float, float]:\n",
    "    '''\n",
    "    Step of the discriminator. Maximize log(D(x)) + log(1 - D(G(z))) - tries to \n",
    "    to improve the prediction that the real images have 1. scores and the \n",
    "    generated images have have 0. scores.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pictures: torch.Tensor\n",
    "        Batch of real images that we're trying to imitate.\n",
    "    generation: torch.Tensor\n",
    "        Batch of generated images.\n",
    "    discriminator: torch.nn.Module\n",
    "        Model that we optimise.\n",
    "    optimizer: torch.optim.Optimizer\n",
    "        Optimizet that uses weights of the dicriminator.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    out: tuple[float, float, float]\n",
    "        - Mean prediction for real images.\n",
    "        - Mean predictoin for fake images.\n",
    "        - Total loss value on both real and fake images.\n",
    "    '''\n",
    "\n",
    "    batch_size = pictures.shape[0]\n",
    "    discriminator.zero_grad()\n",
    "\n",
    "    # Gradient accumulation on real images\n",
    "    # Model should predict scores close to 1\n",
    "    label = torch.full((batch_size,), 1., dtype=torch.float, device=DEVICE)\n",
    "    output = discriminator(pictures)\n",
    "    errD_real = binary_cross_entropy(output, label)\n",
    "    errD_real.backward()\n",
    "    D_x = output.mean().item()\n",
    "\n",
    "    # Gradient accumulation on fake images\n",
    "    # Model should predicst scores close to 0\n",
    "    label.fill_(0.)\n",
    "    # Note: generation here bypasses the discriminator without gradient \n",
    "    # accumulation because we don't need generator gradients to optimize the \n",
    "    # discriminator.\n",
    "    output = discriminator(generation.detach())\n",
    "    errD_fake = binary_cross_entropy(output, label)\n",
    "    errD_fake.backward()\n",
    "    D_G_z1 = output.mean().item()\n",
    "\n",
    "\n",
    "    errD = errD_real + errD_fake\n",
    "\n",
    "    # Step based on accumulated gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    return D_x, D_G_z1, errD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this generator step, by calculating the loss (the difference between the discriminator’s judgment and the target “real” label), the generator learns the extent of improvement needed. This loss is used to compute gradients, guiding adjustments to the generator's weights to make future outputs more realistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_step(\n",
    "    generation: torch.Tensor,\n",
    "    discriminator: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    generator: torch.nn.Module\n",
    ") -> tuple[float, float]:\n",
    "    '''\n",
    "    Step of the generator.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    genration: torch.Tensor\n",
    "        Set of objects generated by generator.\n",
    "    disciminator: torch.nn.Module\n",
    "        Model which decision will determine behavior of the the generator.\n",
    "    optimizer: torch.optim.Optimizer\n",
    "        The optimizer that changes weights of the generator.\n",
    "    generator: torch.nn.Module\n",
    "        The model whose weights we are adjusting in this step.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out: float\n",
    "        Mean prediction of the discriminator.\n",
    "    '''\n",
    "\n",
    "    batch_size = generation.shape[0]\n",
    "    generator.zero_grad()\n",
    "    \n",
    "    label = torch.full(\n",
    "        size=(batch_size,), \n",
    "        fill_value=1., \n",
    "        dtype=torch.float, \n",
    "        device=DEVICE\n",
    "    )\n",
    "    output = discriminator(generation)\n",
    "    errG = binary_cross_entropy(output, label)\n",
    "    errG.backward()\n",
    "\n",
    "\n",
    "    D_G_z2 = output.mean().item()\n",
    "    optimizer.step()\n",
    "\n",
    "    return D_G_z2, errG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell implements the training loop that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][0/938]\tLoss_D: 0.3693\tLoss_G: 4.2457\tD(x): 0.9770\tD(G(z)): 0.2622 / 0.0199\n",
      "[0/1][50/938]\tLoss_D: 0.1346\tLoss_G: 4.5018\tD(x): 0.9298\tD(G(z)): 0.0549 / 0.0142\n",
      "[0/1][100/938]\tLoss_D: 0.2139\tLoss_G: 3.2313\tD(x): 0.8846\tD(G(z)): 0.0692 / 0.0519\n",
      "[0/1][150/938]\tLoss_D: 0.3231\tLoss_G: 4.8366\tD(x): 0.7736\tD(G(z)): 0.0141 / 0.0164\n",
      "[0/1][200/938]\tLoss_D: 0.2256\tLoss_G: 3.3409\tD(x): 0.8259\tD(G(z)): 0.0181 / 0.0702\n",
      "[0/1][250/938]\tLoss_D: 0.0987\tLoss_G: 6.3334\tD(x): 0.9259\tD(G(z)): 0.0182 / 0.0179\n",
      "[0/1][300/938]\tLoss_D: 0.8076\tLoss_G: 3.4168\tD(x): 0.5108\tD(G(z)): 0.0059 / 0.0576\n",
      "[0/1][350/938]\tLoss_D: 0.1141\tLoss_G: 4.1980\tD(x): 0.9216\tD(G(z)): 0.0291 / 0.0268\n",
      "[0/1][400/938]\tLoss_D: 0.2928\tLoss_G: 5.7039\tD(x): 0.9785\tD(G(z)): 0.1862 / 0.0092\n",
      "[0/1][450/938]\tLoss_D: 0.5139\tLoss_G: 3.9546\tD(x): 0.9850\tD(G(z)): 0.3296 / 0.0289\n",
      "[0/1][500/938]\tLoss_D: 0.6163\tLoss_G: 3.4078\tD(x): 0.9219\tD(G(z)): 0.3688 / 0.0481\n",
      "[0/1][550/938]\tLoss_D: 0.0580\tLoss_G: 5.9186\tD(x): 0.9507\tD(G(z)): 0.0047 / 0.0072\n",
      "[0/1][600/938]\tLoss_D: 0.3366\tLoss_G: 2.6737\tD(x): 0.8332\tD(G(z)): 0.1254 / 0.0942\n",
      "[0/1][650/938]\tLoss_D: 0.2499\tLoss_G: 3.6629\tD(x): 0.9385\tD(G(z)): 0.1608 / 0.0343\n",
      "[0/1][700/938]\tLoss_D: 0.1997\tLoss_G: 2.9224\tD(x): 0.9109\tD(G(z)): 0.0918 / 0.0701\n",
      "[0/1][750/938]\tLoss_D: 0.3967\tLoss_G: 2.2101\tD(x): 0.8347\tD(G(z)): 0.1390 / 0.1575\n",
      "[0/1][800/938]\tLoss_D: 0.6386\tLoss_G: 5.8890\tD(x): 0.9770\tD(G(z)): 0.3996 / 0.0057\n",
      "[0/1][850/938]\tLoss_D: 0.3644\tLoss_G: 2.3480\tD(x): 0.7719\tD(G(z)): 0.0624 / 0.1278\n",
      "[0/1][900/938]\tLoss_D: 0.3432\tLoss_G: 2.3281\tD(x): 0.7960\tD(G(z)): 0.0666 / 0.1484\n"
     ]
    }
   ],
   "source": [
    "feature_map_size = 64\n",
    "noise_size = 100\n",
    "\n",
    "netG = Generator(\n",
    "    feature_map_size=feature_map_size, \n",
    "    number_channels=1, \n",
    "    noize_size=noise_size\n",
    ").to(DEVICE)\n",
    "netG = netG.apply(weights_init)\n",
    "\n",
    "netD = Discriminator(\n",
    "    feature_map_size=feature_map_size, \n",
    "    number_channels=1\n",
    ").to(DEVICE)\n",
    "netD = netD.apply(weights_init)\n",
    "\n",
    "lr = 0.001\n",
    "beta1 = 0.5\n",
    "\n",
    "# We'll draw images from the same input to compare results.\n",
    "fixed_noise = torch.randn(64, noise_size, device=DEVICE)\n",
    "\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (pictures, _) in enumerate(DATA_LOADER, 0):\n",
    "        pictures = pictures.to(DEVICE)\n",
    "        batch_size = pictures.size(0)\n",
    "        \n",
    "        # Getting generated (\"fake\") picture that tries to trick discriminator\n",
    "        noise = torch.randn(batch_size, noise_size, device=DEVICE)\n",
    "        generation = netG(noise)\n",
    "\n",
    "        D_x, D_G_z1, errD = discriminator_step(\n",
    "            pictures=pictures,\n",
    "            generation=generation,\n",
    "            discriminator=netD,\n",
    "            optimizer=optimizerD\n",
    "        )\n",
    "\n",
    "        # Maximizing for generator log(D(G(z)))\n",
    "        D_G_z2, errG = generator_step(\n",
    "            generation=generation,\n",
    "            discriminator=netD,\n",
    "            optimizer=optimizerG,\n",
    "            generator=netG\n",
    "        )\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(\n",
    "                \"[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    num_epochs,\n",
    "                    i,\n",
    "                    len(DATA_LOADER),\n",
    "                    errD.item(),\n",
    "                    errG.item(),\n",
    "                    D_x,\n",
    "                    D_G_z1,\n",
    "                    D_G_z2,\n",
    "                )\n",
    "            )\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        if (iters % 500 == 0) or (\n",
    "            (epoch == num_epochs - 1) and (i == len(DATA_LOADER) - 1)\n",
    "        ):\n",
    "            with torch.no_grad():\n",
    "                generation = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(generation, padding=2, normalize=True))\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "noize = torch.randn(1, noise_size, device=DEVICE)\n",
    "plt.imshow(netG(noize).squeeze().detach().numpy())\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional\n",
    "\n",
    "Here, we consider a modification of the previous model. The model shown in this section will draw a picture of the given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator's feature map\n",
    "ngf = 64\n",
    "# Input channels\n",
    "nc = 1\n",
    "# Size of the vector from which generator will create a picture\n",
    "nz = 100\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=10,\n",
    "            embedding_dim=10\n",
    "        )\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # (nz) x 1 x 1\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=nz+10,\n",
    "                out_channels=ngf * 2, \n",
    "                kernel_size=7,\n",
    "                stride=1, \n",
    "                padding=0, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (ngf*2) x 7 x 7\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (ngf) x 14 x 14\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "            # (nc) x 28 x 28\n",
    "        )\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        ohe_labels = self.embedding(labels)[:, :, None, None]\n",
    "        input = torch.cat([input, ohe_labels], dim=1)\n",
    "        return self.main(input)\n",
    "\n",
    "# Discriminator's feature map\n",
    "ndf = 64\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(\n",
    "            num_embeddings=10, \n",
    "            embedding_dim=28 * 28\n",
    "        )\n",
    "        self.main = nn.Sequential(\n",
    "            # (nc) x 28 x 28\n",
    "            nn.Conv2d(\n",
    "                in_channels=nc+1, \n",
    "                out_channels=ndf, \n",
    "                kernel_size=4, \n",
    "                stride=2, \n",
    "                padding=1, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "\n",
    "            # (ndf) x 14 x 14\n",
    "            nn.Conv2d(\n",
    "                in_channels=ndf, \n",
    "                out_channels=ndf * 2, \n",
    "                kernel_size=4, \n",
    "                stride=2, \n",
    "                padding=1, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "\n",
    "            # (ndf*2) x 7 x 7\n",
    "            nn.Conv2d(\n",
    "                in_channels=ndf * 2, \n",
    "                out_channels=1,\n",
    "                kernel_size=7, \n",
    "                stride=1, \n",
    "                padding=0, \n",
    "                bias=False\n",
    "            ),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input, labels):\n",
    "        labels = self.embedding(labels).reshape_as(input)\n",
    "        input = torch.cat([input, labels], dim=1)\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator().to(DEVICE)\n",
    "netG = netG.apply(weights_init)\n",
    "\n",
    "netD = Discriminator().to(DEVICE)\n",
    "netD = netD.apply(weights_init)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "lr = 0.001\n",
    "beta1 = 0.5\n",
    "\n",
    "# We'll draw images from the same input to compare results.\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=DEVICE)\n",
    "\n",
    "# Labels for real and fake images\n",
    "real_label, fake_label = 1., 0.\n",
    "\n",
    "optimizerD = torch.optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (X, y) in enumerate(DATA_LOADER, 0):\n",
    "        real_batch = X.to(DEVICE)\n",
    "        batch_size = real_batch.size(0)\n",
    "        \n",
    "\n",
    "        # Maximazing over discriminator log(D(x)) + log(1 - D(G(z)))\n",
    "        # Step on the real image\n",
    "        netD.zero_grad()\n",
    "        label = torch.full((batch_size,), real_label, dtype=torch.float, device=DEVICE)\n",
    "        output = netD(real_batch, y).view(-1)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # Step on the fake image\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=DEVICE)\n",
    "        fake = netG(noise, y)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach(), y).view(-1)\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        # Maximizing for generator log(D(G(z)))\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)\n",
    "        output = netD(fake, y).view(-1)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(\n",
    "                \"[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    num_epochs,\n",
    "                    i,\n",
    "                    len(DATA_LOADER),\n",
    "                    errD.item(),\n",
    "                    errG.item(),\n",
    "                    D_x,\n",
    "                    D_G_z1,\n",
    "                    D_G_z2,\n",
    "                )\n",
    "            )\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        if (iters % 500 == 0) or (\n",
    "            (epoch == num_epochs - 1) and (i == len(DATA_LOADER) - 1)\n",
    "        ):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise, y).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows the result of the obtained network. In the `number` variable, you can specify the number to draw, and the result from the model will be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "noise = torch.randn(1, nz, 1, 1, device=DEVICE)\n",
    "number = 0\n",
    "\n",
    "res = netG(\n",
    "    noise, \n",
    "    torch.tensor([number])\n",
    ")\n",
    "plt.imshow(res.squeeze().detach().numpy())\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
