{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67beb034-0408-4f3e-ae5f-b9d134adba64",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    "Layer in torch is some transformation with some inputs and some outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fcbd104-4e79-406c-99b9-d6e2979b2d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85e4a60-103f-4cf2-a6e6-32ac1dc99c10",
   "metadata": {},
   "source": [
    "To perform a layer transformation on the tensor `X`, simply use the syntax `layer(X)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6bd924-b58f-459c-a344-793248e069c9",
   "metadata": {},
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a22a9c3-620a-4e09-82be-0363292487de",
   "metadata": {},
   "source": [
    "The `torch.nn.Linear` layer performs the following operation:\n",
    "\n",
    "$$X_{n \\times l} \\cdot \\omega_{l \\times k} + b_k$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $l$ - number of inputs\n",
    "- $k$ - number of outputs\n",
    "- $n$ - number of input samples\n",
    "- $X_{n \\times l}$ - input tensor\n",
    "- $\\omega_{l \\times k}$ - weight matrix of the layer\n",
    "- $b_k$ - bias vector of the layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf498d04",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell applies the tensor to some data and manually performs the same transformation. The results should be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90385899-ed31-4bd7-8601-d5ef8f966acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer transformation\n",
      "[-0.6235317587852478, -1.1335619688034058, -0.70122230052948]\n",
      "X@w+b\n",
      "[-0.6235317587852478, -1.1335619688034058, -0.70122230052948]\n"
     ]
    }
   ],
   "source": [
    "in_features = 5\n",
    "out_features = 3\n",
    "\n",
    "linear = nn.Linear(\n",
    "    in_features = in_features, \n",
    "    out_features = out_features\n",
    ")\n",
    "\n",
    "X = torch.rand(in_features)\n",
    "\n",
    "print(\"Layer transformation\")\n",
    "print(linear(X).tolist())\n",
    "print(\"X@w+b\")\n",
    "print((linear.weight@X + linear.bias).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2efedf",
   "metadata": {},
   "source": [
    "### Define values\n",
    "\n",
    "To define custom values for tensors you have to use access `weight` and `bias` fater layer creation. They are belongs to the `type(linear_layer.weight)` data type. So you have to use method `copy_` under `torch.no_grad` context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085fc399",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Hereâ€™s an example of how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91af648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = nn.Linear(in_features=3, out_features=4)\n",
    "\n",
    "default_weights = torch.ones_like(linear_layer.weight)\n",
    "default_biases = torch.zeros_like(linear_layer.bias)\n",
    "\n",
    "with torch.no_grad():\n",
    "    linear_layer.weight.copy_(default_weights)\n",
    "    linear_layer.bias.copy_(default_biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5013a97b",
   "metadata": {},
   "source": [
    "After completing the process, we have the `weight` tensor initialized with ones and the `bias` tensor initialized with zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d5ceadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(linear_layer.weight)\n",
    "print(linear_layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d80a2c",
   "metadata": {},
   "source": [
    "### dtype\n",
    "\n",
    "By default, tensors used in `torch.nn.Linear` have a `float32` data type. This can lead to issues when processing tensors with different data types. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1660663",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines a tensor with a `float16` data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a604411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.3904e+04,  3.6992e+00,  3.5763e-07],\n",
       "        [ 0.0000e+00, -5.1200e+02,         nan],\n",
       "        [-5.1200e+02,  4.3789e+00,  2.0000e+00]], dtype=torch.float16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_size = 3\n",
    "\n",
    "tensor = torch.empty(\n",
    "    size=(tensor_size, tensor_size), \n",
    "    dtype=torch.float16\n",
    ")\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e652a465",
   "metadata": {},
   "source": [
    "The following cell defines a tensor with a `float32` data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2f28cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(tensor_size, tensor_size)\n",
    "for p in layer.parameters():\n",
    "    print(p.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1c8738",
   "metadata": {},
   "source": [
    "Trying to apply the `layer` to the tensor will result in an error stating that the data types are incompatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71a1ae9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Half and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Half and Float"
     ]
    }
   ],
   "source": [
    "layer(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f70496",
   "metadata": {},
   "source": [
    "The following cell demonstrates how to change the data type of tensors used in `nn.Linear`. After modifying the data types, you can successfully apply the `layer` to the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6f9ae10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6652.0000, 24528.0000, -9664.0000],\n",
       "        [       nan,        nan,        nan],\n",
       "        [   76.9375,  -286.0000,   113.6875]], dtype=torch.float16,\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for p in layer.parameters():\n",
    "    p.data = p.data.to(torch.float16)\n",
    "\n",
    "layer(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932cfaf1",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "A dropout layer randomly sets some components of the input tensor to zero with a given probability $p$. During training, the remaining non-zero components are scaled by a factor of $ \\frac{1}{1-p}$ to prevent signal attenuation. Formally, if we start with a tensor $x_i$, where $i \\in \\mathbb{N}^k$ represents the indices of the $k$-dimensional tensor, the output after applying dropout is given by:\n",
    "\n",
    "$$\n",
    "x'_i = x_i \\cdot p_i \\cdot \\frac{1}{1-p},\n",
    "$$\n",
    "\n",
    "where $p_i$ is sampled from a Bernoulli distribution with parameter $p$, i.e., $p_i \\sim \\text{Bernoulli}(p)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085d31ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This example demonstrates the transformation of a tensor after passing through a dropout layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "49ae0a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]], dtype=torch.float64)\n",
      "\n",
      "Dropout result:\n",
      "tensor([[ 1.4286,  2.8571,  0.0000],\n",
      "        [ 5.7143,  0.0000,  8.5714],\n",
      "        [10.0000,  0.0000, 12.8571]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(111)\n",
    "\n",
    "dropout_layer = nn.Dropout(p=0.3)\n",
    "tensor = (torch.arange(3 * 3, dtype=float) + 1).reshape((3, 3))\n",
    "\n",
    "print(\"Original tensor:\")\n",
    "print(tensor)\n",
    "print()\n",
    "print(\"Dropout result:\")\n",
    "print(dropout_layer(tensor))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
