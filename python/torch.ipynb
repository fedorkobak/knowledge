{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch\n",
    "\n",
    "Is a great library that provides really convenient and flexible interfaces for building neural networks.\n",
    "\n",
    "For installation check [this page](https://pytorch.org/get-started/locally/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor\n",
    "\n",
    "Tensor is a generalisation of a matrix to the case of arbitrary dimensionality. Basic entity with wich torch operates is tensor. FInd out more in [specific page](torch/tensor.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following example demonstrates how to create a specific tensor. In this tensor, the elements are denoted as $\\left[ijk\\right]$, where $i$ represents the layer index in the third dimension, $j$ denotes the row index, and $k$ indicates the column index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[111, 112, 113, 114],\n",
       "         [121, 122, 123, 124],\n",
       "         [131, 132, 133, 134]],\n",
       "\n",
       "        [[211, 212, 213, 214],\n",
       "         [221, 222, 223, 224],\n",
       "         [231, 232, 233, 244]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([\n",
    "    [\n",
    "        [111,112,113,114],\n",
    "        [121,122,123,124],\n",
    "        [131,132,133,134]\n",
    "    ],\n",
    "    [\n",
    "        [211,212,213,214],\n",
    "        [221,222,223,224],\n",
    "        [231,232,233,244]\n",
    "    ],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "\n",
    "A key feature of PyTorch that sets it apart from NumPy is its ability to automatically compute gradients for tensors involved in computations. You just need to call the `backward` method on the result of your computations. The tensors that participated in these computations will then have a `grad` attribute containing the gradients. Find out more on the [relevant page](torch/differentiation.ipynb).\n",
    "\n",
    "---\n",
    "\n",
    "As example consider fuction:\n",
    "\n",
    "$$f(\\overline{X})=\\sum_i x_i^2, \\overline{X} = (x_1, x_2, x_3)$$\n",
    "\n",
    "Suppose we want to calculate the gradient of the $f$ on $x$ in point $(1,2,3)$:\n",
    "\n",
    "$$\\nabla f=(2x_1, 2x_2, 2x_3) \\Rightarrow \\nabla f(1,2,3)=(2,4,6)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat the same procedure with the torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([1,2,3], dtype=torch.float, requires_grad=True)\n",
    "res = (X**2).sum()\n",
    "res.backward()\n",
    "X.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "Torch implements some functions typical for neural networks. They are defined in the `torch.nn.functional` module. It's typical to define this module with an `F` alias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions\n",
    "\n",
    "Torch implements common loss functions. The following table shows some of them:\n",
    "\n",
    "| Loss Function                         | Description                              |\n",
    "|--------------------------------------|------------------------------------------|\n",
    "| `torch.nn.functional.binary_cross_entropy` | Binary Cross Entropy                     |\n",
    "| `torch.nn.functional.binary_cross_entropy_with_logits` | Binary Cross Entropy with Logits        |\n",
    "| `torch.nn.functional.cross_entropy`       | Cross Entropy Loss                       |\n",
    "| `torch.nn.functional.hinge_embedding_loss` | Hinge Embedding Loss                     |\n",
    "| `torch.nn.functional.kl_div`              | Kullback-Leibler Divergence Loss         |\n",
    "| `torch.nn.functional.l1_loss`             | Mean Absolute Error Loss                |\n",
    "| `torch.nn.functional.mse_loss`            | Mean Squared Error Loss                  |\n",
    "| `torch.nn.functional.margin_ranking_loss` | Margin Ranking Loss                      |\n",
    "| `torch.nn.functional.multi_label_margin_loss` | Multi-Label Margin Loss                |\n",
    "| `torch.nn.functional.multi_label_soft_margin_loss` | Multi-Label Soft Margin Loss           |\n",
    "| `torch.nn.functional.smooth_l1_loss`      | Smooth L1 Loss                           |\n",
    "| `torch.nn.functional.triplet_margin_loss` | Triplet Margin Loss                      |\n",
    "| `torch.nn.functional.nll_loss`            | Negative Log Likelihood Loss            |\n",
    "| `torch.nn.functional.cosine_embedding_loss` | Cosine Embedding Loss                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The followgin cell shows applying `mse_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(\n",
    "    torch.tensor([1,2,3], dtype=torch.float),\n",
    "    torch.tensor([2,3,4], dtype=torch.float)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reduction` parameter allows you to specify the type of aggregation to apply to the results of the function. The three commonly used values are `none`, `mean`, and `sum`.\n",
    "\n",
    "---\n",
    "\n",
    "The following cell demonstrates how different types of reduction are applied to the same inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduction - mean, res=1.0\n",
      "reduction - sum, res=3.0\n",
      "reduction - none, res=tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "tens1 = torch.tensor([1,2,3], dtype=torch.float)\n",
    "tens2 = torch.tensor([2,3,4], dtype=torch.float)\n",
    "\n",
    "for reduction in [\"mean\", \"sum\", \"none\"]:\n",
    "    res = F.mse_loss(tens1, tens2, reduction=reduction)\n",
    "    print(f\"reduction - {reduction}, res={res}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
