{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch\n",
    "\n",
    "Is a great library that provides really convenient and flexible interfaces for building neural networks.\n",
    "\n",
    "For installation check [this page](https://pytorch.org/get-started/locally/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor\n",
    "\n",
    "Tensor is a generalisation of a matrix to the case of arbitrary dimensionality. Basic entity with wich torch operates is tensor. FInd out more in [specific page](torch/tensor.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following example demonstrates how to create a specific tensor. In this tensor, the elements are denoted as $\\left[ijk\\right]$, where $i$ represents the layer index in the third dimension, $j$ denotes the row index, and $k$ indicates the column index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[111, 112, 113, 114],\n",
       "         [121, 122, 123, 124],\n",
       "         [131, 132, 133, 134]],\n",
       "\n",
       "        [[211, 212, 213, 214],\n",
       "         [221, 222, 223, 224],\n",
       "         [231, 232, 233, 244]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([\n",
    "    [\n",
    "        [111,112,113,114],\n",
    "        [121,122,123,124],\n",
    "        [131,132,133,134]\n",
    "    ],\n",
    "    [\n",
    "        [211,212,213,214],\n",
    "        [221,222,223,224],\n",
    "        [231,232,233,244]\n",
    "    ],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "\n",
    "A key feature of PyTorch that sets it apart from NumPy is its ability to automatically compute gradients for tensors involved in computations. You just need to call the `backward` method on the result of your computations. The tensors that participated in these computations will then have a `grad` attribute containing the gradients. Find out more on the [relevant page](torch/differentiation.ipynb).\n",
    "\n",
    "---\n",
    "\n",
    "As example consider fuction:\n",
    "\n",
    "$$f(\\overline{X})=\\sum_i x_i^2, \\overline{X} = (x_1, x_2, x_3)$$\n",
    "\n",
    "Suppose we want to calculate the gradient of the $f$ on $x$ in point $(1,2,3)$:\n",
    "\n",
    "$$\\nabla f=(2x_1, 2x_2, 2x_3) \\Rightarrow \\nabla f(1,2,3)=(2,4,6)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat the same procedure with the torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([1,2,3], dtype=torch.float, requires_grad=True)\n",
    "res = (X**2).sum()\n",
    "res.backward()\n",
    "X.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions\n",
    "\n",
    "Torch implements common loss functions. The following table shows some of them:\n",
    "\n",
    "| Loss Function                         | Description                              |\n",
    "|--------------------------------------|------------------------------------------|\n",
    "| `torch.nn.functional.binary_cross_entropy` | Binary Cross Entropy                     |\n",
    "| `torch.nn.functional.binary_cross_entropy_with_logits` | Binary Cross Entropy with Logits        |\n",
    "| `torch.nn.functional.cross_entropy`       | Cross Entropy Loss                       |\n",
    "| `torch.nn.functional.hinge_embedding_loss` | Hinge Embedding Loss                     |\n",
    "| `torch.nn.functional.kl_div`              | Kullback-Leibler Divergence Loss         |\n",
    "| `torch.nn.functional.l1_loss`             | Mean Absolute Error Loss                |\n",
    "| `torch.nn.functional.mse_loss`            | Mean Squared Error Loss                  |\n",
    "| `torch.nn.functional.margin_ranking_loss` | Margin Ranking Loss                      |\n",
    "| `torch.nn.functional.multi_label_margin_loss` | Multi-Label Margin Loss                |\n",
    "| `torch.nn.functional.multi_label_soft_margin_loss` | Multi-Label Soft Margin Loss           |\n",
    "| `torch.nn.functional.smooth_l1_loss`      | Smooth L1 Loss                           |\n",
    "| `torch.nn.functional.triplet_margin_loss` | Triplet Margin Loss                      |\n",
    "| `torch.nn.functional.nll_loss`            | Negative Log Likelihood Loss            |\n",
    "| `torch.nn.functional.cosine_embedding_loss` | Cosine Embedding Loss                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The followgin cell shows applying `mse_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(\n",
    "    torch.tensor([1,2,3], dtype=torch.float),\n",
    "    torch.tensor([2,3,4], dtype=torch.float)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reduction` parameter allows you to specify the type of aggregation to apply to the results of the function. The three commonly used values are `none`, `mean`, and `sum`.\n",
    "\n",
    "---\n",
    "\n",
    "The following cell demonstrates how different types of reduction are applied to the same inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduction - mean, res=1.0\n",
      "reduction - sum, res=3.0\n",
      "reduction - none, res=tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "tens1 = torch.tensor([1,2,3], dtype=torch.float)\n",
    "tens2 = torch.tensor([2,3,4], dtype=torch.float)\n",
    "\n",
    "for reduction in [\"mean\", \"sum\", \"none\"]:\n",
    "    res = F.mse_loss(tens1, tens2, reduction=reduction)\n",
    "    print(f\"reduction - {reduction}, res={res}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "PyTorch provides a variety of tools for creating neural network layers. Find out more on the [relevant page](torch/layers.ipynb).\n",
    "\n",
    "**Note:** In theory, the term \"layer\" often refers to a combination of connections and activation functions. However, PyTorch has a more specific abstraction where there are dedicated layers for different functionalities. It's important to keep this in mind to avoid confusion.\n",
    "\n",
    "The following table lists the layers available in PyTorch:\n",
    "\n",
    "| Category          | Layer                    | Description                                  |\n",
    "|-------------------|--------------------------|----------------------------------------------|\n",
    "| **Linear Layers** | `torch.nn.Linear`        | Fully connected (dense) layer               |\n",
    "| **Convolutional Layers** | `torch.nn.Conv1d`      | 1D convolutional layer                      |\n",
    "|                   | `torch.nn.Conv2d`        | 2D convolutional layer                      |\n",
    "|                   | `torch.nn.Conv3d`        | 3D convolutional layer                      |\n",
    "| **Pooling Layers**| `torch.nn.MaxPool1d`     | 1D max pooling layer                       |\n",
    "|                   | `torch.nn.MaxPool2d`     | 2D max pooling layer                       |\n",
    "|                   | `torch.nn.MaxPool3d`     | 3D max pooling layer                       |\n",
    "|                   | `torch.nn.AvgPool1d`     | 1D average pooling layer                   |\n",
    "|                   | `torch.nn.AvgPool2d`     | 2D average pooling layer                   |\n",
    "|                   | `torch.nn.AvgPool3d`     | 3D average pooling layer                   |\n",
    "| **Normalization Layers** | `torch.nn.BatchNorm1d` | 1D batch normalization                      |\n",
    "|                   | `torch.nn.BatchNorm2d`   | 2D batch normalization                      |\n",
    "|                   | `torch.nn.BatchNorm3d`   | 3D batch normalization                      |\n",
    "|                   | `torch.nn.LayerNorm`     | Layer normalization                         |\n",
    "|                   | `torch.nn.InstanceNorm1d`| 1D instance normalization                   |\n",
    "|                   | `torch.nn.InstanceNorm2d`| 2D instance normalization                   |\n",
    "|                   | `torch.nn.InstanceNorm3d`| 3D instance normalization                   |\n",
    "| **Activation Functions** | `torch.nn.ReLU`        | Rectified Linear Unit                       |\n",
    "|                   | `torch.nn.Sigmoid`       | Sigmoid activation function                 |\n",
    "|                   | `torch.nn.Tanh`          | Hyperbolic tangent activation function       |\n",
    "|                   | `torch.nn.LeakyReLU`     | Leaky Rectified Linear Unit                 |\n",
    "|                   | `torch.nn.Softmax`       | Softmax activation function                 |\n",
    "|                   | `torch.nn.Softplus`      | Softplus activation function                |\n",
    "|                   | `torch.nn.Softshrink`    | Softshrink activation function              |\n",
    "| **Recurrent Layers** | `torch.nn.RNN`          | Recurrent Neural Network layer              |\n",
    "|                   | `torch.nn.LSTM`          | Long Short-Term Memory layer                |\n",
    "|                   | `torch.nn.GRU`           | Gated Recurrent Unit layer                  |\n",
    "| **Other Layers**  | `torch.nn.Embedding`     | Lookup table for embeddings                 |\n",
    "|                   | `torch.nn.Dropout`       | Dropout layer for regularization            |\n",
    "|                   | `torch.nn.Transformer`   | Transformer model                           |\n",
    "|                   | `torch.nn.TransformerEncoder` | Transformer encoder                     |\n",
    "|                   | `torch.nn.TransformerDecoder` | Transformer decoder                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider typical features of such objets. As an example, let's take a linear layer without going into its peculiarities.\n",
    "\n",
    "---\n",
    "\n",
    "The following cell shows that you can apply layer to the operand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5347, -0.0643, -0.2821],\n",
       "        [ 0.2541,  0.2737, -0.2114],\n",
       "        [ 0.4634,  0.2516, -0.2575]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = torch.nn.Linear(10, 3)\n",
    "layer(torch.rand(3, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use a layer as part of the computation, and it can participate in the `backward` pass to compute gradients. The following cell demonstrates how to obtain the gradient for the `weight` attribute of a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer(torch.rand(3, 10)).sum().backward()\n",
    "layer.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network composition\n",
    "\n",
    "Neural networks are built by composing layers. PyTorch provides powerful tools and concepts for building these compositions. This section will delve into these important concepts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential\n",
    "\n",
    "You can use `torch.nn.Sequential` to combine multiple network layers into a sequential chain. Find out more in the [specific page](torch/sequential.ipynb).\n",
    "\n",
    "---\n",
    "\n",
    "The following cell demonstrates a basic example where a linear transformation is applied to the input, followed by a ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.8781],\n",
       "        [0.4362, 0.0000, 0.7350],\n",
       "        [0.0000, 0.0000, 1.1225]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 3\n",
    "\n",
    "sequential = torch.nn.Sequential(\n",
    "    torch.nn.Linear(size, size, bias=False),\n",
    "    torch.nn.ReLU()\n",
    ")\n",
    "\n",
    "X = torch.randn([3, 3])\n",
    "sequential(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate class\n",
    "\n",
    "You can define a neural network as a separate class, which allows you to add custom logic for initialization or network-specific procedures. To create a network class, follow these rules:\n",
    "\n",
    "- **Inherit from `torch.nn.Module`:** This establishes your class as a PyTorch module, providing access to its functionality.\n",
    "- **Call `super().__init__()` in the constructor:** This initializes the base `nn.Module` class, ensuring proper setup.\n",
    "- **Define a `forward` method:** This method implements the computational procedure of your network. It defines how input data flows through your layers to produce output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines a set of Linear layers whose size is determined during class creation. The forward method standardizes the data before applying the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleNetwork(torch.nn.Module):\n",
    "    def __init__(self, layers_number: int, neurons: int):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = torch.nn.Sequential(*[\n",
    "            torch.nn.Linear(neurons, neurons)\n",
    "            for i in range(layers_number)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, X: torch.Tensor):\n",
    "        X = (X - X.mean(axis=0, keepdim=True))/X.std(axis=0, keepdim=True)\n",
    "        return self.network(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the network we've defined works as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2482,  0.0882,  0.4507],\n",
       "        [-0.2465,  0.0897,  0.4466],\n",
       "        [-0.2531,  0.0827,  0.4587],\n",
       "        [-0.2463,  0.0899,  0.4459],\n",
       "        [-0.2461,  0.0892,  0.4429]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExampleNetwork(layers_number=10, neurons=3)(X = torch.randn([5, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device\n",
    "\n",
    "For tensors and the model you are using, you can select the device in which the tensor is to be used. Find out more in the [specific page](torch/devices.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following example shows how to check the `device` for your tensor. By default it's cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.randn([5, 5]).device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, most objects typically encapsulate tensors, allowing you to access their device through the tensors' devices. The following example demonstrates how to access the devices for the weights and biases of two linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cpu\n",
      "cpu\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "example_sequential = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3,3),\n",
    "    torch.nn.Linear(3,3)\n",
    ")\n",
    "\n",
    "for param in example_sequential.parameters():\n",
    "    print(param.device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
