{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f130a6a-e954-4d9d-b27f-12e98167ca3b",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "Here are common metrics that have been designed or adapted specifically for recommendation systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcc4487f-9c98-48c4-a90d-01b6a779617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import unittest\n",
    "\n",
    "from IPython.display import HTML, Latex\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from surprise.prediction_algorithms.slope_one import SlopeOne\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.dataset import Dataset\n",
    "from surprise.reader import Reader\n",
    "from surprise.prediction_algorithms.knns import KNNBasic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed69734f-ba0d-4229-9ba6-aae8d3656592",
   "metadata": {},
   "source": [
    "**Sources**\n",
    "\n",
    "- Article on TSD [Evaluation Metrics for Recommendation Systems â€” An Overview](https://towardsdatascience.com/evaluation-metrics-for-recommendation-systems-an-overview-71290690ecba);\n",
    "- [Mean average precision for ranking and classification](https://www.evidentlyai.com/ranking-metrics/mean-average-precision-map)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3fe2e2-c2b7-4222-a4ca-c4ba0bbd564d",
   "metadata": {},
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f8b8a6-b56a-45c7-aa7d-c1000c9eb926",
   "metadata": {},
   "source": [
    "The following cell has generated a taskt that we will use as an example.  It is generated in the format `<obtect/item> <-> relevance`. \n",
    "\n",
    "All the sources I checked describe how to estimate the performance of the models in the case of binary output, where there are \"relevant\" and \"non-relevant\". Despite the fact that in life often occur and I have met with tasks where the pair object/item put in correlation to non-binary values (ratings or even preferences expressed in spent money), for simplicity in the beginning let's consider the classical variant. So we'll have following definition:\n",
    "\n",
    "$$r_{ij} =\\begin{cases}\n",
    "1 - \\text{i-th item is relevant for j-th object}\\\\\n",
    "0 - \\text{other case}.\n",
    "\\end{cases}, i=\\overline{1,n}, j=\\overline{1,m}.$$\n",
    "\n",
    "Where:\n",
    "- $n$ - number of the items under consideration;\n",
    "- $m$ - number of the objects under consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04718cc3-b48c-49f7-a2f1-f31c6a4058fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object</th>\n",
       "      <th>item</th>\n",
       "      <th>relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     object  item  relevant\n",
       "148       4    28         0\n",
       "147       4    27         1\n",
       "154       5     4         1\n",
       "8         0     8         1\n",
       "105       3    15         1\n",
       "24        0    24         0\n",
       "125       4     5         0\n",
       "276       9     6         0\n",
       "130       4    10         1\n",
       "226       7    16         0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_width = 10\n",
    "r_height = 30\n",
    "np.random.seed(10)\n",
    "\n",
    "R, c = make_blobs(\n",
    "    n_samples=r_height,\n",
    "    n_features=r_width,\n",
    "    centers=3,\n",
    "    random_state=10,\n",
    "    cluster_std=1\n",
    ")\n",
    "R = np.round((R-R.min())/(R.max()-R.min())).astype(int)\n",
    "\n",
    "# genrating combinations of object/item to be empty\n",
    "combination_counts = 20\n",
    "nan_combinations = np.concatenate(\n",
    "    [\n",
    "        np.random.randint(0, R.shape[0], [combination_counts,1]),\n",
    "        np.random.randint(0, R.shape[1], [combination_counts,1])\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "R_frame = pd.Series(\n",
    "    R.ravel(),\n",
    "    index = pd.MultiIndex.from_tuples(\n",
    "            [\n",
    "                (j,i) \n",
    "                for j in np.arange(R.shape[1]) \n",
    "                for i in np.arange(R.shape[0])\n",
    "            ],\n",
    "            names = [\"object\", \"item\"]\n",
    "    ),\n",
    "    name = \"relevant\"\n",
    ").reset_index()\n",
    "\n",
    "R_frame.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259a3e4c-a6ce-4596-9efa-e72a0ef21c4e",
   "metadata": {},
   "source": [
    "The process of dividing observations into test and train sets is crucial. The methods employed to predict expected values often tend to reproduce the same numbers they have already encountered. Therefore, incorporating the test/train split becomes highly significant as it introduces necessary errors to learn specifics of the metrics under consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8134dbf6-6b4f-4a57-b448-df08db8e9083",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_train, R_test = train_test_split(\n",
    "    R_frame, stratify=R_frame[[\"object\"]], \n",
    "    test_size=0.25,\n",
    "    random_state = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3a3dd6-88e2-47ca-86e9-e05c8b1045f9",
   "metadata": {},
   "source": [
    "### Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ea5cac-bf51-41a2-8152-6ccf89d2c209",
   "metadata": {},
   "source": [
    "We need some algorithms result to compute metrics for them. So here I use some basic approaches from the `surprise` library to build some solutions. We will then compare metrics between them and learn why this or that approach is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9c7492e-2437-4e33-806a-5f9c2d8085ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object</th>\n",
       "      <th>item</th>\n",
       "      <th>relevant</th>\n",
       "      <th>model1_scores</th>\n",
       "      <th>model2_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.274269</td>\n",
       "      <td>0.365826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.803087</td>\n",
       "      <td>0.803087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.579057</td>\n",
       "      <td>0.617377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.817919</td>\n",
       "      <td>0.817919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.332373</td>\n",
       "      <td>0.440908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     object  item  relevant  model1_scores  model2_scores\n",
       "241       8     1         0       0.274269       0.365826\n",
       "159       5     9         0       0.803087       0.803087\n",
       "158       5     8         0       0.579057       0.617377\n",
       "189       6     9         1       0.817919       0.817919\n",
       "31        1     1         1       0.332373       0.440908"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = Reader(rating_scale=(0,1))\n",
    "surp_dataset = Dataset.load_from_df(\n",
    "    R_train[[\"object\", \"item\", 'relevant']], \n",
    "    reader\n",
    ")\n",
    "my_data_set = surp_dataset.build_full_trainset()\n",
    "\n",
    "model = KNNBasic(k=25,verbose=False)\n",
    "model = model.fit(my_data_set)\n",
    "R_test[\"model1_scores\"] = R_test[[\"object\", \"item\"]].apply(\n",
    "    lambda row: model.predict(\n",
    "        row[\"object\"], row[\"item\"]\n",
    "    ).est, \n",
    "    axis = 1\n",
    ")\n",
    "\n",
    "model = KNNBasic(k=5, verbose=False)\n",
    "model.fit(my_data_set)\n",
    "R_test[\"model2_scores\"] = R_test[[\"object\", \"item\"]].apply(\n",
    "    lambda row: model.predict(\n",
    "        row[\"object\"], row[\"item\"]\n",
    "    ).est, \n",
    "    axis = 1\n",
    ")\n",
    "R_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0227841-313c-4f8c-a4b5-75431180d201",
   "metadata": {},
   "source": [
    "Now, for each object $j=\\overline{1,m}$, we have two arrays of scores: $S_{1j}$ and $S_{2j}$, generated by the first and second models, respectively. The array for the $M$-th model should be represented as $S_{M,j} = \\{s_{M,j,1}, s_{M,j,2}, ..., s_{M,j,n}\\}$. If $s_{M,j,t} > s_{M,j,k}$, it indicates that the $t$-th item is considered more relevant than the $k$-th item for object $j$, according to the $M$-th model.\n",
    "\n",
    "So now we can order items according to relevance by model. So we can define orders of the items:\n",
    "\n",
    "$$I_{M,j}=\\{i_1, i_2, ... , i_n\\}: k<t \\Leftrightarrow s_{M,j,i_k} > s_{M,j,i_t}.$$\n",
    "\n",
    "Or, in simple words, in $I_{M,j}$ items go in descending order of preference for the $j$-th object according to the $M$-th model.\n",
    "\n",
    "The following cell displays $I_{1,j}$ and $I_{2,j}$ for selected $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a5b0e944-d1a5-42cc-972f-b05ad3feac77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object</th>\n",
       "      <th>item</th>\n",
       "      <th>relevant</th>\n",
       "      <th>model1_scores</th>\n",
       "      <th>model2_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.274269</td>\n",
       "      <td>0.365826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.416936</td>\n",
       "      <td>0.450703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688437</td>\n",
       "      <td>0.585741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.602008</td>\n",
       "      <td>0.631656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.438645</td>\n",
       "      <td>0.582476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.459117</td>\n",
       "      <td>0.363809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.638770</td>\n",
       "      <td>0.581646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     object  item  relevant  model1_scores  model2_scores\n",
       "241       8     1         0       0.274269       0.365826\n",
       "247       8     7         1       0.416936       0.450703\n",
       "268       8    28         0       0.688437       0.585741\n",
       "267       8    27         1       0.602008       0.631656\n",
       "251       8    11         1       0.438645       0.582476\n",
       "257       8    17         0       0.459117       0.363809\n",
       "266       8    26         0       0.638770       0.581646"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$I_{1,8}=\\{ 28,26,27,17,11,7,1 \\}$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$I_{2,8}=\\{ 27,28,11,26,7,1,17 \\}$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "object_ind = 8\n",
    "temp_object = R_test[R_test[\"object\"] == object_ind]\n",
    "display(temp_object)\n",
    "\n",
    "get_order = lambda scores_name: \",\".join(\n",
    "    temp_object\n",
    "    .sort_values(scores_name, ascending=False)[\"item\"]\n",
    "    .astype(str)\n",
    "    .to_list()\n",
    ")\n",
    "\n",
    "model1_order = get_order(\"model1_scores\")\n",
    "display(Latex(f\"$I_{{1,{object_ind}}}=\\{{ {model1_order} \\}}$\"))\n",
    "model2_order = get_order(\"model2_scores\")\n",
    "display(Latex(f\"$I_{{2,{object_ind}}}=\\{{ {model2_order} \\}}$\"))\n",
    "\n",
    "del object_ind, temp_object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0cebe1-fb26-4873-a866-e48ad0c85935",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## recall@k\n",
    "\n",
    "$recall_j@k$ gives a measure of how many of the relevant items are present in top $k$ out of all the relevant items, where $k$ is the number of recommendations generated for a $j$-th object. Or more formally:\n",
    "\n",
    "$$recall_j@k = \\frac{\\sum_{i=1}^k r_i}{\\sum_{i=1}^n r_i}$$\n",
    "\n",
    "Where:\n",
    "- items are sorted according to their preference for $j$-th object for the model under consideration;\n",
    "- $\\sum_{i=1}^k r_i$ - number of relevant items in first $k$ items;\n",
    "- $\\sum_{i=1}^n r_i$ - total number of relevant items for $j$-th object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b345486e-5490-4208-bf9d-6403eff5532d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Consider specific"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016fe917-d67a-4a83-86af-57798792c780",
   "metadata": {},
   "source": [
    "Let's examine a specific object to gain a clear understanding of the situation and calculate the recall at 3 ($recall@3$) for it. We will compare model 1 and model 2 to discern any differences. In the following cell, we have extracted a subframe for the specific object and sorted it based on the results from the models. The example has been selected to highlight the disparity in $recall@3$ between the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "760f1133-1456-428e-bc0a-0e9e36c802f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style='display: flex;justify-content: space-around;'>\n",
       "        <div>\n",
       "            <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevant</th>\n",
       "      <th>model1_results</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.763529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.604313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>0.595839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.452637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.424210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0.380536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "            <p style='font-size:20px'>\n",
       "                recall@3 - 40.0%\n",
       "            </p>\n",
       "        </div>\n",
       "        <div>\n",
       "            <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevant</th>\n",
       "      <th>model2_results</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>0.805351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.763529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.613521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.576874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.568975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0.443676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "            <p style='font-size:20px'>\n",
       "                recall@3 - 60.0%\n",
       "            </p>\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = 3\n",
    "obj = 4\n",
    "\n",
    "model1_tab = R_test.loc[\n",
    "    R_frame[\"object\"] == obj,\n",
    "    [\n",
    "        \"item\",\n",
    "        \"relevant\",\n",
    "        \"model1_results\"\n",
    "    ]\n",
    "].sort_values(\n",
    "    \"model1_results\", \n",
    "    ascending=False\n",
    ").set_index(\"item\")\n",
    "\n",
    "model2_tab = R_test.loc[\n",
    "    R_frame[\"object\"] == obj,\n",
    "    [\n",
    "        \"item\",\n",
    "        \"relevant\",\n",
    "        \"model2_results\"\n",
    "    ]\n",
    "].sort_values(\n",
    "    \"model2_results\", \n",
    "    ascending=False\n",
    ").set_index(\"item\")\n",
    "\n",
    "model1_recall = (\n",
    "    model1_tab[\"relevant\"].iloc[:k].sum()/\n",
    "    model1_tab[\"relevant\"].sum()\n",
    ")\n",
    "model2_recall = (\n",
    "    model2_tab[\"relevant\"].iloc[:k].sum()/\n",
    "    model2_tab[\"relevant\"].sum()\n",
    ")\n",
    "\n",
    "display(HTML(\n",
    "    f\"\"\"\n",
    "    <div style='display: flex;justify-content: space-around;'>\n",
    "        <div>\n",
    "            {model1_tab.to_html()}\n",
    "            <p style='font-size:20px'>\n",
    "                recall@{k} - {round(model1_recall*100,2)}%\n",
    "            </p>\n",
    "        </div>\n",
    "        <div>\n",
    "            {model2_tab.to_html()}\n",
    "            <p style='font-size:20px'>\n",
    "                recall@{k} - {round(model2_recall*100,2)}%\n",
    "            </p>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b47043-2431-4859-a25a-74e822d0629c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Python code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be31fc1-844c-45cd-8ab2-26c8a7abc0da",
   "metadata": {},
   "source": [
    "There is a function that represents the realisation of $recall@k$ in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75a28cf1-ef6a-49ae-a3f6-f2744ee05fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_k(relevance_array, pred_score, k):\n",
    "    '''\n",
    "    The calculation of recall@k is a metric that measures \n",
    "    the proportion of relevant items present within the top k \n",
    "    recommendations out of all relevant elements. It signifies \n",
    "    the ability to identify and include relevant items in the \n",
    "    initial recommendations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    relevance_array : numpy.array\n",
    "        binary array marking observations that were relevant;\n",
    "    pred_score : numpy.array\n",
    "        predicted scores are expected to be \n",
    "        higher the more relevant item is.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    out : float\n",
    "        realisation of the metric.\n",
    "    '''\n",
    "    if len(relevance_array)!=len(pred_score):\n",
    "        raise ValueError(\n",
    "            \"`relevance_array` and `pred_score` must be the same size\"\n",
    "        )\n",
    "    elif len(relevance_array) < k:\n",
    "        raise ValueError(\n",
    "            \"k is greater than the number of observations\"\n",
    "        )\n",
    "    \n",
    "    relevant_in_k = np.sum(\n",
    "        relevance_array[np.argsort(pred_score)[::-1]][:k]\n",
    "    )\n",
    "    relevant_total = np.sum(relevance_array)\n",
    "    return relevant_in_k/relevant_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed7bed-8de5-4262-b7c9-e5a6971bb8ba",
   "metadata": {},
   "source": [
    "Here is some unitests for function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f3dd6ab-b8dd-4dad-a157-a48543f8e24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_computions (__main__.TestRecall)\n",
      "Just basic test with known result ... ok\n",
      "test_different_sizes (__main__.TestRecall)\n",
      "We must check that if the sizes of arrays with ... ok\n",
      "test_k_more_obs (__main__.TestRecall)\n",
      "K cannot be more than the number of observations ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.001s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "class TestRecall(unittest.TestCase):\n",
    "    def test_different_sizes(self):\n",
    "        '''\n",
    "        We must check that if the sizes of arrays with \n",
    "        relevance and prediction differ, an error must \n",
    "        be rased.\n",
    "        '''\n",
    "        with self.assertRaises(ValueError):\n",
    "            recall_k(\n",
    "                np.array([1, 1, 0]),\n",
    "                np.array([0.3, 0.2, 0.3, 0.2]),\n",
    "                1\n",
    "            )\n",
    "\n",
    "    def test_k_more_obs(self):\n",
    "        '''\n",
    "        K cannot be more than the number of observations \n",
    "        we are considering.\n",
    "        '''\n",
    "        with self.assertRaises(ValueError):\n",
    "            recall_k(\n",
    "                np.array([1, 1, 0, 0, 1]),\n",
    "                np.array([0.4, 0.1, 0.2, 0.5, 0.3]),\n",
    "                10\n",
    "            )\n",
    "    \n",
    "    def test_computions(self):\n",
    "        '''\n",
    "        Just basic test with known result\n",
    "        '''\n",
    "        real_ans = recall_k(\n",
    "            np.array([1, 1, 0, 0, 1]),\n",
    "            np.array([0.4, 0.1, 0.2, 0.5, 0.3]),\n",
    "            3\n",
    "        )\n",
    "        exp_ans = 2/3\n",
    "        self.assertAlmostEqual(real_ans, exp_ans, delta=0.000001)\n",
    "ans = unittest.main(argv=[''], verbosity=2, exit=False)\n",
    "del TestRecall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bfcae6-53ac-4652-b716-018d0df8c8d6",
   "metadata": {},
   "source": [
    "The following cell shows the code to calculate the recall for our example. We calculated it for each object, but then took the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ea4c8fa-c105-493e-aba5-acbf58f75345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall for model 1</th>\n",
       "      <th>recall for model2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>object</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        recall for model 1  recall for model2\n",
       "object                                       \n",
       "0                 0.666667           0.500000\n",
       "1                 0.800000           0.800000\n",
       "2                 1.000000           1.000000\n",
       "3                 0.500000           0.500000\n",
       "4                 0.400000           0.600000\n",
       "5                 0.666667           0.666667\n",
       "6                 0.666667           0.666667\n",
       "7                 0.250000           0.250000\n",
       "8                 0.333333           0.666667\n",
       "9                 0.600000           0.600000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall for model 1</th>\n",
       "      <th>recall for model2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean value</th>\n",
       "      <td>0.588333</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            recall for model 1  recall for model2\n",
       "mean value            0.588333              0.625"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show = R_test.groupby(\"object\").apply(\n",
    "    lambda object: pd.Series({\n",
    "        \"recall for model 1\" : recall_k(\n",
    "            relevance_array=object[\"relevant\"].to_numpy(),\n",
    "            pred_score=object[\"model1_results\"].to_numpy(),\n",
    "            k=4\n",
    "        ),\n",
    "        \"recall for model2\" : recall_k(\n",
    "            relevance_array=object[\"relevant\"].to_numpy(),\n",
    "            pred_score=object[\"model2_results\"].to_numpy(),\n",
    "            k=4\n",
    "        )\n",
    "    }),\n",
    "    include_groups=False\n",
    ")\n",
    "display(show)\n",
    "display(show.mean().rename(\"mean value\").to_frame().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e37a35c-d8d7-4f2f-937f-380b8d7bf1a9",
   "metadata": {},
   "source": [
    "## precision@k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95ace30-0ef3-40fa-b77a-b4a6bf88f65a",
   "metadata": {},
   "source": [
    "$precision@k$ is a fraction of relevant elements in the first $k$ recommendations. Or more formally:\n",
    "\n",
    "$$precision@k = \\frac{\\sum_{i=1}^k r_i}{k}$$\n",
    "\n",
    "Where:\n",
    "- items are sorted according to their preference for $j$-th object for the model under consideration;\n",
    "- $\\sum_{i=1}^k r_i$ - number of relevant items in first $k$ items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f7d900-e118-498d-8117-2615461aae91",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Consider specific"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2b3f2b-a423-40b0-9d6b-d67dd337ce4f",
   "metadata": {},
   "source": [
    "Let's examine a specific object to gain a clear understanding of the situation and calculate the recall at 3 ($precision@3$) for it. We will compare model 1 and model 2 to discern any differences. In the following cell, we have extracted a subframe for the specific object and sorted it based on the results from the models. The example has been selected to highlight the disparity in $precision@3$ between the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79ce8b89-05ef-4226-8657-cb98fc9a9cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style='display: flex;justify-content: space-around;'>\n",
       "        <div>\n",
       "            <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevant</th>\n",
       "      <th>model1_results</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.763529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.604313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>0.595839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.452637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.424210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0.380536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "            <p style='font-size:20px'>\n",
       "                recall@3 - 66.67%\n",
       "            </p>\n",
       "        </div>\n",
       "        <div>\n",
       "            <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevant</th>\n",
       "      <th>model2_results</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>0.805351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.763529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.613521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.576874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.568975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0.443676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "            <p style='font-size:20px'>\n",
       "                recall@3 - 100.0%\n",
       "            </p>\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = 3\n",
    "obj = 4\n",
    "\n",
    "model1_tab = R_test.loc[\n",
    "    R_frame[\"object\"] == obj,\n",
    "    [\n",
    "        \"item\",\n",
    "        \"relevant\",\n",
    "        \"model1_results\"\n",
    "    ]\n",
    "].sort_values(\n",
    "    \"model1_results\", \n",
    "    ascending=False\n",
    ").set_index(\"item\")\n",
    "\n",
    "model2_tab = R_test.loc[\n",
    "    R_frame[\"object\"] == obj,\n",
    "    [\n",
    "        \"item\",\n",
    "        \"relevant\",\n",
    "        \"model2_results\"\n",
    "    ]\n",
    "].sort_values(\n",
    "    \"model2_results\", \n",
    "    ascending=False\n",
    ").set_index(\"item\")\n",
    "\n",
    "model1_recall = (\n",
    "    model1_tab[\"relevant\"].iloc[:k].mean()\n",
    ")\n",
    "model2_recall = (\n",
    "    model2_tab[\"relevant\"].iloc[:k].mean()\n",
    ")\n",
    "\n",
    "display(HTML(\n",
    "    f\"\"\"\n",
    "    <div style='display: flex;justify-content: space-around;'>\n",
    "        <div>\n",
    "            {model1_tab.to_html()}\n",
    "            <p style='font-size:20px'>\n",
    "                recall@{k} - {round(model1_recall*100,2)}%\n",
    "            </p>\n",
    "        </div>\n",
    "        <div>\n",
    "            {model2_tab.to_html()}\n",
    "            <p style='font-size:20px'>\n",
    "                recall@{k} - {round(model2_recall*100,2)}%\n",
    "            </p>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb54eed-cbf5-4d60-9178-fb6ff4dfaf1e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Python code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eb42f4-9378-4b27-869e-543a0328e60c",
   "metadata": {},
   "source": [
    "There is a function that represents the realisation of $precision@k$ in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00d2b9cc-764f-4ada-9118-e281d4944f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_k(relevance_array, pred_score, k):\n",
    "    '''\n",
    "    Calculation Precision@k. This is a metric used to \n",
    "    assess the accuracy of recommendations by calculating \n",
    "    the proportion of relevant items in the first k \n",
    "    recommendations out of all the items recommended. \n",
    "    It quantifies the precision and effectiveness of \n",
    "    the recommendation system in providing highly relevant \n",
    "    suggestions within the initial set of recommendations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    relevance_array : numpy.array\n",
    "        binary array marking observations that were relevant;\n",
    "    pred_score : numpy.array\n",
    "        predicted scores are expected to be \n",
    "        higher the more relevant item is.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    out : float\n",
    "        realisation of the metric.\n",
    "    '''\n",
    "    if len(relevance_array)!=len(pred_score):\n",
    "        raise ValueError(\n",
    "            \"`relevance_array` and `pred_score` must be the same size\"\n",
    "        )\n",
    "    elif len(relevance_array) < k:\n",
    "        raise ValueError(\n",
    "            \"k is greater than the number of observations\"\n",
    "        )\n",
    "    return np.mean(\n",
    "        relevance_array[np.argsort(pred_score)[::-1]][:k]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14069c7-294e-4652-9f30-707be0cada37",
   "metadata": {},
   "source": [
    "Here is some unitests for function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5da231f0-b361-4ca5-8aa2-38509eb2fd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_computions (__main__.TestPrecision)\n",
      "Just basic test with known result ... ok\n",
      "test_different_sizes (__main__.TestPrecision)\n",
      "We must check that if the sizes of arrays with ... ok\n",
      "test_k_more_obs (__main__.TestPrecision)\n",
      "K cannot be more than the number of observations ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "class TestPrecision(unittest.TestCase):\n",
    "    def test_different_sizes(self):\n",
    "        '''\n",
    "        We must check that if the sizes of arrays with \n",
    "        relevance and prediction differ, an error must \n",
    "        be rased.\n",
    "        '''\n",
    "        with self.assertRaises(ValueError):\n",
    "            precision_k(\n",
    "                np.array([1, 1, 0]),\n",
    "                np.array([0.3, 0.2, 0.3, 0.2]),\n",
    "                1\n",
    "            )\n",
    "\n",
    "    def test_k_more_obs(self):\n",
    "        '''\n",
    "        K cannot be more than the number of observations \n",
    "        we are considering.\n",
    "        '''\n",
    "        with self.assertRaises(ValueError):\n",
    "            precision_k(\n",
    "                np.array([1, 1, 0, 0, 1]),\n",
    "                np.array([0.4, 0.1, 0.2, 0.5, 0.3]),\n",
    "                10\n",
    "            )\n",
    "    \n",
    "    def test_computions(self):\n",
    "        '''\n",
    "        Just basic test with known result\n",
    "        '''\n",
    "        real_ans = precision_k(\n",
    "            np.array([1, 1, 0, 0, 1]),\n",
    "            np.array([0.4, 0.1, 0.2, 0.5, 0.3]),\n",
    "            3\n",
    "        )\n",
    "        exp_ans = 2/3\n",
    "        self.assertAlmostEqual(real_ans, exp_ans, delta=0.000001)\n",
    "\n",
    "ans = unittest.main(argv=[''], verbosity=2, exit=False)\n",
    "del TestPrecision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b61bf6f-2bce-48b3-8658-12a2ca5ea334",
   "metadata": {},
   "source": [
    "The following cell shows the code to calculate the precision for our example. We calculated it for each object, but then took the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5494d524-8947-4022-921b-3ed0824ee9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision for model 1</th>\n",
       "      <th>precision for model2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>object</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        precision for model 1  precision for model2\n",
       "object                                             \n",
       "0                        1.00                  0.75\n",
       "1                        1.00                  1.00\n",
       "2                        0.75                  0.75\n",
       "3                        0.75                  0.75\n",
       "4                        0.50                  0.75\n",
       "5                        0.50                  0.50\n",
       "6                        1.00                  1.00\n",
       "7                        0.25                  0.25\n",
       "8                        0.25                  0.50\n",
       "9                        0.75                  0.75"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision for model 1</th>\n",
       "      <th>precision for model2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean value</th>\n",
       "      <td>0.675</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            precision for model 1  precision for model2\n",
       "mean value                  0.675                   0.7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show = R_test.groupby(\"object\").apply(\n",
    "    lambda object: pd.Series({\n",
    "        \"precision for model 1\" : precision_k(\n",
    "            relevance_array=object[\"relevant\"].to_numpy(),\n",
    "            pred_score=object[\"model1_results\"].to_numpy(),\n",
    "            k=4\n",
    "        ),\n",
    "        \"precision for model2\" : precision_k(\n",
    "            relevance_array=object[\"relevant\"].to_numpy(),\n",
    "            pred_score=object[\"model2_results\"].to_numpy(),\n",
    "            k=4\n",
    "        )\n",
    "    }),\n",
    "    include_groups=False\n",
    ")\n",
    "display(show)\n",
    "display(show.mean().rename(\"mean value\").to_frame().T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
