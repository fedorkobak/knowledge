{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c01d4f65-fdbe-4546-9168-891beba49c89",
   "metadata": {},
   "source": [
    "# Confusion matrix\n",
    "\n",
    "Inaccuracy matrix is a very important concept for evaluating classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12396198-f3de-4139-b424-36b2e9debedb",
   "metadata": {},
   "source": [
    "## Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b3fc5f-2b74-4f1c-ac0b-8f1e621b5dcb",
   "metadata": {},
   "source": [
    "Consider a binary classification problem. We have two classes, Positive and Negative.\n",
    "\n",
    "Let be:\n",
    "\n",
    "- $P$ is the number of positive observations in the sample;\n",
    "- $N$ is the number of negative observations in the sample.\n",
    "\n",
    "Now, suppose we have formed some classifier. We have the following groups of observations.\n",
    "\n",
    "- True positive - observations that were positive in the sample and we correctly predicted them as positive. We will denote their number as $TP$;\n",
    "- True negative - observations that were negative in the sample and we correcrly predicted then as negative. We will denote their number as $TN$;\n",
    "- False positve - observations that were negative in the sample, but which we then mistakenly predicted to be positive. We will denote their number as $FP$;\n",
    "- False negative - observations that were positive in the sample, but wich we then mistakenly predicted to be negative. We will denote their number as $FN$.\n",
    "\n",
    "So, if you put the actual value on the rows and the predicted value on the columns, you will get a confusion matrix.\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th></th>\n",
    "      <th>Predicted $N$</th>\n",
    "      <th>Predicted $P$</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Actual $N$</td>\n",
    "      <td>$TN$</td>\n",
    "      <td>$FP$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Actual $P$</td>\n",
    "      <td>$FN$</td>\n",
    "      <td>$TP$</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8cbeaf-36e1-446f-819a-2c81b344bda2",
   "metadata": {},
   "source": [
    "Also valuable is the representation of the confusion matrix using relative values.\n",
    "\n",
    "Let be:\n",
    "\n",
    "- $P^* = TP + FP$ - number of observations from the sample predicted as positive;\n",
    "- $N^* = TN + FN$ - number of observations from the sample predicted as negative;\n",
    "- $TNR = TN/N^*$ - true negative rate, the proportion of correct predictions among observations that are predicted negative;\n",
    "- $FNR = FN/N^*$ - false negative rate, the proportion of incorrect predictions among observations that are predicted to be negative;\n",
    "- $TPR = TP/P^*$ - true positive rate, the proportion of correct predictions among observations that are predicted to be positive;\n",
    "- $FPR = FP/P^*$ - false positve rate, the proportion of incorrect predicitons among observations that are predicted to be negative.\n",
    "\n",
    "\n",
    "So using these notations the confusion matrix can also be written:\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th></th>\n",
    "      <th>Predicted $N$</th>\n",
    "      <th>Predicted $P$</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Actual $N$</td>\n",
    "      <td>$TNR$</td>\n",
    "      <td>$FPR$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Actual $P$</td>\n",
    "      <td>$FNR$</td>\n",
    "      <td>$TPR$</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a37552f-be87-4bce-b36e-1bb7d474f4d3",
   "metadata": {},
   "source": [
    "## Confusion table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c502da-4730-4218-a712-74df61d57499",
   "metadata": {},
   "source": [
    "Many classification models allow to return a score that indicates the probability that a particular object belongs to the positive class. You can select the threshold above which you consider the object under consideration to be positive. Different treshold values will consequently produce different confusion matrixes.\n",
    "\n",
    "The table that puts in correspondence to some selected threshold the table of contiguity will be called the confusion table.\n",
    "\n",
    "| treshold   | $TNR$ | $FPR$ | $FNR$ | $TPR$ |\n",
    "|:-----------|:-----:|:-----:|:-----:|:-----:|\n",
    "| $t_1$      | $TNR_1$| $FPR_1$| $FNR_1$| $TPR_1$|\n",
    "| $t_2$      | $TNR_2$| $FPR_2$| $FNR_2$| $TPR_2$|\n",
    "| ...        | ...    | ...    | ...    | ...    |\n",
    "| $t_i$      | $TNR_i$| $FPR_i$| $FNR_i$| $TPR_i$|\n",
    "| ...        | ...    | ...    | ...    | ...    |\n",
    "| $t_n$      | $TNR_n$| $FPR_n$| $FNR_n$| $TPR_n$|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c7b7a54-9b4e-4862-ab78-43311b40e4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def confusion_table(\n",
    "    y_true,\n",
    "    y_score,\n",
    "    tresholds=None\n",
    "):\n",
    "    if tresholds is None:\n",
    "        tresholds = y_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
