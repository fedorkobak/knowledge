{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f433428",
   "metadata": {},
   "source": [
    "# Assets bundles\n",
    "\n",
    "Asset bundles are a way to define the databricks project as a code. You can develop your project locally by following the typical Databricks project patterns, and deploy it to the platform using your CI/CD pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743dcdfd",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "The bundle's configuration of the bundle is defined in the `databricks.yml` file. Consider the configuration options for the bundle. Check the official desriptions in the [Databricks Asset Bundle configuration](https://docs.databricks.com/aws/en/dev-tools/bundles/settings).\n",
    "\n",
    "- `bundle`: Specifies the Databricks environment and the bundle's basic properties.\n",
    "- `include`: allows to specify other configuration files. When configuration is relatively complex, it's convenient to keep some configurations in the other files.\n",
    "- [`scripts`](https://docs.databricks.com/aws/en/dev-tools/bundles/settings#scripts): Define a script to be run in the local environemt. But the configuration specific to the Databricks environemnt that corresponds to the bunlde will be applied. You can use a command like `databricks bundle run <name specified for the script>`. \n",
    "- [`sync`](https://docs.databricks.com/aws/en/dev-tools/bundles/settings#sync): Specifies which files will be pushed to the Databricks environemtn during `databricks bundle deploy`.\n",
    "- [`artifacts`](https://docs.databricks.com/aws/en/dev-tools/bundles/settings#artifacts): if your project is supposed to produce some output files during build (python whl, java jar, etc.) you have to specify this using `artifacts` attribute. The most important detail is that here is defined the script that generates the artifact; this script will be executed with the `databricks bundle build` command.\n",
    "- [`variables`](https://docs.databricks.com/aws/en/dev-tools/bundles/settings#variables): here, you can define variables that can be used in subtitutions.\n",
    "- [`resources`](https://docs.databricks.com/aws/en/dev-tools/bundles/settings#resources): specifies the Databricks [resources](https://docs.databricks.com/aws/en/dev-tools/bundles/resources#supported-resources). It is literaly the features of the Databricks used by the project lie: jobs, dashboards, clusters etc.\n",
    "- [`targets`](https://docs.databricks.com/aws/en/dev-tools/bundles/settings#targets): sometimes you need several setups for the same project. The most popular cases are `dev` and `production`. The `targets` allows to specify exactly this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9235778b",
   "metadata": {},
   "source": [
    "### Artifacts\n",
    "\n",
    "Consider the simpliest possible `artifact` usage. The following code specifies the artifact as the `result` file.\n",
    "\n",
    "```yaml\n",
    "bundle:\n",
    "  name: knowledge\n",
    "\n",
    "artifacts:\n",
    "  default:\n",
    "    build: echo \"this is new configuration\" > result\n",
    "```\n",
    "\n",
    "Running the command `databricks bundle build` will create the `result` file, which will then published in the Databricks environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5275087",
   "metadata": {},
   "source": [
    "## Substitutions\n",
    "\n",
    "With substitutions mechanisms you will be able to retrieve some values and substitute them to the config during `bundle build` or `bundle run`. As typcail you have to define your substitutions in the `${<variable name>}` format. Check more about substitutions in the [Substitutions](https://docs.databricks.com/aws/en/dev-tools/bundles/variables) page of the official documentation.\n",
    "\n",
    "For example the following pattern in the configuration:\n",
    "\n",
    "```yaml\n",
    "artifacts:\n",
    "  default:\n",
    "    build: echo \"This is ${bundle.name} bundle\" > ${bundle.target}\n",
    "```\n",
    "\n",
    "It will create a file with the same name as the bundle's target and save the string that containing the bundle name there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d3d13a",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "Variables can be sepcified using following symtax:\n",
    "```yaml\n",
    "variables:\n",
    "  <var_name1>:\n",
    "    ...\n",
    "  <var_name2>:\n",
    "    ...\n",
    "```\n",
    "\n",
    "To **pass a value** to a variable:\n",
    "\n",
    "- Use the environment variable that follows the pattern `BUNDLE_VAR_<name of variable>`, databricks CLI commands executed from corresponding environement will automatically substitute this value.\n",
    "- Use the `--var=\"<var_name1>=<var_value1>,<var_name2>=<var_value2>\"` options of the `databricks bundle deploy`.\n",
    "\n",
    "For more check the [Custom variables](https://docs.databricks.com/aws/en/dev-tools/bundles/variables#custom-variables) section of the official documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8638ca3b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As exmaple consider the following configuration for variables:\n",
    "\n",
    "```yaml\n",
    "variables:\n",
    "  var1:\n",
    "    default: value1\n",
    "  var2:\n",
    "    default: value2\n",
    "\n",
    "artifacts:\n",
    "  default:\n",
    "    build: echo \"${var.var1} and ${var.var2}\" > result\n",
    "```\n",
    "\n",
    "The values `var1` and `var2` are defined in the bundle, and then used in the command that creates the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8bd2c0",
   "metadata": {},
   "source": [
    "After running the pipeline, the content of the `result` file content will contain the default values of the variables.\n",
    "\n",
    "```bash\n",
    "$ databricks bundle deploy\n",
    "\n",
    "Building default...\n",
    "Uploading bundle files to /Workspace/Users/fedor.kobak@innowise.com/.bundle/python_default/dev/files...\n",
    "Deploying resources...\n",
    "Updating deployment state...\n",
    "Deployment complete!\n",
    "\n",
    "$ cat result\n",
    "\n",
    "value1 and value2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179cff4c",
   "metadata": {},
   "source": [
    "The following cell shows how the default values of the variables can be replaced:\n",
    "\n",
    "- The `var1` is specified through `BUNDLE_VAR_var1=\"hello\"`. \n",
    "- The `var2` is specified through `--var=\"var2=world\"`.\n",
    "\n",
    "```bash\n",
    "$ BUNDLE_VAR_var1=\"hello\" databricks bundle deploy --var=\"var2=world\"\n",
    "\n",
    "Building default...\n",
    "Uploading bundle files to /Workspace/Users/fedor.kobak@innowise.com/.bundle/knowledge/dev/files...\n",
    "Deploying resources...\n",
    "Deployment complete!\n",
    "\n",
    "$ cat result \n",
    "\n",
    "hello and world\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab3429e",
   "metadata": {},
   "source": [
    "## Execute scripts\n",
    "\n",
    "To execute scripts with a bundle configuration cretedentials use `databricks bundle run [reference to the script]`. The script can be defined *inline* or specified in *databricks.yml*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aea9524",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "For example if you try to access the `DATABRICKS_HOST` from the local raw python environment, you will receive an error:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafbd465",
   "metadata": {},
   "source": [
    "\n",
    "```bash\n",
    "$python3 -c 'import os; print(os.environ[\"DATABRICKS_HOST\"])'\n",
    "Traceback (most recent call last):\n",
    "  File \"<string>\", line 1, in <module>\n",
    "    import os; print(os.environ[\"DATABRICKS_HOST\"])\n",
    "                     ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n",
    "  File \"<frozen os>\", line 717, in __getitem__\n",
    "KeyError: 'DATABRICKS_HOST'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35ce6cc",
   "metadata": {},
   "source": [
    "But the same command wroks fine under the Databricks CLI.\n",
    "\n",
    "```bash\n",
    "$ databricks bundle run -- python3 -c 'import os; print(os.environ[\"DATABRICKS_HOST\"][:20])'\n",
    "https://dbc-da0651ae\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e5a25f",
   "metadata": {},
   "source": [
    "## Targets\n",
    "\n",
    "The `targets` allow you to define multiple behavior patterns for the bundle. For example, you may need to organize development and production environments.\n",
    "\n",
    "The target definition may have the following syntax:\n",
    "\n",
    "```yaml\n",
    "targets:\n",
    "  <tagtet1 name>:\n",
    "    <configuration>\n",
    "  <target2 name>:\n",
    "    <configuration>\n",
    "```\n",
    "\n",
    "The typical way to specify the target is to use `-t <target name>` option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929787de",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As example consider the following configuration:\n",
    "\n",
    "```yaml\n",
    "artifacts:\n",
    "  default:\n",
    "    build: echo \"My target is ${bundle.target}\" > result\n",
    "\n",
    "targets:\n",
    "  dev:\n",
    "    mode: development\n",
    "    default: true\n",
    "  prod:\n",
    "    mode: development\n",
    "```\n",
    "\n",
    "The information about the used target was forwarded through the artifact. \n",
    "\n",
    "**Note.** Here, the `mode` for the `prod` target is set to the `development` value just to keep things simple, since the other options require additional configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd53b8b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "With the typical run system returns the `dev` as value for `bundle.target`.\n",
    "\n",
    "```bash\n",
    "$ databricks bundle deploy\n",
    "\n",
    "Building default...\n",
    "Uploading bundle files to /Workspace/Users/fedor.kobak@innowise.com/.bundle/python_default/dev/files...\n",
    "Deploying resources...\n",
    "Updating deployment state...\n",
    "Deployment complete!\n",
    "\n",
    "$ cat result\n",
    "\n",
    "My target is dev\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad9aa40",
   "metadata": {},
   "source": [
    "The following cell runs `databricks bundle deploy -t prod` which forces CLI to use `prod` target. \n",
    "\n",
    "```bash\n",
    "$ databricks bundle deploy -t prod\n",
    "\n",
    "Building default...\n",
    "Uploading bundle files to /Workspace/Users/fedor.kobak@innowise.com/.bundle/python_default/prod/files...\n",
    "Deploying resources...\n",
    "Updating deployment state...\n",
    "Deployment complete!\n",
    "\n",
    "$ cat result\n",
    "\n",
    "My target is prod\n",
    "```\n",
    "\n",
    "There is corresponding result in the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9231b51",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "In this section, we will discuss how to define resources in Databricks. A resource is a feature of Databricks that can be used by our application.\n",
    "\n",
    "They are defined using the following syntax:\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "    jobs:\n",
    "        <list of the jobs>\n",
    "    apps:\n",
    "        <list of the apps>\n",
    "    ...\n",
    "```\n",
    "\n",
    "Check the [Supported resources](https://docs.databricks.com/aws/en/dev-tools/bundles/resources#supported-resources)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1867f679",
   "metadata": {},
   "source": [
    "### Jobs\n",
    "\n",
    "Probably the most popular type of the resource in the Databricks. Job is an automated workflow defined in the Databricks.\n",
    "\n",
    "The most imporatant aspects of the job configuration are:\n",
    "\n",
    "- `name`: Defines the name of the job.\n",
    "- `tasks`: lists the tasks awailable for the job.\n",
    "- `shedule`: sets up the rules when task have to be executed.\n",
    "\n",
    "Check more in:\n",
    "\n",
    "- [Job](https://docs.databricks.com/aws/en/dev-tools/bundles/resources#job) description for keys of assests bundles.\n",
    "- [Job configuration](https://docs.databricks.com/aws/en/dev-tools/bundles/examples#job) in bundle configuration examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56b6dee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The most minimalistic definition of the job may take form:\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  jobs:\n",
    "    job1:\n",
    "      name: job1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5f51ed",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "There are several ways to manage third-party dependencies in assests bundles.\n",
    "\n",
    "- The `libraries` option allows you to define the packages that will be installed on the cluster during deployment. This option obviously won't work for serverless configuration.\n",
    "- The `environment` option allows you to specify the environments that will be created when the job starts. This means that a new environment is created each time you create a job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d3810b",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "Typcailly, your project will depend on the third-third party libraries. You are supposed to specify this in the `library` key. This can be specified either for the entire `cluster` or for a particular task.\n",
    "\n",
    "**Note:** libraries are not supported by the serverless computation type. The best solution is probably to create a new Databricks environment where you can set up packages you like [Manage serverless base environments](https://docs.databricks.com/aws/en/admin/workspace-settings/base-environment).\n",
    "\n",
    "For more check the [Dataricks Assets Bundles library dependencies](https://docs.databricks.com/aws/en/dev-tools/bundles/library-dependencies)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df5f7b3",
   "metadata": {},
   "source": [
    "The following code illustrates how to specify libraries for a paritcular task.\n",
    "\n",
    "---\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  jobs:\n",
    "    my_job:\n",
    "      tasks:\n",
    "        - task_key: my_task\n",
    "          libraries:\n",
    "            - pypi:\n",
    "                package: cowsay==6.1 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb177c8",
   "metadata": {},
   "source": [
    "### Environments\n",
    "\n",
    "Environments are specified for the task. There is `environments` key, each element of this list species a mapping that describes single environment. Keys available in the environment mapping are described in the following table:\n",
    "\n",
    "| Key                        | Description                                                                     |\n",
    "|----------------------------|---------------------------------------------------------------------------------|\n",
    "| `environment_key`          | The environment identifier that is used by tasks to reference this environment. | \n",
    "| `spec.environment_version` | The [version of the Databricks envrionment](https://docs.databricks.com/aws/en/release-notes/serverless/environment-version/). |\n",
    "| `spec.depencies`           | List where each key complements the `pip install` command.                      |\n",
    "\n",
    "For each task, you must specify the `environment_key` pointing to the environment key to the described environment. **Note.** it is not possible to use this approach with the Jupyter notebook. For some reason, it note that you have to use the `%pip install` magic command in your notebook code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8751bbe2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following code defines the `default_python` environment and uses it as the environment for the `my_task`:\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  jobs:\n",
    "     my_job:\n",
    "      name: cow_say_job\n",
    "      environments:\n",
    "        - environment_key: default_python\n",
    "          spec:\n",
    "            environment_version: '4'\n",
    "            dependencies:\n",
    "              - cowsay==6.1\n",
    "      tasks:\n",
    "        - task_key: my_task\n",
    "          spark_python_task:\n",
    "            python_file: ./my_file.py\n",
    "          environment_key: default_python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af2cdc8",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "A task is a stage in a job. The following important details are associated with task definitions:\n",
    "\n",
    "- The configuration of tasks is located at the following yaml path: `resources.jobs.{job_name}.tasks`.\n",
    "- Each task configuration begins with `- task_key: task_identifier` list element.\n",
    "- There is set of keys in the task configuration that determine the type of the task. Behind this path, there is a configuration specific to the taks type: `notebook_task`, `sql_task`, `pipeline_task`, `spark_python_task`, and so on.\n",
    "- There is a set of keys that generally descirbe the task: [other task settings](https://docs.databricks.com/aws/en/dev-tools/bundles/job-task-types#other-task-settings)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec98768d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The configuration of the task minght look like this:\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  jobs:\n",
    "    my_job:\n",
    "      name: cow_say_job\n",
    "      tasks:\n",
    "        - task_key: task1\n",
    "          notebook_task:\n",
    "            notebook_path: ./my_file.py\n",
    "        - task_key: task2\n",
    "          sql_task:\n",
    "            path: ./my_file.sql\n",
    "```\n",
    "\n",
    "Check the [task settings](https://docs.databricks.com/aws/en/dev-tools/bundles/job-task-types#task-settings) lists different types of tasks and their configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa4134",
   "metadata": {},
   "source": [
    "### Notebook task\n",
    "\n",
    "The Notebook Task allows to set up a task that will execute a notebook. The general form of definition is:\n",
    "\n",
    "```yaml\n",
    "- task_key: some_task\n",
    "  notebook_taks:\n",
    "    notebook_path: my_file.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399d874f",
   "metadata": {},
   "source": [
    "#### `.py` as notebook\n",
    "\n",
    "You can use a regular `.py` file as a notebook by adding the line `# Databricks notebook source` as the beginning of the file. After deployment databricks will treat it as a notebook.\n",
    "\n",
    "---\n",
    "\n",
    "Consider following resources configuration:\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  jobs:\n",
    "    job1:\n",
    "      name: job1\n",
    "      tasks:\n",
    "        - task_key: task1\n",
    "          notebook_task:\n",
    "            notebook_path: file.py\n",
    "```\n",
    "\n",
    "Where `file.py` is:\n",
    "\n",
    "```python\n",
    "print(\"hello world\")\n",
    "```\n",
    "\n",
    "Attempt to deploy the bundle fails:\n",
    "\n",
    "```bash\n",
    "$ databricks bundle deploy\n",
    "Error: expected a notebook for \"resources.jobs.job1.tasks[0].notebook_task.notebook_path\" but got a file: file at /tmp/databricks_experiments/file.py is not a notebook\n",
    "```\n",
    "\n",
    "However, if the file is defined slightly different:\n",
    "\n",
    "```python\n",
    "# Databricks notebook source\n",
    "print(\"hello world\")\n",
    "```\n",
    "\n",
    "The deployemnt goes fine:\n",
    "\n",
    "```bash\n",
    "$ databricks bundle deploy\n",
    "\n",
    "Uploading bundle files to /Workspace/Users/fedor.kobak@innowise.com/.bundle/my_bundle/dev/files...\n",
    "Deploying resources...\n",
    "Updating deployment state...\n",
    "Deployment complete!\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
