{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a585528",
   "metadata": {},
   "source": [
    "# Databricks\n",
    "\n",
    "Databricks is a platform for manipulating data and data related processes: analitics and ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1281a828",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Consider how databricks keeps data. There are:\n",
    "\n",
    "- **Catalogs**: top level container, containing schemas.\n",
    "- **Schemas**: or databases: Contains data objects.\n",
    "- **Data objects** can be: **Volume**, **Table**, **View**, **Function** or **Model**.\n",
    "\n",
    "Check the:\n",
    "\n",
    "- [Database objects in Databricks](https://docs.databricks.com/aws/en/database-objects/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413304ad",
   "metadata": {},
   "source": [
    "## Feature store\n",
    "\n",
    "You can manipulate the feature store using the databricks Python SDK, module: `databricks.feature_engineering`. This is not provided with the Databricks Python SDK out of the box - install the separatre [PyPI published package](https://pypi.org/project/databricks-feature-engineering/).\n",
    "\n",
    "Create the feature store with code:\n",
    "\n",
    "```python\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "fe.create_table(\n",
    "    name=\"<catalog>.<schema>.<table name>\",\n",
    "    primary_keys=[\"<primary key 1>\", \"<primary key2>\"],\n",
    "    df=data,\n",
    "    description=\"This is some sort of description\",\n",
    "    tags={\"source\": \"bronze\", \"format\": \"delta\"}\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b637096b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Jobs&Workflows\n",
    "\n",
    "Jobs and workflows allows to orchestrate tasks, wich are pieces of code that perform actions on the platform, and build relationships between them.\n",
    "\n",
    "The following table lists teh ways you can define the databricks tasks.\n",
    "\n",
    "| Task Type | Description | Primary Use Case |\n",
    "| :--- | :--- | :--- |\n",
    "| **Notebook Task** | Runs a Databricks notebook written in Python, Scala, SQL, or R. | Executing interactive code, ETL logic, or ML training pipelines. |\n",
    "| **Pipeline Task** | Runs a specified Delta Live Tables (DLT) pipeline. | Orchestrating end-to-end declarative ETL/streaming data pipelines. |\n",
    "| **SQL File Task** | Executes a SQL script file stored in the workspace or a Git repository. | Running complex SQL transformations, DDL, or DML statements. |\n",
    "| **Python Script Task** | Executes a Python file on the cluster using `spark-submit`. | Running standard Python code, often with Spark (PySpark) libraries. |\n",
    "| **Python Wheel Task** | Runs a Python function packaged within a Python Wheel (`.whl`) file. | Running production-grade, modular, and version-controlled Python code. |\n",
    "| **JAR Task** | Executes a compiled Java or Scala application packaged as a JAR file. | Running compiled, production-ready code, typically for complex logic. |\n",
    "| **Spark-Submit Task** | Allows submission of a generic Spark application via the `spark-submit` command. | Running custom or highly specialized Spark applications. |\n",
    "| **dbt Task** | Runs one or more `dbt` (data build tool) commands. | Orchestrating and running dbt projects for data transformation. |\n",
    "| **Run Job Task** | Executes another Databricks Job as a task. | Creating nested, modular, or reusable workflows (Parent-Child jobs). |\n",
    "| **If/Else Condition Task**| Evaluates a condition and controls the execution flow of subsequent tasks. | Adding conditional logic (branching) to a workflow. |\n",
    "| **For Each Task** | Iterates over a collection of input values and runs a nested task for each value. | Parallel processing or batch operations over a list of items. |\n",
    "| **Dashboard Task** | Updates a Databricks SQL Dashboard. | Automating the refresh of business intelligence dashboards. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd71220",
   "metadata": {},
   "source": [
    "### Tasks communication\n",
    "\n",
    "To communicate between tasks you can set and read \"tasks values\".\n",
    "\n",
    "In python code use for that [dbutils.jobs.tasksValue](https://docs.databricks.com/aws/en/dev-tools/databricks-utils#taskvalues-subutility-dbutilsjobstaskvalues):\n",
    "\n",
    "- `dbutils.jobs.taskValues.set(key=\"<key>\", value=\"<value>\")` for setting a value.\n",
    "- `dbutils.jobs.taskValues.get(taskKey=\"<name of the previous task>\", key='key_from_script')` for reading the value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6744f361",
   "metadata": {},
   "source": [
    "## CLI\n",
    "\n",
    "The Databricks CLI allows you to manipulate your Databricks worksspace/account your machine command line. The following table shows corresponding subcommands:\n",
    "\n",
    "| Command group        | Description / purpose                                                                               |\n",
    "| -------------------- | --------------------------------------------------------------------------------------------------- |\n",
    "| **fs**               | Manage files in DBFS / file system (list, copy, delete, read)                                       |\n",
    "| **git-credentials**  | Manage personal access tokens for Databricks to perform operations on behalf of user                |\n",
    "| **repos**            | Manage Git repos within Databricks (import, sync, permissions)                                      |\n",
    "| **secrets**          | Manage secrets, scopes, and access control for secrets                                              |\n",
    "| **workspace**        | Handle workspace contents (notebooks, folders) and permissions                                      |\n",
    "| **cluster-policies** | Control rules and policies for cluster configurations                                               |\n",
    "| **clusters**         | Manage cluster lifecycle and settings                                                               |\n",
    "| **api**              | Call any Databricks REST API directly (for advanced or unsupported endpoints)                       |\n",
    "| **completion**       | Generate shell autocompletion scripts                                                               |\n",
    "| **configure**        | Set up and configure the Databricks CLI (e.g. host, profile)                                        |\n",
    "| **help**             | Display summary and help information for commands                                                   |\n",
    "| **bundle**           | Manage Databricks Asset Bundles (CI/CD-style deployments)                                           |\n",
    "| **labs**             | Work with experimental Labs applications and features in Databricks                                 |\n",
    "| **auth**             | Manage authentication, login, profiles, and tokens                                                  |\n",
    "| **current-user**     | Show information about the currently authenticated user or service principal                        |\n",
    "| **model-registry**   | Manage the workspaceâ€™s MLflow Model Registry: models, versions, transitions, metadata, and webhooks |\n",
    "\n",
    "Check more in:\n",
    "- [What is the Databricks CLI](https://docs.databricks.com/aws/en/dev-tools/cli/).\n",
    "- [Installation guide](https://docs.databricks.com/aws/en/dev-tools/cli/install)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a71514",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "If you have the Databricks CLI installed on your system, you should be able to run following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6337533",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databricks CLI\n",
      "\n",
      "Usage:\n",
      "  databricks [command]\n",
      "\n",
      "Databricks Workspace\n",
      "  fs                                     Filesystem related commands\n",
      "  git-credentials                        Registers personal access token for Databricks to do operations on behalf of the user.\n",
      "  repos                                  The Repos API allows users to manage their git repos.\n",
      "  secrets                                The Secrets API allows you to manage secrets, secret scopes, and access permissions.\n",
      "  workspace                              The Workspace API allows you to list, import, export, and delete notebooks and folders.\n",
      "\n",
      "Compute\n",
      "  cluster-policies                       You can use cluster policies to control users' ability to configure clusters based on a set of rules.\n",
      "  clusters                               The Clusters API allows you to create, start, edit, list, terminate, and delete clusters.\n",
      "  global-init-scripts                    The Global Init Scripts API enables Workspace administrators to configure global initialization scripts for their workspace.\n",
      "  instance-pools                         Instance Pools API are used to create, edit, delete and list instance pools by using ready-to-use cloud instances which reduces a cluster start and auto-scaling times.\n",
      "  instance-profiles                      The Instance Profiles API allows admins to add, list, and remove instance profiles that users can launch clusters with.\n",
      "  libraries                              The Libraries API allows you to install and uninstall libraries and get the status of libraries on a cluster.\n",
      "  policy-compliance-for-clusters         The policy compliance APIs allow you to view and manage the policy compliance status of clusters in your workspace.\n"
     ]
    }
   ],
   "source": [
    "databricks --help | head -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae1d3b8",
   "metadata": {},
   "source": [
    "## Assets bundles\n",
    "\n",
    "The assets bundles is an instruction in a YAML file for managing a Databricks project.\n",
    "\n",
    "There are two importatn concepts in the databricks assets bundles:\n",
    "\n",
    "- The `databricks.yml` file and its configuration allow you to specify the bundle.\n",
    "- The `databricks bundle` subcommand of the databricks CLI allows you to manipulate the bundle.\n",
    "\n",
    "For more details check:\n",
    "\n",
    "- [What is Databricks Asset Bundles](https://docs.databricks.com/aws/en/dev-tools/bundles/).\n",
    "- [Develop Databricks Asset Bundles](https://docs.databricks.com/aws/en/dev-tools/bundles/work-tasks): will guide you through the process of creating and deploying a bundle.\n",
    "- The [`bundle` command group](https://docs.databricks.com/aws/en/dev-tools/cli/bundle-commands) describes the details of the databricks CLI that are responsible for managing bundles.\n",
    "- The [assets bundles](databricks/assets_bundles.ipynb) page in the site."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666bba3e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider the process of creating the simpliest asset bundle.\n",
    "\n",
    "Create the folder and the `databricks.yml` file within it:\n",
    "\n",
    "```yaml\n",
    "bundle:\n",
    "  name: knowledge\n",
    "\n",
    "resources:\n",
    "  jobs:\n",
    "    hello-job:\n",
    "      name: hello-job\n",
    "      tasks:\n",
    "        - task_key: hello-task\n",
    "          notebook_task:\n",
    "            notebook_path: ./hello.ipynb\n",
    "\n",
    "targets:\n",
    "  dev:\n",
    "    default: true\n",
    "```\n",
    "\n",
    "Create `hello.ipynb` as \"project\" defines task based on it.\n",
    "\n",
    "Use the command: `databricks bundle deploy` to push your bundle to the Databricks environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6425b4",
   "metadata": {},
   "source": [
    "After these manipulations, you have to have the corresponding folder in the `.bundles` folder of your db environment. And `hello-job` will be listed in the jobs list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3e1520",
   "metadata": {},
   "source": [
    "To delete the bundle (only in the databricks environment) use the `databricks bundle destroy`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d52d3",
   "metadata": {},
   "source": [
    "## AI&ML\n",
    "\n",
    "Databricks provides a range of tools for building and deploying machine learning solutions. Check the [AI and machine learning on Databricks](https://docs.databricks.com/aws/en/machine-learning/) page for more information.\n",
    "\n",
    "The most usefull services are:\n",
    "\n",
    "- Databricks provides **OpenAI-compatible models** endpoints, so you can access some models using only your databricks credentials. Check more [Get started querying LLMs on Databricks](https://docs.databricks.com/aws/en/large-language-models/llm-serving-intro).\n",
    "- The [Mozaic AI Vector Search](https://docs.databricks.com/aws/en/vector-search/vector-search) for embeddings retrieval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
