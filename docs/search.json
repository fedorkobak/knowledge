[
  {
    "objectID": "math/basics.html",
    "href": "math/basics.html",
    "title": "Basics",
    "section": "",
    "text": "Let we have \\(A\\) event and \\(B\\) event.\nIf \\(A\\) and \\(B\\) occur at the same time, we denote it by the intersection of the events \\(A \\cap B\\).\nIf at least one of the events \\(A\\) and \\(B\\) occurs, we denote it as a union of events \\(A \\cup B\\).\nThe probabilities of all events are related by the formula:\n\\[P(A\\cup B)=P(A)+P(B)-P(A \\cap B)\\]\nSometimes it can be confusing that you need to know the probability that events will occur at the same time to calculate the probability that at least one will occur. But think about it more careful - individual probabilities do not contain any information about how often these events occur together, so how often these events occur together must be learnt separately."
  },
  {
    "objectID": "math/conditional_probability.html",
    "href": "math/conditional_probability.html",
    "title": "Сonditional probability",
    "section": "",
    "text": "Сonditional probability of event A if event B occurs: \\(P(A|B)\\).\nThe expression is valid: \\[P(A|B)P(B)=P(A\\cap B). \\tag{1}\\]\nConditions \\(A\\) and \\(B\\) independent if: \\[P(A|B)=P(A).\\]\nProperty:\n\\[\\sum_{i=1}^n P(A_i)=1 \\Leftrightarrow\\sum_{i=1}^nP(A_i|B)=1.\\]\nIf \\(A_i\\) are only events that can happen, then one of them will definitely occur even under the \\(B\\) condition.\nAnd a useful practical consequence:\n\\[P(A|B) + P(\\overline A|B) = 1.\\]"
  },
  {
    "objectID": "math/conditional_probability.html#basic-ideas",
    "href": "math/conditional_probability.html#basic-ideas",
    "title": "Сonditional probability",
    "section": "",
    "text": "Сonditional probability of event A if event B occurs: \\(P(A|B)\\).\nThe expression is valid: \\[P(A|B)P(B)=P(A\\cap B). \\tag{1}\\]\nConditions \\(A\\) and \\(B\\) independent if: \\[P(A|B)=P(A).\\]\nProperty:\n\\[\\sum_{i=1}^n P(A_i)=1 \\Leftrightarrow\\sum_{i=1}^nP(A_i|B)=1.\\]\nIf \\(A_i\\) are only events that can happen, then one of them will definitely occur even under the \\(B\\) condition.\nAnd a useful practical consequence:\n\\[P(A|B) + P(\\overline A|B) = 1.\\]"
  },
  {
    "objectID": "math/conditional_probability.html#law-of-total-probability",
    "href": "math/conditional_probability.html#law-of-total-probability",
    "title": "Сonditional probability",
    "section": "Law of total probability",
    "text": "Law of total probability\nIf we have event \\(A\\) that depends on \\(n\\) events \\(B_i, i=\\overline{(1,n)}\\) probability of \\(A\\) can be written as:\n\\[P(A)=\\sum_{i=1}^n P(A|B_i)P(B_i)\\]\nWhere \\(B_i, i=\\overline{1,n}\\) - events that don’t happen at the same time, but one of them will.\n\nInteresting task\nSuppose we have an online shop and we have agreed to support a key partner. In a normal situation, users buy the goods of this partner with a probability of 0.05 (without special displays). If we specially show the product on the homepage, the probability of purchase will be 0.2.\nWe promised the partner 1000 sales per day, and the traffic per day is 10000 users (one user makes exactly one purchase per day). What is the minimum probability of showing the product on the homepage so that the sales correspond to our agreement?\nLet’s denote:\n\n\\(A\\) - user bought the good;\n\\(B\\) - good was shown to the user on the main page.\n\nSo:\n\nWe need \\(1000\\) realisations of \\(A\\) in \\(10000\\) events so \\(P(A) = \\frac{1000}{10000}=\\frac{1}{10}\\);\nThe event “we showed good on the main page and it was bought” can be written \\(A|B\\), so \\(P(A|B)=0.2\\);\nThe event “good bought without being displayed on the main page” can be written as \\(A|\\overline B\\), so \\(P(A|\\overline B)=0.05\\).\n\nAnd \\(P(B)\\) is our goal.\nUsing law of total probability we can get:\n\\[P(A)=P(A|B)P(B)+P(A|\\overline B)P(\\overline B) \\Rightarrow\\] \\[P(A)=P(A|B)P(B)+P(A|\\overline B)[1-P(B)]\\]\nIs school equation - \\(P(B)=1/3\\).\nInteresting that my original solution was slightly different:\nLet’s denote:\n\n\\(x\\) - number of users who were shown the good on the main page;\n\\(y\\) - number of users who weren’t shown the good on the main page.\n\nThen the system of equations:\n\\[\n\\begin{cases}\nx+y=10000;\\\\\n0.2*x+0.05*y=1000.\n\\end{cases}\n\\]\nWhich soluton - \\(x=333, y= 667\\). So the final result is \\(333/10000 = 1/3\\).\nBut it’s really the same thing as solution throw law of total probability. Valid expressions:\n\n\\(x = P(B)*10000\\);\n\\(y = P(\\overline B)*10000.\\)\n\nLet’s substitute them into the system of equations:\n\\[\n\\begin{cases}\nP(B)*10000+P(\\overline B)*10000=10000;\\\\\n0.2*P(B)*10000+0.05*P(\\overline B)*10000=1000.\n\\end{cases}\n\\]\nAnd divide both equations by \\(10000\\):\n\\[\n\\begin{cases}\nP(\\overline B)=1-P(B);\\\\\n0.2*P(B)+0.05*P(\\overline B)=1000/10000.\n\\end{cases}\n\\]\nWe are essentially back to the final expression from the first solution."
  },
  {
    "objectID": "math/conditional_probability.html#bayes-formula",
    "href": "math/conditional_probability.html#bayes-formula",
    "title": "Сonditional probability",
    "section": "Bayes formula",
    "text": "Bayes formula\n\nDerivation\nConsider the expression \\((1)\\) - it can be written similarly for \\(B\\):\n\\[P(B|A)P(A)=p(A \\cap B)\\]\nSo we got that:\n\\[P(A|B)P(B)=P(A\\cap B)=P(B|A)P(A)\\]\nFrom here we can get Bayes formula:\n\\[P(A|B)=\\frac{P(B|A)P(A)}{P(B)}.\\]\n\n\nExample task\nTo make clearer why we may need bayes formula let’s consider task.\nWe’ve made two batches of the product, and it’s known that:\n\nIn the first batch there were 5000 products and the probability of rejection is 0.1;\nIn the second batch there were 10000 products and the probability of defect is 0.2.\n\nSuppose we take a random batch. It turned out to be defective. We need to calculate the probability that we took a part from the first batch.\nLet:\n\n\\(A_i, i=\\overline{1,n}\\) - probability that detail is taken from batch \\(i\\);\n\\(B\\) is probability that detail is broken.\n\nSo we have \\(P(A_1)=5000/15000 = 1/3, P(A_2)=10000/15000=2/3, P(B|A_1)=0.1, P(B|A_2)=0.2\\).\nAnd need to find \\(P(A_1|B)\\).\nUsing Bayes formula \\(P(A_1|B)=\\frac{P(B|A_1)P(A_1)}{P(B)}=0.05\\)"
  },
  {
    "objectID": "python/fastapi/run_application.html",
    "href": "python/fastapi/run_application.html",
    "title": "Run application",
    "section": "",
    "text": "In this page I will introduce how to run fastapi applications in general and in particular how to run examples related to fastapi in the site.\nHere you can find out more details about using fastapi in the docker."
  },
  {
    "objectID": "python/fastapi/run_application.html#sec-dockerfile",
    "href": "python/fastapi/run_application.html#sec-dockerfile",
    "title": "Run application",
    "section": "dockerfile",
    "text": "dockerfile\nIn the next cell is the docker file I am using for this example.\n\n%%writefile run_application_files/dockerfile\nFROM python:3.11\nCOPY requrements.txt requrements.txt\nRUN pip3 install -r requrements.txt\nEXPOSE 8000"
  },
  {
    "objectID": "python/fastapi/run_application.html#requrements.txt",
    "href": "python/fastapi/run_application.html#requrements.txt",
    "title": "Run application",
    "section": "requrements.txt",
    "text": "requrements.txt\nPython libraries you only needed to run the fastapi server. It is supposed to be used in the dockerfile described above.\n\n%%writefile run_application_files/requrements.txt\nfastapi==0.103.1\nuvicorn==0.23.2"
  },
  {
    "objectID": "python/fastapi/run_application.html#sec-prog",
    "href": "python/fastapi/run_application.html#sec-prog",
    "title": "Run application",
    "section": "Programme",
    "text": "Programme\nYou need to declare an object of class fastapi.fastAPI. Then use its decorators to add to your functions the ability to respond to certain requests.\nSo in the following example, I create fastapi.fastAPI under the name my_first_app, and create a function that will always respond hello to a get request.\n\n%%writefile run_application_files/get_started.py\nfrom fastapi import FastAPI\n\nmy_first_app = FastAPI()\n\n@my_first_app.get(\"/\")\ndef say_hello():\n    return \"hello\"\n\nOverwriting run_application_files/get_started.py"
  },
  {
    "objectID": "python/fastapi/run_application.html#run",
    "href": "python/fastapi/run_application.html#run",
    "title": "Run application",
    "section": "Run",
    "text": "Run\nTo run the fastapi official documentation recomend to use uvicorn as web-server. So you need to use command:\nuvicon &lt;path to python file with program&gt;:&lt;name of fastapi.fastAPI object in your program&gt;.\nSo in the following example, a docker container is run, tested and stopped:\n\nCreated from the image named fastapi_experiment described in the docker file above;\nWith the name test_container for container;\nThe default port for fastapi is 8000, so port 8000 on the container is connected to port 8000 on the local machine;\nWith volume that allows to read programme;\nAnd with the command created from the required pattern:\n\n--host 0.0.0.0 is used to make the application visible from the container, it’s not necessary if you are using uvicorn without docker.\n\n\n\n!docker run --rm -itd\\\n    --name test_container\\\n    -v ./run_application_files/get_started.py:/get_started.py\\\n    -p 8000:8000 \\\n    fastapi_experiment \\\n    uvicorn --host 0.0.0.0 get_started:my_first_app &gt;/dev/null\n\nSo now you can try it in your browser, but here I use the curl utility - it returns “hello”, just as I declared in the programme.\n\n!curl localhost:8000\n\n\"hello\"\n\n\nAnd we can also check what is happening inside the container. Note that the last line here is the log line for the http request from the previous cell.\n\n!docker logs test_container\n\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\nINFO:     172.17.0.1:45308 - \"GET / HTTP/1.1\" 200 OK\n\n\nDon’t forget to stop the image when you’ve finished playing with the container.\n\n!docker stop test_container &&gt; /dev/null"
  },
  {
    "objectID": "python/fastapi/run_application.html#update-program",
    "href": "python/fastapi/run_application.html#update-program",
    "title": "Run application",
    "section": "Update program",
    "text": "Update program\nThe most convenient way to experiment with a container containing fastapi is to swap the program on the fly, so you can run many examples through one container. That’s why I usually connect the used executing py file as a volume (so that changes on the computer get into the container at once).\nBut to implement it, you also need to run uvicorn with the --reload flag, which will make it track changes in the programme and update with it.\nSo, in the following cells:\n\nThe container stats with a program that sends initial line as a response;\nTry in with curl - all right, got initial line;\nThen change the reload.py file to respond with updated line;\nImmediately try again with the same curl - we’ve got an updated line as response.\n\n\n%%writefile run_application_files/reload.py\nfrom fastapi import FastAPI\n\nmy_first_app = FastAPI()\n\n@my_first_app.get(\"/\")\ndef say_hello():\n    return \"initial line\"\n\nOverwriting run_application_files/reload.py\n\n\n\n!docker run --rm -itd\\\n    --name test_container\\\n    -v ./run_application_files/reload.py:/reload.py\\\n    -p 8000:8000 \\\n    fastapi_experiment \\\n    uvicorn --host 0.0.0.0 --reload reload:my_first_app\n\n752d9632c61a1927a227daf2a8b3de375385e107af1213a0df98c2b7d482d2ef\n\n\n\n!curl localhost:8000\n\n\"initial line\"\n\n\n\n%%writefile run_application_files/reload.py\nfrom fastapi import FastAPI\n\nmy_first_app = FastAPI()\n\n@my_first_app.get(\"/\")\ndef say_hello():\n    return \"updated line\"\n\nOverwriting run_application_files/reload.py\n\n\n\n!curl localhost:8000\n\n\"updated line\"\n\n\n\n!docker stop test_container"
  },
  {
    "objectID": "python/fastapi/pass_arguments.html",
    "href": "python/fastapi/pass_arguments.html",
    "title": "Pass arguments",
    "section": "",
    "text": "To make your programs useful, you need to give them some arguments. On this page I’ll describe how to do that."
  },
  {
    "objectID": "python/fastapi/pass_arguments.html#preparing",
    "href": "python/fastapi/pass_arguments.html#preparing",
    "title": "Pass arguments",
    "section": "Preparing",
    "text": "Preparing\nTo run the examples on this page, you will need:\n\nStart a docker container with fastapi (check run application);\nImport some libraries.\n\n\n# requests is cnetral library\n# to try make requests from python\nimport requests\n\n!docker run --rm -itd\\\n    --name test_container\\\n    -v ./pass_arguments_files/app.py:/app.py\\\n    -p 8000:8000 \\\n    fastapi_experiment \\\n    uvicorn --host 0.0.0.0 --reload app:app\n\n56ef4d5a14722445162bef8427289ec48ba07ff681612b7f4da5f2748295f7ee\n\n\nDon’t forget to stop the container when you’ve finished playing with the examples on this page.\n\n!docker stop test_container\n\ntest_container"
  },
  {
    "objectID": "python/fastapi/pass_arguments.html#query-params",
    "href": "python/fastapi/pass_arguments.html#query-params",
    "title": "Pass arguments",
    "section": "Query params",
    "text": "Query params\nTo define url with parameters in fastapi, you need to define method with parameters and wrap it with fastapi object decorators.\n\nBasics\nIn order to pass an argument using ulr we need to write a construction ?param1=argument1&param2=argument2&...&paramN=argumentN at the end of url (in web development, the name for this construction is query params).\nSo in the following example I have written a program to divide two numbers and use the syntax /divide?a=10&b=2 in the url to complete the division.\n\n%%writefile pass_arguments_files/app.py\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/divide\")\ndef divide(a:int, b:int) -&gt; int:\n    return a/b\n\nOverwriting pass_arguments_files/app.py\n\n\n\nresponse = requests.get(\"http://localhost:8000/divide?a=10&b=2\")\nresponse.text\n\n'5'\n\n\n\n\nTake datetime\nYou are allowed to use datetime.datetime as input type in fastapi. But note that you have to use ISO 8601 fromat for datetime objects - &lt;YYYY-MM-DDTHH-MM-SS&gt;. Where:\n\nYYYYY - year (e.g. 2022);\nMM - month (from 01 to 12);\nDD - day (from 01 to 31);\nT - separator between date and time;\nHH - hour (00 to 23);\nMM - minutes (from 00 to 59);\nSS - seconds (00 to 59).\n\nSo in the following example is the service that takes any date and returns the date for the next day:\n\n%%writefile pass_arguments_files/app.py\nfrom fastapi import FastAPI\nfrom datetime import datetime, timedelta\n\napp = FastAPI()\n\n@app.get(\"/add_year\")\ndef add_year(dt : datetime):\n    return dt + timedelta(days = 1)\n\nOverwriting pass_arguments_files/app.py\n\n\n\nimport requests\nresponse = requests.get(\n    \"http://localhost:8000/add_year?dt=2022-10-05T20:10:10\"\n)\nresponse.text\n\n'\"2022-10-06T20:10:10\"'"
  },
  {
    "objectID": "python/fastapi/pass_arguments.html#json-input",
    "href": "python/fastapi/pass_arguments.html#json-input",
    "title": "Pass arguments",
    "section": "JSON input",
    "text": "JSON input\nTo pass json data to your endpoint, you need to declare a descendant class of pydantic.BaseModel where you describe fields and their pytes to be passed to the endpoint as json. And use an instance of that class as a parameter to your endpoint.\nSo the following fastapi program defines Item which expects json with keys param1 and param2 and just returns line describing what data we’ve got.\n\n%%writefile pass_arguments_files/app.py\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\nclass Item(BaseModel):\n    param1: int\n    param2: str\n\napp = FastAPI()\n\n@app.post(\"/\")\ndef read_json(item:Item):\n    return f\"\"\"I have got:\n    param1={item.param1};\n    param2={item.param2}.\"\"\"\n\nOverwriting pass_arguments_files/app.py\n\n\nAnd here is an example of how you can send a query to such an end point and process its result.\n\nimport requests\nimport json\n\ndata = {\"param1\" : 2, \"param2\": \"test line\"}\nresponse = requests.post(\n    \"http://localhost:8000/\", \n    json=data\n)\nprint(response.content.decode(\"utf-8\").replace(\"\\\\n\", \"\\n\"))\n\n\"I have got:\n    param1=2;\n    param2=test line.\""
  },
  {
    "objectID": "python/fastapi/pass_arguments.html#data-types",
    "href": "python/fastapi/pass_arguments.html#data-types",
    "title": "Pass arguments",
    "section": "Data types",
    "text": "Data types\n\nParameter types\nYou need to declare data types for the arguments otherwise the call will not work correctly.\nThe following example describes a programme without input datatypes. The request to the server causes the error:\n\n%%writefile pass_arguments_files/app.py\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/divide\")\ndef divide(a, b) -&gt; int:\n    return a/b\n\nOverwriting pass_arguments_files/app.py\n\n\n\nresponse = requests.get(\"http://localhost:8000/divide?a=10&b=2\")\nresponse.text\n\n'Internal Server Error'\n\n\n\n\nOutput datatype\nIt does not have to be specified. So in the following example the output data type is not specified and the query is executed without problems.\n\n%%writefile pass_arguments_files/app.py\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/divide\")\ndef divide(a : int, b : int):\n    return a/b\n\nOverwriting pass_arguments_files/app.py\n\n\n\nresponse = requests.get(\"http://localhost:8000/divide?a=10&b=2\")\nresponse.text\n\n'5.0'\n\n\nBut if you have specified a type, you must follow it.\nIn the following example, the GET response function is configured to use int as output.\n\nThe first request called in such a way as to return float - that’s why you got the error;\nThe result of the second request can be interpreted as int, so all is well.\n\n\n%%writefile pass_arguments_files/app.py\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/divide\")\ndef divide(a : int, b : int) -&gt; int:\n    return a/b\n\nOverwriting pass_arguments_files/app.py\n\n\n\nresponse = requests.get(\"http://localhost:8000/divide?a=1&b=2\")\nresponse.text\n\n'Internal Server Error'\n\n\n\nresponse = requests.get(\"http://localhost:8000/divide?a=4&b=2\")\nresponse.text\n\n'2'"
  },
  {
    "objectID": "python/sklearn/columns_transformer.html",
    "href": "python/sklearn/columns_transformer.html",
    "title": "Specific processing for column",
    "section": "",
    "text": "Sometimes different columns need to be transformed in different ways. The most obvious example is the different processing of categorical and numerical columns:\nIt’s easy to build such a transformation yourself, but it’s convenient that sklearn has an out-of-the-box solution that can be easily integrated into sklearn type pipelines - sklearn.compose.ColumnTransformer.\nLearn more here.\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import (\n    OneHotEncoder,\n    StandardScaler,\n    FunctionTransformer\n)\nfrom sklearn.compose import ColumnTransformer\nfrom IPython.display import HTML"
  },
  {
    "objectID": "python/sklearn/columns_transformer.html#basic-example",
    "href": "python/sklearn/columns_transformer.html#basic-example",
    "title": "Specific processing for column",
    "section": "Basic example",
    "text": "Basic example\nSo in the next cell a random data frame is generated, with some categorical and some numerical columns. Let’s show how can be builded component of the pipeline that process categorical columns in one way and numeric in other.\n\nsample_size = 500\nnp.random.seed(10)\n\ngenerate_word = lambda: \"\".join([\n    chr(val) for val in \n    np.random.randint(ord(\"a\"), ord(\"z\") + 1, 10)\n])\nget_cat_var = lambda: np.random.choice(\n    [\n        generate_word() for i in \n        range(np.random.randint(2,7))\n    ], \n    sample_size\n)\nget_num_var = lambda: np.random.normal(\n    np.random.uniform(-1,1), \n    np.random.uniform(1,10),\n    sample_size\n)\n\nvariables_generator = [get_cat_var, get_num_var]\n\ndata_frame = pd.concat(\n    {\n        f\"var {i}\" : \\\n        pd.Series(np.random.choice(variables_generator)())\n        for i in range(20)\n    },\n    axis = 1\n)\n\ndata_frame.head()\n\n\n\n\n\n\n\n\nvar 0\nvar 1\nvar 2\nvar 3\nvar 4\nvar 5\nvar 6\nvar 7\nvar 8\nvar 9\nvar 10\nvar 11\nvar 12\nvar 13\nvar 14\nvar 15\nvar 16\nvar 17\nvar 18\nvar 19\n\n\n\n\n0\n6.352738\nghfmmekjzz\newvspmvrkg\n-3.916784\n0.579251\n3.078876\njljighbmio\niieafcivri\n-3.503851\nhwadgiwzth\nzderdinjyy\n-0.851043\n-7.137998\n-0.990391\n4.128471\nlduutwjjin\n-6.858011\n-3.455499\nkdzpmsglss\nfjogwgrkig\n\n\n1\n-1.562264\nghfmmekjzz\ndlfjbofnbr\n-1.458950\n0.755219\n-0.498048\nphrxnjsbae\niieafcivri\n-5.212578\nyxickhmgkp\nkpqepphruh\n-6.878257\n-1.712574\n-7.783903\n-3.623413\nlduutwjjin\n3.198987\n-7.290196\neywzqkuzza\nfjogwgrkig\n\n\n2\n-2.453819\nbooaisyeuj\ndlfjbofnbr\n-0.124566\n4.070167\n-2.271910\nlzsssmsaim\nvhfoucvgil\n-3.504522\npdzajvgbzz\nynhwdgvtke\n-0.838181\n1.898630\n-6.632060\n-1.394765\nzghwqxiakd\n-14.830121\n10.490557\nirrdfszbwf\nvoumadgklp\n\n\n3\n-0.042513\nbooaisyeuj\nkkagxtgiko\n-6.897858\n-0.065287\n-3.459478\nphrxnjsbae\nyfmijifvmo\n0.742066\nwectjxhbio\nkpqepphruh\n-0.087694\n-1.808818\n0.053985\n0.494845\nlduutwjjin\n-0.341344\n4.539596\neywzqkuzza\ndzlpowvufa\n\n\n4\n-5.946806\nxmtwmxfxpz\ndlfjbofnbr\n7.453730\n-3.450039\n0.091773\njljighbmio\nvhfoucvgil\n1.830545\nhwadgiwzth\nynhwdgvtke\n-0.218426\n0.492733\n-2.954776\n-2.614179\nzghwqxiakd\n-2.672298\n6.436154\nkdzpmsglss\ndzlpowvufa\n\n\n\n\n\n\n\nTo prepare a transformer that handles different columns in different ways, you need to pass a list of your transformers to the transformers parameter of the sklearn.compose.ColumnTransformer constructor.\nEach element of the transformers list should be of the form (&lt;transformer name&gt;, &lt;transformer class&gt;, &lt;columns that will use this transformer&gt;).\nSo in the following cell we have created such an object, showing how it will look in the Jupyter output and possible results of this transformation for the data frame described above.\n\nnumeric_columns = list(data_frame.select_dtypes(\"number\").columns)\ncategorical_columns = list(set(data_frame.columns) - set(numeric_columns))\n\nmy_transformer = ColumnTransformer(\n    transformers = [\n        (\"one_hot_encoder\", OneHotEncoder(), categorical_columns),\n        (\"standart_scaler\", StandardScaler(), numeric_columns)\n    ]\n)\n\ndisplay(HTML(\"&lt;p style=\\\"font-size:20px\\\"&gt;Class display in jupyter&lt;/p&gt;\"))\ndisplay(my_transformer)\ndisplay(HTML(\"&lt;p style=\\\"font-size:20px\\\"&gt;Fit and transfrom result&lt;/p&gt;\"))\ndisplay(\n    pd.DataFrame(\n        my_transformer.fit_transform(data_frame)\n    ).head()\n)\n\nClass display in jupyter\n\n\nColumnTransformer(transformers=[('one_hot_encoder', OneHotEncoder(),\n                                 ['var 6', 'var 2', 'var 7', 'var 9', 'var 1',\n                                  'var 10', 'var 15', 'var 18', 'var 19']),\n                                ('standart_scaler', StandardScaler(),\n                                 ['var 0', 'var 3', 'var 4', 'var 5', 'var 8',\n                                  'var 11', 'var 12', 'var 13', 'var 14',\n                                  'var 16', 'var 17'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformerColumnTransformer(transformers=[('one_hot_encoder', OneHotEncoder(),\n                                 ['var 6', 'var 2', 'var 7', 'var 9', 'var 1',\n                                  'var 10', 'var 15', 'var 18', 'var 19']),\n                                ('standart_scaler', StandardScaler(),\n                                 ['var 0', 'var 3', 'var 4', 'var 5', 'var 8',\n                                  'var 11', 'var 12', 'var 13', 'var 14',\n                                  'var 16', 'var 17'])])one_hot_encoder['var 6', 'var 2', 'var 7', 'var 9', 'var 1', 'var 10', 'var 15', 'var 18', 'var 19']OneHotEncoderOneHotEncoder()standart_scaler['var 0', 'var 3', 'var 4', 'var 5', 'var 8', 'var 11', 'var 12', 'var 13', 'var 14', 'var 16', 'var 17']StandardScalerStandardScaler()\n\n\nFit and transfrom result\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n\n\n\n\n0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n-1.169878\n0.128906\n1.319012\n-0.962918\n-0.316245\n-1.378868\n-0.008420\n1.003985\n-0.811118\n-0.795706\n\n\n1\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n...\n-0.391908\n0.160042\n-0.407646\n-1.467765\n-2.575318\n-0.360064\n-1.728754\n-1.025626\n0.436845\n-1.531170\n\n\n2\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.030460\n0.746601\n-1.263927\n-0.963117\n-0.311424\n0.318060\n-1.437070\n-0.442118\n-1.800370\n1.879037\n\n\n3\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n...\n-2.113468\n0.014859\n-1.837191\n0.291547\n-0.030132\n-0.378137\n0.256049\n0.052623\n-0.002472\n0.737690\n\n\n4\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n2.429194\n-0.584051\n-0.122927\n0.613140\n-0.079132\n0.054056\n-0.505865\n-0.761387\n-0.291717\n1.101435\n\n\n\n\n5 rows × 47 columns"
  },
  {
    "objectID": "python/sklearn/columns_transformer.html#no-transformations",
    "href": "python/sklearn/columns_transformer.html#no-transformations",
    "title": "Specific processing for column",
    "section": "No transformations",
    "text": "No transformations\nIf you need to build a transformer that doesn’t change some of the columns in any way, you can use FunctionTransformer(lambda x: x) for columns you need to keep value in- is a transformer that returns the orginal value of the input column. So you’ll get the original value of the column.\nIn the following example we use a dummy transformer and a standard scaler for each input column. As you can see, this type of transformation leaves the columns untransformed.\n\nnp.random.seed(10)\nsample_size = 10\n\ndf = pd.DataFrame({\n    \"col1\" : np.random.uniform(5, 10, sample_size),\n    \"col2\" : np.random.normal(5, 10, sample_size)\n})\n\ndisplay(HTML(\"&lt;h3&gt;Input frame&lt;/h3&gt;\"))\ndisplay(df)\n\ndf = ColumnTransformer(\n    transformers = [\n        (\"dummy\", FunctionTransformer(lambda x: x), [\"col1\", \"col2\"]),\n        (\"standart_scaler\", StandardScaler(), [\"col1\", \"col2\"])\n    ]\n).fit_transform(df)\ndisplay(HTML(\"&lt;h3&gt;Transformation result&lt;/h3&gt;\"))\npd.DataFrame(df)\n\nInput frame\n\n\n\n\n\n\n\n\n\ncol1\ncol2\n\n\n\n\n0\n8.856603\n7.655116\n\n\n1\n5.103760\n6.085485\n\n\n2\n8.168241\n5.042914\n\n\n3\n8.744019\n3.253998\n\n\n4\n7.492535\n9.330262\n\n\n5\n6.123983\n17.030374\n\n\n6\n5.990314\n-4.650657\n\n\n7\n8.802654\n15.282741\n\n\n8\n5.845554\n7.286301\n\n\n9\n5.441699\n9.451376\n\n\n\n\n\n\n\nTransformation result\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n8.856603\n7.655116\n1.258270\n0.013588\n\n\n1\n5.103760\n6.085485\n-1.365599\n-0.258714\n\n\n2\n8.168241\n5.042914\n0.776989\n-0.439580\n\n\n3\n8.744019\n3.253998\n1.179555\n-0.749924\n\n\n4\n7.492535\n9.330262\n0.304557\n0.304194\n\n\n5\n6.123983\n17.030374\n-0.652291\n1.640020\n\n\n6\n5.990314\n-4.650657\n-0.745748\n-2.121234\n\n\n7\n8.802654\n15.282741\n1.220550\n1.336838\n\n\n8\n5.845554\n7.286301\n-0.846960\n-0.050395\n\n\n9\n5.441699\n9.451376\n-1.129323\n0.325205"
  },
  {
    "objectID": "python/pandas/groupby.html",
    "href": "python/pandas/groupby.html",
    "title": "Groupby",
    "section": "",
    "text": "pd.DataFrame.groupby is a very useful tool, but sometimes working with it can be a bit confusing. So in this page I want to pay more attention to some functions and cases.\nThe most useful page for learning is GroupBy object in pandas documentation."
  },
  {
    "objectID": "python/pandas/groupby.html#basic-frame",
    "href": "python/pandas/groupby.html#basic-frame",
    "title": "Groupby",
    "section": "Basic frame",
    "text": "Basic frame\nThere are many examples of the same type in this section, so unless specified by default I will use the dataset declared below.\n\nimport pandas as pd\nfrom IPython.display import HTML\n\nbasic_frame = pd.DataFrame({'A': ['a', 'a', 'b', 'b', 'c', 'c'],\n                   'B': [2, 1, 3, 4, 6, 5],\n                   'C': [10, 20, 30, 40, 50, 60]})\n\nbasic_frame\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\na\n2\n10\n\n\n1\na\n1\n20\n\n\n2\nb\n3\n30\n\n\n3\nb\n4\n40\n\n\n4\nc\n6\n50\n\n\n5\nc\n5\n60"
  },
  {
    "objectID": "python/pandas/groupby.html#pd.groupby",
    "href": "python/pandas/groupby.html#pd.groupby",
    "title": "Groupby",
    "section": "pd.groupby",
    "text": "pd.groupby\nHere I describe basic usage of pandas.groupby function. For a formal description, see the official pandas documntation for this function.\n\nas_idnex\nSetting this value to True allows you to say that the aggregation variable should not be used as an index.\nSo in the following example I show the difference.\nNote In the first case the return is padnas.Series just because I called it that way, but in the second case it’s not a dataframe that pandas has to use as a result.\n\ndisplay(HTML(\"&lt;b&gt;as_index=True&lt;/b&gt;\"))\ndisplay(basic_frame.groupby(\"A\", as_index=True)[\"C\"].sum())\ndisplay(HTML(\"&lt;b&gt;as_index=False&lt;/b&gt;\"))\ndisplay(basic_frame.groupby(\"A\", as_index=False)[\"C\"].sum())\n\nas_index=True\n\n\nA\na     30\nb     70\nc    110\nName: C, dtype: int64\n\n\nas_index=False\n\n\n\n\n\n\n\n\n\nA\nC\n\n\n\n\n0\na\n30\n\n\n1\nb\n70\n\n\n2\nc\n110\n\n\n\n\n\n\n\n\n\nobserved\nIn the categorical datatype there is a possible case where a category exists but never appears in series'. This parameter describes whether unobserved catetories will be used ingroupby` results (False) or only observed categories will be used (True).\nSo in the following example I changed a datatype for the A column to category, added a new category l but no new observation corresponding to this category, and finally tried all options for the observed parameter. In the first case we don’t have l in the groupby result index, in the second we do.\n\nexample_frame = basic_frame.copy()\nexample_frame[\"A\"] = example_frame[\"A\"].\\\n                        astype(\"category\").cat.\\\n                        add_categories(\"l\")\n\ndisplay(\n    HTML(\"&lt;b style=\\\"font-size:120%\\\"&gt;=====observed=True=====&lt;/b&gt;\")\n)\ndisplay(\n    example_frame.groupby(\"A\", observed=True).sum()\n)\n\ndisplay(\n    HTML(\"&lt;b style=\\\"font-size:120%\\\"&gt;=====observed=False=====&lt;/b&gt;\")\n)\ndisplay(\n    example_frame.groupby(\"A\", observed=False).sum()\n)\n\n=====observed=True=====\n\n\n\n\n\n\n\n\n\nB\nC\n\n\nA\n\n\n\n\n\n\na\n3\n30\n\n\nb\n7\n70\n\n\nc\n11\n110\n\n\n\n\n\n\n\n=====observed=False=====\n\n\n\n\n\n\n\n\n\nB\nC\n\n\nA\n\n\n\n\n\n\na\n3\n30\n\n\nb\n7\n70\n\n\nc\n11\n110\n\n\nl\n0\n0"
  },
  {
    "objectID": "python/pandas/groupby.html#iterating",
    "href": "python/pandas/groupby.html#iterating",
    "title": "Groupby",
    "section": "Iterating",
    "text": "Iterating\nYou can iterate trow pandas.DataFrame.groupby retults. In each eteration you will get tuple of two values:\n\nValue of the grouping variable for this iteration;\nSub-sampling from the original data set corresponding to the considered value of the grouping variable.\n\nSo in the following example I show the result of the first iteration under pandas.DataFrameGroupby result and then show a case of using it in the cycle.\n\ndisplay(HTML(\"&lt;b&gt;Some iteration returns&lt;/b&gt;\"))\ndisplay(next(basic_frame.groupby(\"A\").__iter__()))\n\ndisplay(HTML(\"&lt;b&gt;Whole cycle&lt;/b&gt;\"))\nfor a_val, subframe in basic_frame.groupby(\"A\"):\n    print(\"====\" + a_val + \"=====\")\n    display(subframe)\n\nSome iteration returns\n\n\n('a',\n    A  B   C\n 0  a  2  10\n 1  a  1  20)\n\n\nWhole cycle\n\n\n====a=====\n====b=====\n====c=====\n\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\na\n2\n10\n\n\n1\na\n1\n20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n2\nb\n3\n30\n\n\n3\nb\n4\n40\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n4\nc\n6\n50\n\n\n5\nc\n5\n60"
  },
  {
    "objectID": "python/pandas/groupby.html#external-group-array",
    "href": "python/pandas/groupby.html#external-group-array",
    "title": "Groupby",
    "section": "External group array",
    "text": "External group array\nYou can use an arbitrary array (that is not a column of the dataframe being grouped) for grouping.\nSo in the following example I use list shat markers to split the dataframe into two groups x and y.\n\ngroup_list = [\"x\", \"x\", \"x\", \"y\", \"y\", \"y\"]\ndisplay(HTML(\"&lt;b&gt;Input dataframe&lt;/b&gt;\"))\ndisplay(basic_frame)\ndisplay(HTML(\"&lt;b&gt;Group variable&lt;/b&gt;\"))\ndisplay(group_list)\nbasic_frame.groupby(group_list).sum()\n\nInput dataframe\n\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\na\n2\n10\n\n\n1\na\n1\n20\n\n\n2\nb\n3\n30\n\n\n3\nb\n4\n40\n\n\n4\nc\n6\n50\n\n\n5\nc\n5\n60\n\n\n\n\n\n\n\nGroup variable\n\n\n['x', 'x', 'x', 'y', 'y', 'y']\n\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\nx\naab\n6\n60\n\n\ny\nbcc\n15\n150\n\n\n\n\n\n\n\nYou can even mix two external variables.\n\ngroup_list1 = [\"x\", \"x\", \"x\", \"y\", \"y\", \"y\"]\ngroup_list2 = [1,1,2,2,2,1]\ndisplay(HTML(\"&lt;b&gt;Group variables&lt;/b&gt;\"))\ndisplay(group_list1, group_list2)\nbasic_frame.groupby([group_list1, group_list2]).sum()\n\nGroup variables\n\n\n['x', 'x', 'x', 'y', 'y', 'y']\n\n\n[1, 1, 2, 2, 2, 1]\n\n\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\nx\n1\naa\n3\n30\n\n\n2\nb\n3\n30\n\n\ny\n1\nc\n5\n60\n\n\n2\nbc\n10\n90\n\n\n\n\n\n\n\nOr mix external and internal variables in a groupby.\n\ngroup_list = [\"x\", \"x\", \"x\", \"y\", \"y\", \"y\"]\ndisplay(HTML(\"&lt;b&gt;Group variable&lt;/b&gt;\"))\ndisplay(group_list)\nbasic_frame.groupby([group_list1, \"A\"]).sum()\n\nGroup variable\n\n\n['x', 'x', 'x', 'y', 'y', 'y']\n\n\n\n\n\n\n\n\n\n\nB\nC\n\n\n\nA\n\n\n\n\n\n\nx\na\n3\n30\n\n\nb\n3\n30\n\n\ny\nb\n4\n40\n\n\nc\n11\n110"
  },
  {
    "objectID": "python/pandas/groupby.html#agg---rule-by-dict",
    "href": "python/pandas/groupby.html#agg---rule-by-dict",
    "title": "Groupby",
    "section": "agg - rule by dict",
    "text": "agg - rule by dict\nThis is a way to apply aggregation functions using syntax {&lt;var_name_1&gt;:&lt;aggregation_function_1&gt;, &lt;var_name_2&gt;:&lt;aggregation_function_2&gt;, ...}.\nSo in the following example, I use the above syntax to aggregate max B values and sum of C values by A subsets:\n\ndisplay(HTML(\"&lt;b&gt;Aggregation&lt;/b&gt;\"))\ndisplay(basic_frame.groupby(\"A\").agg({\"B\":\"max\", \"C\":\"sum\"}))\n\nAggregation\n\n\n\n\n\n\n\n\n\nB\nC\n\n\nA\n\n\n\n\n\n\na\n2\n30\n\n\nb\n4\n70\n\n\nc\n6\n110"
  },
  {
    "objectID": "python/pandas/groupby.html#apply---combine-results",
    "href": "python/pandas/groupby.html#apply---combine-results",
    "title": "Groupby",
    "section": "apply - combine results",
    "text": "apply - combine results\nPandas documentation about apply function.\n\nBasic idea\nThe peculiarity of this method is that it uses pandas.DataFrame as the input for the aggregation function.\nThe following example shows this: `example_funtion’ just prints the input and it always prints a DataFrame for each “A” variable option.\n\ndef example_funtion(subdf):\n    print(\"=========\")\n    print(subdf)\n    return 5\n\nres = basic_frame.groupby(\"A\")[\n    [\"A\", \"B\", \"C\"]\n].apply(example_funtion)\n\n=========\n   A  B   C\n0  a  2  10\n1  a  1  20\n=========\n   A  B   C\n2  b  3  30\n3  b  4  40\n=========\n   A  B   C\n4  c  6  50\n5  c  5  60\n\n\n\n\nUse case\nSo it’s perfect for cases where you need to get, for each variant of variable A, some value of variable C conditioned on the value of variable B.\nIn particular, the following example shows how to obtain for each option of “A” the “C” value corresponding to the minimum “B” value.\n\nFor \"A\" == \"a\" I got \"C\" == 20, because it corresponds to \"B\"== 1, which is the minimum for every \"A\" == \"a\";\nFor \"A\" == \"b\" I got \"C\" == 30, because it corresponds to \"B\"== 3, which is the minimum for every \"A\" == \"b\";\nFor \"A\" == \"c\" I got \"C\" == 60, because it corresponds to \"B\"== 5, which is the minimum for every \"A\" == \"c\".\n\n\nresult = basic_frame.groupby(\"A\")[[\"B\", \"C\"]].apply(\n    lambda subset: subset.loc[subset[\"B\"].idxmin(), \"C\"]\n)\ndisplay(HTML(\"&lt;b&gt;Result&lt;/b&gt;\"))\nresult.rename(\"C\").to_frame()\n\nResult\n\n\n\n\n\n\n\n\n\nC\n\n\nA\n\n\n\n\n\na\n20\n\n\nb\n30\n\n\nc\n60\n\n\n\n\n\n\n\n\n\nvs agg\nOther common function may seem useless because this function can do everything they can. However, according to the pandas documentation, they may work a little faster. I have not been able to test this yet."
  },
  {
    "objectID": "python/pandas/groupby.html#transform",
    "href": "python/pandas/groupby.html#transform",
    "title": "Groupby",
    "section": "transform",
    "text": "transform\nThis is a function that allows you to get aggregations as pandas.Series/pandas.DataFrame indexed like the original pandas.DataFrame.\nFor example, in the following cell, throw the transform function, for each record in the original pandas.DataFrame I got the mean value of B for each group in A.\n\ntemp_frame = basic_frame.copy()\n\ntemp_frame[\"mean B by A\"] = (\n    temp_frame.\n    groupby(\"A\")[\"B\"].\n    transform(\"mean\")\n)\ndisplay(temp_frame)\n\n\n\n\n\n\n\n\nA\nB\nC\nmean B by A\n\n\n\n\n0\na\n2\n10\n1.5\n\n\n1\na\n1\n20\n1.5\n\n\n2\nb\n3\n30\n3.5\n\n\n3\nb\n4\n40\n3.5\n\n\n4\nc\n6\n50\n5.5\n\n\n5\nc\n5\n60\n5.5\n\n\n\n\n\n\n\nHere I have a pandas.DataFrame that for each record from the original pandas.DataFrame matches the mean value of the B and C columns to the A column in a command.\n\ndisplay(\n    temp_frame.\n    groupby(\"A\")[[\"B\", \"C\"]].\n    transform(\"mean\")\n)\n\n\n\n\n\n\n\n\nB\nC\n\n\n\n\n0\n1.5\n15.0\n\n\n1\n1.5\n15.0\n\n\n2\n3.5\n35.0\n\n\n3\n3.5\n35.0\n\n\n4\n5.5\n55.0\n\n\n5\n5.5\n55.0"
  },
  {
    "objectID": "python/pandas/groupby.html#sum",
    "href": "python/pandas/groupby.html#sum",
    "title": "Groupby",
    "section": "sum",
    "text": "sum\nThe basic function that allows you to get sums by groups.\n\nFor str dtype\nIf you apply the sum function to a variable containing a str datatype, it will concatenate observations by groups.\nSo in the following example, this just happened with the group text column of the test dataframe.\n\ntest_df = pd.DataFrame({\n    \"group class\" : [\"a\", \"a\", \"b\", \"b\"],\n    \"group numeric\" : [3,4,5,1],\n    \"group text\" : [\"hello\", \"test\", \"line3\", \"superline\"]\n})\ndisplay(HTML(\"&lt;b&gt;Initial frame&lt;/b&gt;\"))\ndisplay(test_df)\ndisplay(HTML(\"&lt;b&gt;Aggregation result&lt;/b&gt;\"))\ntest_df.groupby(\"group class\").sum()\n\nInitial frame\n\n\n\n\n\n\n\n\n\ngroup class\ngroup numeric\ngroup text\n\n\n\n\n0\na\n3\nhello\n\n\n1\na\n4\ntest\n\n\n2\nb\n5\nline3\n\n\n3\nb\n1\nsuperline\n\n\n\n\n\n\n\nAggregation result\n\n\n\n\n\n\n\n\n\ngroup numeric\ngroup text\n\n\ngroup class\n\n\n\n\n\n\na\n7\nhellotest\n\n\nb\n6\nline3superline"
  },
  {
    "objectID": "python/pandas/loc.html",
    "href": "python/pandas/loc.html",
    "title": "<DataFrame/Series>.loc",
    "section": "",
    "text": "loc is a way of selecting elements from DataFrames/Series using index/column names.\nIn the next cell I create the data frame for the experiments.\nimport pandas as pd\nimport numpy as np\nfrom IPython.display import HTML\n\nsample_size = 10\n\ntest_df = pd.DataFrame(\n    {\n        \"a\":np.random.choice(range(10), sample_size),\n        \"b\":np.random.choice(range(10), sample_size)\n    },\n    index = [chr(i) for i in range(ord(\"a\"), ord(\"a\")+sample_size)]\n)\ntest_df\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\na\n1\n3\n\n\nb\n0\n6\n\n\nc\n7\n7\n\n\nd\n1\n6\n\n\ne\n3\n9\n\n\nf\n4\n8\n\n\ng\n4\n6\n\n\nh\n1\n0\n\n\ni\n9\n1\n\n\nj\n8\n5"
  },
  {
    "objectID": "python/pandas/loc.html#basic",
    "href": "python/pandas/loc.html#basic",
    "title": "<DataFrame/Series>.loc",
    "section": "Basic",
    "text": "Basic\nYou can apply slices by index values.\nNote Although basic python slices use don’t include the last element, pandas slices ignore this rule and will include it.\nSo the following example just shows it - I’ve got a slice from e to h including h:\n\ntest_df[\"e\":\"h\"]\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\ne\n6\n6\n\n\nf\n1\n8\n\n\ng\n4\n0\n\n\nh\n6\n7"
  },
  {
    "objectID": "python/pandas/loc.html#only-df-order",
    "href": "python/pandas/loc.html#only-df-order",
    "title": "<DataFrame/Series>.loc",
    "section": "Only df order",
    "text": "Only df order\nYou should only mention elements in the order in which they appear in the dataframe - there’s no ascending rule for slicing.\nIn the following example I’m trying to apply the same slicing as before to inverted dataframe and got the empty slice because in dataframe’s order “h” element is higher than “e”.\n\ninv_df = test_df.iloc[::-1]\ndisplay(HTML(\"&lt;b&gt;Indersed dataframe&lt;/b&gt;\"))\ndisplay(inv_df)\ndisplay(HTML(\"&lt;b&gt;Slice&lt;/b&gt;\"))\ndisplay(inv_df[\"e\":\"h\"])\n\nIndersed dataframe\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\nj\n9\n7\n\n\ni\n5\n3\n\n\nh\n6\n7\n\n\ng\n4\n0\n\n\nf\n1\n8\n\n\ne\n6\n6\n\n\nd\n0\n9\n\n\nc\n8\n8\n\n\nb\n9\n8\n\n\na\n0\n9\n\n\n\n\n\n\n\nSlice\n\n\n\n\n\n\n\n\n\na\nb"
  },
  {
    "objectID": "python/pandas/data_types/columns_by_types.html",
    "href": "python/pandas/data_types/columns_by_types.html",
    "title": "Columns by types",
    "section": "",
    "text": "Sometimes it’s really useful to be able to select columns with specific data types. For example, in machine learning it’s very common to process numeric and categorical columns in different ways."
  },
  {
    "objectID": "python/pandas/data_types/columns_by_types.html#experimental-frame",
    "href": "python/pandas/data_types/columns_by_types.html#experimental-frame",
    "title": "Columns by types",
    "section": "Experimental frame",
    "text": "Experimental frame\nThe following cell creates a data frame with columns of different datatypes.\n\nimport pandas as pd\nimport numpy as np\n\nfrom IPython.display import HTML\n\nnp.random.seed(10)\n     \nsample_shape = (100, 20)\ngenerate_numeric = lambda sample_size: np.random.normal(0, 1, sample_size)\ngenerate_str = lambda sample_size:[\n    \"\".join(map(chr, np.random.randint(low = ord(\"a\"), high = ord(\"z\"), size=10))) \n    for i in range(sample_size)\n]\n\ntest_frame = pd.DataFrame({\n    f\"column{i}\" : np.random.choice([generate_numeric, generate_str])(sample_shape[0])\n    for i in range(sample_shape[1])\n})\n\ntest_frame.head(5)\n\n\n\n\n\n\n\n\ncolumn0\ncolumn1\ncolumn2\ncolumn3\ncolumn4\ncolumn5\ncolumn6\ncolumn7\ncolumn8\ncolumn9\ncolumn10\ncolumn11\ncolumn12\ncolumn13\ncolumn14\ncolumn15\ncolumn16\ncolumn17\ncolumn18\ncolumn19\n\n\n\n\n0\neparqrijak\n-0.885237\nicveuvsnde\ncbrwxbagsa\njoojouhnsa\ncmuhdgunmu\nnmfjqsnnvq\n0.900117\n-1.269680\n1.673854\n-0.981047\nwxejdwfwks\nywexlugyio\noueqgttxsx\neksymjwner\nxyowiyxxfa\ngxcccqjhsp\ntwhcjgunju\ngrccysdkoh\n0.114989\n\n\n1\niwetqeplwy\n0.322590\nbfkypaqefr\nfjkhhelgvn\ncfpvrmueij\ncrsvpfjehm\npfhardywns\n1.762525\n1.519998\n0.085414\n1.083912\npwtchygtok\ndpvswtdmvg\nlvsxcxvnpp\ngwunqbkhdh\nubcrvcnpgs\nmauwtacklp\nyvfdtqbixn\nxwrvapaqbs\n1.247267\n\n\n2\nlbieortwnf\n-0.449772\nsiavscpewk\nmhnjrhtnau\nxaxeyfoxvc\ntbsrqpmmew\ntxeqmsqaau\n-1.020418\n0.898286\n1.805783\n1.103664\nxitnomidxt\nfgyktnnake\nrnxbuhgnnt\nbcjbulycwx\novqimfwygl\njmdnepbygg\nnrgyocwhqx\nytwxdqablj\n-1.462726\n\n\n3\nntnwmbesnw\n0.790567\nksqksaxcvf\nxdfgbltktd\nrwmjlcjiun\njeixeeqhgc\njaksucxefd\n-0.017661\n-0.274982\n-0.077126\n-0.643199\ngddfxpsqfv\ndnjqsbynyw\nqxjyvrnewn\nxnyptpulyq\nepwspoqbyg\nbsvjpmulwr\ncwywpacbma\nkrhtlbypnu\n-0.517101\n\n\n4\nxylkyjpsqw\n1.690074\nrebhymnhun\nrjbandmwys\nqectgjuiwb\nwcrrgxwbdl\nnutkvjctxv\n-1.259420\n-0.229280\n-0.698492\n-0.351349\nqooeitqcjb\nudrqckibws\nqacerqmmxk\nbctmrcaypo\nexnlqjeilv\njmcoelgcbu\nycqvdihklu\noqakhlnuux\n0.121970"
  },
  {
    "objectID": "python/pandas/data_types/columns_by_types.html#select-numeric-columns",
    "href": "python/pandas/data_types/columns_by_types.html#select-numeric-columns",
    "title": "Columns by types",
    "section": "Select numeric columns",
    "text": "Select numeric columns\nJust use pandas.DataFrame.select_dtypes(\"number\"). So in the following cell I’ll use this syntax and show that the results are correct.\n\nnumeric_columns = test_frame.select_dtypes(\"number\")\n\ndisplay(HTML(\"&lt;b&gt;Head&lt;/b&gt;\"))\ndisplay(numeric_columns.head())\ndisplay(HTML(\"&lt;b&gt;Data types&lt;/b&gt;\"))\ndisplay(pd.Series(numeric_columns.dtypes, name = \"Data type\").to_frame())\n\nHead\n\n\n\n\n\n\n\n\n\ncolumn1\ncolumn7\ncolumn8\ncolumn9\ncolumn10\ncolumn19\n\n\n\n\n0\n-0.885237\n0.900117\n-1.269680\n1.673854\n-0.981047\n0.114989\n\n\n1\n0.322590\n1.762525\n1.519998\n0.085414\n1.083912\n1.247267\n\n\n2\n-0.449772\n-1.020418\n0.898286\n1.805783\n1.103664\n-1.462726\n\n\n3\n0.790567\n-0.017661\n-0.274982\n-0.077126\n-0.643199\n-0.517101\n\n\n4\n1.690074\n-1.259420\n-0.229280\n-0.698492\n-0.351349\n0.121970\n\n\n\n\n\n\n\nData types\n\n\n\n\n\n\n\n\n\nData type\n\n\n\n\ncolumn1\nfloat64\n\n\ncolumn7\nfloat64\n\n\ncolumn8\nfloat64\n\n\ncolumn9\nfloat64\n\n\ncolumn10\nfloat64\n\n\ncolumn19\nfloat64"
  },
  {
    "objectID": "python/pandas/data_visualisation.html",
    "href": "python/pandas/data_visualisation.html",
    "title": "Data visualisation",
    "section": "",
    "text": "Pandas has some integration with matplotlib. It’s difficult to create advanced plots from pandas, but for purposes of instant visualisation it can be helpful.\n\nplot\npandas.DataFrame has a plot function that is only for line plots using variables from the dataframe. You can specify:\n\nx variable name for x-axis, x-axis title will be the same;\ny variable name for y-axis, y-axis title will be the same;\nfigsize to adjust the size of the diagram;\nMany parameters used in the classic matplotlib.pyplot.plot function.\n\n\nimport numpy as np\nimport pandas as pd\n\n\nx = np.arange(0, 10, 0.1)\ndf = pd.DataFrame({\n    \"x\" : x, 'y':x*3 + np.random.normal(0, 1, len(x)),\n})\n\nans = df.plot(\n    x=\"x\", y=\"y\",\n    figsize = (14,5),\n    grid = True\n)\n\n\n\n\n\n\nhist\nYou can create a histogram based on the values of some pandas.Series from this object only.\nThe arguments are really close to matplotlib.hist except that:\n\nfigsize you can set figure size just from that function.\n\nSo in the following example, I use all these features to show the skewness of a normally distributed variable.\n\nimport numpy as np\nimport pandas as pd\n\nvis_ser = pd.Series(np.random.normal(0, 1, 1000), name = \"some variable\")\nans = vis_ser.hist(bins = 20, figsize = (3, 3))"
  },
  {
    "objectID": "python/advanced/multi_threads.html",
    "href": "python/advanced/multi_threads.html",
    "title": "Multithreads in python",
    "section": "",
    "text": "Official documentarion for threading module;\nThird-party tutorial with examples (python2)."
  },
  {
    "objectID": "python/advanced/multi_threads.html#sources",
    "href": "python/advanced/multi_threads.html#sources",
    "title": "Multithreads in python",
    "section": "",
    "text": "Official documentarion for threading module;\nThird-party tutorial with examples (python2)."
  },
  {
    "objectID": "python/advanced/multi_threads.html#example1---runtime-messages",
    "href": "python/advanced/multi_threads.html#example1---runtime-messages",
    "title": "Multithreads in python",
    "section": "Example1 - runtime messages",
    "text": "Example1 - runtime messages\nIn following exmaple:\n\nDefine the myThread class as an ancestor of the threading.Thread class - it will describe the behaviour of the thread:\n\nThe run method is executed when the thread is started - in this case method:\n\nSpends time according to the delay parameter of the class;\nIncreases the value of counter by one.\n\n\nCreate and run two instances of myThread:\n\nWith delay 0.5 and 1;\nWith name “first” and “second”;\n\nIn cycle every 0.5 seconds check is threads still alife and print counter values for each instance;\nResult:\n\ncounter of instance named “first” updated at each step;\ncounter of instance named “second” is updated once every two steps.\n\n\n\nimport threading\nimport time\n\nsteps_count = 5\n\nclass myThread(threading.Thread):\n    def __init__(self, delay, name):\n        super().__init__()\n        self.counter = 0\n        self.delay = delay\n        self.name = name\n\n    def run(self):\n        for i in range(steps_count):\n            time.sleep(self.delay)\n            self.counter += 1\n\nthreads = []\n\n# Create new threads\nthread1 = myThread(0.5, \"first\")\nthread2 = myThread(1, \"second\")\n\nbegin_time = time.time()\n# Start new Threads\nthread1.start()\nthread2.start()\n\n# Add threads to thread list\nthreads.append(thread1)\nthreads.append(thread2)\n\nis_any_thread_live = True\nwhile is_any_thread_live:\n    time.sleep(0.5)\n\n    dislpay_line = \\\n        f\"====={round(time.time() - begin_time, 1)}\" + \\\n        \" seconds after begining=====\"\n    print(dislpay_line)\n    for t in threads:\n        print(t.name + \" counter = \" + str(t.counter))\n    \n    for t in threads:\n        if t.is_alive():\n            # if any thread alife\n            # i leave the cycle\n            break\n    else:\n        # if we tried all threads\n        # and there aren't any alife \n        # set flag for cycle leaving\n        is_any_thread_live = False\n\n=====0.5 seconds after begining=====\nfirst counter = 1\nsecond counter = 0\n=====1.0 seconds after begining=====\nfirst counter = 2\nsecond counter = 1\n=====1.5 seconds after begining=====\nfirst counter = 3\nsecond counter = 1\n=====2.0 seconds after begining=====\nfirst counter = 4\nsecond counter = 2\n=====2.5 seconds after begining=====\nfirst counter = 5\nsecond counter = 2\n=====3.0 seconds after begining=====\nfirst counter = 5\nsecond counter = 3\n=====3.5 seconds after begining=====\nfirst counter = 5\nsecond counter = 3\n=====4.0 seconds after begining=====\nfirst counter = 5\nsecond counter = 4\n=====4.5 seconds after begining=====\nfirst counter = 5\nsecond counter = 4\n=====5.0 seconds after begining=====\nfirst counter = 5\nsecond counter = 5"
  },
  {
    "objectID": "python/advanced/excel_export.html",
    "href": "python/advanced/excel_export.html",
    "title": "To excel import",
    "section": "",
    "text": "from IPython.display import HTML\nHTML('''\n&lt;style&gt;\n    h1 {text-align:center}\n&lt;/style&gt;\n''')"
  },
  {
    "objectID": "python/advanced/excel_export.html#several-pd.dataframe-on-list",
    "href": "python/advanced/excel_export.html#several-pd.dataframe-on-list",
    "title": "To excel import",
    "section": "Several pd.DataFrame on list",
    "text": "Several pd.DataFrame on list\n\nimport pandas as pd\n\ndf1 = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\ndf2 = pd.DataFrame({'C': [7, 8, 9], 'D': [10, 11, 12]})\n\nwriter = pd.ExcelWriter('excel_export_files/output.xlsx')\n\ndf1.to_excel(writer, sheet_name='Sheet1')\ndf2.to_excel(writer, sheet_name='Sheet1', startrow=df1.shape[0] + 2)\n\ndf1.to_excel(writer, sheet_name='Sheet2')\ndf2.to_excel(writer, sheet_name='Sheet2', startrow=df1.shape[0] + 2)\n\nwriter.close()"
  },
  {
    "objectID": "python/advanced/airflow/run_airflow.html",
    "href": "python/advanced/airflow/run_airflow.html",
    "title": "Run airflow",
    "section": "",
    "text": "To play with airflow we run airflow. In this section I will describe how to get airflow up in a docker container, and by default I will use this method in any airflow related questions onthis site."
  },
  {
    "objectID": "python/advanced/airflow/run_airflow.html#image",
    "href": "python/advanced/airflow/run_airflow.html#image",
    "title": "Run airflow",
    "section": "Image",
    "text": "Image\nThe following docker file should be used to build the image.\nFeatures an image built from this dockerfile:\n\nAdmin username is admin and password is admin;\nCofigure disables automatic loading of example dags, so it will be more convenient to concentrate on writing your own dags.\n\n\n%%writefile run_airflow/dockerfile\nFROM python:3.10\n\n# Script for airflow installation\n# described below\nCOPY install_airflow.sh install_airflow.sh\nRUN bash install_airflow.sh\n\n# we need to create user in other case\n# airflow will create it by itself with\n# random password\n# for some reason it asks to run airflow db init\n# before creating a user\nRUN airflow db init; \\\n    airflow users create \\\n        --username admin \\\n        --password admin \\\n        --firstname Fedor \\\n        --lastname Kobak \\\n        --role Admin \\\n        --email spiderman@superhero.org;\n\n# Here is the command that replaces the load_examples\n# parameter from True to False. So if you\n# airflow, you won't get any examples in the list of dags.\nRUN sed -i 's/load_examples = True/load_examples = False/g' /root/airflow/airflow.cfg\n\nCMD [\"airflow\", \"standalone\"]\n\nOverwriting run_airflow/dockerfile\n\n\ninstall_airflow.sh just copied from airflow quick start page.\n\n%%writefile run_airflow/isntall_airflow.sh\nAIRFLOW_VERSION=2.7.1\n\n# Extract the version of Python you have installed. If you're currently using Python 3.11 you may want to set this manually as noted above, Python 3.11 is not yet supported.\nPYTHON_VERSION=\"$(python --version | cut -d \" \" -f 2 | cut -d \".\" -f 1-2)\"\n\nCONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\"\n# For example this would install 2.7.1 with python 3.8: https://raw.githubusercontent.com/apache/airflow/constraints-2.7.1/constraints-3.8.txt\n\npip install \"apache-airflow==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"\n\nWriting run_airflow/isntall_airflow.sh"
  },
  {
    "objectID": "python/advanced/airflow/run_airflow.html#start-the-container",
    "href": "python/advanced/airflow/run_airflow.html#start-the-container",
    "title": "Run airflow",
    "section": "Start the container",
    "text": "Start the container\nIn the following cell command to start the container. We need port 8080 to get access to the airflow browser client.\n\n!docker run -itd --rm\\\n    --name start_airflow\\\n    -p 8080:8080\\\n    airflow_tests &&gt; /dev/null\n\nNow you can try localhost:8080 on your browser (use username - admin and password - admin).\nDon’t forget to stop the container when you don’t need it.\n\n!docker stop start_airflow &&gt; /dev/null"
  },
  {
    "objectID": "python/advanced/airflow/bash_tasks.html",
    "href": "python/advanced/airflow/bash_tasks.html",
    "title": "Bash tasks",
    "section": "",
    "text": "This page will cover the peculiarities of using bash to describe tasks in airflow."
  },
  {
    "objectID": "python/advanced/airflow/bash_tasks.html#dag",
    "href": "python/advanced/airflow/bash_tasks.html#dag",
    "title": "Bash tasks",
    "section": "DAG",
    "text": "DAG\n\n%%writefile bash_tasks/bash_tasks.py\n\nfrom datetime import datetime, timedelta\nfrom textwrap import dedent\n\nfrom airflow import DAG\n\nfrom airflow.operators.bash import BashOperator\nwith DAG(\n    \"bash_tasks\",\n    default_args={\n        \"depends_on_past\": False,\n    },\n    description=\"A simple tutorial DAG\",\n    schedule=timedelta(days=1),\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    print_bash_date = BashOperator(\n        task_id=\"print_date\",\n        bash_command=\"\"\"\n        for i in {0..3}\n        do\n            echo \"current date: $(date)\"\n        done\n        \"\"\"\n    )\n\n    run_external_script = BashOperator(\n        task_id=\"external_script\",\n        bash_command=\"\"\"\n        for i in {0..3}\n        do\n            echo \"current date: $(date)\"\n        done\n        \"\"\"\n    )\n\nOverwriting bash_tasks/bash_tasks.py"
  },
  {
    "objectID": "python/advanced/airflow/bash_tasks.html#container",
    "href": "python/advanced/airflow/bash_tasks.html#container",
    "title": "Bash tasks",
    "section": "Container",
    "text": "Container\n\n%%bash\ndocker run -d --rm\\\n    --name bash_tasks\\\n    -p 8080:8080\\\n    -v ./bash_tasks:/root/airflow/dags\\\n    airflow_tests &&gt; /dev/null\n\ndocker exec bash_tasks airflow dags list\n\ndag_id     | filepath      | owner   | paused\n===========+===============+=========+=======\nbash_tasks | bash_tasks.py | airflow | True  \n                                             \n\n\n\n!docker exec bash_tasks airflow db migrate\n\nDB: sqlite:////root/airflow/airflow.db\nPerforming upgrade to the metadata database sqlite:////root/airflow/airflow.db\n[2023-09-25T11:23:17.657+0000] {migration.py:213} INFO - Context impl SQLiteImpl.\n[2023-09-25T11:23:17.658+0000] {migration.py:216} INFO - Will assume non-transactional DDL.\n[2023-09-25T11:23:17.659+0000] {db.py:1622} INFO - Creating tables\nINFO  [alembic.runtime.migration] Context impl SQLiteImpl.\nINFO  [alembic.runtime.migration] Will assume non-transactional DDL.\nDatabase migrating done!\n\n\n\n!docker stop bash_tasks\n\nbash_tasks"
  },
  {
    "objectID": "python/advanced/airflow/bash_tasks.html#print_bash_date",
    "href": "python/advanced/airflow/bash_tasks.html#print_bash_date",
    "title": "Bash tasks",
    "section": "print_bash_date",
    "text": "print_bash_date\n\n!docker exec bash_tasks airflow tasks test bash_tasks print_date 2015-06-01\n#| grep \" - current date\"\n\n[2023-09-25T11:23:20.435+0000] {dagbag.py:539} INFO - Filling up the DagBag from /root/airflow/dags\n[2023-09-25T11:23:20.558+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=&lt;TaskInstance: bash_tasks.print_date __airflow_temporary_run_2023-09-25T11:21:25.160046+00:00__ [None]&gt;\n[2023-09-25T11:23:20.561+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=&lt;TaskInstance: bash_tasks.print_date __airflow_temporary_run_2023-09-25T11:21:25.160046+00:00__ [None]&gt;\n[2023-09-25T11:23:20.562+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 2\n[2023-09-25T11:23:20.562+0000] {taskinstance.py:1428} WARNING - cannot record queued_duration for task print_date because previous state change time has not been saved\n[2023-09-25T11:23:20.562+0000] {taskinstance.py:1380} INFO - Executing &lt;Task(BashOperator): print_date&gt; on 2015-06-01 00:00:00+00:00\n[2023-09-25T11:23:20.581+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='bash_tasks' AIRFLOW_CTX_TASK_ID='print_date' AIRFLOW_CTX_EXECUTION_DATE='2015-06-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='__airflow_temporary_run_2023-09-25T11:21:25.160046+00:00__'\n[2023-09-25T11:23:20.582+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp\n[2023-09-25T11:23:20.582+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', '\\n        for i in {0..3}\\n        do\\n            echo \"current date: $(date)\"\\n        done\\n        ']\n[2023-09-25T11:23:20.586+0000] {subprocess.py:86} INFO - Output:\n[2023-09-25T11:23:20.587+0000] {subprocess.py:93} INFO - current date: Mon Sep 25 11:23:20 UTC 2023\n[2023-09-25T11:23:20.588+0000] {subprocess.py:93} INFO - current date: Mon Sep 25 11:23:20 UTC 2023\n[2023-09-25T11:23:20.588+0000] {subprocess.py:93} INFO - current date: Mon Sep 25 11:23:20 UTC 2023\n[2023-09-25T11:23:20.589+0000] {subprocess.py:93} INFO - current date: Mon Sep 25 11:23:20 UTC 2023\n[2023-09-25T11:23:20.589+0000] {subprocess.py:97} INFO - Command exited with return code 0\n[2023-09-25T11:23:20.599+0000] {taskinstance.py:1398} INFO - Marking task as SUCCESS. dag_id=bash_tasks, task_id=print_date, execution_date=20150601T000000, start_date=, end_date=20230925T112320"
  },
  {
    "objectID": "python/advanced/geopy/request.html",
    "href": "python/advanced/geopy/request.html",
    "title": "Request",
    "section": "",
    "text": "In this page I will describe how to make different types of requests to geopy.\nGeopy documentation.\nTo use it you need to create a Nominatim object and pass it user_agent. In my opinion you can pass whatever you want as user_agent, I don’t really understand how this option is used.\nfrom geopy.geocoders import Nominatim\ngeolocator = Nominatim(user_agent = \"knowledge\")"
  },
  {
    "objectID": "python/advanced/geopy/request.html#arbitrary-request",
    "href": "python/advanced/geopy/request.html#arbitrary-request",
    "title": "Request",
    "section": "Arbitrary request",
    "text": "Arbitrary request\nUsing the geocode method, you can query the system address as text.\n\ngeolocator.geocode(\"Minsk\", language = \"en\").raw\n\n{'place_id': 184576951,\n 'licence': 'Data © OpenStreetMap contributors, ODbL 1.0. http://osm.org/copyright',\n 'osm_type': 'relation',\n 'osm_id': 59195,\n 'lat': '53.9024716',\n 'lon': '27.5618225',\n 'class': 'boundary',\n 'type': 'administrative',\n 'place_rank': 7,\n 'importance': 0.6699404177567978,\n 'addresstype': 'city',\n 'name': 'Minsk',\n 'display_name': 'Minsk, Belarus',\n 'boundingbox': ['53.7938470', '53.9717897', '27.3740176', '28.0799469']}"
  },
  {
    "objectID": "python/advanced/geopy/request.html#request-as-coordinates",
    "href": "python/advanced/geopy/request.html#request-as-coordinates",
    "title": "Request",
    "section": "Request as coordinates",
    "text": "Request as coordinates\nUsing reverse method you can pass any coordinates you like as tuple and got answer.\n\ngeolocator.reverse((43, 23), language = \"ru\").raw\n\n{'place_id': 82486720,\n 'licence': 'Data © OpenStreetMap contributors, ODbL 1.0. http://osm.org/copyright',\n 'osm_type': 'way',\n 'osm_id': 706822395,\n 'lat': '42.999751989667615',\n 'lon': '23.000019012462104',\n 'class': 'highway',\n 'type': 'tertiary',\n 'place_rank': 26,\n 'importance': 0.10000999999999993,\n 'addresstype': 'road',\n 'name': '813',\n 'display_name': '813, Бондин хан, Туден, Годеч, Софийская область, 2240, Болгария',\n 'address': {'road': '813',\n  'neighbourhood': 'Бондин хан',\n  'village': 'Туден',\n  'municipality': 'Годеч',\n  'county': 'Софийская область',\n  'ISO3166-2-lvl6': 'BG-23',\n  'postcode': '2240',\n  'country': 'Болгария',\n  'country_code': 'bg'},\n 'boundingbox': ['42.9936767', '42.9998986', '22.9871460', '23.0008811']}"
  },
  {
    "objectID": "python/advanced/geopy/request.html#location-object",
    "href": "python/advanced/geopy/request.html#location-object",
    "title": "Request",
    "section": "Location object",
    "text": "Location object\nRequests to geopy will return the geopy.location.Location object.\nThe next cell lists the interesting fields.\n\nlocation_opject = geolocator.geocode(\"Minsk\", language = \"ru\")\n\nfor field in dir(location_opject): \n    if field[0] != \"_\": print(field)\n\naddress\naltitude\nlatitude\nlongitude\npoint\nraw\n\n\nraw field is crusical it contains original answer from api.\n\nlocation_opject.raw\n\n{'place_id': 152512099,\n 'licence': 'Data © OpenStreetMap contributors, ODbL 1.0. http://osm.org/copyright',\n 'osm_type': 'relation',\n 'osm_id': 59195,\n 'lat': '53.9024716',\n 'lon': '27.5618225',\n 'class': 'boundary',\n 'type': 'administrative',\n 'place_rank': 7,\n 'importance': 0.6699404177567978,\n 'addresstype': 'city',\n 'name': 'Минск',\n 'display_name': 'Минск, Беларусь',\n 'boundingbox': ['53.7938470', '53.9717897', '27.3740176', '28.0799469']}"
  },
  {
    "objectID": "python/advanced/transformers.html",
    "href": "python/advanced/transformers.html",
    "title": "Transformers",
    "section": "",
    "text": "transformers is a library in python that allows you to use pre-trained machine learning models that belong to the transformers architecture. So in the following example, the pre-trained Bert model has been loaded and deiaplayed:\nimport numpy as np"
  },
  {
    "objectID": "python/advanced/transformers.html#load-model",
    "href": "python/advanced/transformers.html#load-model",
    "title": "Transformers",
    "section": "Load model",
    "text": "Load model\n\nfrom transformers import BertModel\nmodel = BertModel.from_pretrained('bert-base-cased')\nmodel\n\nBertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"
  },
  {
    "objectID": "python/advanced/transformers.html#tokenizer",
    "href": "python/advanced/transformers.html#tokenizer",
    "title": "Transformers",
    "section": "Tokenizer",
    "text": "Tokenizer\nTo use the model correctly, you may need a tokiniser - a program that trains text to tokens. Trainformers also have special tools:\n\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\ntokenizer\n\nBertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n    0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n    103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}\n\n\nSo here is example how “Hello!” phrase can be tokinized.\n\ntokenizer.encode_plus(\n    'Hello!', \n    add_special_tokens=True, \n    return_token_type_ids=False, \n    return_tensors='pt'\n)\n\n{'input_ids': tensor([[ 101, 8667,  106,  102]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
  },
  {
    "objectID": "python/advanced/transformers.html#model-usage",
    "href": "python/advanced/transformers.html#model-usage",
    "title": "Transformers",
    "section": "Model usage",
    "text": "Model usage\nHere we combine the downloaded tokiniser and model and use it to run some phrase through the model:\n\nencoding = tokenizer.encode_plus(\n    'Hello!', \n    add_special_tokens=True, \n    return_token_type_ids=False, \n    return_tensors='pt'\n)\nres = model(**encoding)\n\nWe’ve got back an instance of a specific class that implements the result of the BERT model:\n\ntype(res)\n\ntransformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions\n\n\nIt contains two objects:\n\nlast_hidden_state - state of last hidden layer;\npooler_output - output of the model.\n\n\nfrom IPython.display import HTML\n\ndisplay(HTML(\"&lt;b&gt;Last hidden state:&lt;/b&gt;\"))\nprint(res.last_hidden_state)\ndisplay(HTML(\"&lt;br&gt;&lt;b&gt;Pooler output:&lt;/b&gt;\"))\nprint(res.pooler_output)\n\nLast hidden state:\n\n\ntensor([[[ 0.6283,  0.2166,  0.5605,  ...,  0.0136,  0.6158, -0.1712],\n         [ 0.6108, -0.2253,  0.9263,  ..., -0.3028,  0.4500, -0.0714],\n         [ 0.8040,  0.1809,  0.7076,  ..., -0.0685,  0.4837, -0.0774],\n         [ 1.3290,  0.2360,  0.4567,  ...,  0.1509,  0.9621, -0.4841]]],\n       grad_fn=&lt;NativeLayerNormBackward0&gt;)\ntensor([[-0.7105,  0.4876,  0.9999, -0.9947,  0.9599,  0.9521,  0.9767, -0.9946,\n         -0.9815, -0.6238,  0.9776,  0.9984, -0.9989, -0.9998,  0.8559, -0.9755,\n          0.9895, -0.5281, -1.0000, -0.7414, -0.7056, -0.9999,  0.2901,  0.9786,\n          0.9729,  0.0734,  0.9828,  1.0000,  0.8981, -0.1109,  0.2780, -0.9920,\n          0.8693, -0.9985,  0.1461,  0.2067,  0.8092, -0.2430,  0.8580, -0.9585,\n         -0.8130, -0.6138,  0.7961, -0.5727,  0.9737,  0.2362, -0.1194, -0.0789,\n          0.0031,  0.9997, -0.9519,  0.9899, -0.9962,  0.9931,  0.9950,  0.5050,\n          0.9952,  0.1090, -0.9994,  0.3416,  0.9792,  0.2506,  0.8923, -0.2238,\n          0.3518, -0.5293, -0.9570,  0.1357, -0.3313,  0.1627, -0.0078,  0.3608,\n          0.9833, -0.9160,  0.0196, -0.9141,  0.2075, -0.9999,  0.9449,  1.0000,\n          0.7796, -0.9997,  0.9935, -0.2309, -0.7830,  0.8880, -0.9994, -0.9994,\n          0.0160, -0.6875,  0.9554, -0.9846,  0.7813, -0.9322,  1.0000, -0.9511,\n         -0.1583,  0.3866,  0.9699, -0.8283, -0.7689,  0.9346,  0.9994, -0.9965,\n          0.9992,  0.8548, -0.9459, -0.9451,  0.8292,  0.0406,  0.9891, -0.9857,\n         -0.9555,  0.0784,  0.9803, -0.9302,  0.9901,  0.8020, -0.2408,  1.0000,\n         -0.2367,  0.9689,  0.9982,  0.8761, -0.8891, -0.2167, -0.7227,  0.9385,\n         -0.7771, -0.5691,  0.8276, -0.9881, -0.9987,  0.9994, -0.2452,  1.0000,\n         -0.9992,  0.9935, -0.9999, -0.8808, -0.7632, -0.1224, -0.9886,  0.0386,\n          0.9893,  0.0950, -0.9691, -0.8293,  0.7363, -0.9115,  0.4936,  0.6483,\n         -0.9581,  0.9715,  0.9982,  0.9653,  0.9890,  0.1850, -0.9722,  0.8544,\n          0.9774, -0.9995,  0.8701, -0.9960,  0.9993,  0.9746,  0.8500, -0.9971,\n          0.9999, -0.8101,  0.0206,  0.0232,  0.0982, -0.9993,  0.4584,  0.4759,\n          0.8759,  0.9993, -0.9945,  0.9995,  0.8257, -0.0958,  0.8097,  0.9992,\n         -0.9966, -0.9701, -0.9847,  0.2290,  0.8039,  0.8126,  0.4859,  0.9628,\n          0.9992,  0.7803, -0.9974, -0.3289,  0.9692, -0.1398,  1.0000, -0.4062,\n         -0.9998, -0.7585,  0.9499,  0.9883, -0.2351,  0.9828, -0.7602, -0.1856,\n          0.9882, -0.9727,  0.9989,  0.4863,  0.9102,  0.8847,  0.9915, -0.9258,\n         -0.0788,  0.1543, -0.7543,  0.9999, -0.9996, -0.2332,  0.5874, -0.9942,\n         -0.9979,  0.9794, -0.0194, -0.8693, -0.2168,  0.7979,  0.2004,  0.9521,\n          0.9900, -0.6276, -0.7824, -0.9998, -0.9979, -0.8998, -0.9672,  0.0430,\n          0.6920, -0.3973, -0.9400, -0.9990,  0.9653,  0.4508, -0.8941,  0.1173,\n         -0.7708, -0.9993,  0.6723, -0.9301, -0.9982,  0.9996, -0.7676,  0.9977,\n          0.9586, -0.9940,  0.8469, -0.9993, -0.1126, -0.9871,  0.6339,  0.6951,\n         -0.6402, -0.0494,  0.9932, -0.9665, -0.8052,  0.8924, -0.9999,  0.9337,\n         -0.2121,  0.9991,  0.8133,  0.2323,  0.9850,  0.9504, -0.9842, -0.9998,\n          0.9709,  0.8598, -0.9926, -0.1464,  0.9999, -0.9989, -0.8643, -0.9607,\n         -0.9940, -0.9996,  0.2709, -0.8859,  0.2709,  0.9882,  0.6940,  0.1279,\n          0.9929,  0.9910,  0.2286, -0.3951,  0.1043, -0.9768, -0.9314,  0.9270,\n          0.1190, -1.0000,  0.9999, -0.9937,  0.9782,  0.9644, -0.9963,  0.8284,\n          0.1232, -0.9757,  0.0153,  0.9999,  0.9855, -0.1870,  0.2675,  0.9221,\n         -0.3209,  0.6954, -0.8953, -0.7505,  0.2031, -0.9330,  0.9959,  0.7163,\n         -0.9901,  0.9979,  0.0020,  0.8427, -0.8604,  0.9138,  0.9909, -0.1418,\n         -0.6551,  0.0506, -0.1576, -0.9825,  0.2171, -0.9974, -0.5776,  0.9791,\n          0.9842, -0.9881,  0.9842, -0.0714,  0.9470, -0.9988,  1.0000, -0.9961,\n          0.0919,  0.8178, -0.8827, -0.6587,  0.9920,  0.9926,  0.9770, -0.9782,\n         -0.8553,  0.8854,  0.9670, -0.9807, -0.0805, -0.9996, -0.8613,  0.9954,\n          0.9971,  0.0548, -0.1328, -0.9985,  0.9674, -0.8499, -0.9574, -0.0873,\n         -0.8494,  0.8687,  0.9986, -0.7471,  0.7234,  0.1623, -0.9843,  0.9408,\n          0.9139,  0.9998, -0.9575,  0.6504,  0.9878, -0.2068, -0.8754,  0.6124,\n          0.9995, -0.9634, -0.2490, -0.9995, -0.0428, -0.6623, -0.3127, -0.6662,\n          0.0496, -0.8922,  0.9753,  0.0668,  0.8677, -0.4532,  0.9877, -0.1256,\n         -0.0084, -0.3633, -0.4310,  0.5263,  0.2795,  0.9855, -0.9623,  0.9997,\n         -0.2454, -1.0000, -0.9985, -0.8149, -0.9996,  0.8664, -0.9936,  0.9833,\n          0.9645, -0.9990, -0.9995, -0.9971, -0.9827,  0.8944,  0.7312, -0.0281,\n          0.3282, -0.0801, -0.0505, -0.5034,  0.0436, -0.9379, -0.6098, -0.9991,\n          0.9075, -1.0000, -0.8943,  0.9983, -0.9974, -0.9702, -0.9073, -0.5143,\n         -0.8728,  0.5948,  0.9871, -0.4346, -0.8071, -0.9995,  0.9870, -0.8455,\n          0.0911, -0.9254, -0.9747,  0.9997,  0.8546, -0.1713, -0.0070, -0.9989,\n          0.9919, -0.9512, -0.9604, -0.9775,  0.2517, -0.9669, -0.9998,  0.0274,\n          0.9981,  0.9918,  0.9872,  0.3586, -0.4554, -0.9641,  0.1662, -0.9999,\n          0.8734,  0.9199, -0.9861, -0.7962,  0.9937,  0.9731, -0.9745, -0.9908,\n          0.9605,  0.3886,  0.9752, -0.6621, -0.5456,  0.3875,  0.0179, -0.9900,\n         -0.9324,  0.9966, -0.9996,  0.9857,  0.9975,  0.9992, -0.2936,  0.1672,\n         -0.9914, -0.9807, -0.5163,  0.3407, -0.9999,  0.9999, -1.0000,  0.4725,\n         -0.8529,  0.9192,  0.9880, -0.4080, -0.9999, -0.9998,  0.3443,  0.1171,\n          0.9887,  0.4144,  0.1948, -0.6747, -0.3843,  0.9973, -0.8765, -0.8126,\n         -0.9988,  0.9997,  0.4865, -0.9982,  0.9962, -0.9995,  0.8606,  0.9813,\n          0.8977,  0.9750, -0.9995,  1.0000, -0.9998,  0.9968, -1.0000, -0.9994,\n          0.9998, -0.9914, -0.8091, -0.9997, -0.9992,  0.7226,  0.1572, -0.5659,\n          0.9883, -0.9998, -0.9987, -0.4541, -0.9309, -0.8762,  0.9972, -0.8189,\n          0.9894, -0.0908,  0.9623,  0.3485,  0.9983,  0.9907, -0.7728, -0.8770,\n         -0.9934,  0.9906, -0.6931,  0.4029,  0.9709, -0.0175, -0.8430,  0.3529,\n         -0.9967,  0.5958,  0.1322,  0.9368,  0.9241,  0.8697, -0.1084, -0.5757,\n         -0.3093, -0.9923,  0.5895, -0.9995,  0.9794, -0.9611,  0.0368, -0.4532,\n          0.2907, -0.9616,  0.9996,  0.9988, -0.9890,  0.0842,  0.9875, -0.8991,\n          0.9725, -0.9923,  0.0591,  0.9806, -0.7373,  0.9830, -0.0013,  0.0637,\n          0.9887, -0.9946, -0.9119, -0.6857,  0.3650,  0.0621, -0.9661,  0.0115,\n          0.9902, -0.4771, -0.9997,  0.9412, -0.9993, -0.1202,  0.9763, -0.0377,\n          0.9999, -0.8283,  0.1905,  0.1534, -0.9998, -0.9995,  0.0540, -0.1655,\n         -0.9095,  0.9996, -0.3539,  0.8774, -0.9999,  0.3085,  0.9981,  0.2593,\n          0.8318, -0.9150, -0.9679, -0.9740, -0.7623,  0.0018,  0.8898, -0.9830,\n         -0.6928, -0.9300,  1.0000, -0.9982, -0.9065, -0.9884,  0.7844,  0.9037,\n          0.4408,  0.1464, -0.9218,  0.9302, -0.9442,  0.9973, -0.9939, -0.9963,\n          0.9998,  0.5109, -0.9965,  0.2434, -0.4521,  0.3922,  0.1305,  0.8537,\n         -0.8603, -0.2880, -0.9964,  0.8409, -0.9161, -0.9866, -0.6739, -0.3227,\n         -0.9776,  0.9935,  0.9731,  0.9999, -0.9998,  0.9386, -0.0222,  0.9991,\n          0.0548, -0.7093,  0.9169,  0.9997, -0.7628,  0.8665, -0.1445,  0.0659,\n          0.4782, -0.4443,  0.9984, -0.9318,  0.0735, -0.9737, -0.9999,  0.9999,\n         -0.0253,  0.9916,  0.3062,  0.8545, -0.9106,  0.9857, -0.9849, -0.9302,\n         -1.0000,  0.1720, -0.9895, -0.9877, -0.1270,  0.9847, -0.9996, -0.9917,\n         -0.4165, -1.0000,  0.9550, -0.9948, -0.8871, -0.9896,  0.9988, -0.2867,\n         -0.8413,  0.9756, -0.9648,  0.9592,  0.9145, -0.4687,  0.2182,  0.1638,\n         -0.8462, -0.9960, -0.9356, -0.9699,  0.9437, -0.9875, -0.8543,  0.9966,\n          0.9835, -0.9992, -0.9944,  0.9973,  0.2491,  0.9927, -0.5164, -0.9998,\n         -0.9999,  0.0790,  0.2047,  0.9947, -0.3889,  0.9576,  0.8522, -0.5662,\n          0.6047, -0.8014, -0.2958, -0.6084, -0.2891,  1.0000, -0.9179,  0.9894]],\n       grad_fn=&lt;TanhBackward0&gt;)\n\n\nPooler output:"
  },
  {
    "objectID": "python/advanced/transformers.html#apply-to-dataset",
    "href": "python/advanced/transformers.html#apply-to-dataset",
    "title": "Transformers",
    "section": "Apply to dataset",
    "text": "Apply to dataset\nThis section shows how the BERT model can be applied to a dataset.\n\nDataset\nWe will be working with the datasets library, which is usually used in conjunction with the transformers library. So in the following cell we have extracted the imdb library, which contains reviews for the movies. We use a small subset of the dataframe to reduce the amount of calculations.\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"imdb\", split=\"train\")\n\nnp.random.seed(100)\nidx = np.random.randint(len(dataset), size=200)\ndataset = dataset.select(idx)\n\ndataset[0]\n\n{'text': \"The Film must have been shot in a day,there are scenes where you can see the camera reflections and its red pointer,even the scenery's green light that blends with the actors!!!The plot and the lines are really awful without even the slightest inspiration(At least as a thriller genre movie).Everything that got to do with Poe in the movie,has a shallow and childish approach.The film is full of clise and no thrilling.If you want to watch a funny b-movie for a relaxing evening with friends then go for it you will enjoy it (As I Did) but there's no way to take this film seriously!\",\n 'label': 0}\n\n\nApplying tokenization to the dataset. Finally for each element of the dataset:\n\ndef tokenization(example):\n    return tokenizer.batch_encode_plus(\n        example['text'],\n        add_special_tokens=True, \n        return_token_type_ids=False, \n        truncation=True\n    )\ndataset = dataset.map(\n    tokenization, batched=True\n)\ndataset.set_format(\n    type=\"torch\", \n    columns=[\"input_ids\", \"attention_mask\"]\n)\n\nAfter that we have extra keys for each object input_ids and attention_mask - wich is required for bert model.\n\ndisplay(HTML(\"&lt;b&gt;Keys:&lt;/b&gt;\"))\nprint(list(dataset[0].keys()))\ndisplay(HTML(\"&lt;br&gt;&lt;b&gt;Tokens:&lt;/b&gt;\"))\nprint(dataset[0]['input_ids'])\ndisplay(HTML(\"&lt;br&gt;&lt;b&gt;Mask:&lt;/b&gt;\"))\nprint(dataset[0]['attention_mask'])\n\nKeys:\n\n\n['input_ids', 'attention_mask']\ntensor([  101,  1109,  2352,  1538,  1138,  1151,  2046,  1107,   170,  1285,\n          117,  1175,  1132,  4429,  1187,  1128,  1169,  1267,  1103,  4504,\n        26906,  1105,  1157,  1894,  1553,  1200,   117,  1256,  1103, 19335,\n          112,   188,  2448,  1609,  1115, 13390,  1116,  1114,  1103,  5681,\n          106,   106,   106,  1109,  4928,  1105,  1103,  2442,  1132,  1541,\n         9684,  1443,  1256,  1103, 16960,  7670,   113,  1335,  1655,  1112,\n          170, 11826,  6453,  2523,   114,   119,  5268,  1115,  1400,  1106,\n         1202,  1114, 21377,  1107,  1103,  2523,   117,  1144,   170,  8327,\n         1105,  2027,  2944,  3136,   119,  1109,  1273,  1110,  1554,  1104,\n          172,  6137,  1162,  1105,  1185, 21401,  1158,   119,  1409,  1128,\n         1328,  1106,  2824,   170,  6276,   171,   118,  2523,  1111,   170,\n        22187,  3440,  1114,  2053,  1173,  1301,  1111,  1122,  1128,  1209,\n         5548,  1122,   113,  1249,   146,  2966,   114,  1133,  1175,   112,\n          188,  1185,  1236,  1106,  1321,  1142,  1273,  5536,   106,   102])\ntensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n\n\nTokens:\n\n\nMask:\n\n\n\n\nLoader\nPreparing the classical torch.loader:\n\nfrom torch.utils.data import DataLoader\nfrom transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\nloader = DataLoader(\n    dataset, \n    batch_size=32, \n    collate_fn=data_collator, \n    pin_memory=True,\n    shuffle=False\n)\n\nNow let’s look at what the first element of loader will be - it’s dict, with all the necessary stuff for fitting and forwarding through bert model:\n\nitem = next(iter(loader))\nitem.keys()\n\nYou're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n\n\ndict_keys(['input_ids', 'attention_mask'])\n\n\nFinally you can pass item to the model to get prediction, but you need high-performance hardware to run this:\nmodel.eval()\nmodel(**item)[\"pooler_output\"]\nFinally, we need to model all the batches. In the following cell we have just extracted embeddings for the whole imdb dataset.\nNote:\n\nThis cell should be started on powerful hardware;\nThe function nested in torch.inference_mode is crucial, without it the model takes too much memory.\n\n\nimport torch\nfrom tqdm import tqdm\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\n@torch.inference_mode()\ndef get_embeddings(model, loader, device):\n    model.to(device)\n    model.eval()\n    \n    total_embeddings = []\n    labels = []\n    \n    for batch in tqdm(loader):\n\n        batch = {\n            key: batch[key].to(device) \n            for key in ['attention_mask', 'input_ids']\n        }\n        \n        embeddings = model(**batch)['last_hidden_state'][:, 0, :]\n        total_embeddings.append(embeddings)\n\n    return torch.cat(total_embeddings, dim=0)\n\nembeddings = get_embeddings(model, loader, device)\n\ncpu\n\n\n100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [01:37&lt;00:00, 13.89s/it]\n\n\nSo for each line from the dataset we got an embedding from the model. It’s representation and shape is shown below:\n\ndisplay(HTML(\"&lt;b&gt;Embeddings:&lt;/b&gt;\"))\ndisplay(embeddings)\ndisplay(HTML(\"&lt;br&gt;&lt;b&gt;Shape:&lt;/b&gt;\"))\ndisplay(embeddings.shape)\n\nEmbeddings:\n\n\ntensor([[ 0.6028,  0.1125, -0.2223,  ..., -0.1698,  0.1655,  0.0645],\n        [ 0.6183,  0.0239, -0.2428,  ..., -0.1532,  0.1792,  0.1111],\n        [ 0.3977,  0.1045, -0.1627,  ..., -0.0737,  0.2369,  0.1243],\n        ...,\n        [ 0.5241,  0.1857, -0.4136,  ..., -0.2503,  0.0959,  0.2350],\n        [ 0.6121,  0.0789, -0.1658,  ..., -0.1467,  0.4310,  0.1626],\n        [ 0.6343,  0.0062, -0.1479,  ..., -0.2116,  0.1337,  0.1664]])\n\n\nShape:\n\n\ntorch.Size([200, 768])"
  },
  {
    "objectID": "python/dash/select_clear_all.html",
    "href": "python/dash/select_clear_all.html",
    "title": "Select/Clear All",
    "section": "",
    "text": "It’s a common task to organise a mechanism to select or disable all available options. So here I describe possible ways to do it.\n\nSelect\nIn the following example, there is a checklist and all the boxes can be selected by pressing the “Select all” button. You should use prevent_initial_call = True for select all button callback in order to avoid all options in dashboard starting.\n\nfrom jupyter_dash import JupyterDash\nfrom dash import (\n    html,\n    dcc,\n    callback, \n    Input, \n    Output, \n    State\n)\nfrom IPython.display import clear_output\n\napp = JupyterDash(__name__)\n\napp.layout = html.Div([\n    html.Button(\n        \"Select All\",\n        id=\"select-all-button\",\n    ),\n    dcc.Checklist(\n        [\n            \"BelarusSales\",\n            \"SalesNetworkBelarus\",\n            \"BelSalesNetwork\",\n            \"SalesNetworkBel\",\n            \"SalesOfBelarus\",\n            \"BelSalesNetworks\",\n            \"BelarusSalesNetwork\",\n            \"SalesInBelarus\",\n        ],\n        id = \"check-list\",\n        value = []\n    )\n])\n\n@callback(\n    Output(\"check-list\", \"value\"),\n    State(\"check-list\", \"options\"),\n    Input(\"select-all-button\", \"n_clicks\"),\n    prevent_initial_call = True\n)\ndef select_all_button(\n    options: list,\n    n_clicks: int\n) -&gt; list:\n    '''\n        Callback that allows you to do \n        select all options in CheckList\n        from pressing \"select-all-button\".\n\n        Arguments\n        -----------\n\n        options : (list) list with options of Checkbox;\n        n_clicks: (int) number of clicks on button.\n\n        Returns\n        -----------\n        (list) list of selected options of Checklist.\n    '''\n    return options\n        \n\napp.run_server(debug=True, port = 8051)\nclear_output()\n\n\n\n\nClear\nIn the following example, I create a button that deselects all values in the checklist. It’s really similar ideas to “Select all” and even easier. You have to set prevent_initial_call = True as well.\n\nfrom jupyter_dash import JupyterDash\nfrom dash import (\n    html,\n    dcc,\n    callback,\n    Input,\n    Output,\n    State\n)\nfrom IPython.display import clear_output\n\ninitial_clicks = 0\n\napp = JupyterDash(__name__)\napp.layout = html.Div([\n    html.Button(\n        \"Clear\", id = \"clear-button\", \n        n_clicks = initial_clicks\n    ),\n    dcc.Checklist(\n        [\n            \"BelarusSales\",\n            \"SalesNetworkBelarus\",\n            \"BelSalesNetwork\",\n            \"SalesNetworkBel\",\n            \"SalesOfBelarus\",\n            \"BelSalesNetworks\",\n            \"BelarusSalesNetwork\",\n            \"SalesInBelarus\",\n        ],\n        id = \"check-list\",\n        value = []\n    )\n])\n\n@callback(\n    Output(\"check-list\", \"value\"),\n    Input(\"clear-button\", \"n_clicks\"),\n    prevent_initial_call = True\n)\ndef button_click(n_clicks: int) -&gt; list:\n    '''\n        The callback for clear button.\n\n        Arguments\n        -----------\n        value : (list) the value of Checklist;\n\n        Returns\n        -----------\n\n        (list) a list of options to be selected in \n               the Checklist after pressing the call button;\n    '''\n    return []\n\napp.run(port=8052)\nclear_output()"
  },
  {
    "objectID": "python/dash/callbacks.html",
    "href": "python/dash/callbacks.html",
    "title": "Callbacks",
    "section": "",
    "text": "It is a mechanism that allows you to create a function that will be called when you perform an action. It realised. This is implemented via the dash.callback decorator. More details in the documentation. I will focus on some practical features.\n\nSources\n\nUsing dash inside jupyter;\nCheck list component in dash;\nDash basic callbacks;\nCallback without output - github discussion.\n\n\n\nInput/output format\nInput/Output, implemented by dash.Input/dash.Output, which should be passed as arguments to the callback decorator. Constructors of the classes require the following syntax (\"&lt;object-id&gt;\", \"&lt;property&gt;\"), so you can choose which property to pass to the callback and which to change.\nIn the following example, I simply take dcc.Checklist.values and link it to ddc.Slider.marks - the markers on the dcc.slider will exactly match the selected checkboxes on the dcc.checklist.\n\nfrom dash import dcc, html, Input, Output, callback\nfrom jupyter_dash import JupyterDash\nfrom IPython.display import clear_output\n\napp = JupyterDash(__name__)\noptions = list(range(0,20))\nvalue = [1,5]\n\nlst_val_to_slider_marks = lambda value: {val:str(val) for val in value}\n\napp.layout =  html.Div(\n    [\n        dcc.Checklist(\n            options,\n            value = value,\n            id = \"check-lst\",\n            inline = True\n        ),\n        dcc.Slider(\n            min(options), max(options),\n            step = None,\n            marks = lst_val_to_slider_marks(value),\n            id = \"slider\"\n        )\n    ],\n    style={'display': 'flex', 'flex-direction': 'column'}\n)\n\n@callback(\n    Output(\"slider\", \"marks\"),\n    Input(\"check-lst\", \"value\")\n)\ndef my_callback(val:list) -&gt; dict:\n    return lst_val_to_slider_marks(val)\n\nif __name__ == '__main__':\n    app.run_server(debug=True, port=8051)\nclear_output()\n\nIn site it will looks like:  Any check box you click - it will add one more marker on slider.\n\n\nState callbacks\nAny dash.dcc.Input will trigger the callback when the related item has changed. But sometimes it’s useful to have an element that sends its state, but only when some other element triggers the callback. Such purpose should be completed by dash.dcc.State.\nSo the following example is the same as in the Input/Output Format section, but it updates the slider not when the new checkbox is selected, but only when the button is pressed.\n\nfrom dash import dcc, html, Input, Output, callback, State\nfrom jupyter_dash import JupyterDash\nfrom IPython.display import clear_output\n\napp = JupyterDash(__name__)\noptions = list(range(0,20))\nvalue = [1,5]\n\nlst_val_to_slider_marks = lambda value: {val:str(val) for val in value}\n\napp.layout =  html.Div(\n    [\n        dcc.Checklist(\n            options,\n            value = value,\n            id = \"check-lst\",\n            inline = True\n        ),\n        dcc.Slider(\n            min(options), max(options),\n            step = None,\n            marks = lst_val_to_slider_marks(value),\n            id = \"slider\"\n        ),\n        html.Button(\n            \"Update bar\", id = \"button\"\n        )\n    ],\n    style={'display': 'flex', 'flex-direction': 'column'}\n)\n\n@callback(\n    Output(\"slider\", \"marks\"),\n    State(\"check-lst\", \"value\"),\n    Input(\"button\", \"n_clicks\")\n)\ndef my_callback(val: list, n_clicks : int) -&gt; dict:\n    '''\n        Callback for button.\n\n        Arguments\n        -----------\n        \n        val : (list) value passed from check list by State\n                caontains camptions of selected boxes;\n        n_clicks : (int) n_clicks of button passed by Input;\n\n        Returns\n        -----------\n\n        (dict) which maps values and it's markers on slider.\n    '''\n    return lst_val_to_slider_marks(val)\n\napp.run_server(debug=True)\nclear_output()\n\n\n\n\nChained callbacks\nThis section is just a variant of that material. But better described.\nThis example shows how one event can trigger a chain of different callbacks.\n\nSo we have a button that adds a new option to the checkbox list with a button_click callback;\nWhen something changes options in the checklist, it triggers check_list_options_changed, which only sets the last option selected;\nWhen something changes selected options, check_list_values_changed will be triggered and show selected options like line.\n\n\nfrom dash import dcc, html, Input, Output, callback\nfrom IPython.display import clear_output\nfrom jupyter_dash import JupyterDash\n\napp = JupyterDash(__name__)\napp.layout = html.Div([\n    html.Button(\"Add new box\", id = \"add-button\", n_clicks = 0),\n    dcc.Checklist(id=\"check-list\"),\n    html.P(id=\"disp-sel-boxes\")\n])\n\n@callback(\n    Output(\"check-list\", \"options\"),\n    Input(\"add-button\", \"n_clicks\")\n)\ndef button_click(n_clicks):\n    '''\n        Callback makes in listbox exactly\n        number of boxes as the number of\n        button is clicked.\n\n        Arguments\n        ------------\n\n        n_clicks : (int) count of clicks on button;\n\n        Returns\n        -----------\n\n        (list) captions of boxes which will \n        be displayed in check-list.\n    '''\n    return [f\"box {i}\" for i in range(n_clicks+1)]\n\n@callback(\n    Output(\"check-list\", \"value\"),\n    Input(\"check-list\", \"options\")\n)\ndef check_list_options_changed(options):\n    '''\n        Callback that will be called when the list of\n        of options in the checklist is updated. It sets\n        only the last option as a value.\n\n        Arguments\n        -----------\n\n        options : (list) list of available options.\n\n        Returns\n        -----------\n\n        (list) contains only the last option.\n    '''\n    return [options[-1]]\n\n@callback(\n    Output(\"disp-sel-boxes\", \"children\"),\n    Input(\"check-list\", \"value\")\n)\ndef check_list_values_changed(value):\n    '''\n        Called when you select/unselect a check.\n\n        Arguments\n        -----------\n        \n        value : (list) captions of selected boxes;\n\n        Returns\n        -----------\n        \n        (str) line that describes selected options\n        as \"Selected boxes: &lt;box1&gt;, ..., &lt;boxn&gt;\".\n    '''\n    return \"Selected boxes: \" + \", \".join([val for val in value])\n\napp.run(debug=True)\nclear_output()\n\n\n\n\nCallback without output\nIt turns out that dash has no callback mechanism without output. So the only tricky way is to create a dummy object and set it as the output object. In the following example, I use html.Div(id='dummy'), or rather its children property. I also print out some messages with changes to dcc.Checklist to prove that everything is working. You can find py file with the following example in “callbacks_examples/no_output_callback.py”.\n\nfrom dash import dcc, html, Input, Output, callback, Dash\nfrom IPython.display import clear_output\nfrom jupyter_dash import JupyterDash\n\napp = Dash(__name__)\n\ncheck_values = [\"value1\", \"value2\", \"value3\"]\n\napp.layout = html.Div([\n    html.Div(id='dummy'),\n    dcc.Checklist(\n        check_values,\n        id = \"check-lst\"\n    )\n])\n\nclicks_counter = 0\n\n@callback(\n    Output(\"dummy\", \"children\"),\n    Input(\"check-lst\", \"value\")\n)\ndef test_callback(checklist_value):\n    global clicks_counter\n    clicks_counter += 1\n\n    print(\"==========================\")\n    print(f\"    CLICK {clicks_counter}     \")\n    print(\"==========================\")\n    \n    print(\"-------value-------\")\n    print(checklist_value)\n    return None\n\nif __name__ == '__main__':\n    app.run_server(debug=False)\n\nclear_output()"
  },
  {
    "objectID": "python/dash/multipage_applications.html",
    "href": "python/dash/multipage_applications.html",
    "title": "Multipage applications",
    "section": "",
    "text": "Official information about creating multi-page applications in dash can be found here. In this page I just want to write down some of my experiments related to this feature."
  },
  {
    "objectID": "python/dash/multipage_applications.html#dash.page_registry",
    "href": "python/dash/multipage_applications.html#dash.page_registry",
    "title": "Multipage applications",
    "section": "dash.page_registry",
    "text": "dash.page_registry\ndash.page_registry is a special dictionary that contains information about the pages in the current application. So in the following example an application with two pages has been created. Page with button has a special button that stores dash.page_registry.\n\n%%writefile multipage_applications_files/app.py\nimport dash\nfrom dash import Dash, html, dcc\n\napp = Dash(__name__, use_pages=True)\n\napp.layout = html.Div([\n    html.H1('Multi-page app with Dash Pages'),\n    html.Div([\n        html.Div(\n            dcc.Link(f\"{page['name']} - {page['path']}\", href=page[\"relative_path\"])\n        ) for page in dash.page_registry.values()\n    ]),\n    dash.page_container\n])\n\nif __name__ == '__main__':\n    app.run(debug=True)\n\nOverwriting multipage_applications_files/app.py\n\n\n\n%%writefile multipage_applications_files/pages/some_page.py\nimport dash\nfrom dash import html, callback, Input, Output\n\nimport pickle\n\ndash.register_page(__name__)\nlayout = html.Div([\n    html.H1('Page with button'),\n    html.Button(\n        \"Save page_registry\",\n        id=\"save-button\",\n    ),\n    html.Div(id='dummy')\n])\n\n@callback(\n    Output(\"dummy\", \"children\"),\n    Input(\"save-button\", \"n_clicks\")\n)\ndef save_callback(n_clicks):\n    with open(\"page_registry\", \"wb\") as f:\n        pickle.dump(dash.page_registry, f)\n\nOverwriting multipage_applications_files/pages/some_page.py\n\n\n\n%%writefile multipage_applications_files/pages/home.py\nimport dash\nfrom dash import html\n\ndash.register_page(__name__, path=\"/\")\nlayout = html.Div([\n    html.H1('Basic home page')\n])\n\nOverwriting multipage_applications_files/pages/home.py\n\n\nBy running the following cell, you can run the dash application described in the previous three cells.\n\n%%bash\ncd multipage_applications_files\npython3 app.py\n\nDash is running on http://127.0.0.1:8050/\n\n * Serving Flask app 'app'\n * Debug mode: on\nError while terminating subprocess (pid=19264): \n\n\nHere you can explore the layers of the dash.page_registry.\n\nimport pickle\nfrom IPython.display import JSON, HTML\n\nwith open(\"multipage_applications_files/page_registry\", \"rb\") as f:\n    j = pickle.load(f)\n\nJSON(j)\n\n&lt;IPython.core.display.JSON object&gt;"
  },
  {
    "objectID": "python/plotly/dragmode.html",
    "href": "python/plotly/dragmode.html",
    "title": "Dragmode",
    "section": "",
    "text": "By default, you can adjust the scale and display sector of Plotly plots by dragging them with your mouse. Sometimes this feature is not suitable, so you can disable it by setting the dragmode layout property to False.\nIn the following example I build absolutely inetic plots, but in the first case dragmode=True (defalut case), in the second case dragmode=False. So try it yourself - you can drag the first plot and cat the second.\n\nimport numpy as np\nimport plotly.graph_objects as go\n\nsamples_counts = 1000\nx_values = np.random.uniform(0, 100, samples_counts)\ny_values = np.random.uniform(0, 100, samples_counts)\n\nmy_scatter = go.Scatter(\n    x = x_values,\n    y = y_values,\n    mode = \"markers\"\n)\n\ngo.Figure(my_scatter).show()\n\ngo.Figure(\n    data = my_scatter,\n    layout=dict(dragmode = False)\n).show()"
  },
  {
    "objectID": "python/torch/differentiation.html",
    "href": "python/torch/differentiation.html",
    "title": "Differentiation",
    "section": "",
    "text": "Torch has tools that allow you to calculate derivatives. This page created for this feature.\nimport torch"
  },
  {
    "objectID": "python/torch/differentiation.html#basic",
    "href": "python/torch/differentiation.html#basic",
    "title": "Differentiation",
    "section": "Basic",
    "text": "Basic\nTo compute derivative in torch you need to create torch.tensor with the property requires_grad = True so that torch will look for the gradient of any function this tensor is involved in.\nThen we need to define a function that depends on the tensor under consideration. And call the backward method from it - it will compute partial derivatives for each tensor on which it depends.\nAfter previous step you will have derivative values in grad field of tensor under consideration.\n\nExample 1\nLet’s say we have function:\n\\[y(\\omega)=\\sum_i^n 3\\omega_i.\\]\nAnd we need to find the derivative of the function on the variables \\(\\omega_i, i\\in\\overline{1,n}\\). Let’s do it by hand at first:\n\\[\\frac{dy}{d \\omega_i} = \\sum_j^n\\frac{d3\\omega_j}{d \\omega_i} = \\sum_j^n3\\frac{d\\omega_j}{d \\omega_i}.\\]\nAnd that’s considering the fact that:\n\\[\\frac{d \\omega_i}{d \\omega_j} = \\begin{cases} 0 , i\\neq j; \\\\ 1 , i=j.\\end{cases}\\]\nWe got:\n\\[\\frac{dy}{d \\omega_i} = 3.\\]\nThe implementation of this example in Torch is listed in the cell below:\n\nn = 5\nw = torch.rand(n, requires_grad = True)\ny = torch.sum(w*3)\ny.backward()\nw.grad\n\ntensor([3., 3., 3., 3., 3.])\n\n\n\n\nExample 2\nNow we have a slightly more complicated function:\n\\[y(\\omega, \\gamma)=\\sum_i^n \\omega_i \\gamma_i.\\]\nSo derivatives by \\(\\omega_i\\) and \\(\\gamma_i, i \\in \\overline{1,n}\\) accordingly:\n\\[\\frac{dy}{d \\omega_i} = \\sum_j^n\\frac{d\\omega_j\\gamma_j}{d \\omega_i} = \\sum_j^n\\gamma_j\\frac{d\\omega_j}{d \\omega_i}=\\gamma_i;\\] \\[\\frac{dy}{d \\gamma_i} = \\sum_j^n\\frac{d\\omega_j\\gamma_j}{d \\gamma_i} = \\sum_j^n\\omega_j\\frac{d\\gamma_j}{d \\gamma_i}=\\omega_i.\\]\nAnd the implementation in the Torch for this case will look like the following cell. The main purpose of this example is to show that if the derivative contains a variable, its value is substituted into the expression. So in the example:\n\n\\(\\omega=(1,2,3,4)\\) - so derivatives of the \\(\\gamma_i\\) take these values;\nlikewise \\(\\gamma=(5,6,7,8)\\) - derivatvies of the \\(\\omega_i\\) take these values.\n\n\nw = torch.tensor([1, 2, 3, 4], dtype = torch.float, requires_grad = True)\ng = torch.tensor([5, 6, 7, 8], dtype = torch.float, requires_grad = True)\n\ny = w @ g\ny.backward()\nprint(\"omega gradient value\")\nprint(w.grad)\nprint(\"gamma gradient value\")\nprint(g.grad)\n\nomega gradient value\ntensor([5., 6., 7., 8.])\ngamma gradient value\ntensor([1., 2., 3., 4.])"
  },
  {
    "objectID": "python/torch/differentiation.html#gradient-descent",
    "href": "python/torch/differentiation.html#gradient-descent",
    "title": "Differentiation",
    "section": "Gradient descent",
    "text": "Gradient descent\nThe torch’s ability to calculate derivatives is extremely useful for gradient descent. So here is a simple example of a gradient descent implementation using torch derivatives.\nSample generation:\n\nn_features = 2\nn_objects = 300\n\nw_true = torch.randn(n_features)\nX = (torch.rand(n_objects, n_features) - 0.5) * 5\nY = X @ w_true + torch.randn(n_objects) / 2\n\nThe implementation of the algorithm is shown in the following cell.\nAt each iteration: - The predictions for the current weights are computed; - The MSE for the current prediction is calculated; - A gradient of MSE on the weights is taken with line MSE.backward(); - In the weights space, a step is made in the direction of the antigradient. We need to wrap this operation with torch.no_grad() so that this calculation isn’t used for gradient calculations.\n\nstep_size = 1e-2\nw = torch.rand(X.shape[1], requires_grad = True)\nfor i in range(5000):\n    y_pred = torch.matmul(X,w)\n    MSE = torch.mean((y_pred - Y)**2)\n    MSE.backward()\n    \n    with torch.no_grad():\n        w -= w.grad * step_size\n\n    w.grad.zero_()\n\nSo let’s compare the real coefficients and the result of the approximation - it’s pretty close.\n\nprint(\"Real coefs:\", w_true.tolist())\nprint(\"Approximation coefs:\", w.tolist())\n\nReal coefs: [-0.1252637803554535, 0.5651034116744995]\nApproximation coefs: [-0.1096344143152237, 0.5543152689933777]"
  },
  {
    "objectID": "python/torch/nn_module.html",
    "href": "python/torch/nn_module.html",
    "title": "nn.Module class",
    "section": "",
    "text": "Is a class that allows you to define complex neural networks in Torch. Simply use this class as a descendant.\nimport torch\nfrom torch import nn"
  },
  {
    "objectID": "python/torch/nn_module.html#parameters",
    "href": "python/torch/nn_module.html#parameters",
    "title": "nn.Module class",
    "section": "parameters()",
    "text": "parameters()\nOfficial documentation.\nTo optimise the network, you need access to the parameters that will change as the model is optimised. The Parameters method fulfils this role. It looks like it somehow understands that the fields it contains are descendants of the class nn.Module and extracts their parameters.\nTwo following cells allow you to compare what parameters return if we use just empty ancestor of nn.Module and ancestor that have some fields that actually implementations of nn.Module.\nIn the following cell we have an empty nn.Module - so when we try to unpack it generator to list we have just an empty list:\n\nclass EmptyNetwork(nn.Module):\n    pass\nempty_network = EmptyNetwork()\n[i for i in empty_network.parameters()]\n\n[]\n\n\nThis cell implements such a descendant of the nn.Module, taking some parameters from its files. To be more specific, there are two fully connected layers defined here. So we end up with four tensors, two matrices for fully connected layers and their biases:\n\nclass ParametersNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.foo = nn.Linear(3, 3)\n        self.bar = nn.Linear(5, 5)\n\nnetwork = ParametersNetwork()\nfor i in network.parameters():\n    print(i.data)\n\ntensor([[ 0.0302, -0.4218,  0.3589],\n        [ 0.0500, -0.0864,  0.2156],\n        [-0.5365,  0.4816,  0.2940]])\ntensor([-0.0345, -0.4456, -0.4284])\ntensor([[ 0.1643,  0.3919,  0.4324, -0.2549,  0.0276],\n        [-0.0170,  0.2218, -0.3960,  0.1335,  0.3155],\n        [ 0.0246, -0.0172,  0.3138,  0.4222,  0.0569],\n        [ 0.2502,  0.3730, -0.1723,  0.1000,  0.1558],\n        [ 0.2668, -0.3949,  0.1780,  0.2885,  0.2406]])\ntensor([-0.0279, -0.2638,  0.1210, -0.1394, -0.0539])"
  },
  {
    "objectID": "python/torch/regression_network.html",
    "href": "python/torch/regression_network.html",
    "title": "Regression in torch",
    "section": "",
    "text": "import torch\nfrom torch import nn, optim\n\nfrom torch.utils.data import TensorDataset, DataLoader"
  },
  {
    "objectID": "python/torch/regression_network.html#create-network",
    "href": "python/torch/regression_network.html#create-network",
    "title": "Regression in torch",
    "section": "Create network",
    "text": "Create network\nYou need to create a network object that describes the sequence of transformations performed by the model under consideration.\n\nnet = nn.Sequential(\n    nn.Linear(in_features = 100, out_features = 10),\n    nn.ReLU(),\n    nn.Linear(in_features = 10, out_features = 1)\n)\nnet\n\nSequential(\n  (0): Linear(in_features=100, out_features=10, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=10, out_features=1, bias=True)\n)"
  },
  {
    "objectID": "python/torch/regression_network.html#optimizer",
    "href": "python/torch/regression_network.html#optimizer",
    "title": "Regression in torch",
    "section": "Optimizer",
    "text": "Optimizer\nIs a special object that allows you to change the weights of the network to make it better.\n\noptim.Adam(net.parameters())\n\nAdam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.001\n    maximize: False\n    weight_decay: 0\n)"
  },
  {
    "objectID": "python/torch/regression_network.html#loss",
    "href": "python/torch/regression_network.html#loss",
    "title": "Regression in torch",
    "section": "Loss",
    "text": "Loss\nIn torch there are classes that implement loss functions.\n\nloss = nn.MSELoss()\nloss(torch.rand(100), torch.rand(100))\n\ntensor(0.1757)"
  },
  {
    "objectID": "python/torch/regression_network.html#dataset",
    "href": "python/torch/regression_network.html#dataset",
    "title": "Regression in torch",
    "section": "Dataset",
    "text": "Dataset\nThere are classes that implement a convenient way of organising your data sets. The advantage is that you just pass a sequence of tensors describing your objects and just by using the indexing syntax you can access any object you want.\n\ndataset = TensorDataset(\n    torch.rand(100, 2), \n    torch.rand(100, 1), \n    torch.rand(100, 5)\n)\ndataset[5]\n\n(tensor([0.5017, 0.2694]),\n tensor([0.7112]),\n tensor([0.3316, 0.2062, 0.4393, 0.0639, 0.5028]))"
  },
  {
    "objectID": "python/torch/regression_network.html#dataloader",
    "href": "python/torch/regression_network.html#dataloader",
    "title": "Regression in torch",
    "section": "Dataloader",
    "text": "Dataloader\nObject that implements the partitioning of the dataset into batches. You just pass the dataset object and the required batch size. By iterating on the gotten object you will get batches.\n\nmy_loader = DataLoader(\n    TensorDataset(\n        torch.rand(10, 3),\n        torch.rand(10)\n    ),\n    batch_size = 2\n)\n\nfor a,b in my_loader:\n    print(a,b)\n\ntensor([[0.9273, 0.7325, 0.2644],\n        [0.8303, 0.4975, 0.5738]]) tensor([0.9972, 0.5394])\ntensor([[0.4124, 0.5600, 0.0088],\n        [0.5078, 0.0942, 0.3956]]) tensor([0.5644, 0.7418])\ntensor([[0.7397, 0.4297, 0.1283],\n        [0.0926, 0.3512, 0.6297]]) tensor([0.4397, 0.8969])\ntensor([[0.6764, 0.2055, 0.4628],\n        [0.3138, 0.7287, 0.8516]]) tensor([0.5232, 0.4896])\ntensor([[0.6208, 0.2415, 0.4725],\n        [0.4048, 0.2538, 0.6035]]) tensor([0.8369, 0.3332])"
  },
  {
    "objectID": "python/torch/regression_network.html#fit-model",
    "href": "python/torch/regression_network.html#fit-model",
    "title": "Regression in torch",
    "section": "Fit model",
    "text": "Fit model\nNow lets combine all previous steps to show simple model fit.\nSample generation.\n\nn_features = 2\nn_objects = 300\n\nw_true = torch.randn(n_features)\nX = (torch.rand(n_objects, n_features) - 0.5) * 5\nY = X @ w_true + torch.randn(n_objects) / 2\n\nFitting algorithm.\n\ndata_loader = DataLoader(TensorDataset(X, Y), batch_size = 20)\nnet = nn.Sequential(\n    nn.Linear(in_features = n_features, out_features = 1, bias = False)\n)\noptimizer = optim.Adam(net.parameters(), lr = 0.1)\nloss_fn = nn.MSELoss()\nloss_values = []\n\nfor x, y in data_loader:\n    \n    optimizer.zero_grad()\n    # forward path\n    output = net(x)\n    # loss compution\n    loss_val = loss_fn(output.ravel(), y)\n    loss_values.append(loss_val.item())\n    # packward path\n    loss_val.backward()\n    optimizer.step()\n\nprint(\"Loss path \", \"-&gt;\".join([str(round(val, 2)) for val in loss_values]))\nprint(\"True weights \" , w_true)\nprint(\"Estimated weights\", list(net.parameters())[0][0])\n\nLoss path  1.58-&gt;0.94-&gt;1.17-&gt;0.52-&gt;0.39-&gt;0.23-&gt;0.34-&gt;0.34-&gt;0.22-&gt;0.24-&gt;0.29-&gt;0.33-&gt;0.43-&gt;0.2-&gt;0.42\nTrue weights  tensor([-0.7069, -0.2510])\nEstimated weights tensor([-0.9058, -0.2342], grad_fn=&lt;SelectBackward0&gt;)"
  },
  {
    "objectID": "python/torch/logistic_regression.html",
    "href": "python/torch/logistic_regression.html",
    "title": "Logistic regression",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn, optim\n\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom sklearn.metrics import roc_auc_score"
  },
  {
    "objectID": "python/torch/logistic_regression.html#create-sample",
    "href": "python/torch/logistic_regression.html#create-sample",
    "title": "Logistic regression",
    "section": "Create sample",
    "text": "Create sample\n\nsample_size = 1000\n\nX = torch.rand(sample_size, 2)\nY = torch.tensor([\n    np.random.choice([0, 1], p = [1-p.item(), p.item()]) \n    for p in X.mean(1)\n])\n\n\nplt.scatter(X[:,0], X[:,1], c = Y)\nplt.show()"
  },
  {
    "objectID": "python/torch/logistic_regression.html#model",
    "href": "python/torch/logistic_regression.html#model",
    "title": "Logistic regression",
    "section": "Model",
    "text": "Model\n\nnetwork = nn.Sequential(\n    nn.Linear(in_features= 2, out_features = 1, bias = False),\n    nn.Sigmoid()\n)"
  },
  {
    "objectID": "python/torch/logistic_regression.html#fit-model",
    "href": "python/torch/logistic_regression.html#fit-model",
    "title": "Logistic regression",
    "section": "Fit model",
    "text": "Fit model\n\ndata_loder = DataLoader(\n    TensorDataset(X, Y), batch_size = 100\n)\noptimizer = optim.Adam(network.parameters(), lr = 0.001)\nloss = nn.CrossEntropyLoss()\n\nloss_values = []\nfor i in range(1000):\n    for x, y in data_loder:\n        optimizer.zero_grad()\n        predict = network(x)\n        loss_val = loss(predict.ravel(), y.float())\n        loss_values.append(loss_val.item())\n        loss_val.backward()\n        optimizer.step()"
  },
  {
    "objectID": "python/torch/logistic_regression.html#roc_auc",
    "href": "python/torch/logistic_regression.html#roc_auc",
    "title": "Logistic regression",
    "section": "\\(ROC_{auc}\\)",
    "text": "\\(ROC_{auc}\\)\n\nroc_auc_score(Y.tolist(), network(X).ravel().tolist())\n\n0.7214294001624318"
  },
  {
    "objectID": "python/torch/tensor/element_wise.html",
    "href": "python/torch/tensor/element_wise.html",
    "title": "Element-wise operations",
    "section": "",
    "text": "If you have a tensor with dimension \\((n_1,n_2,...,n_k, ...,n_m)\\) and you want to perform some operation on dimension \\(k\\). You need to create a tensor with dimension \\((n_k, 1, 1, ..., 1) \\in \\mathbb{N}^k\\). Then you can just use basic algebraic operations.\nimport torch\nimport numpy as np"
  },
  {
    "objectID": "python/torch/tensor/element_wise.html#two-dimensional-example",
    "href": "python/torch/tensor/element_wise.html#two-dimensional-example",
    "title": "Element-wise operations",
    "section": "Two-dimensional example",
    "text": "Two-dimensional example\nSuppose you need to add a zero tensor to an array containing two numbers. So in the following cells the tensors a and b are created, which we will use for example:\n\nnp.random.seed(10)\n\na = torch.zeros((2, 3))\nb = torch.tensor([2,3])\n\n\nprint(\"Tensor a\")\nprint(a)\nprint(\"shape - \", list(a.shape), end = \"\\n\\n\\n\")\n\n\nprint(\"Tensor b\")\nprint(b)\nprint(\"shape - \", list(b.shape))\n\nTensor a\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\nshape -  [2, 3]\n\n\nTensor b\ntensor([2, 3])\nshape -  [2]\n\n\nThe added tensor must be converted so that \\(n_1=m_1\\).\nWhere : - \\(n_i\\) - zeros tensor \\(i-th\\) dimention size; - \\(m_i\\) - addimg tensor \\(i-th\\) dimetion size.\nSo in the following example we will use unsqueeze for this purpose.\n\nb_unsqueezed = b.unsqueeze(1)\nprint(b_unsqueezed)\nprint(\"shape - \", list(b_unsqueezed.shape))\n\ntensor([[2],\n        [3]])\nshape -  [2, 1]\n\n\nAnd finally result of sum. Essentially we just added the corresponding values of array b to each element in the row of array a:\n\na + b_unsqueezed\n\ntensor([[2., 2., 2.],\n        [3., 3., 3.]])"
  },
  {
    "objectID": "python/torch/tensor/element_wise.html#arbitrary-dimention-example",
    "href": "python/torch/tensor/element_wise.html#arbitrary-dimention-example",
    "title": "Element-wise operations",
    "section": "Arbitrary dimention example",
    "text": "Arbitrary dimention example\nThis shows how to add a tensor of any dimension to another tensor along the selected axis. The following cell creates the tensors we’ll use as an example:\n\nnp.random.seed(10)\n\na = torch.zeros((3, 2, 3, 3))\nb = torch.tensor([2,3])\nprint(\"Tensor a\")\nprint(a)\nprint(\"shape - \", list(a.shape), end = \"\\n\\n\\n\")\n\nprint(\"Tensor b\")\nprint(b)Transformation of the adding tensor to the power sum:\nprint(\"shape - \", list(b.shape))\n\nTensor a\ntensor([[[[0., 0., 0.],\n          [0., 0., 0.],\n          [0., 0., 0.]],\n\n         [[0., 0., 0.],\n          [0., 0., 0.],\n          [0., 0., 0.]]],\n\n\n        [[[0., 0., 0.],\n          [0., 0., 0.],\n          [0., 0., 0.]],\n\n         [[0., 0., 0.],\n          [0., 0., 0.],\n          [0., 0., 0.]]],\n\n\n        [[[0., 0., 0.],\n          [0., 0., 0.],\n          [0., 0., 0.]],\n\n         [[0., 0., 0.],\n          [0., 0., 0.],\n          [0., 0., 0.]]]])\nshape -  [3, 2, 3, 3]\n\n\nTensor b\ntensor([2, 3])\nshape -  [2]\n\n\nTransformation of adding tensor to pefrorm sum:\n\nb_unsqueezed = b.unsqueeze(1).unsqueeze(1)\nprint(\"tensor\")\nprint(b_unsqueezed)\nprint(\"shape - \", list(b_unsqueezed.shape))\n\ntensor\ntensor([[[2]],\n\n        [[3]]])\nshape -  [2, 1, 1]\n\n\nAnd finally do the sum. So for each \\(i-th; i \\in \\overline{1,n_k}\\) within the tensor with dimension \\((n_{k+1},...,n_{m})\\) the \\(i-th\\) number from the adding tensor is added.\n\na+b_unsqueezed\n\ntensor([[[[2., 2., 2.],\n          [2., 2., 2.],\n          [2., 2., 2.]],\n\n         [[3., 3., 3.],\n          [3., 3., 3.],\n          [3., 3., 3.]]],\n\n\n        [[[2., 2., 2.],\n          [2., 2., 2.],\n          [2., 2., 2.]],\n\n         [[3., 3., 3.],\n          [3., 3., 3.],\n          [3., 3., 3.]]],\n\n\n        [[[2., 2., 2.],\n          [2., 2., 2.],\n          [2., 2., 2.]],\n\n         [[3., 3., 3.],\n          [3., 3., 3.],\n          [3., 3., 3.]]]])"
  },
  {
    "objectID": "python/torch/vision/mnist.html",
    "href": "python/torch/vision/mnist.html",
    "title": "MNIST",
    "section": "",
    "text": "A popular dataframe for first steps and experiments in computer vision. So here are my solutions for MNIST using torch.\nfrom tqdm import tqdm\n\nimport pandas as pd\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader\n\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\n\nfrom IPython.display import clear_output\n\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "python/torch/vision/mnist.html#downloading-data",
    "href": "python/torch/vision/mnist.html#downloading-data",
    "title": "MNIST",
    "section": "Downloading data",
    "text": "Downloading data\nThere is a special module in Torch that allows you to download the MNIST dataset to your computer.\n\ndata_train = MNIST(\n    \"mnist_files\",\n    train = True,\n    download = True,\n    transform = transforms.ToTensor()\n)\ndata_valid = MNIST(\n    \"mnist_files\",\n    train = False,\n    download = True,\n    transform = transforms.ToTensor()\n)\n\n\ntrain_loader = DataLoader(data_train, batch_size = 64, shuffle = True)\nvalid_loader = DataLoader(data_valid, batch_size = 64, shuffle = True)"
  },
  {
    "objectID": "python/torch/vision/mnist.html#data-description",
    "href": "python/torch/vision/mnist.html#data-description",
    "title": "MNIST",
    "section": "Data description",
    "text": "Data description\nInput features is a matrix describing images with dimensions (n_channels, picture_width, picture_height).\n\ndata_train[0][0].shape\n\ntorch.Size([1, 28, 28])\n\n\nTarget is an integer that indicates which number will be displayed on the image.\n\ndata_train[0][1]\n\n5"
  },
  {
    "objectID": "python/torch/vision/mnist.html#basic-network",
    "href": "python/torch/vision/mnist.html#basic-network",
    "title": "MNIST",
    "section": "Basic network",
    "text": "Basic network\n\nDefinition and usage\nHere is a model that implements a transformation that takes tesnsor describing picture and returns model 10 numbers.\n\nbasic_network = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(28*28, 256),\n    nn.ReLU(),\n    nn.Linear(256, 10)\n)\n\nbasic_network(data_train[0][0]).tolist()\n\n[[-0.22059409320354462,\n  -0.1914120316505432,\n  0.007970903068780899,\n  -0.05525556206703186,\n  -0.05393986031413078,\n  0.0036779269576072693,\n  0.051963288336992264,\n  0.03920704871416092,\n  0.06251650303602219,\n  -0.1357821673154831]]\n\n\nWe understand the intex of the maximum value as the predicted class. To extract it from the model result, you need to use code like in the cell below.\n\npred_number, pred_class = torch.max(basic_network(data_train[0][0]), 1)\nprint(\"predicted number\", pred_number.item())\nprint(\"predicted class\", pred_class.item())\n\npredicted number 0.06251650303602219\npredicted class 8\n\n\nOr the same code applied to the whole batch.\n\nX, y = next(iter(train_loader))\nvalues, classes = torch.max(basic_network(X), 1)\n\npd.DataFrame({\n    \"values\" : values.tolist(),\n    \"classes\" : classes.tolist()\n}).head()\n\n\n\n\n\n\n\n\nvalues\nclasses\n\n\n\n\n0\n0.109403\n6\n\n\n1\n0.158294\n6\n\n\n2\n0.126944\n6\n\n\n3\n0.137046\n8\n\n\n4\n0.135719\n8\n\n\n\n\n\n\n\n\n\nOptimization\nBelow is the function that implements the model training epoch.\n\ndef epoch(\n    model: nn.Module, \n    data_loader: DataLoader, \n    optimizer: optim.Optimizer, \n    loss_fn\n):\n    model.train()\n    loss_values = []\n    \n    for x, y in tqdm(data_loader, desc = \"train\"):\n        optimizer.zero_grad()\n        output = model(x)\n        loss_val = loss_fn(output, y)\n        loss_values.append(loss_val.item())\n        loss_val.backward()\n        optimizer.step()\n\n    return sum(loss_values)/len(loss_values)\n\n\n\nEvaluation\nHere is a function that allows you to estimate the loss and accuracy of the model for test data.\n\noptimizer = optim.Adam(basic_network.parameters(), lr = 1e-3)\nloss_fn = nn.CrossEntropyLoss()\n\n@torch.inference_mode()\ndef evaluate(\n    model: nn.Module, \n    data_loader: DataLoader, \n    loss_fn\n    ):\n    loss_values = []\n    model.eval()\n\n    total = 0\n    correct = 0\n \n    for x, y in tqdm(data_loader, desc = \"evaluation\"):\n        output = model(x)\n        \n        loss_val = loss_fn(output, y)\n        loss_values.append(loss_val.item())\n\n        _, classes = torch.max(model(x), 1)\n        \n        total += len(x)\n        correct += (classes == y).sum().item()\n    \n    return sum(loss_values)/len(loss_values), correct/total\n\n\n\nFitting\nHere is a cycle that runs a few epochs of network load.\n\ntrain_scores, test_scores, accuracies = [], [], []\n\nfor i in range(100):\n    train_scores.append(epoch(\n        model = basic_network,\n        data_loader = train_loader,\n        optimizer = optimizer,\n        loss_fn = loss_fn\n    ))\n    test_score, accuracy = evaluate(\n        model = basic_network, \n        data_loader = valid_loader,\n        loss_fn = loss_fn\n    )\n    test_scores.append(test_score)\n    accuracies.append(accuracy)\n\n\n    clear_output()\n    if accuracy &gt; 0.98:\n        break\n\nVisualise the progress of the model in the plots below.\n\nplt.figure(figsize = [10, 3])\nplt.subplot(121)\nplt.title(\"Accuracy\")\nplt.plot(range(len(accuracies)), accuracies)\nplt.subplot(122)\nplt.title(\"Learn curves\")\nplt.plot(range(len(train_scores)), train_scores)\nplt.plot(range(len(test_scores)), test_scores)\nplt.legend([\"train\", \"test\"])\nplt.ylim(0,0.4)\nplt.show()"
  },
  {
    "objectID": "python/torch/vision/mnist.html#сonvolutional-neural-network",
    "href": "python/torch/vision/mnist.html#сonvolutional-neural-network",
    "title": "MNIST",
    "section": "Сonvolutional neural network",
    "text": "Сonvolutional neural network\nYou may think that result of the previous model is pretty good, but you can imporove it iven more using convolutinal layers.\nIn the next cell, we’ve just defined the model that we’re going to fit.\n\ncnn = nn.Sequential(\n    nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 5),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size =  2),\n\n    nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 5),\n    nn.ReLU(),\n    nn.MaxPool2d(kernel_size = 2),\n\n    nn.Flatten(),\n    nn.Linear(4*4*64, 256),\n    nn.ReLU(),\n    nn.Linear(256, 10)\n)\n\nWe can use the same methods for fitting as for the basic model.\nNote Don’t forget to pass the weights of the new model to the optimiser, otherwise it will continue to work with the weights of the base model.\n\noptimizer = optim.Adam(cnn.parameters(), lr = 1e-3)\n\ntrain_scores, test_scores, accuracies = [], [], []\n\nfor i in range(100):\n    train_scores.append(epoch(\n        model = cnn,\n        data_loader = train_loader,\n        optimizer = optimizer,\n        loss_fn = loss_fn\n    ))\n    test_score, accuracy = evaluate(\n        model = cnn, \n        data_loader = valid_loader,\n        loss_fn = loss_fn\n    )\n    test_scores.append(test_score)\n    accuracies.append(accuracy)\n\n\n    clear_output()\n    if accuracy &gt; 0.993:\n        break\n\nAnd visualising of the results:\n\nplt.figure(figsize = [10, 3])\nplt.subplot(121)\nplt.title(\"Accuracy\")\nplt.plot(range(len(accuracies)), accuracies)\nplt.subplot(122)\nplt.title(\"Learn curves\")\nplt.plot(range(len(train_scores)), train_scores)\nplt.plot(range(len(test_scores)), test_scores)\nplt.legend([\"train\", \"test\"])\nplt.ylim(0,0.4)\nplt.show()"
  },
  {
    "objectID": "python/torch/vision/augmentations.html",
    "href": "python/torch/vision/augmentations.html",
    "title": "Augmentations",
    "section": "",
    "text": "Is some transformation of data that allows you to get more data or transfrom your data in specific way.\nfrom torchvision import transforms\nfrom PIL import Image\n\nfrom IPython.display import HTML\nTo show the result of the transformations, the image loaded and displayed below is used.\nimg = Image.open(\"cat.jpg\")\nimg"
  },
  {
    "objectID": "python/torch/vision/augmentations.html#resize",
    "href": "python/torch/vision/augmentations.html#resize",
    "title": "Augmentations",
    "section": "Resize",
    "text": "Resize\nIs a transformation that allows you to set a specific size for the image. So in the following cell I show how it can be done for a specific image.\n\ndisplay(HTML(\"&lt;h4&gt;Transforemd image&lt;/h4&gt;\"))\ndisplay(transforms.Resize((224, 224))(img))\n\nTransforemd image"
  },
  {
    "objectID": "python/torch/vision/augmentations.html#horizontal-flip",
    "href": "python/torch/vision/augmentations.html#horizontal-flip",
    "title": "Augmentations",
    "section": "Horizontal flip",
    "text": "Horizontal flip\nYou can use torchvision.transforms.RandomHorizontalFlip for this purpose. But this transforms the flip randomly, you have to set the p parameter as the probability of the flip.\nIn the following cell I is showed RandomHorizontalFlip(p=1) that always will transform picture.\n\ntransforms.RandomHorizontalFlip(p=1)(img)"
  },
  {
    "objectID": "python/torch/vision/unet.html",
    "href": "python/torch/vision/unet.html",
    "title": "Unet",
    "section": "",
    "text": "This page describes the implementation of the unet network architecture in Torch.\nfrom tqdm import tqdm\nimport pickle\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\nimport torchvision\nfrom torchvision.datasets import OxfordIIITPet\nfrom torchvision import transforms as T\n\ntest_picture_index = 500"
  },
  {
    "objectID": "python/torch/vision/unet.html#data-set",
    "href": "python/torch/vision/unet.html#data-set",
    "title": "Unet",
    "section": "Data set",
    "text": "Data set\nWill be used OxfordIIITPet. We will load it with target_types = 'segmentation' so for each picutre we will have picture itself and mask for it.\n\nPictures\nHere is an example of what we deal with as images.\n\ntrain_dataset = OxfordIIITPet(\n    \"OxfordIIITPet/\", \n    target_types = \"segmentation\", \n    download = True\n)\ntest_dataset = OxfordIIITPet(\n    \"OxfordIIITPet/\", \n    target_types = \"segmentation\", \n    download = True,\n    split = \"test\"\n)\n\nplt.figure(figsize = (10, 9))\nplt.subplot(121)\nplt.imshow(train_dataset[test_picture_index][0])\nplt.xticks([]);plt.yticks([])\nplt.title(\"Picture\")\n\nplt.subplot(122)\nplt.imshow(train_dataset[test_picture_index][1])\nplt.xticks([]);plt.yticks([])\nplt.title(\"Mask\")\n\nplt.show()\n\n\n\n\n\n\nTransfomations\nThe network does not understand images. So we have to create tensors from images. Here we have added some transformations to our dataset. Let’s see what they do.\n\ntransform = T.Compose([\n    T.Resize((256, 256)),\n    T.ToTensor()\n])\ntarget_transfrom = T.Compose([\n    T.Resize((256, 256)),\n    T.PILToTensor(),\n    # classes are marked starting with 1\n    # but in python it's easier to work with\n    # with sets that start with 0, so the following\n    # transformation fixes this inconvenience\n    T.Lambda(lambda x: (x - 1).long())\n])\n\ntransforms = (\n    torchvision.datasets.\n    vision.StandardTransform(transform, target_transfrom)\n)\n\ntrain_dataset.transforms = transforms\ntest_dataset.transforms = transforms\n\nThis is the shape of the tensor used as input to the model - it’s regular three channel picture.\n\ntrain_dataset[test_picture_index][0].shape\n\ntorch.Size([3, 256, 256])\n\n\nThis is the shape and unique values that appear in the target. So it’s just an array whose shape is the same as the shape of the input images, but it only takes three values.\n\nprint(\"Shape:\", list(train_dataset[test_picture_index][1].shape))\nprint(\"Values:\", list(train_dataset[test_picture_index][1].unique()))\n\nShape: [1, 256, 256]\nValues: [tensor(0), tensor(1), tensor(2)]"
  },
  {
    "objectID": "python/torch/vision/unet.html#architecture",
    "href": "python/torch/vision/unet.html#architecture",
    "title": "Unet",
    "section": "Architecture",
    "text": "Architecture\nHere is a description of the transformation sequence that we usually understand as the UNET architecture.\nTo show sequence of trainsformations we will take one picture from dataset. But all transformations are usually performed on a batch of objects, so we need unsqueeze(0) to simulate a batch of one object.\n\nexample_image = train_dataset[test_picture_index][0].unsqueeze(0)\nexample_image.shape\n\ntorch.Size([1, 3, 256, 256])\n\n\nbase_channels is hyper parameter of the UNET. Usually more base_channels leads to a more complex model.\n\nbase_channels = 16\n\n\nOperations\nThe following cell defines a function that returns a transformation that allows to change the number of channels of input data. It’s a very common operation for the UNET architecture.\n\ndef conv_plus_conv(in_channels: int, out_channels: int):\n    \"\"\"\n    Makes UNet block\n    :param in_channels: input channels\n    :param out_channels: output channels\n    :return: UNet block\n    \"\"\"\n    return nn.Sequential(\n        nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        ),\n        nn.BatchNorm2d(num_features=out_channels),\n        nn.LeakyReLU(0.2),\n        nn.Conv2d(\n            in_channels=out_channels,\n            out_channels=out_channels,\n            kernel_size=3,\n            stride=1,\n            padding=1\n        ),\n        nn.BatchNorm2d(num_features=out_channels),\n        nn.LeakyReLU(0.2),\n    )\n\nWe also need a transformation that reduces the resolution of the data for the down blocks.\n\ndownsample = nn.MaxPool2d(kernel_size=2, stride=2)\n\n\n\nDown blocks\nEach down block should first increase the number of channels and make the image smaller - downsample the image. But we need to save the state of the data after increasing the channels to use it later in up blocks.\n\nprint(\"Input shape:\", example_image.shape)\ndown1 = conv_plus_conv(in_channels = 3, out_channels = base_channels)\nresidual1 = down1(example_image)\nprint(\"Increased channels shape:\", residual1.shape)\ndown_result = downsample(residual1)\nprint(\"Downsampled resolution shape:\", down_result.shape)\n\nInput shape: torch.Size([1, 3, 256, 256])\nIncreased channels shape: torch.Size([1, 16, 256, 256])\nDownsampled resolution shape: torch.Size([1, 16, 128, 128])\n\n\nWe repeat this operation several times. For example, let’s do it one more time. Usually more repetitions lead to better results.\n\nprint(\"Input shape:\", down_result.shape)\ndown2 = conv_plus_conv(in_channels = base_channels, out_channels = base_channels*2)\nresidual2 = down2(down_result)\nprint(\"Increased channels shape:\", residual2.shape)\ndown_result = downsample(residual2)\nprint(\"Downsampled resolution shape:\", down_result.shape)\n\nInput shape: torch.Size([1, 16, 128, 128])\nIncreased channels shape: torch.Size([1, 32, 128, 128])\nDownsampled resolution shape: torch.Size([1, 32, 64, 64])\n\n\n\n\nBottle neck\nIs just convolutional transfomation that saves all dimentions of the data.\n\nprint(\"Input shape:\", down_result.shape)\nbottleneck = conv_plus_conv(base_channels * 2, base_channels * 2)\nbottleneck_result = bottleneck(down_result)\nprint(\"Output shape:\", bottleneck_result.shape)\n\nInput shape: torch.Size([1, 32, 64, 64])\nOutput shape: torch.Size([1, 32, 64, 64])\n\n\n\n\nUp blocks\nThe up blocks carry out essentially the reverse operations:\n\nIncrese resolution of the picture;\nDecrease number of channels.\n\nBut it also concatenates channels from down blocks - that is why we stored this information in down blocks.\n\nprint(\"Input shape:\", bottleneck_result.shape)\nup_result = nn.functional.interpolate(bottleneck_result, scale_factor=2)\nprint(\"Upsampled shape:\", up_result.shape)\nup_result = torch.cat((up_result, residual2), dim=1)\nprint(\"Concatenated shape:\", up_result.shape)\nup2 = conv_plus_conv(base_channels * 4, base_channels)\nup_result = up2(up_result)\nprint(\"Channels decreased shape:\", up_result.shape)\n\nInput shape: torch.Size([1, 32, 64, 64])\nUpsampled shape: torch.Size([1, 32, 128, 128])\nConcatenated shape: torch.Size([1, 64, 128, 128])\nChannels decreased shape: torch.Size([1, 16, 128, 128])\n\n\nThis operation is repeated as many times as there have been blocks down. So for this example we need to repeate it one more time:\n\nprint(\"Input shape:\", up_result.shape)\nup_result = nn.functional.interpolate(up_result, scale_factor=2)\nprint(\"Upsampled shape:\", up_result.shape)\nup_result = torch.cat((up_result, residual1), dim=1)\nprint(\"Concatenated shape:\", up_result.shape)\nup1 = conv_plus_conv(base_channels * 2, base_channels)\nup_result = up1(up_result)\nprint(\"Channels decreased shape:\", up_result.shape)\n\nInput shape: torch.Size([1, 16, 128, 128])\nUpsampled shape: torch.Size([1, 16, 256, 256])\nConcatenated shape: torch.Size([1, 32, 256, 256])\nChannels decreased shape: torch.Size([1, 16, 256, 256])\n\n\n\n\nOut\nAnd the final prediction is produced by the base convolutional layer, which returns the original number of classes.\n\nout = nn.Conv2d(\n    in_channels=base_channels, \n    out_channels=3, \n    kernel_size=1\n)\nresult = out(up_result)\nresult.shape\n\ntorch.Size([1, 3, 256, 256])"
  },
  {
    "objectID": "python/torch/vision/unet.html#model-fitting",
    "href": "python/torch/vision/unet.html#model-fitting",
    "title": "Unet",
    "section": "Model fitting",
    "text": "Model fitting\nNow let’s try to build model based on described architecture.\n\nDefining model\nFollowing class implements UNET in torch.\n\nclass UNET(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        base_channels = 32\n\n        self.down1 = conv_plus_conv(3, base_channels)\n        self.down2 = conv_plus_conv(base_channels, base_channels * 2)\n        self.down3 = conv_plus_conv(base_channels*2, base_channels * 4)\n        self.down4 = conv_plus_conv(base_channels*4, base_channels * 8)\n\n        self.up1 = conv_plus_conv(base_channels * 2, base_channels)\n        self.up2 = conv_plus_conv(base_channels * 4, base_channels)\n        self.up3 = conv_plus_conv(base_channels * 8, base_channels*2)\n        self.up4 = conv_plus_conv(base_channels * 16, base_channels*4)\n\n        self.bottleneck = conv_plus_conv(base_channels * 8, base_channels * 8)\n\n        self.out = nn.Conv2d(in_channels=base_channels, out_channels=3, kernel_size=1)\n\n        self.downsample = nn.MaxPool2d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n\n        residual1 = self.down1(x)\n        x = self.downsample(residual1)\n        residual2 = self.down2(x)\n        x = self.downsample(residual2)\n        residual3 = self.down3(x)\n        x = self.downsample(residual3)\n        residual4 = self.down4(x)\n        x = self.downsample(residual4)\n\n        x = self.bottleneck(x)\n        \n        x = nn.functional.interpolate(x, scale_factor=2)\n        x = torch.cat((x, residual4), dim=1)\n        x = self.up4(x)\n        x = nn.functional.interpolate(x, scale_factor=2)\n        x = torch.cat((x, residual3), dim=1)\n        x = self.up3(x)\n        x = nn.functional.interpolate(x, scale_factor=2)\n        x = torch.cat((x, residual2), dim=1)\n        x = self.up2(x)\n        x = nn.functional.interpolate(x, scale_factor=2)\n        x = torch.cat((x, residual1), dim=1)\n        x = self.up1(x)\n\n        x = self.out(x)\n\n        return x\n\nLet’s check that everything goes fine, transformations are performed without errors and the output dimension is as expected.\n\nmodel = UNET()\nmodel(test_dataset[0][0].unsqueeze(0)).shape\n\ntorch.Size([1, 3, 256, 256])\n\n\n\n\nTrain functions\nThis is where we define the functions we’re going to use for the train model and evaluate its accuracy.\n\ndef train(\n    model, device, optimizer, loss_fn, loader\n) -&gt; float:\n    '''\n    Traning algorithm.\n    '''\n    \n    model.train()\n\n    train_loss = 0\n    total = 0\n    correct = 0\n\n    for x, y in tqdm(loader, desc='Train'):\n        bs = y.size(0)\n        x, y = x.to(device), y.squeeze(1).to(device)\n        optimizer.zero_grad()\n\n        output = model(x)\n        loss = loss_fn(output.reshape(bs, 3, -1), y.reshape(bs, -1))\n        train_loss += loss.item()\n        loss.backward()\n\n        optimizer.step()\n\n        _, y_pred = output.max(dim=1)\n        total += y.size(0) * y.size(1) * y.size(2)\n        correct += (y == y_pred).sum().item()\n\n    train_loss /= len(loader)\n    accuracy = correct / total\n\n    return train_loss, accuracy\n\n@torch.inference_mode()\ndef evaluate(model, loader, device, loss_fn) -&gt; tuple[float, float]:\n    '''\n    Evaluation function.\n    '''\n    model.eval()\n\n    total_loss = 0\n    total = 0\n    correct = 0\n\n    for x, y in tqdm(loader, desc='Evaluation'):\n        bs = y.size(0)\n\n        x, y = x.to(device), y.squeeze(1).to(device)\n\n        output = model(x)\n\n        loss = loss_fn(output.reshape(bs, 3, -1), y.reshape(bs, -1))\n\n        total_loss += loss.item()\n\n        _, y_pred = output.max(dim=1)\n        total += y.size(0) * y.size(1) * y.size(2)\n        correct += (y == y_pred).sum().item()\n\n    total_loss /= len(loader)\n    accuracy = correct / total\n\n    return total_loss, accuracy\n\n\n\nToy example\nHere is an example of a toy with a small data subset and small batches. So it can be fitted with weak devices. It just to make sure that everything goes fine.\nLet’s check train function.\nnp.random.seed(150)\nidx = np.random.randint(len(train_dataset), size=200)\ntoy_loader = DataLoader([train_dataset[i] for i in idx], batch_size = 10)\ntoy_model = UNET()\n\nfor epoch in range(10):\n    train(\n        model = toy_model,\n        device = torch.device('cpu'),\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3),\n        loss_fn = nn.CrossEntropyLoss(),\n        loader = toy_loader\n    )\ntorch.save(toy_model.state_dict(), 'unet_files/toy_model.pt')\nAnd evaluate function as well.\nidx = np.random.randint(len(test_dataset), size=100)\nevaluate(\n    model = toy_model,\n    loader = DataLoader([test_dataset[i] for i in idx], batch_size = 10),\n    device = torch.device('cpu'),\n    loss_fn = nn.CrossEntropyLoss()\n)\nDespite the fact that we have simplified the learning process as much as possible, it still takes quite a long time. For this reason and for the reproducibility of the experiment, we have saved the state of the model and will unload it in the next cell.\n\ntoy_model = UNET()\ntoy_model.load_state_dict(torch.load('unet_files/toy_model.pt'))\n\n&lt;All keys matched successfully&gt;\n\n\nNow let’s see if we can get something interesting. Let’s look at one of the images we used for training - does it look like something is working? All the results weren’t even close, but I picked one where the object’s features are clearly visible in the prediction. It might be worth trying to fit the model more seriously.\n\nplt.figure(figsize = (10, 9))\n\npicture_ind = 2812\n\nplt.subplot(131)\nplt.title(\"Input\")\nplt.imshow(T.ToPILImage()(train_dataset[picture_ind][0]))\nplt.xticks([]); plt.yticks([])\n\nplt.subplot(132)\nplt.title(\"Target\")\nplt.imshow(T.ToPILImage()(train_dataset[picture_ind][1].to(torch.uint8)))\nplt.xticks([]); plt.yticks([])\n\nplt.subplot(133)\nplt.title(\"Prediction\")\nprediction = toy_model(train_dataset[picture_ind][0].unsqueeze(0))[0].max(dim=0)[1].to(torch.uint8)\nplt.imshow(prediction)\nplt.xticks([]); plt.yticks([])\n\nplt.show()\n\n\n\n\n\n\nFinal model\nHere is the final, serious fitting of the model. If available, the GPU will be used, so we’ll be able to make the batch bigger and evaluate the model on the train just during the training run.\nThird-party services providing computing power were used to fit this model. But the code used for fitting is presented below.\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\nprint(device)\nif torch.cuda.is_available():\n    print(\"Using device_name: \", torch.cuda.get_device_name())\n\nloss_fn = nn.CrossEntropyLoss()\nmodel = UNET().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=64, \n    shuffle=True, \n    num_workers=4, \n    pin_memory=True\n)\ntest_loader = DataLoader(\n    test_dataset, \n    batch_size=64, \n    shuffle=True, \n    num_workers=4, \n    pin_memory=True\n)\nfit_monitoring = {\n    \"train_losses\" : [],\n    \"train_accuracies\" : [],\n    \"test_losses\" : [],\n    \"test_accuracies\" : []\n}\n\nfor epoch in range(25):\n    train_loss, train_accuracy = train(\n        model = model,\n        device = device,\n        optimizer = optimizer,\n        loss_fn = loss_fn,\n        loader = train_loader\n    )\n    test_loss, test_accuracy = evaluate(\n        model = model,\n        loader = test_loader,\n        device = device,\n        loss_fn = loss_fn\n    )\n\n    fit_monitoring[\"train_losses\"].append(train_loss)\n    fit_monitoring[\"train_accuracies\"].append(train_accuracy)\n    fit_monitoring[\"test_losses\"].append(test_loss)\n    fit_monitoring[\"test_accuracies\"].append(test_accuracy)\n\n\ntorch.save(model.state_dict(), 'model.pt')\npickle.dump(fit_monitoring, open(\"fit_monitoring.pck\", \"wb\"))\nThe model quality scores by epoch are shown in the cell below - final model gets almost 90% pixel accuracy on test sample.\n\nfit_monitoring = pickle.load(open(\"unet_files/fit_monitoring.pck\", \"rb\"))\n\nfig = plt.figure(figsize = (15, 5))\n\nplt.subplot(121)\nplt.title(\"Loss\")\nplt.plot(fit_monitoring[\"train_losses\"])\nplt.plot(fit_monitoring[\"test_losses\"])\n\nplt.subplot(122)\nplt.title(\"Pixel accuracy\")\nplt.plot(fit_monitoring[\"train_accuracies\"])\nplt.plot(fit_monitoring[\"test_accuracies\"])\n\nfinal_accuracy = fit_monitoring[\"test_accuracies\"][-1]\nplt.axhline(\n    final_accuracy,\n    color = \"gray\",\n    linestyle = \"--\"\n)\nplt.yticks(\n    np.concatenate([\n        plt.yticks()[0],\n        [round(final_accuracy,3)]\n    ])\n)\n\nfig.legend([\"Train\", \"Test\"])\n\nplt.show()"
  },
  {
    "objectID": "python/torch/vision/unet.html#visual-benchmark",
    "href": "python/torch/vision/unet.html#visual-benchmark",
    "title": "Unet",
    "section": "Visual benchmark",
    "text": "Visual benchmark\nFormal bemchmark is great but who actually cares what number you scored - let’s check on picutres. Everyone will draw their own conclusions about the quality of the model.\n\n# importing the model\nmodel = UNET()\nmodel.load_state_dict(\n    torch.load(\n        'unet_files/model.pt', \n        map_location=torch.device('cpu')\n    )\n)\nmodel = model.eval()\n\nnp.random.seed(20)\n\nindx = np.random.randint(0, len(test_dataset), 20)\n\ninputs = torch.stack([test_dataset[i][0] for i in indx])\ntargets = torch.stack([test_dataset[i][1] for i in indx]).to(torch.uint8)\npredicts = model(inputs).max(dim = 1)[1].to(torch.uint8)\n\ndef plot_result(input, target, predict):\n    picture_index = 543\n\n    plt.figure(figsize = (10, 9))\n    \n    plt.subplot(131)\n    plt.title(\"Input\")\n    plt.imshow(T.ToPILImage()(input))\n    plt.xticks([]);plt.yticks([])\n    \n    plt.subplot(132)\n    plt.title(\"Target\")\n    plt.imshow(T.ToPILImage()(target))\n    plt.xticks([]);plt.yticks([])\n    \n    plt.subplot(133)\n    plt.title(\"Predict\")\n    plt.imshow(predict)\n    plt.xticks([]);plt.yticks([])\n    \n    plt.show()\n\nfor i in range(len(indx)):\n    plot_result(inputs[i], targets[i], predicts[i])"
  },
  {
    "objectID": "python/basics/numpy/functions.html",
    "href": "python/basics/numpy/functions.html",
    "title": "fromfunciton",
    "section": "",
    "text": "Тут я собираю сведения о функция numpy которымы должен пользоваться или постоянно забываю как\n\nimport numpy as np\n\nПозволяет создать numpy.array используя правило задаваемое через функцию.\n\nnp.fromfunction(lambda i, j: i*3 + j, (4,4))\n\narray([[ 0.,  1.,  2.,  3.],\n       [ 3.,  4.,  5.,  6.],\n       [ 6.,  7.,  8.,  9.],\n       [ 9., 10., 11., 12.]])\n\n\n\nnumpy всегда оперирует массивами\nСледует помнить, что numpy всегда оперирует массивами. Рассмотрим пример кода из следующей ячейки. Тут предполагается возвращать для четной суммы индексов строковое значение “четная” а для не четной суммы индексов “нечетная”.\n\nnp.fromfunction(\n    lambda i,j:\"Четая\" if i&gt;j else \"Не четная\",\n    (4,4)\n)\n\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n\n\nДля того, чтобы разобраться почему так происходит, выведем параметы, которые приходят в функцию-правило переданную fromfunction.\n\ndef test_func(i,j):\n    print(i)\n    print(j)\n    return 0\n\nnp.fromfunction(test_func, (4,4))\n\n[[0. 0. 0. 0.]\n [1. 1. 1. 1.]\n [2. 2. 2. 2.]\n [3. 3. 3. 3.]]\n[[0. 1. 2. 3.]\n [0. 1. 2. 3.]\n [0. 1. 2. 3.]\n [0. 1. 2. 3.]]\n\n\n0\n\n\nИтак, приходят матрицы с индексами по строчкам для аргумента i и по столбцам для аргумента j.\n\nnp.fromfunction(\n    lambda i,j: np.where((i+j)%2==0, \"четная\", \"нечетная\"),\n    (4,4)\n)\n\narray([['четная', 'нечетная', 'четная', 'нечетная'],\n       ['нечетная', 'четная', 'нечетная', 'четная'],\n       ['четная', 'нечетная', 'четная', 'нечетная'],\n       ['нечетная', 'четная', 'нечетная', 'четная']], dtype='&lt;U8')"
  },
  {
    "objectID": "python/basics/functions.html",
    "href": "python/basics/functions.html",
    "title": "Functions",
    "section": "",
    "text": "Arguments vs parameters\nParameters - local variables to which values are assigned at the moment of its call.\nArguments are information passed to the function. They are real values corresponding to the parameters that were specified when the function was declared.\nSo in following example: - val is a parameter of some_function; - \"test\" is an argument passed to some_function.\n\ndef some_function(val):\n    print(\"I got\", val)\n\nsome_function(\"test\")\n\nI got test"
  },
  {
    "objectID": "python/basics/tkinter/basics.html",
    "href": "python/basics/tkinter/basics.html",
    "title": "Источники",
    "section": "",
    "text": "Отсновы библиотеки tkinter\n\nфайл предоставленный на курсах от belhard по программированию на python (см. в тойже папке, в которой лежит этот .ipynb);"
  },
  {
    "objectID": "python/basics/paths_settings/pathlib.html",
    "href": "python/basics/paths_settings/pathlib.html",
    "title": "Pathlib",
    "section": "",
    "text": "Pathlib is a special library built into Python that makes it easier to work with paths in Python. This page is focused on usage of this library."
  },
  {
    "objectID": "python/basics/paths_settings/pathlib.html#path-class",
    "href": "python/basics/paths_settings/pathlib.html#path-class",
    "title": "Pathlib",
    "section": "Path class",
    "text": "Path class\nYou should pass your folder path to the constructor, and you’ll get an instance that encapsulates the path behaviour.\nYou can:\n\nGet the path to the parent folder using the parent field;\nGet path to child folder with syntax &lt;pathlib.Path instance&gt; / &lt;name of child folder&gt; (just use division operator).\n\nAll these features are shown below:\n\nfrom pathlib import Path\nmy_path = Path(os.getcwd())\n\nprint(\"Original path: \", my_path)\nprint(\"Parent path: \", my_path.parent)\nprint(\"Using '/' operator: \", my_path/\"folder\"/\"file\")\n\nOriginal path:  /home/fedor/Documents/knowledge/python/basics/paths_settings\nParent path:  /home/fedor/Documents/knowledge/python/basics\nUsing '/' operator:  /home/fedor/Documents/knowledge/python/basics/paths_settings/folder/file"
  },
  {
    "objectID": "python/basics/basic_datatypes/dict.html",
    "href": "python/basics/basic_datatypes/dict.html",
    "title": "dict",
    "section": "",
    "text": "Allow to build arrays that has key&lt;-&gt;value relationship."
  },
  {
    "objectID": "python/basics/basic_datatypes/dict.html#get-extract-element",
    "href": "python/basics/basic_datatypes/dict.html#get-extract-element",
    "title": "dict",
    "section": "get extract element",
    "text": "get extract element\nBy using this function you can get value by key from the dict.\n\nPurpose\nIt may not be clear why this is necessary, as there is a [] operator which allows you to perform exactly the same actions. The difference from the [] operator is that it doesn’t cause an error if there is no specified key in the dictionary.\nSo the following cells show that the [] operator caused an error when you tried to load a non-existent key, but in the identical example the get method just returned None.\n\ntest_dict = {\"a\":1, \"b\":2}\nprint(test_dict[\"c\"])\n\nKeyError: 'c'\n\n\n\ntest_dict = {\"a\":1, \"b\":2}\nprint(test_dict.get(\"c\"))\n\nNone\n\n\n\n\ndefault argument\nAllows to set value to be returned If there is no item with the requested key.\n\ntest_dict = {\"a\":1, \"b\":2}\nprint(test_dict.get(\"c\", \"default value\"))\n\ndefault value"
  },
  {
    "objectID": "python/basics/basic_datatypes/dict.html#delete-specific-item",
    "href": "python/basics/basic_datatypes/dict.html#delete-specific-item",
    "title": "dict",
    "section": "Delete specific item",
    "text": "Delete specific item\n\ndel operator\nYou can simply apply the del operator to the specific element. As in the following example:\n\ntest_dict = {\"a\":1, \"b\":2}\ndel test_dict[\"a\"]\nprint(test_dict)\n\n{'b': 2}\n\n\nNote If there is no item in the dict that you want to delete in this way, you will get the error. The next cell shows exactly this case:\n\ntest_dict = {\"a\":1, \"b\":2}\ndel test_dict[\"c\"]\nprint(test_dict)\n\nKeyError: 'c'\n\n\n\n\npop method\nIt will remove element by key passed as an argument to this method and return value as a result of the function. The following example shows how it works:\n\nIn the first print we show the result of the pop method;\nIn the second part, we’ll show that the dictionary no longer has an element with the key “a”.\n\n\ntest_dict = {\"a\":1, \"b\":2}\nprint(test_dict.pop(\"a\"))\nprint(test_dict)\n\n1\n{'b': 2}\n\n\nNote this method will still cause an error if you apply it to a key that doesn’t exist in the dictionary.\n\ntest_dict = {\"a\":1, \"b\":2}\ntest_dict.pop(\"C\")\n\nKeyError: 'C'"
  },
  {
    "objectID": "python/basics/basic_datatypes/dict.html#clear-delete-all",
    "href": "python/basics/basic_datatypes/dict.html#clear-delete-all",
    "title": "dict",
    "section": "clear delete all",
    "text": "clear delete all\nThis method allows you to delete all the values written in the dictionary. The following example shows what it can look like.\n\ntest_dict = {\"a\":1, \"b\":2}\ntest_dict.clear()\nprint(test_dict)\n\n{}"
  },
  {
    "objectID": "python/basics/basic_datatypes/dict.html#in-operator",
    "href": "python/basics/basic_datatypes/dict.html#in-operator",
    "title": "dict",
    "section": "in operator",
    "text": "in operator\nChecks if the element on the left side of in is in the keys (not values) of the dictionary on the right side of in.\n\ntest_dict = {\"a\":1, \"b\":2}\nprint(\"a\" in test_dict)\nprint(1 in test_dict)\n\nTrue\nFalse"
  },
  {
    "objectID": "python/basics/basic_datatypes/dict.html#key-of-max",
    "href": "python/basics/basic_datatypes/dict.html#key-of-max",
    "title": "dict",
    "section": "Key of max",
    "text": "Key of max\nIt was a common task for me to find a key of dict that matches the max/min value of the dictionary. The most elegant solution I found is to use dict as an argument for the max/min basic Python function, but as key argument use the dict.get method.\nThe logic is as follows - the function will iterate over the dictionary keys, and use only the results of execution of the function passed as an argument to the key parameter for comparison. Since we pass to the get function the corresponding value to the key, the comparison will be performed on the values of the dictionary.\nSo in the following example, I apply this solution to the randomly generated dictionary:\n\nfrom random import randint\nfrom IPython.display import HTML\n\nexample_dict = {\n    chr(iscii_code): randint(0,10)\n    for iscii_code in range(ord(\"a\"), ord(\"g\"))\n}\ndisplay(HTML(\"&lt;b style=\\\"font-size:120%\\\"&gt;=====Input dict=====&lt;/b&gt;\"))\ndisplay(example_dict)\ndisplay(HTML(\"&lt;b style=\\\"font-size:120%\\\"&gt;=====Result=====&lt;/b&gt;\"))\ndisplay(HTML(max(example_dict, key=example_dict.get)))\n\n=====Input dict=====\n\n\n{'a': 3, 'b': 8, 'c': 6, 'd': 9, 'e': 6, 'f': 3}\n\n\n=====Result=====\n\n\nd"
  },
  {
    "objectID": "python/basics/basic_datatypes/frozenset.html",
    "href": "python/basics/basic_datatypes/frozenset.html",
    "title": "frozenset",
    "section": "",
    "text": "set in Python is a mutable data type, so you cannot use it in many cases (e.g. as a key for dict or as an element of another set). To fix this, Python provides a special data type frozenset which is like set but immutable.\nSo in the following example I try to use frozenset and set as elements of another set:\n\nWith frozenset all is well;\nRegular set causes an error.\n\n\nregular_set = {1,3}\nfrozen_set = frozenset([1,2,3,4])\n\nprint(\"Using a frozenset:\", {1,2,3, frozen_set})\nprint(\"Using a regular set:\", {1, 2, 3, regular_set, 1})\n\nUsing a frozenset: {1, 2, 3, frozenset({1, 2, 3, 4})}\n\n\nTypeError: unhashable type: 'set'"
  },
  {
    "objectID": "python/basics/basic_datatypes/set.html",
    "href": "python/basics/basic_datatypes/set.html",
    "title": "set",
    "section": "",
    "text": "Array of unique values. You can create it by using {&lt;element1&gt;, &lt;element2&gt;, ..., &lt;elementn&gt;} or tuple function.\nIn the following example, I show that it can contain only unique elements. Although element 2 is specified twice, it appears only once as a result.\n{1,2,3,4,2}\n\n{1, 2, 3, 4}"
  },
  {
    "objectID": "python/basics/basic_datatypes/set.html#empty-set",
    "href": "python/basics/basic_datatypes/set.html#empty-set",
    "title": "set",
    "section": "Empty set",
    "text": "Empty set\nTo create an empty set you should:\n\nUse the set function without arguments;\nDo not use empty curly brackets, it will create empty dict not set;\nDo not use curly curly brackets with a comma (lite it works for tuple), it will cause an error.\n\nThe following cell shows that curly brackets alone create a tuple but not a set. And the set() expression will in turn create an instance of set.\n\nprint(type({}))\nprint(type(set()))\n\n&lt;class 'dict'&gt;\n&lt;class 'set'&gt;\n\n\nThe following cell shows that using braces with a comma is an error.\n\n{,}\n\nSyntaxError: invalid syntax (3645013520.py, line 1)\n\n\nInterestingly, applying the print function to an empty set results in set() output.\n\nprint(set())\n\nset()"
  },
  {
    "objectID": "python/basics/basic_datatypes/set.html#no-order",
    "href": "python/basics/basic_datatypes/set.html#no-order",
    "title": "set",
    "section": "No order",
    "text": "No order\nOne of the main features of the set datatype is that it doesn’t have any order. This has important implications:\n\nYou can’t apply indexing syntax (the [] operator) to sets;\nThere are no functions associated with indexing (e.g. pop in the list datatype)."
  },
  {
    "objectID": "python/basics/basic_datatypes/set.html#only-immutable",
    "href": "python/basics/basic_datatypes/set.html#only-immutable",
    "title": "set",
    "section": "Only immutable",
    "text": "Only immutable\nIn sets, you can only use immutable datatypes. The following cells show that if you add a mutable type to a tuple (e.g. list), you will get an error.\n\n{1,\"1\", 9.4, (1,3,4)}\n\n{(1, 3, 4), 1, '1', 9.4}\n\n\n\n{1,\"1\", 9.4, (1,3,4), [3,4,5]}\n\nTypeError: unhashable type: 'list'"
  },
  {
    "objectID": "python/basics/basic_datatypes/set.html#add-new-value",
    "href": "python/basics/basic_datatypes/set.html#add-new-value",
    "title": "set",
    "section": "add new value",
    "text": "add new value\n\ntest_set = {1,2,3}\nprint(\"initital set\", test_set)\ntest_set.add(\"s\")\nprint(\"result set\", test_set)\n\ninitital set {1, 2, 3}\nresult set {1, 2, 3, 's'}\n\n\nNote that adding an element that already exists in the set won’t cause an error, it will simply be ignored.\n\ntest_set = {1,2,3,4}\nprint(\"initial set\", test_set)\ntest_set.add(3)\nprint(\"restult set\", test_set)\n\ninitial set {1, 2, 3, 4}\nrestult set {1, 2, 3, 4}"
  },
  {
    "objectID": "python/basics/basic_datatypes/set.html#remove-delete-by-value",
    "href": "python/basics/basic_datatypes/set.html#remove-delete-by-value",
    "title": "set",
    "section": "remove delete by value",
    "text": "remove delete by value\nYou can delete element by value.\n\ntest_set = {1,2,3,4,5}\nprint(\"initial list\", test_set)\ntest_set.remove(4)\nprint(\"resutl list\", test_set)\n\ninitial list {1, 2, 3, 4, 5}\nresutl list {1, 2, 3, 5}\n\n\nNote that if there’s no removing value in the source list, you’ll have an error.\n\ntest_set = {5,3,4,1}\ntest_set.remove(6)\n\nKeyError: 6"
  },
  {
    "objectID": "python/basics/basic_datatypes/set.html#intercection",
    "href": "python/basics/basic_datatypes/set.html#intercection",
    "title": "set",
    "section": "intercection",
    "text": "intercection\nThis method allows you to get a set that contains only common elements for argument sets.\n\ntest_set1 = {1,2,3}\ntest_set2 = {2,3,4}\ntest_set1.intersection(test_set2)\n\n{2, 3}"
  },
  {
    "objectID": "python/basics/basic_datatypes/set.html#union",
    "href": "python/basics/basic_datatypes/set.html#union",
    "title": "set",
    "section": "union",
    "text": "union\nThis method allows you to get a set that contains all elements from both argument sets.\n\ntest_set1 = {1,2,3}\ntest_set2 = {2,3,4}\ntest_set1.union(test_set2)\n\n{1, 2, 3, 4}"
  },
  {
    "objectID": "python/basics/basic_datatypes/set.html#symmetric_difference",
    "href": "python/basics/basic_datatypes/set.html#symmetric_difference",
    "title": "set",
    "section": "symmetric_difference",
    "text": "symmetric_difference\nThis method allows you to get a set that contains elements that are present in only one of the argument sets, but not in both.\n\ntest_set1 = {1,2,3}\ntest_set2 = {2,3,4}\ntest_set1.symmetric_difference(test_set2)\n\n{1, 4}"
  },
  {
    "objectID": "python/basics/basic_datatypes/set.html#sets-difference-a-b",
    "href": "python/basics/basic_datatypes/set.html#sets-difference-a-b",
    "title": "set",
    "section": "Sets difference A-B",
    "text": "Sets difference A-B\nIn case you subtract set A from set B you get a set of elements from the set not contained in set B.\n\ntest_set1 = {1,2,3}\ntest_set2 = {2,3,4}\ntest_set1 - test_set2\n\n{1}"
  },
  {
    "objectID": "python/basics/cli_arguments/manually.html",
    "href": "python/basics/cli_arguments/manually.html",
    "title": "Manually",
    "section": "",
    "text": "Using the sys.argv object, you can access all the values passed to Python interpreter during the current run.\nSo the following example defines a script that shows that sys.argv contains a list of string values, including those passed to the program from the run directory.\n\n%%writefile manually_files/example.py\nimport sys\nfor v in sys.argv: print(v)\n\nOverwriting manually_files/example.py\n\n\n\n%%bash \npython3 manually_files/example.py value{1..10}\n\nmanually_files/example.py\nvalue1\nvalue2\nvalue3\nvalue4\nvalue5\nvalue6\nvalue7\nvalue8\nvalue9\nvalue10"
  },
  {
    "objectID": "python/basics/packages.html",
    "href": "python/basics/packages.html",
    "title": "Packages",
    "section": "",
    "text": "This page describes features related to packages in Python.\n\nVersion\nTo get the version of a package imported into the environment just from python program, you can call the __version__ attribute.\nIn the following cell I just do it to pandas installed in the operating system.\n\nimport pandas\npandas.__version__\n\n'2.0.1'"
  },
  {
    "objectID": "python/basics/logging.html",
    "href": "python/basics/logging.html",
    "title": "Logging",
    "section": "",
    "text": "Logging in python: developer guide (rus);\n\n\nimport os\nimport sys\nimport logging\nimport random"
  },
  {
    "objectID": "python/basics/logging.html#error-types",
    "href": "python/basics/logging.html#error-types",
    "title": "Logging",
    "section": "Error types",
    "text": "Error types\nThere are five levels of logging:\n\nDebug;\nInfo;\nWarning;\nError;\nCritical.\n\nYou can call them by using the relevant functions:\n\nlogging.debug(\"A DEBUG Message\")\nlogging.info(\"An INFO\")\nlogging.warning(\"A WARNING\")\nlogging.error(\"An ERROR\")\nlogging.critical(\"A message of CRITICAL severity\")\n\nWARNING:root:A WARNING\nERROR:root:An ERROR\nCRITICAL:root:A message of CRITICAL severity\n\n\nIn previous examle debug and info errors wasn’t printed. It happened becaulse default value for logging.level setted to logging.WARNING, therefore only those messages above WARNING will be displayed."
  },
  {
    "objectID": "python/basics/logging.html#basicconfig",
    "href": "python/basics/logging.html#basicconfig",
    "title": "Logging",
    "section": "basicConfig",
    "text": "basicConfig\n\nlogging.basicConfig(level=logging.INFO)\n\nlogging.debug(\"A DEBUG Message\")\nlogging.info(\"An INFO\")\nlogging.warning(\"A WARNING\")\nlogging.error(\"An ERROR\")\nlogging.critical(\"A message of CRITICAL severity\")\n\nINFO:root:An INFO\nWARNING:root:A WARNING\nERROR:root:An ERROR\nCRITICAL:root:A message of CRITICAL severity"
  },
  {
    "objectID": "python/basics/logging.html#getlogger---creating-loggers",
    "href": "python/basics/logging.html#getlogger---creating-loggers",
    "title": "Logging",
    "section": "getLogger - creating loggers",
    "text": "getLogger - creating loggers\nFor some reason you can’t set basic config twice with logging lib. I showing it in the following example:\n\nSet level=logging.DEBUG and, as expected, got every type of message;\nSet level=logging.ERROR and, but any way got every type of message.\n\n\nprint(\"=====logging run1=====\", file=sys.stderr)\nlogging.basicConfig(level=logging.DEBUG)\nlogging.debug(\"A DEBUG Message\")\nlogging.info(\"An INFO\")\nlogging.warning(\"A WARNING\")\nlogging.error(\"An ERROR\")\nlogging.critical(\"A message of CRITICAL severity\")\n\nprint(\"=====logging run2=====\", file=sys.stderr)\nlogging.basicConfig(level=logging.ERROR)\nlogging.debug(\"A DEBUG Message\")\nlogging.info(\"An INFO\")\nlogging.warning(\"A WARNING\")\nlogging.error(\"An ERROR\")\nlogging.critical(\"A message of CRITICAL severity\")\n\n=====logging run1=====\nDEBUG:root:A DEBUG Message\nINFO:root:An INFO\nWARNING:root:A WARNING\nERROR:root:An ERROR\nCRITICAL:root:A message of CRITICAL severity\n=====logging run2=====\nDEBUG:root:A DEBUG Message\nINFO:root:An INFO\nWARNING:root:A WARNING\nERROR:root:An ERROR\nCRITICAL:root:A message of CRITICAL severity\n\n\nThe solution for different situations is to create different loggers with different options. In the following example, I create different loggers with different level options, so that logger2 successfully sets the ERROR option.\n\nlogger1 = logging.getLogger(\"logger1\")\nlogger1.setLevel(logging.INFO)\n\nprint(\"=====logger1=====\", file=sys.stderr)\nlogger1.debug(\"A DEBUG Message\")\nlogger1.info(\"An INFO\")\nlogger1.warning(\"A WARNING\")\nlogger1.error(\"An ERROR\")\nlogger1.critical(\"A message of CRITICAL severity\")\n\n\nlogger2 = logging.getLogger(\"logger2\")\nlogger2.setLevel(logging.ERROR)\n\nprint(\"=====logger2=====\", file=sys.stderr)\nlogger2.debug(\"A DEBUG Message\")\nlogger2.info(\"An INFO\")\nlogger2.warning(\"A WARNING\")\nlogger2.error(\"An ERROR\")\nlogger2.critical(\"A message of CRITICAL severity\")\n\n=====logger1=====\nINFO:logger1:An INFO\nWARNING:logger1:A WARNING\nERROR:logger1:An ERROR\nCRITICAL:logger1:A message of CRITICAL severity\n=====logger2=====\nERROR:logger2:An ERROR\nCRITICAL:logger2:A message of CRITICAL severity"
  },
  {
    "objectID": "python/basics/virtual_environment.html",
    "href": "python/basics/virtual_environment.html",
    "title": "Virutal environment in python",
    "section": "",
    "text": "Official documentation on virtual invironment in python;\nExtended tutorial;\nJupyter Notebook Kernels: How to Add, Change, Remove."
  },
  {
    "objectID": "python/basics/virtual_environment.html#wrong-way",
    "href": "python/basics/virtual_environment.html#wrong-way",
    "title": "Virutal environment in python",
    "section": "Wrong way",
    "text": "Wrong way\nThe wrong way to do this is to just copy the virtual environment folder. It will lead to broken paths for the Python interpreter.\nIn the following example, I copy the python3 environment from venv1 to venv2, but when I run python from the copied environment and check sys.path, it’s still pointing to the path for venv1.\n\n%%bash\ncd venv_files\n\nmkdir venv1 venv2\n\npython3.10 -m venv venv1/venv\ncp -r venv1/venv venv2/venv\n\nsource venv2/venv/bin/activate\necho \"=====sys.path[-1]=====\"\npython3 -c \"import sys; print(sys.path[-1])\"\n\nrm -r venv1 venv2\n\n=====sys.path[-1]=====\n/home/fedor/Documents/knowledge_bank/python/venv_files/venv1/venv/lib/python3.10/site-packages"
  },
  {
    "objectID": "python/basics/virtual_environment.html#correct-way",
    "href": "python/basics/virtual_environment.html#correct-way",
    "title": "Virutal environment in python",
    "section": "Correct way",
    "text": "Correct way\nhttps://stackoverflow.com/questions/7438681/how-to-duplicate-virtualenv"
  },
  {
    "objectID": "python/basics/virtual_environment.html#add-to-kernelspec",
    "href": "python/basics/virtual_environment.html#add-to-kernelspec",
    "title": "Virutal environment in python",
    "section": "Add to kernelspec",
    "text": "Add to kernelspec\nFor more information, click here.\n\nCreate:\n\nInstall jupyter in environment pip install jupyter;\nRun for environment ipython kernel install --name \"&lt;environment name&gt;\" --user;\n\nList all available environments jupyter kernelspec list;\nDelete jupyter kernelspec remove &lt;kernel-name&gt;\n\nThe following example shows the use of all the above commands.\n\n%%bash\ncd venv_files\npython3 -m venv test_venv\n\necho \"=====instalation=====\"\nsource test_venv/bin/activate\npip install jupyter &&gt; /dev/null\nipython kernel install --name \"test-env\" --user\ndeactivate\n\necho \"=====list available kernels=====\"\njupyter kernelspec list\n\necho \"====delete test_environment=====\"\njupyter kernelspec remove -y test-env\n\nrm -r test_venv\n\n=====instalation=====\nInstalled kernelspec test-env in /home/fedor/.local/share/jupyter/kernels/test-env\n=====list available kernels=====\nAvailable kernels:\n  python3     /home/fedor/.local/share/jupyter/kernels/python3\n  test-env    /home/fedor/.local/share/jupyter/kernels/test-env\n====delete test_environment=====\nRemoved /home/fedor/.local/share/jupyter/kernels/test-env\n\n\nNote after reloading for example jupyter lab you will have refreshed list of kernels."
  },
  {
    "objectID": "python/basics/virtual_environment.html#run-from-environment",
    "href": "python/basics/virtual_environment.html#run-from-environment",
    "title": "Virutal environment in python",
    "section": "Run from environment",
    "text": "Run from environment\nWhen you run jupyter ... in your system, it will run from the base system python, whether or not any environment is enabled. But jupyter is actually just a Python runnable package, and you can do python3 -m jupyter ..., in which case it will use the appropriate environment to run a jupyter ....\nI can’t show it using notebook cells. But you should use such commands:\n# create environment\npython3 -m venv some_venv\n# activate environment\nsource some_venv/bin/activate\n# install jupyter\npip install jupyter\n# run jupyter using direct call of jupyter from python\npython3 -m jupyter lab"
  },
  {
    "objectID": "python/basics/operators/arithmetic.html",
    "href": "python/basics/operators/arithmetic.html",
    "title": "Arithmetic",
    "section": "",
    "text": "It always returns the data type float, even if the result can be interpreted as int.\n\ntype(4/2)\n\nfloat"
  },
  {
    "objectID": "python/basics/operators/arithmetic.html#division-operator",
    "href": "python/basics/operators/arithmetic.html#division-operator",
    "title": "Arithmetic",
    "section": "",
    "text": "It always returns the data type float, even if the result can be interpreted as int.\n\ntype(4/2)\n\nfloat"
  },
  {
    "objectID": "python/basics/operators/arithmetic.html#integer-division",
    "href": "python/basics/operators/arithmetic.html#integer-division",
    "title": "Arithmetic",
    "section": "// - integer division",
    "text": "// - integer division\nThis operator simply divides the operands and returns the result without the floating point part.\n\n5//2\n\n2\n\n\nUnlike regular division, it always returns int data type.\n\nprint(type(6/2))\nprint(type(6//2))\n\nint\n\n\nNote that the rounding is always on the lower side. So if we apply this operator to the negative number, we get the lower number that is the result of the even division.\n\n-5//2\n\n-3"
  },
  {
    "objectID": "python/basics/operators/break.html",
    "href": "python/basics/operators/break.html",
    "title": "Leave cycle (break)",
    "section": "",
    "text": "Lets you exit the cycle."
  },
  {
    "objectID": "python/basics/operators/break.html#nested-loops",
    "href": "python/basics/operators/break.html#nested-loops",
    "title": "Leave cycle (break)",
    "section": "Nested loops",
    "text": "Nested loops\nWhat will happens if you put cycle incide two cycles? Will it leave both cycles or just the internal one?\nIt will leave only intertal loop.\nSo in the following example we will use the break operator inside the internal cycle only if we loop to a variable of the internal cycle that is equal to or greater than 2. As a result, we go through all the operations provided by the outer loop, but each interation of the internal loop is ended when the loop variable enriches 2.\n\nfor i in range(4):\n    print(\"external loop\", i)\n    for j in range(4):\n        if j &gt;= 2:\n            break\n        print(\"    internal loop\", j)\n\nexternal loop 0\n    internal loop 0\n    internal loop 1\nexternal loop 1\n    internal loop 0\n    internal loop 1\nexternal loop 2\n    internal loop 0\n    internal loop 1\nexternal loop 3\n    internal loop 0\n    internal loop 1\n\n\nSolution the best solution for today is to organise a flag that will cause a break operator in the external cycle.\nSo in the following example I have added a flag that gets a True value before exiting the internal loop, and if after exiting the internal loop flag==True it will cause exiting the external cycle as well.\nNo better solution has been found to date.\n\nfor i in range(4):\n    print(\"external loop\", i)\n    for j in range(4):\n        if j &gt;= 2:\n            flag = True\n            break\n        print(\"    internal loop\", j)\n\n    if flag: break\n\nexternal loop 0\n    internal loop 0\n    internal loop 1"
  },
  {
    "objectID": "python/basics/operators/yield.html",
    "href": "python/basics/operators/yield.html",
    "title": "yield",
    "section": "",
    "text": "yield is a keyword in Python that is used to return from a function while preserving the state of its local variables, and when such a function is called again, execution continues from the yield statement where it was interrupted. Any function containing the yield keyword is called a generator. We can say that yield is what makes it a generator."
  },
  {
    "objectID": "python/basics/operators/yield.html#example",
    "href": "python/basics/operators/yield.html#example",
    "title": "yield",
    "section": "Example",
    "text": "Example\nConsider an example where we extract only even numbers from a list. basic_fun represents the solution without using yield and yield_fun represents the solution with the yield function.\nYou can use yield just like return, but remember - it returns to the function for the next iteration.\n\ndef basic_fun(list_of_nums):\n    return [i for i in list_of_nums if (i%2) == 0]\n\ndef yield_fun(list_of_nums):\n    for i in list_of_nums:\n        if i % 2 == 0:\n            yield i\n\nSo here both functions have been applied to a list of values. And of course we got the same result:\n\nlist_of_nums = [1, 2, 3, 8, 15, 42]\nprint(\n    \"basic_fun result:\", \n    basic_fun(list_of_nums)\n)\nprint(\n    \"get_even result:\",\n    list(get_even(list_of_nums))\n)\n\nbasic_fun result: [2, 8, 42]\nget_even result: [2, 8, 42]"
  },
  {
    "objectID": "python/basics/operators/yield.html#return-value",
    "href": "python/basics/operators/yield.html#return-value",
    "title": "yield",
    "section": "Return value",
    "text": "Return value\nFunctions that exits with yield returns generator - is special object in python.\nThe following example simply iterates over a list of numbers using the yield operator and prints each element.\nThe key idea here is that\n\nDuring the function call, the function wasn’t actually executed - we don’t have any output from print, and we got the generator object as a result;\nBut when we unpacked the generator into a list, we got messages and a list as a result.\n\n\ndef experiment_fun(list_of_nums):\n    for v in list_of_nums:\n        print(f\"processing object: {v}\")\n        yield v\n\nlist_of_nums = [10,20,30,40,50]\n\nprint(\"Function execution:\")\ngen = experiment_fun(list_of_nums)\nprint(\"Result:\", gen, end = \"\\n\\n\")\n\nprint(\"Generator unpacking:\")\nprint(\"Result:\", list(gen))\n\nFunction execution:\nResult: &lt;generator object experiment_fun at 0x7fada84d4b30&gt;\n\nGenerator unpacking:\nprocessing object: 10\nprocessing object: 20\nprocessing object: 30\nprocessing object: 40\nprocessing object: 50\nResult: [10, 20, 30, 40, 50]"
  },
  {
    "objectID": "python/basics/operators/yield.html#yield-and-return",
    "href": "python/basics/operators/yield.html#yield-and-return",
    "title": "yield",
    "section": "yield and return",
    "text": "yield and return\nBut what happens if you use both yield and return in the same function? Nothing special - the function will create generator, but if it has return in it - it will just stop iterating over generator.\nSo the following cell just shows how it might look - it iterates over the passed array while for the first three elements, but then return is executed so that elements after the third have not been processed.\n\ndef experiment_fun(list_of_nums):\n    for i, v in enumerate(list_of_nums):\n        print(f\"processing object: {v, i}\")\n        \n        yield v\n        \n        if i &gt; 1:\n            return \"test\"\n\nlist_of_nums = [10,20,30,40,50]\nlist(experiment_fun(list_of_nums))\n\nprocessing object: (10, 0)\nprocessing object: (20, 1)\nprocessing object: (30, 2)\n\n\n[10, 20, 30]"
  },
  {
    "objectID": "python/sqlalchemy/relations.html",
    "href": "python/sqlalchemy/relations.html",
    "title": "Relations",
    "section": "",
    "text": "In this page I want to describe the mechanism of sqlalchemy that allows to perform sql JOIN operation in a really natural for python way. For each “main” table, you can define related essentials and you’ll be able to access them only from queries on the “main” table."
  },
  {
    "objectID": "python/sqlalchemy/relations.html#prepare",
    "href": "python/sqlalchemy/relations.html#prepare",
    "title": "Relations",
    "section": "Prepare",
    "text": "Prepare\nIn the following cell we have just defined basic things for sqlalchemy.\n\n!docker run -d --rm\\\n    --name relations_example\\\n    -e POSTGRES_PASSWORD=postgres\\\n    -p 5000:5432\\\n    postgres:15.4 &&gt; /dev/null\n!sleep 5\n\nfrom sqlalchemy import (\n    create_engine, \n    Column, \n    Integer,\n    String,\n    ForeignKey\n)\nfrom sqlalchemy.orm import (\n    sessionmaker, \n    declarative_base, \n    relationship\n)\nfrom random import randint\n\nURL = \"postgresql://postgres:postgres@localhost:5000/postgres\"\nengine = create_engine(URL)\n\nSessionLocal = sessionmaker(\n    autoflush=False,\n    autocommit=False,\n    bind=engine\n)\nsession = SessionLocal()\n\n# defining data model\nBase = declarative_base()"
  },
  {
    "objectID": "python/sqlalchemy/relations.html#data-model",
    "href": "python/sqlalchemy/relations.html#data-model",
    "title": "Relations",
    "section": "Data model",
    "text": "Data model\nData model is a key feature of this page. There are:\n\nMainTable, which contains\n\nSome information that is unique to each record - id and text_var;\nSome information that can be mapped to the other describe table;\n\nDescribeTable - auxiliary table that describes certain states of the MainTable.\n\nSo we need a mechanism to tell sqlalcemy that it needs to join some infromation from DescribeTable into MainTable.\nWith DescribeTable it’s very simple, you just define primary key and text_var that simulate some content to be joined.\nMainTable has:\n\nid as primary key;\ntext_var that simulate some content;\ndescribe_id in its declaration contains ForeignKey which indicates which field of their third-party table this field is associated with;\ndescribe is a field where objects of DescribeTable class will be written to so that they can be accessed.\n\n\nclass DescribeTable(Base):\n    __tablename__=\"describe_table\"\n    id = Column(Integer, primary_key=True)\n    text_var=Column(String)\n\nclass MainTable(Base):\n    __tablename__=\"main_table\"\n    id = Column(Integer, primary_key=True)\n    text_var = Column(String)\n    describe_id = Column(\n        Integer, ForeignKey(\"describe_table.id\")\n    )\n    describe = relationship(\"DescribeTable\")\n\n# duplicate datamodel in the database\nBase.metadata.create_all(engine)"
  },
  {
    "objectID": "python/sqlalchemy/relations.html#filling-of-the-table",
    "href": "python/sqlalchemy/relations.html#filling-of-the-table",
    "title": "Relations",
    "section": "Filling of the table",
    "text": "Filling of the table\nBelow content of the tables is created. Few records that defines mapping for the descriptions. And some random content for the MainTable.\n\nfor i in range(2):\n    session.add(DescribeTable(\n        text_var = f\"descrion {i+1}\"\n    ))\n\nfor i in range(10):\n    session.add(MainTable(\n        text_var = \"\".join([chr(randint(97,107)) for i in range(10)]),\n        describe_id = randint(1,2)\n    ))\n\nsession.commit()\n\n\n%%bash\ndocker exec -i relations_example psql -U postgres -d postgres\nSELECT * FROM describe_table;\nSELECT * FROM main_table;\n\n id |  text_var  \n----+------------\n  1 | descrion 1\n  2 | descrion 2\n(2 rows)\n\n id |  text_var  | describe_id \n----+------------+-------------\n  1 | kciihbjiii |           1\n  2 | cgdebdabhh |           1\n  3 | bichegaghb |           2\n  4 | kkaidfdiki |           1\n  5 | icicbkcihj |           1\n  6 | eijbdfjkff |           1\n  7 | dhacdkgahd |           2\n  8 | hhiebijhbc |           2\n  9 | dccdgfkiee |           2\n 10 | fhbgideidg |           2\n(10 rows)"
  },
  {
    "objectID": "python/sqlalchemy/relations.html#query",
    "href": "python/sqlalchemy/relations.html#query",
    "title": "Relations",
    "section": "Query",
    "text": "Query\nAn object that describes the entity of the linked table has been declared in the main table, it is through this object that the content of the linked table can be retrieved.\nSo despite the fact that the query was made in MainTable the corresponding data from DescribeTable are automatically pulled into the results.\n\nfor main_table_instance in session.query(MainTable).all():\n    print(\"============================\")\n    print(\n        \"text_var:\", main_table_instance.text_var, \"\\n\"\n        \"describe_id:\", main_table_instance.describe_id, \"\\n\"\n        \"description content:\", main_table_instance.describe.text_var\n    )\n\n============================\ntext_var: kciihbjiii \ndescribe_id: 1 \ndescription content: descrion 1\n============================\ntext_var: cgdebdabhh \ndescribe_id: 1 \ndescription content: descrion 1\n============================\ntext_var: bichegaghb \ndescribe_id: 2 \ndescription content: descrion 2\n============================\ntext_var: kkaidfdiki \ndescribe_id: 1 \ndescription content: descrion 1\n============================\ntext_var: icicbkcihj \ndescribe_id: 1 \ndescription content: descrion 1\n============================\ntext_var: eijbdfjkff \ndescribe_id: 1 \ndescription content: descrion 1\n============================\ntext_var: dhacdkgahd \ndescribe_id: 2 \ndescription content: descrion 2\n============================\ntext_var: hhiebijhbc \ndescribe_id: 2 \ndescription content: descrion 2\n============================\ntext_var: dccdgfkiee \ndescribe_id: 2 \ndescription content: descrion 2\n============================\ntext_var: fhbgideidg \ndescribe_id: 2 \ndescription content: descrion 2"
  },
  {
    "objectID": "python/sqlalchemy/relations.html#stop-container",
    "href": "python/sqlalchemy/relations.html#stop-container",
    "title": "Relations",
    "section": "Stop container",
    "text": "Stop container\n\n!docker stop relations_example\n\nrelations_example"
  },
  {
    "objectID": "python/sqlalchemy/query.html",
    "href": "python/sqlalchemy/query.html",
    "title": "Query",
    "section": "",
    "text": "In this page I’ll describe how to load records from the database using sqlalchemy."
  },
  {
    "objectID": "python/sqlalchemy/query.html#start-container",
    "href": "python/sqlalchemy/query.html#start-container",
    "title": "Query",
    "section": "Start container",
    "text": "Start container\nContainer with a postgres database, and creating tables that will be useful for the examples described on this page.\n\n%%bash\ndocker run -d --rm\\\n    --name read_table_example\\\n    -e POSTGRES_PASSWORD=postgres\\\n    -p 5000:5432\\\n    postgres:15.4 &&gt; /dev/null\nsleep 5\ndocker exec -i read_table_example psql -U postgres -d postgres\n\nCREATE TABLE test_table (\n    numeric_var INT PRIMARY KEY,\n    text_var VARCHAR\n);\n\nINSERT INTO test_table (numeric_var, text_var) VALUES\n(1, 'a'),\n(2, 'a'),\n(3, 'b'),\n(4, 'c');\n\nCREATE TABLE\nINSERT 0 4\n\n\nDon’t forget to stop the container when you’ve finished playing with the examples on this page.\n\n!docker stop read_table_example\n\nread_table_example"
  },
  {
    "objectID": "python/sqlalchemy/query.html#creating-map",
    "href": "python/sqlalchemy/query.html#creating-map",
    "title": "Query",
    "section": "Creating map",
    "text": "Creating map\nIt’s a preparatory step:\n\nCreate session;\nRecreate the data model corresponding to the database you are working in.\n\n\nfrom sqlalchemy import (\n    create_engine, \n    Column, \n    Integer,\n    String\n)\nfrom sqlalchemy.orm import sessionmaker, declarative_base\n\nURL = \"postgresql://postgres:postgres@localhost:5000/postgres\"\nengine = create_engine(URL)\nLocalSession = sessionmaker(\n    autocommit=False,\n    autoflush=False,\n    bind=engine\n)\nsession = LocalSession()\n\nBase = declarative_base()\nclass TestTable(Base):\n    __tablename__ = \"test_table\"\n    numeric_var = Column(Integer, primary_key = True)\n    text_var = Column(String)"
  },
  {
    "objectID": "python/sqlalchemy/query.html#extract-all-records",
    "href": "python/sqlalchemy/query.html#extract-all-records",
    "title": "Query",
    "section": "Extract all records",
    "text": "Extract all records\nYou need:\n\nUse session.query(&lt;table class&gt;) to extract values from a table related to &lt;table class&gt;;\nsession.query.all() returns a list of instances of &lt;table class&gt; corresponding to each record in the database:\n\nIn the example, the list was printed;\nAnd in the example, all fields were printed.\n\n\n\nquery_res = session.query(TestTable)\n\nprint(\"list of TestTable -\", query_res.all())\n\nprint(\"\\nExtract fields:\")\nfor record in query_res.all():\n    print(record.numeric_var, record.text_var)\n\nlist of TestTable - [&lt;__main__.TestTable object at 0x7f8fd0f6d720&gt;, &lt;__main__.TestTable object at 0x7f8fd0f6da50&gt;, &lt;__main__.TestTable object at 0x7f8fd0f6e530&gt;, &lt;__main__.TestTable object at 0x7f8fd0f6ee30&gt;]\n\nExtract fields:\n1 a\n2 a\n3 b\n4 c"
  },
  {
    "objectID": "python/sqlalchemy/query.html#filters",
    "href": "python/sqlalchemy/query.html#filters",
    "title": "Query",
    "section": "Filters",
    "text": "Filters\nTo add filters to the query, you need to call the fiter method on the result of the query method. As an argument you must pass conditions in syntax such as &lt;table class&gt;.&lt;field name&gt; == &lt;value&gt;.\nSo in the following example all these tips are used to query all records that have a as the value of the text_var.\n\nfor record in (\n    session.\n    query(TestTable).\n    filter(TestTable.text_var==\"a\").\n    all()\n):\n    print(record.numeric_var, record.text_var)\n\n1 a\n2 a"
  },
  {
    "objectID": "sql/special_cases.html",
    "href": "sql/special_cases.html",
    "title": "Special cases",
    "section": "",
    "text": "In this section, I’ll describe some common tasks associated with posgres and their solutions.\nThe following cell creates all needed for examples in this page.\n%%bash\ndocker run --rm -d\\\n    -e POSTGRES_PASSWORD=postgres \\\n    --name special_cases \\\n    postgres:15.4 &&gt; /dev/null\nsleep 5\ndocker exec -i special_cases psql -U postgres -d postgres\n\nCREATE TABLE simple_table(\n    id TEXT NOT NULL,\n    text TEXT NOT NULL\n);\nINSERT INTO simple_table (id, text) VALUES\n(0, 'Text1'),\n(1, 'tExT2'),\n(3, 'TEXT3');\n\nCREATE TABLE\nINSERT 0 3\nThe table to be used for the experiments.\n%%bash \ndocker exec -i special_cases psql -U postgres -d postgres\n\nSELECT * FROM simple_table;\n\n id | text  \n----+-------\n 0  | Text1\n 1  | tExT2\n 3  | TEXT3\n(3 rows)\nStop container! After playing with examples, you should turn it off with the following command.\n!docker stop special_cases &&gt; /dev/null"
  },
  {
    "objectID": "sql/special_cases.html#columns-names",
    "href": "sql/special_cases.html#columns-names",
    "title": "Special cases",
    "section": "Columns names",
    "text": "Columns names\nTo get names of all columns you should use SELECT column_name FROM information_schema.columns WHERE table_name=. Just like in following example:\n\n%%bash \ndocker exec -i special_cases psql -U postgres -d postgres\n\nSELECT column_name\nFROM information_schema.columns\nWHERE table_name = 'simple_table';\n\n column_name \n-------------\n id\n text\n(2 rows)"
  },
  {
    "objectID": "sql/special_cases.html#count-raws",
    "href": "sql/special_cases.html#count-raws",
    "title": "Special cases",
    "section": "Count raws",
    "text": "Count raws\nTo get counts of observations in table you can use command: SELECT COUNT(*) FROM &lt;table_name&gt;.\n\n%%bash \ndocker exec -i special_cases psql -U postgres -d postgres\n\nSELECT COUNT(*) FROM simple_table\n\n count \n-------\n     3\n(1 row)"
  },
  {
    "objectID": "sql/special_cases.html#list-tables",
    "href": "sql/special_cases.html#list-tables",
    "title": "Special cases",
    "section": "List tables",
    "text": "List tables\nIt’s a common task to list all available tables for the current database. So in this section I want to mention some options.\n\n\\dt\nVery simple method, but it doesn’t always work.\n\n%%bash \ndocker exec -i special_cases psql -U postgres -d postgres\n\\dt;\n\n            List of relations\n Schema |     Name     | Type  |  Owner   \n--------+--------------+-------+----------\n public | simple_table | table | postgres\n(1 row)\n\n\n\n\n\npg_catalog.pg_tables table\nThis is a service table that contains the tables available for this base.\nIn the following example, I have simply selected everything from it. By default it contains some service tables.\n\n%%bash \ndocker exec -i special_cases psql -U postgres -d postgres\nSELECT * FROM pg_catalog.pg_tables;\n\n     schemaname     |        tablename         | tableowner | tablespace | hasindexes | hasrules | hastriggers | rowsecurity \n--------------------+--------------------------+------------+------------+------------+----------+-------------+-------------\n public             | simple_table             | postgres   |            | f          | f        | f           | f\n pg_catalog         | pg_statistic             | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_type                  | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_foreign_table         | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_authid                | postgres   | pg_global  | t          | f        | f           | f\n pg_catalog         | pg_statistic_ext_data    | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_user_mapping          | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_subscription          | postgres   | pg_global  | t          | f        | f           | f\n pg_catalog         | pg_attribute             | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_proc                  | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_class                 | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_attrdef               | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_constraint            | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_inherits              | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_index                 | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_operator              | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_opfamily              | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_opclass               | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_am                    | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_amop                  | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_amproc                | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_language              | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_largeobject_metadata  | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_aggregate             | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_statistic_ext         | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_rewrite               | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_trigger               | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_event_trigger         | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_description           | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_cast                  | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_enum                  | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_namespace             | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_conversion            | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_depend                | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_database              | postgres   | pg_global  | t          | f        | f           | f\n pg_catalog         | pg_db_role_setting       | postgres   | pg_global  | t          | f        | f           | f\n pg_catalog         | pg_tablespace            | postgres   | pg_global  | t          | f        | f           | f\n pg_catalog         | pg_auth_members          | postgres   | pg_global  | t          | f        | f           | f\n pg_catalog         | pg_shdepend              | postgres   | pg_global  | t          | f        | f           | f\n pg_catalog         | pg_shdescription         | postgres   | pg_global  | t          | f        | f           | f\n pg_catalog         | pg_ts_config             | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_ts_config_map         | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_ts_dict               | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_ts_parser             | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_ts_template           | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_extension             | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_foreign_data_wrapper  | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_foreign_server        | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_policy                | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_replication_origin    | postgres   | pg_global  | t          | f        | f           | f\n pg_catalog         | pg_default_acl           | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_init_privs            | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_seclabel              | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_shseclabel            | postgres   | pg_global  | t          | f        | f           | f\n pg_catalog         | pg_collation             | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_parameter_acl         | postgres   | pg_global  | t          | f        | f           | f\n pg_catalog         | pg_partitioned_table     | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_range                 | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_transform             | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_sequence              | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_publication           | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_publication_namespace | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_publication_rel       | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_subscription_rel      | postgres   |            | t          | f        | f           | f\n pg_catalog         | pg_largeobject           | postgres   |            | t          | f        | f           | f\n information_schema | sql_features             | postgres   |            | f          | f        | f           | f\n information_schema | sql_implementation_info  | postgres   |            | f          | f        | f           | f\n information_schema | sql_parts                | postgres   |            | f          | f        | f           | f\n information_schema | sql_sizing               | postgres   |            | f          | f        | f           | f\n(69 rows)\n\n\n\nBut we can impose restrictions, for example, on the schemaname column and get only those tables we are interested in.\n\n%%bash \ndocker exec -i special_cases psql -U postgres -d postgres\nSELECT * FROM pg_catalog.pg_tables WHERE schemaname='public';\n\n schemaname |  tablename   | tableowner | tablespace | hasindexes | hasrules | hastriggers | rowsecurity \n------------+--------------+------------+------------+------------+----------+-------------+-------------\n public     | simple_table | postgres   |            | f          | f        | f           | f\n(1 row)"
  },
  {
    "objectID": "sql/select/unique_values.html",
    "href": "sql/select/unique_values.html",
    "title": "Unique values (DISTINCT)",
    "section": "",
    "text": "Using the syntax SELECT DISTINCT ... you can get unique values in any column. Check official postgres documentation.\nIn the next cell, I create everything I need for the examples in this page.\n%%bash\ndocker run --rm -d\\\n    -e POSTGRES_PASSWORD=postgres \\\n    --name sql_DISTINCT_examples \\\n    postgres:15.4 &&gt; /dev/null\nsleep 5\ndocker exec -i sql_DISTINCT_examples psql -U postgres -d postgres\n\n\\set QUIET on\nCREATE TABLE non_unique_values(\n    col1 TEXT,\n    col2 TEXT\n);\nINSERT INTO non_unique_values(col1, col2) VALUES\n('A', 'Z'),\n('A', 'X'),\n('A', 'Y'),\n('A', 'X'),\n('B', 'X'),\n('B', 'Z'),\n('C', 'Z'),\n('C', 'Y'),\n('C', 'Y');\nThe next cell shows the table I will use for examples in this section.\n%%bash\ndocker exec -i sql_DISTINCT_examples psql -U postgres -d postgres\n\nSELECT * FROM non_unique_values;\n\n col1 | col2 \n------+------\n A    | Z\n A    | X\n A    | Y\n A    | X\n B    | X\n B    | Z\n C    | Z\n C    | Y\n C    | Y\n(9 rows)\nNote don’t forget to stop the container when you finish playing with examples.\n!docker stop sql_DISTINCT_examples &&gt; /dev/null"
  },
  {
    "objectID": "sql/select/unique_values.html#specific-column",
    "href": "sql/select/unique_values.html#specific-column",
    "title": "Unique values (DISTINCT)",
    "section": "Specific column",
    "text": "Specific column\nIf you use only one column as the result of a SELECT, the unique values of that column will be retrieved.\n\n%%bash\ndocker exec -i sql_DISTINCT_examples psql -U postgres -d postgres\n\nSELECT DISTINCT col1 FROM non_unique_values;\n\n col1 \n------\n B\n C\n A\n(3 rows)"
  },
  {
    "objectID": "sql/select/unique_values.html#columns-combination",
    "href": "sql/select/unique_values.html#columns-combination",
    "title": "Unique values (DISTINCT)",
    "section": "Columns combination",
    "text": "Columns combination\nIf you use multiple columns as the result of a SELECT, you will get every possible combination of values in the columns once.\n\n%%bash\ndocker exec -i sql_DISTINCT_examples psql -U postgres -d postgres\n\nSELECT DISTINCT col1, col2 FROM non_unique_values;\n\n col1 | col2 \n------+------\n A    | Z\n C    | Z\n B    | X\n A    | Y\n A    | X\n C    | Y\n B    | Z\n(7 rows)"
  },
  {
    "objectID": "sql/select/unique_values.html#unique-on-specific-column",
    "href": "sql/select/unique_values.html#unique-on-specific-column",
    "title": "Unique values (DISTINCT)",
    "section": "Unique on specific column",
    "text": "Unique on specific column\nBy using DISTINCT ON (expression) you can got the table that contains records uniue by (expression). You can specify the column by which unique records should be selected.\nNote The DISTINCT ON clause is not part of the SQL standard and is sometimes considered bad style because of the potentially indeterminate nature of its results. With judicious use of GROUP BY and subqueries in FROM, this construct can be avoided, but it is often the most convenient alternative.\nNote It is assumed that in the controversial case of which entry to take, the one that occurs first in (expression) is always taken, but no official confirmation of this has yet been found.\nSo in the following example I just use col1 as (expression). As a result we have only unique values in col1 but uniqueness for col2 but uniqueness for col2 was not a concern for the query. Note that col2 only takes values that match the first entry in col1.\n\n%%bash\ndocker exec -i sql_DISTINCT_examples psql -U postgres -d postgres\n\nSELECT DISTINCT ON (col1) \n    col1, col2 \nFROM \n    non_unique_values;\n\n col1 | col2 \n------+------\n A    | Z\n B    | X\n C    | Z\n(3 rows)"
  },
  {
    "objectID": "sql/select/sorting.html",
    "href": "sql/select/sorting.html",
    "title": "Sorting (ORDER BY)",
    "section": "",
    "text": "This is key word that allow you to sort the results of the SELECT.\nIn the next cell, I create everything I need for the examples in this page.\n%%bash\n# creating a container and comming to psql command line\ndocker run -d --rm\\\n    --name order_by_examples\\\n    -e POSTGRES_PASSWORD=docker_app\\\n    -v ./load_tables/CSV/iris_csv.csv:/iris_csv.csv\\\n    postgres:15.4 &&gt; /dev/null\nsleep 5\ndocker exec -i order_by_examples psql -U postgres -d postgres\n\nCREATE TABLE table_for_ordering(\n    int_value INT,\n    text_value TEXT\n);\nINSERT INTO table_for_ordering(int_value, text_value) VALUES\n(4, 'ab'),\n(1, 'bw'),\n(1, 'bc'),\n(3, 'ba'),\n(5, 'cg'),\n(2, 'cd'),\n(7, 'ba');\n\nCREATE TABLE\nINSERT 0 7\nHere is the table used for the examples. As you can see, I’ve put some variables in there and not sorted them all in any systematic way.\n%%bash\ndocker exec -i order_by_examples psql -U postgres -d postgres\nSELECT * FROM table_for_ordering;\n\n int_value | text_value \n-----------+------------\n         4 | ab\n         1 | bw\n         1 | bc\n         3 | ba\n         5 | cg\n         2 | cd\n(6 rows)\nNote don’t forget to stop the container when you finish playing with examples.\n!docker stop order_by_examples &&gt; /dev/null"
  },
  {
    "objectID": "sql/select/sorting.html#numeric-variable",
    "href": "sql/select/sorting.html#numeric-variable",
    "title": "Sorting (ORDER BY)",
    "section": "Numeric variable",
    "text": "Numeric variable\nVery simple - just sort records in ascending order for the selected variable.\n\n%%bash\ndocker exec -i order_by_examples psql -U postgres -d postgres\n\nSELECT * FROM table_for_ordering ORDER BY int_value;\n\n int_value | text_value \n-----------+------------\n         1 | bw\n         1 | bc\n         2 | cd\n         3 | ba\n         4 | ab\n         5 | cg\n(6 rows)"
  },
  {
    "objectID": "sql/select/sorting.html#text-varible",
    "href": "sql/select/sorting.html#text-varible",
    "title": "Sorting (ORDER BY)",
    "section": "Text varible",
    "text": "Text varible\nIt seems that sorting by text variables is done in the usual order according to the encoding table, аnd if the first i characters are the same, the decision will be made based on the i+1th character. The following example confirms it.\n\n%%bash\ndocker exec -i order_by_examples psql -U postgres -d postgres\n\nSELECT * FROM table_for_ordering ORDER BY text_value;\n\n int_value | text_value \n-----------+------------\n         4 | ab\n         3 | ba\n         1 | bc\n         1 | bw\n         2 | cd\n         5 | cg\n(6 rows)"
  },
  {
    "objectID": "sql/select/sorting.html#descending",
    "href": "sql/select/sorting.html#descending",
    "title": "Sorting (ORDER BY)",
    "section": "Descending",
    "text": "Descending\nDy default ORDER BY sort values in ascending order, but by using key word DESC after column name in ORDER BY block you can make posgres use descending order.\nSo in following example I apply this option for int_value.\n\n%%bash\ndocker exec -i order_by_examples psql -U postgres -d postgres\n\nSELECT * FROM table_for_ordering ORDER BY int_value DESC;\n\n int_value | text_value \n-----------+------------\n         7 | ba\n         5 | cg\n         4 | ab\n         3 | ba\n         2 | cd\n         1 | bc\n         1 | bw\n(7 rows)"
  },
  {
    "objectID": "sql/select/sorting.html#order-by-several",
    "href": "sql/select/sorting.html#order-by-several",
    "title": "Sorting (ORDER BY)",
    "section": "Order by several",
    "text": "Order by several\nYou can use multiple columns in the ORDER BY block. In this case the records will be sorted by the ith column, but if there are some options caused by equal values in the ith column, the i+1th column will be used to determine the final order.\nSo in the following example:\n\nThe first query uses int_value, text_value in ORDER BY;\n\nIt has int_value = 1 twice, but the final order is determined by 'bc'&lt;'dw' in text_value;\n\nSecond query uses text_value, int_value DESC in ORDER BY.\n\nIt has text_value = 'ba' twice, but I also mentioned sorting by int_value in descending order, so 7 | ba is preferable to 3 | ba.\n\n\n\n%%bash\ndocker exec -i order_by_examples psql -U postgres -d postgres\n\nSELECT * FROM table_for_ordering ORDER BY int_value, text_value;\nSELECT * FROM table_for_ordering ORDER BY text_value, int_value DESC;\n\n int_value | text_value \n-----------+------------\n         1 | bc\n         1 | bw\n         2 | cd\n         3 | ba\n         4 | ab\n         5 | cg\n         7 | ba\n(7 rows)\n\n int_value | text_value \n-----------+------------\n         4 | ab\n         7 | ba\n         3 | ba\n         1 | bc\n         1 | bw\n         2 | cd\n         5 | cg\n(7 rows)"
  },
  {
    "objectID": "sql/select/aggregation/functions.html",
    "href": "sql/select/aggregation/functions.html",
    "title": "Aggregation functions",
    "section": "",
    "text": "This page is about what operation you can use for aggregations in sql.\nThe next cell starts the container for the playground database.\n%%bash\ndocker run --rm -d\\\n    -e POSTGRES_PASSWORD=docker_app \\\n    --name aggregation_postgres_exmaples \\\n    postgres:15.4 &&gt; /dev/null\nsleep 5\ndocker exec -i aggregation_postgres_exmaples psql -U postgres -d postgres\n\nCREATE TABLE tab (\n    var1 TEXT,\n    var2 BOOL,\n    var3 TEXT\n);\n\nINSERT INTO tab (var1, var2, var3) VALUES\n('foo', true, 'peter'),\n('bar', true, 'alice'),\n('bar', false, 'bob'),\n('foo', true, 'jamie');\n\nSELECT * FROM tab;\n\nCREATE TABLE\nINSERT 0 4\n var1 | var2 | var3  \n------+------+-------\n foo  | t    | peter\n bar  | t    | alice\n bar  | f    | bob\n foo  | t    | jamie\n(4 rows)\nNote Don’t forget to stop the container when you’ve finished playing with it.\n%%bash\ndocker stop aggregation_postgres_exmaples;\n\naggregation_postgres_exmaples"
  },
  {
    "objectID": "sql/select/aggregation/functions.html#logical-operations-bool_or-bool_and",
    "href": "sql/select/aggregation/functions.html#logical-operations-bool_or-bool_and",
    "title": "Aggregation functions",
    "section": "Logical operations (BOOL_OR, BOOL_AND)",
    "text": "Logical operations (BOOL_OR, BOOL_AND)\nSometimes you need to find out if all values of a boolean variable by group are true. For such cases you can use boot_and aggregation function. Also there is function bool_or if you need to get if any value by group is true.\nThe following example shows how to use these functions.\n\n%%bash\ndocker exec -i aggregation_postgres_exmaples psql -U postgres -d postgres\n\nSELECT\n    var1,\n    bool_and(var2),\n    bool_or(var2)\nFROM tab\nGROUP BY var1;\n\n var1 | bool_and | bool_or \n------+----------+---------\n bar  | f        | t\n foo  | t        | t\n(2 rows)"
  },
  {
    "objectID": "sql/select/aggregation/functions.html#strings-concatenations",
    "href": "sql/select/aggregation/functions.html#strings-concatenations",
    "title": "Aggregation functions",
    "section": "Strings concatenations",
    "text": "Strings concatenations\nSuppose you need to concatenate all the values of a text variable into groups. For this purpose there is an argregation function string_agg(&lt;column for concatenation&gt;, &lt;separator&gt;). The result will be all values by group in one line, separated by a separator.\nWe list the values of variable val2 that occur in different groups of variable val1 separated by commas:\n\n%%bash\ndocker exec -i aggregation_postgres_exmaples psql -U postgres -d postgres\nSELECT \n    string_agg(var3, ', ')\nFROM tab\nGROUP BY var1;\n\n  string_agg  \n--------------\n alice, bob\n peter, jamie\n(2 rows)"
  },
  {
    "objectID": "sql/select/first_last_value.html",
    "href": "sql/select/first_last_value.html",
    "title": "FIRST/LAST_VALUE",
    "section": "",
    "text": "In SQL there are the clauses FIRST_VALUE and LAST_VALUE. These allow you to get the first (top) or last (bottom) value in some sort of ordered range. In this page I want to play around with these expressions to learn how they work.\nThe following cell contains a database definition that will be used for the experiments on this page.\n%%bash\ndocker run --rm -d\\\n    -e POSTGRES_PASSWORD=postgres \\\n    --name first_value_examples \\\n    postgres:15.4 &&gt; /dev/null\nsleep 5\ndocker exec -i first_value_examples psql -U postgres -d postgres\n\n\nCREATE TABLE example_table(\n    col1 TEXT,\n    col2 INT\n);\nINSERT INTO example_table(col1, col2) VALUES\n('A', 10),\n('B', 43),\n('C', 3),\n('D', 5),\n('E', 32),\n('F', 31);\n\nCREATE TABLE\nINSERT 0 6\nThe basic tables are available here.\n%%bash\necho \"=====example_table=====\"\ndocker exec first_value_examples \\\n    psql -U postgres -d postgres -c \"SELECT * FROM example_table\";\n\n=====example_table=====\n col1 | col2 \n------+------\n A    |   10\n B    |   43\n C    |    3\n D    |    5\n E    |   32\n F    |   31\n(6 rows)\nNote Don’t forget to stop the container when you’ve finished playing with it.\n%%bash\ndocker stop first_value_examples\n\nfirst_value_examples"
  },
  {
    "objectID": "sql/select/first_last_value.html#find-out-more",
    "href": "sql/select/first_last_value.html#find-out-more",
    "title": "FIRST/LAST_VALUE",
    "section": "Find out more",
    "text": "Find out more\n\nGeeksForGeeks FIRST_VALUE statement;\n Command prompt “How to Use the LAST_VALUE() Function in PostgreSQL?”."
  },
  {
    "objectID": "sql/select/first_last_value.html#basic-example",
    "href": "sql/select/first_last_value.html#basic-example",
    "title": "FIRST/LAST_VALUE",
    "section": "Basic example",
    "text": "Basic example\n\nCorresponds to lowest\nYou can get the first value in any column from any other column using the expression FIRST_VALUE (&lt;value column&gt;) OVER (ORDER BY &lt;sort column&gt;);.\nSo the following example shows how to use an include to query a column that contains a value in col1 that corresponds to the lowest value in col2. Lowest value of col2 = 3, it corresponds to the record where col1 = 'C'. So we have got a column that contains “C” values.\nNote values were sorted by col2.\n\n%%bash\ndocker exec -i first_value_examples psql -U postgres -d postgres\nSELECT \n    col1, \n    col2, \n    FIRST_VALUE (col1) OVER (ORDER BY col2)\nFROM example_table;\n\n col1 | col2 | first_value \n------+------+-------------\n C    |    3 | C\n D    |    5 | C\n A    |   10 | C\n F    |   31 | C\n E    |   32 | C\n B    |   43 | C\n(6 rows)\n\n\n\n\n\nCorresponds to largest\nBy simply adding DESC to the ORDER BY expression, you can get the value that corresponds to the largest value.\nThe following example duplicates the previous one, but we have B in the first_value column because it corresponds to the largest value of col2.\n\n%%bash\ndocker exec -i first_value_examples psql -U postgres -d postgres\nSELECT \n    col1, \n    col2, \n    FIRST_VALUE (col1) OVER (ORDER BY col2 DESC)\nFROM example_table;\n\n col1 | col2 | first_value \n------+------+-------------\n B    |   43 | B\n E    |   32 | B\n F    |   31 | B\n A    |   10 | B\n D    |    5 | B\n C    |    3 | B\n(6 rows)\n\n\n\n\n\nLAST_VALUE\nHere I’m using the LAST_VALUE statement. It works just like `FIRST_VALUE’ but selects the last value in the range.\nThis is almost the same as the result of the Corresponds to largest subsection, but note that the result is records sorted by col2 in ascending order.\n\n%%bash\ndocker exec -i first_value_examples psql -U postgres -d postgres\nSELECT \n    col1, \n    col2, \n    LAST_VALUE (col1) OVER (\n        ORDER BY col2\n        RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n    )\nFROM example_table;\n\n col1 | col2 | last_value \n------+------+------------\n C    |    3 | B\n D    |    5 | B\n A    |   10 | B\n F    |   31 | B\n E    |   32 | B\n B    |   43 | B\n(6 rows)"
  },
  {
    "objectID": "sql/select/NULL_conditions.html",
    "href": "sql/select/NULL_conditions.html",
    "title": "NULL conditions (IS (NOT) NULL)",
    "section": "",
    "text": "It’s a really common task to find records with empty values, or vice versa, to find records where all the information is available.\nIn the next cell, I create everything I need for the examples in this page.\n%%bash\ndocker run --rm -d\\\n    -e POSTGRES_PASSWORD=postgres \\\n    --name null_conditions_example \\\n    postgres:15.4 &&gt; /dev/null\nsleep 5\ndocker exec -i null_conditions_example psql -U postgres -d postgres\n\nCREATE TABLE table_with_nulls(\n    col1 TEXT,\n    col2 INT\n);\nINSERT INTO table_with_nulls(col1, col2) VALUES\n('A', NULL),\n('A', 1),\n('A', 2),\n(NULL, 1),\n('C', 3),\n(NULL, 4);\n\nCREATE TABLE\nINSERT 0 6\nIn the next cell, I’ll show the table I’m going to use as an example.\nNote psql show NULL values just as empty places.\n%%bash\ndocker exec -i null_conditions_example psql -U postgres -d postgres\nSELECT col1, col2 FROM table_with_nulls;\n\n col1 | col2 \n------+------\n A    |     \n A    |    1\n A    |    2\n      |    1\n C    |    3\n      |    4\n(6 rows)\nNote don’t forget to stop the container when you finish playing with examples.\n!docker stop null_conditions_example &&gt; /dev/null"
  },
  {
    "objectID": "sql/select/NULL_conditions.html#is-null",
    "href": "sql/select/NULL_conditions.html#is-null",
    "title": "NULL conditions (IS (NOT) NULL)",
    "section": "IS NULL",
    "text": "IS NULL\nBy using &lt;column name&gt; IS NULL in the WHERE block, you can select records with empty values in a particular column.\nIn the following example I SELECT only values with NULL values in col1.\n\n%%bash\ndocker exec -i null_conditions_example psql -U postgres -d postgres\nSELECT col1, col2 \nFROM table_with_nulls\nWHERE col1 IS NULL;\n\n col1 | col2 \n------+------\n      |    1\n      |    4\n(2 rows)"
  },
  {
    "objectID": "sql/select/NULL_conditions.html#is-not-null",
    "href": "sql/select/NULL_conditions.html#is-not-null",
    "title": "NULL conditions (IS (NOT) NULL)",
    "section": "IS NOT NULL",
    "text": "IS NOT NULL\nBy using &lt;column name&gt; IS NOT NULL in the WHERE block, you can select records with filled values in a particular column.\nIn the following example I SELECT only values that hase something in col1.\n\n%%bash\ndocker exec -i null_conditions_example psql -U postgres -d postgres\nSELECT col1, col2\nFROM table_with_nulls\nWHERE col1 IS NOT NULL;\n\n col1 | col2 \n------+------\n A    |     \n A    |    1\n A    |    2\n C    |    3\n(4 rows)"
  },
  {
    "objectID": "sql/json_datatype.html",
    "href": "sql/json_datatype.html",
    "title": "JSON data type",
    "section": "",
    "text": "By defining a column of type JSONB, you can use JSON objects in your SQL tables. There are special tools that allow you to work with such columns in a dictionary-style way.\nIn the following cell, postgres containers have been used for examples.\n%%bash\ndocker run --rm -d\\\n    -e POSTGRES_PASSWORD=postgres \\\n    --name using_json_datatype \\\n    postgres:15.4 &&gt; /dev/null\nsleep 5\nNote Don’t forget to stop the container when you have finished playing with the examples.\n%%bash\ndocker stop using_json_datatype"
  },
  {
    "objectID": "sql/json_datatype.html#defining-such-column",
    "href": "sql/json_datatype.html#defining-such-column",
    "title": "JSON data type",
    "section": "Defining such column",
    "text": "Defining such column\nIn the following cell I define the data column in example_table as a JSONB datatype. To populate it, just pass string literals containing json-style rows.\n\n%%bash\ndocker exec -i using_json_datatype psql -U postgres -d postgres\n\nCREATE TABLE example_table (\n    id INT PRIMARY KEY,\n    data JSONB\n);\n\nINSERT INTO example_table (\n    id, data\n)\nVALUES\n    (1, '{\"A\": 0, \"B\": 0}'),\n    (2, '{\"A\": 100}'),\n    (3, '{\"B\": -50}'),\n    (4, '{\"A\": 50}'),\n    (5, '{\"B\": -30}'),\n    (6, '{\"B\": 100}'),\n    (7, '{\"B\" : 90, \"A\": -50}')\n;\n\nCREATE TABLE\nINSERT 0 7\n\n\nIn the following cell displayed how JSON type cell will be displayed in the results of the simple query.\n\n%%bash\ndocker exec using_json_datatype\\\n    psql -U postgres -d postgres -c \"SELECT * FROM example_table;\"\n\n id |        data         \n----+---------------------\n  1 | {\"A\": 0, \"B\": 0}\n  2 | {\"A\": 100}\n  3 | {\"B\": -50}\n  4 | {\"A\": 50}\n  5 | {\"B\": -30}\n  6 | {\"B\": 100}\n  7 | {\"A\": -50, \"B\": 90}\n(7 rows)"
  },
  {
    "objectID": "sql/json_datatype.html#extracting-values",
    "href": "sql/json_datatype.html#extracting-values",
    "title": "JSON data type",
    "section": "Extracting values",
    "text": "Extracting values\nYou can access values by key from a JSON type column using the syntax &lt;column name&gt;-&gt;&lt;key name&gt;.\nSo in the following cell, a query has been executed that extracts values under the key ‘A’ from each record.\n\n%%bash\ndocker exec -i using_json_datatype psql -U postgres -d postgres\nSELECT id, data, data-&gt;'A' FROM example_table;\n\n id |        data         | ?column? \n----+---------------------+----------\n  1 | {\"A\": 0, \"B\": 0}    | 0\n  2 | {\"A\": 100}          | 100\n  3 | {\"B\": -50}          | \n  4 | {\"A\": 50}           | 50\n  5 | {\"B\": -30}          | \n  6 | {\"B\": 100}          | \n  7 | {\"A\": -50, \"B\": 90} | -50\n(7 rows)\n\n\n\nYou can access the values by clicking on them and then operate on them as you would any other column."
  },
  {
    "objectID": "sql/datetime/date_components.html",
    "href": "sql/datetime/date_components.html",
    "title": "Date components (DATE_PART)",
    "section": "",
    "text": "Sometimes you need to extract some specific numbers from the date, such as year, month and day. For such purposes you can use the DATE_PART function.\nIn the following cell container, a postgres instance was started and a table was created that will be used for experiments.\n%%bash\ndocker run --rm -d\\\n    -e POSTGRES_PASSWORD=postgres \\\n    --name datediff_example \\\n    postgres:15.4 &&gt; /dev/null\nsleep 5\ndocker exec -i datediff_example psql -U postgres -d postgres\n\nCREATE TABLE tab(\n    var1 DATE\n);\nINSERT INTO tab (var1) VALUES\n('2022-12-20'),\n('2021-10-15'),\n('2018-06-15');\n\nSELECT * FROM tab;\n\nCREATE TABLE\nINSERT 0 3\n    var1    \n------------\n 2022-12-20\n 2021-10-15\n 2018-06-15\n(3 rows)\nNote Don’t forget to stop the container when you finish playing with container.\n%%bash\ndocker stop datediff_example\n\ndatediff_example"
  },
  {
    "objectID": "sql/datetime/date_components.html#example",
    "href": "sql/datetime/date_components.html#example",
    "title": "Date components (DATE_PART)",
    "section": "Example",
    "text": "Example\nHere is an example of how to use DATE_PART. You must use the following syntax: DATE_PART('part', &lt;column name&gt;). So in the following cell all dates will be split into day, month and year:\n\n%%bash\ndocker exec -i datediff_example psql -U postgres -d postgres\n\nSELECT \n    var1 \"Original date\",\n    DATE_PART('year', var1) \"Year\", \n    DATE_PART('month', var1) \"Month\",\n    DATE_PART('day', var1) \"Day\"\nFROM tab;\n\n Original date | Year | Month | Day \n---------------+------+-------+-----\n 2022-12-20    | 2022 |    12 |  20\n 2021-10-15    | 2021 |    10 |  15\n 2018-06-15    | 2018 |     6 |  15\n(3 rows)"
  },
  {
    "objectID": "sql/python_interaction/create_database.html",
    "href": "sql/python_interaction/create_database.html",
    "title": "Create database",
    "section": "",
    "text": "This page discusses issue of creating databases in postgres instance using python scripts.\nIn the following cell, a postgres instance has been deployed in a docker container. No databases were mentioned during creation. Final command shows databases available in just created postgres instance, there are some database looks like it is required by postgres by default.\n\n%%bash\ndocker run --rm -d\\\n    -e POSTGRES_USER=docker_app\\\n    -e POSTGRES_PASSWORD=docker_app\\\n    --name test_postgres\\\n    -p 5431:5432\\\n    postgres:15.4 &&gt; /dev/null\nsleep 5\n\ndocker exec test_postgres \\\n    psql --username docker_app -h localhost -p 5432 -l;\n\n                                                    List of databases\n    Name    |   Owner    | Encoding |  Collate   |   Ctype    | ICU Locale | Locale Provider |     Access privileges     \n------------+------------+----------+------------+------------+------------+-----------------+---------------------------\n docker_app | docker_app | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            | \n postgres   | docker_app | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            | \n template0  | docker_app | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            | =c/docker_app            +\n            |            |          |            |            |            |                 | docker_app=CTc/docker_app\n template1  | docker_app | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            | =c/docker_app            +\n            |            |          |            |            |            |                 | docker_app=CTc/docker_app\n(4 rows)\n\n\n\nNow the central code for this page - creating new databases with psycopg2. Actually it looks like any other query through psycopg with a few features:\n\nYou don’t need to specify a database to connect to;\nYou need to set autocommit = True for the postgres database;\nYou can use the standard sql command CREATE DATABASE to create a database with the required name.\n\nSo in the following cell I just add 10 new databases to the postgres instance.\n\nimport psycopg2\n\nconn = psycopg2.connect(\n    port = \"5431\", # same as when creating a postgres container\n    user = \"docker_app\",\n    password = \"docker_app\",\n    host= \"localhost\"\n)\n\nconn.autocommit = True\ncur = conn.cursor()\nfor i in range(10): cur.execute(f'CREATE DATABASE test_database{i+1}')\n\n# close connection\ncur.close()\nconn.close()\n\nLet’s check the database list now - we have 10 more databases.\n\n%%bash\ndocker exec test_postgres \\\n    psql --username docker_app -h localhost -p 5432 -c '\\l';\n\n                                                      List of databases\n      Name       |   Owner    | Encoding |  Collate   |   Ctype    | ICU Locale | Locale Provider |     Access privileges     \n-----------------+------------+----------+------------+------------+------------+-----------------+---------------------------\n docker_app      | docker_app | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            | \n postgres        | docker_app | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            | \n template0       | docker_app | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            | =c/docker_app            +\n                 |            |          |            |            |            |                 | docker_app=CTc/docker_app\n template1       | docker_app | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            | =c/docker_app            +\n                 |            |          |            |            |            |                 | docker_app=CTc/docker_app\n test_database1  | docker_app | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            | \n test_database10 | docker_app | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            | \n test_database2  | docker_app | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            | \n test_database3  | docker_app | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            | \n test_database4  | docker_app | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            | \n test_database5  | docker_app | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            | \n test_database6  | docker_app | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            | \n test_database7  | docker_app | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            | \n test_database8  | docker_app | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            | \n test_database9  | docker_app | UTF8     | en_US.utf8 | en_US.utf8 |            | libc            | \n(14 rows)\n\n\n\n\n%%bash\ndocker stop test_postgres\n\ntest_postgres"
  },
  {
    "objectID": "sql/build_container.html",
    "href": "sql/build_container.html",
    "title": "Build container",
    "section": "",
    "text": "The postgres section of this site will be inextricably linked to docker, so it is extremely important to dive into the intricacies of running docker containers with postgres.\nThe greatest amount of information for this page is taken from official postgres dockerhub page.\n\nInitialisation scripts\nIf you need to organise the initialisation at container startup, you can mount a folder containing *.sql and *.sh initialisation scripts on /docker-entrypoint-initdb.d. This will execute the files in /docker-entrypoint-initdb.d.\nSo in the following example, I start the docker container with the folder mounted as /docker-entrypoint-initdb.d. This folder only contains a create_table.sql which creates an empty main_table. Then I check the existence of the table that should be created by create_table.sql.\n\n%%bash\ndocker run --rm -d\\\n    -v ./build_container/initialisation_scripts:/docker-entrypoint-initdb.d\\\n    -e POSTGRES_PASSWORD=docker_app \\\n    --name init_scripts_example \\\n    postgres:15.4 &&gt; /dev/null\n\nsleep 5\necho \"=====CHECK TABLE=====\"\ndocker exec init_scripts_example bash -c \"psql -U postgres -d postgres -c \\\"\\dt\\\"\"\n\ndocker stop  init_scripts_example &&gt; /dev/null\n\n=====CHECK TABLE=====\n           List of relations\n Schema |    Name    | Type  |  Owner   \n--------+------------+-------+----------\n public | main_table | table | postgres\n(1 row)"
  },
  {
    "objectID": "jupyter/voila_vs_nbconvert_saving_html/contains_html.html",
    "href": "jupyter/voila_vs_nbconvert_saving_html/contains_html.html",
    "title": "NBconvert",
    "section": "",
    "text": "В этом простом notebook содражться несколько latex выражений. Я намереваю сравнить два случая: - Конвернация в html используя nbconvert; - И используя voila с последующим сохраненнием в html.\nЦель - выяснить, как отличаются части html документов, полученных при использовании каждого из подходов, ответсвенные за latex.\n\\[x_2+3=3232\\]\nА это latex внутри markdown с использованием всего одного знака $ для выделения \\(\\sqrt{x_2}\\).\nКак результат в не слишком продвинух html движках (например базовый viewer jupyter lab) результат получается не очень.\n$$x_2+3=3232$$\nНу а html разметка для той ячейки где сидит latex, выглядит так.\n&lt;div class=\"jp-Cell jp-MarkdownCell jp-Notebook-cell\"&gt;\n&lt;div class=\"jp-Cell-inputWrapper\"&gt;\n&lt;div class=\"jp-Collapser jp-InputCollapser jp-Cell-inputCollapser\"&gt;\n&lt;/div&gt;\n&lt;div class=\"jp-InputArea jp-Cell-inputArea\"&gt;&lt;div class=\"jp-InputPrompt jp-InputArea-prompt\"&gt;\n&lt;/div&gt;&lt;div class=\"jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput \" data-mime-type=\"text/markdown\"&gt;\n$$x_2+3=3232$$\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n\nСохранение страницы из voila\nНа любом из известных графических движков выглядит нормально… Соответсвующий html для ячейки.\n&lt;div class=\"jp-Cell jp-MarkdownCell jp-Notebook-cell\"&gt;\n&lt;div class=\"jp-Cell-inputWrapper\"&gt;\n&lt;div class=\"jp-Collapser jp-InputCollapser jp-Cell-inputCollapser\"&gt;\n&lt;/div&gt;\n&lt;div class=\"jp-InputArea jp-Cell-inputArea\"&gt;&lt;div class=\"jp-RenderedHTMLCommon jp-RenderedMarkdown jp-MarkdownOutput \" data-mime-type=\"text/markdown\"&gt;\n&lt;span class=\"MathJax_Preview\" style=\"color: inherit;\"&gt;&lt;/span&gt;&lt;span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"&gt;&lt;span id=\"MathJax-Element-1-Frame\" class=\"mjx-chtml MathJax_CHTML\" tabindex=\"0\" style=\"font-size: 116%; text-align: center; position: relative;\" data-mathml=\"&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;3232&lt;/mn&gt;&lt;/math&gt;\" role=\"presentation\"&gt;&lt;span id=\"MJXc-Node-1\" class=\"mjx-math\" aria-hidden=\"true\"&gt;&lt;span id=\"MJXc-Node-2\" class=\"mjx-mrow\"&gt;&lt;span id=\"MJXc-Node-3\" class=\"mjx-msubsup\"&gt;&lt;span class=\"mjx-base\"&gt;&lt;span id=\"MJXc-Node-4\" class=\"mjx-mi\"&gt;&lt;span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.249em; padding-bottom: 0.311em;\"&gt;x&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"&gt;&lt;span id=\"MJXc-Node-5\" class=\"mjx-mn\" style=\"\"&gt;&lt;span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.373em; padding-bottom: 0.373em;\"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span id=\"MJXc-Node-6\" class=\"mjx-mo MJXc-space2\"&gt;&lt;span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.311em; padding-bottom: 0.434em;\"&gt;+&lt;/span&gt;&lt;/span&gt;&lt;span id=\"MJXc-Node-7\" class=\"mjx-mn MJXc-space2\"&gt;&lt;span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.373em; padding-bottom: 0.373em;\"&gt;3&lt;/span&gt;&lt;/span&gt;&lt;span id=\"MJXc-Node-8\" class=\"mjx-mo MJXc-space3\"&gt;&lt;span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.065em; padding-bottom: 0.311em;\"&gt;=&lt;/span&gt;&lt;/span&gt;&lt;span id=\"MJXc-Node-9\" class=\"mjx-mn MJXc-space3\"&gt;&lt;span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.373em; padding-bottom: 0.373em;\"&gt;3232&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"&gt;&lt;math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;3232&lt;/mn&gt;&lt;/math&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;script type=\"math/tex; mode=display\" id=\"MathJax-Element-1\"&gt;x_2+3=3232&lt;/script&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\nПонятное дело html полученный из voila выглядит значительно солиднее."
  },
  {
    "objectID": "jupyter/nbconvert/example_notebook.html",
    "href": "jupyter/nbconvert/example_notebook.html",
    "title": "Latex",
    "section": "",
    "text": "Notebook - пример для того чтобы разобраться в особенностсях рабты с nbcnovert\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\n\\[f(x) = x_2+3x+22\\]\n\nMatplotlib график\n\nans = plt.plot([1,2,3], [3,4,5])\n\n\n\n\n\n\nПросто картинка\n\n\n\npandas таблица\n\npd.DataFrame(\n    np.random.rand(20, 30)\n)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n\n0\n0.663143\n0.021464\n0.858000\n0.766506\n0.985039\n0.346647\n0.364177\n0.912234\n0.037032\n0.272790\n...\n0.327494\n0.880709\n0.041401\n0.369179\n0.451213\n0.611814\n0.560559\n0.523025\n0.205788\n0.422654\n\n\n1\n0.827827\n0.649757\n0.215769\n0.179621\n0.570741\n0.088579\n0.845170\n0.596793\n0.161002\n0.002342\n...\n0.034259\n0.278648\n0.882712\n0.355811\n0.766390\n0.151718\n0.863036\n0.730791\n0.114088\n0.379146\n\n\n2\n0.865272\n0.487274\n0.399290\n0.380105\n0.088102\n0.054062\n0.795389\n0.883104\n0.364047\n0.394248\n...\n0.415779\n0.853940\n0.773343\n0.865510\n0.672601\n0.770037\n0.088337\n0.236902\n0.437594\n0.369074\n\n\n3\n0.009670\n0.119128\n0.477052\n0.103209\n0.975513\n0.152730\n0.106975\n0.577861\n0.976428\n0.734008\n...\n0.431997\n0.990676\n0.972629\n0.698313\n0.197316\n0.172827\n0.365104\n0.543878\n0.373947\n0.177256\n\n\n4\n0.209034\n0.328370\n0.747825\n0.912026\n0.684328\n0.730339\n0.222565\n0.723777\n0.249022\n0.515858\n...\n0.344164\n0.825411\n0.107212\n0.722163\n0.064626\n0.557274\n0.979061\n0.617117\n0.129202\n0.828734\n\n\n5\n0.748535\n0.438338\n0.321346\n0.048413\n0.564367\n0.938365\n0.504495\n0.701783\n0.817506\n0.966032\n...\n0.833825\n0.877737\n0.827943\n0.433181\n0.188727\n0.847585\n0.873378\n0.414874\n0.385979\n0.283664\n\n\n6\n0.169217\n0.189379\n0.279181\n0.105524\n0.321006\n0.469294\n0.298573\n0.752147\n0.512748\n0.326833\n...\n0.978364\n0.807084\n0.742420\n0.718420\n0.715044\n0.568432\n0.673553\n0.608451\n0.242622\n0.858917\n\n\n7\n0.318894\n0.703588\n0.918537\n0.798998\n0.097702\n0.837357\n0.734335\n0.260800\n0.746494\n0.996407\n...\n0.895983\n0.674704\n0.269369\n0.561942\n0.527911\n0.219911\n0.668875\n0.098179\n0.040179\n0.268487\n\n\n8\n0.538815\n0.789962\n0.170698\n0.653034\n0.209657\n0.784521\n0.721873\n0.653080\n0.735756\n0.870527\n...\n0.816603\n0.700780\n0.589116\n0.054555\n0.269549\n0.292141\n0.051452\n0.064437\n0.029045\n0.033099\n\n\n9\n0.825010\n0.985221\n0.892499\n0.012812\n0.073686\n0.189973\n0.263994\n0.868104\n0.625486\n0.905932\n...\n0.810490\n0.282077\n0.346684\n0.959549\n0.245090\n0.283466\n0.027302\n0.097790\n0.422846\n0.071181\n\n\n10\n0.071752\n0.566090\n0.576133\n0.042006\n0.724374\n0.617664\n0.646433\n0.685819\n0.075983\n0.945756\n...\n0.379925\n0.579344\n0.854027\n0.674528\n0.111671\n0.101454\n0.556477\n0.319533\n0.891699\n0.777899\n\n\n11\n0.321099\n0.606897\n0.152936\n0.282932\n0.240571\n0.523979\n0.499664\n0.103284\n0.252159\n0.749216\n...\n0.756029\n0.388600\n0.771017\n0.406881\n0.670963\n0.328559\n0.055640\n0.058266\n0.255477\n0.231221\n\n\n12\n0.069434\n0.471599\n0.879479\n0.150907\n0.259604\n0.121494\n0.283101\n0.632702\n0.969049\n0.416081\n...\n0.228064\n0.802990\n0.601474\n0.595615\n0.171583\n0.885227\n0.570803\n0.629680\n0.481346\n0.823556\n\n\n13\n0.008207\n0.210180\n0.048300\n0.360097\n0.540163\n0.947513\n0.017571\n0.152821\n0.986996\n0.352458\n...\n0.460305\n0.102086\n0.617660\n0.219018\n0.832248\n0.308261\n0.002964\n0.548857\n0.822519\n0.257786\n\n\n14\n0.564729\n0.040055\n0.281084\n0.704265\n0.293981\n0.351052\n0.014585\n0.487037\n0.433179\n0.850257\n...\n0.708030\n0.932107\n0.591465\n0.046695\n0.475054\n0.762240\n0.329465\n0.752053\n0.205726\n0.453508\n\n\n15\n0.916010\n0.569487\n0.000574\n0.240208\n0.380580\n0.704447\n0.128708\n0.905343\n0.504579\n0.139354\n...\n0.489544\n0.711177\n0.540197\n0.433703\n0.889278\n0.716258\n0.368589\n0.243502\n0.430579\n0.207018\n\n\n16\n0.555162\n0.159681\n0.611847\n0.286136\n0.345460\n0.190651\n0.211620\n0.289052\n0.445264\n0.415703\n...\n0.782363\n0.835031\n0.996787\n0.903400\n0.236019\n0.176010\n0.306686\n0.103315\n0.948140\n0.046661\n\n\n17\n0.867713\n0.831490\n0.288641\n0.728252\n0.620024\n0.315909\n0.273223\n0.423136\n0.236910\n0.257647\n...\n0.016341\n0.977661\n0.601080\n0.977532\n0.088836\n0.754579\n0.807837\n0.050929\n0.439012\n0.721582\n\n\n18\n0.333013\n0.409006\n0.669115\n0.700568\n0.903307\n0.822934\n0.635862\n0.613437\n0.187538\n0.050552\n...\n0.724519\n0.016494\n0.550508\n0.223219\n0.444358\n0.734620\n0.902881\n0.728414\n0.595171\n0.161489\n\n\n19\n0.614472\n0.792331\n0.300568\n0.345817\n0.407511\n0.816590\n0.950677\n0.037251\n0.316392\n0.058362\n...\n0.296249\n0.254440\n0.963934\n0.222605\n0.509345\n0.495333\n0.685706\n0.500341\n0.956666\n0.333952\n\n\n\n\n20 rows × 30 columns\n\n\n\n\n\nТэги\nВ следующих ячейках записано какой тэг для каждой применен.\nТэг remove_cell для markdown ячейки.\n\nprint(\"remove_input\")\n\nremove_input\n\n\n\nprint(\"remove_output\")\n\nremove_output"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "layout/html/lists.html",
    "href": "layout/html/lists.html",
    "title": "List tags",
    "section": "",
    "text": "Lists are an important part of most documents, so here are ways to create lists in HTML."
  },
  {
    "objectID": "layout/html/lists.html#ulli---unordered-list",
    "href": "layout/html/lists.html#ulli---unordered-list",
    "title": "List tags",
    "section": "<ul>,<li> - unordered list",
    "text": "&lt;ul&gt;,&lt;li&gt; - unordered list\nTags: - &lt;ul&gt; describes an unordered list; - &lt;li&gt; describes a item of list.\n\n%%HTML\n&lt;ul&gt;\n    &lt;li&gt;item 1&lt;/li&gt;\n    &lt;li&gt;item 2&lt;/li&gt;\n    &lt;li&gt;...&lt;/li&gt;\n    &lt;li&gt;item n&lt;/li&gt;\n&lt;/ul&gt;\n\n\n    item 1\n    item 2\n    ...\n    item n\n\n\n\nMarkers can be setted as style=\"list-style-type:&lt;marker type&gt;\". There are following markers available:\n\ndisc;\nsquare;\ncircle.\n\n\n%%HTML\n&lt;text&gt;Marker type for whole list:\n&lt;ul style=\"list-style-type:square\"&gt;\n    &lt;li&gt;item 1&lt;/li&gt;\n    &lt;li&gt;item 2&lt;/li&gt;\n    &lt;li&gt;...&lt;/li&gt;\n    &lt;li&gt;item n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;text&gt;Different marker type for each point:&lt;/text&gt;\n&lt;ul&gt;\n    &lt;li style=\"list-style-type:disc\"&gt;disc&lt;/li&gt;\n    &lt;li style=\"list-style-type:square\"&gt;square&lt;/li&gt;\n    &lt;li style=\"list-style-type:circle\"&gt;circle&lt;/li&gt;\n&lt;/ul&gt;\n\nMarker type for whole list:\n\n    item 1\n    item 2\n    ...\n    item n\n\nDifferent marker type for each point:\n\n    disc\n    square\n    circle"
  },
  {
    "objectID": "layout/html/lists.html#olli---ordered-list",
    "href": "layout/html/lists.html#olli---ordered-list",
    "title": "List tags",
    "section": "<ol>,<li> - ordered list",
    "text": "&lt;ol&gt;,&lt;li&gt; - ordered list\n\n&lt;ol&gt; describes an ordered list;\n&lt;li&gt; describes a item of list.\n\n\n%%HTML\n&lt;ol&gt;\n    &lt;li&gt;item 1&lt;/li&gt;\n    &lt;li&gt;item 2&lt;/li&gt;\n    &lt;li&gt;...&lt;/li&gt;\n    &lt;li&gt;item n&lt;/li&gt;\n&lt;/ol&gt;\n\n\n    item 1\n    item 2\n    ...\n    item n\n\n\n\n\ntype - attribute\nNumeration type: - “1” - arabian numeration; - “A” - Latin caps; - “a” - lowercase Latin letters; - “I” - uppercase Roman numerals; - “i” - lowercase Roman numerals.\nFor some reasons these features can’t be displayed in jupyter, so the following cell generates this file where you can check the result of using these features.\n\nvariants = [\"1\", \"A\", \"a\", \"I\", \"i\"]\nformat_str = '''\n    &lt;ol type=\\\"{}\\\"&gt;\n        &lt;li&gt;item 1&lt;/li&gt;\n        &lt;li&gt;item 2&lt;/li&gt;\n        &lt;li&gt;...&lt;/li&gt;\n        &lt;li&gt;item n&lt;/li&gt;\n    &lt;/ol&gt;\n'''\n\nhtml_line = \"\"\nfor var in variants:\n    html_line += format_str.format(var)\n\nwith open(\"html_files/ordered_lists_types.html\", \"w\") as file:\n    file.write(html_line)\n\n\n\nreversed\nDisplay the list in reverse order\n\n%%HTML\n&lt;ol reversed&gt;\n    &lt;li&gt;item 1&lt;/li&gt;\n    &lt;li&gt;item 2&lt;/li&gt;\n    &lt;li&gt;...&lt;/li&gt;\n    &lt;li&gt;item n&lt;/li&gt;\n&lt;/ol&gt;\n\n\n    item 1\n    item 2\n    ...\n    item n\n\n\n\n\n\nstart - first number\nYou can select the number to use as the first value.\n\n%%HTML\n&lt;ol start = 5&gt;\n    &lt;li&gt;item 1&lt;/li&gt;\n    &lt;li&gt;item 2&lt;/li&gt;\n    &lt;li&gt;...&lt;/li&gt;\n    &lt;li&gt;item n&lt;/li&gt;\n&lt;/ol&gt;\n\n\n    item 1\n    item 2\n    ...\n    item n\n\n\n\n\n\nNegative elements\nI wonder how the list will behave if I start counting from 2 but there are more than two items. As it turns out the list goes beyond natural numbers.\n\n%%HTML\n&lt;ol start = 1 reversed&gt;\n    &lt;li&gt;item 1&lt;/li&gt;\n    &lt;li&gt;item 2&lt;/li&gt;\n    &lt;li&gt;item 3&lt;/li&gt;\n    &lt;li&gt;item 4&lt;/li&gt;\n&lt;/ol&gt;\n\n\n    item 1\n    item 2\n    item 3\n    item 4"
  },
  {
    "objectID": "layout/html/lists.html#dt-dd-dl---definitions-list",
    "href": "layout/html/lists.html#dt-dd-dl---definitions-list",
    "title": "List tags",
    "section": "<dt>, <dd>, <dl> - definitions list",
    "text": "&lt;dt&gt;, &lt;dd&gt;, &lt;dl&gt; - definitions list\n\n&lt;dl&gt; (definitions list) sets a definitions list;\n&lt;dt&gt; (definitions term) sets a new term;\n&lt;dd&gt; (definitions descriptions) describe the term.\n\n\n%%HTML\n&lt;dl&gt;\n    &lt;dt&gt;Term1&lt;/dt&gt;\n    &lt;dd&gt;Definition 1&lt;/dd&gt;\n    &lt;dt&gt;Term2&lt;/dt&gt;\n    &lt;dd&gt;Definition 2&lt;/dd&gt;\n    &lt;dt&gt;...&lt;/dt&gt;\n    &lt;dd&gt;...&lt;/dd&gt;\n    &lt;dt&gt;Term n&lt;/dt&gt;\n    &lt;dd&gt;Definition n&lt;/dd&gt;\n&lt;/dl&gt;\n\n\n    Term1\n    Definition 1\n    Term2\n    Definition 2\n    ...\n    ...\n    Term n\n    Definition n"
  },
  {
    "objectID": "layout/css/overflow.html",
    "href": "layout/css/overflow.html",
    "title": "Overflow",
    "section": "",
    "text": "This property of the object describes how it behaves in the limited space.\nYou can find out more in mdn web docs."
  },
  {
    "objectID": "layout/css/overflow.html#basic-example",
    "href": "layout/css/overflow.html#basic-example",
    "title": "Overflow",
    "section": "Basic example",
    "text": "Basic example\nIn the following example displayed all options:\n\n\n%%HTML\n&lt;details&gt;\n    &lt;summary&gt;scroll&lt;/summary&gt;\n    &lt;div style='height:3cm;overflow:scroll;width:5cm;border:solid;font-size:20px'&gt;Michaelmas term lately over, and the Lord Chancellor sitting in Lincoln's Inn Hall. Implacable November weather. As much mud in the streets as if the waters had but newly retired from the face of the earth.&lt;/div&gt;\n&lt;/details&gt;\n\n&lt;details&gt;\n    &lt;summary&gt;hidden&lt;/summary&gt;\n    &lt;div style='height:3cm;overflow:hidden;width:5cm;border:solid;font-size:20px'&gt;Michaelmas term lately over, and the Lord Chancellor sitting in Lincoln's Inn Hall. Implacable November weather. As much mud in the streets as if the waters had but newly retired from the face of the earth.&lt;/div&gt;\n&lt;/details&gt;\n\n&lt;details&gt;\n    &lt;summary&gt;visible&lt;/summary&gt;\n    &lt;div style='height:3cm;overflow:visible;width:5cm;border:solid;font-size:20px'&gt;Michaelmas term lately over, and the Lord Chancellor sitting in Lincoln's Inn Hall. Implacable November weather. As much mud in the streets as if the waters had but newly retired from the face of the earth.&lt;/div&gt;\n&lt;/details&gt;\n\n&lt;details&gt;\n    &lt;summary&gt;auto&lt;/summary&gt;\n    &lt;div style='height:3cm;overflow:auto;width:5cm;border:solid;font-size:20px'&gt;Michaelmas term lately over, and the Lord Chancellor sitting in Lincoln's Inn Hall. Implacable November weather. As much mud in the streets as if the waters had but newly retired from the face of the earth.&lt;/div&gt;\n&lt;/details&gt;\n\n&lt;details&gt;\n    &lt;summary&gt;clip&lt;/summary&gt;\n    &lt;div style='height:3cm;overflow:clip;width:5cm;border:solid;font-size:20px'&gt;Michaelmas term lately over, and the Lord Chancellor sitting in Lincoln's Inn Hall. Implacable November weather. As much mud in the streets as if the waters had but newly retired from the face of the earth.&lt;/div&gt;\n&lt;/details&gt;\n\n\n    scroll\n    Michaelmas term lately over, and the Lord Chancellor sitting in Lincoln's Inn Hall. Implacable November weather. As much mud in the streets as if the waters had but newly retired from the face of the earth.\n\n\n\n    hidden\n    Michaelmas term lately over, and the Lord Chancellor sitting in Lincoln's Inn Hall. Implacable November weather. As much mud in the streets as if the waters had but newly retired from the face of the earth.\n\n\n\n    visible\n    Michaelmas term lately over, and the Lord Chancellor sitting in Lincoln's Inn Hall. Implacable November weather. As much mud in the streets as if the waters had but newly retired from the face of the earth.\n\n\n\n    auto\n    Michaelmas term lately over, and the Lord Chancellor sitting in Lincoln's Inn Hall. Implacable November weather. As much mud in the streets as if the waters had but newly retired from the face of the earth.\n\n\n\n    clip\n    Michaelmas term lately over, and the Lord Chancellor sitting in Lincoln's Inn Hall. Implacable November weather. As much mud in the streets as if the waters had but newly retired from the face of the earth."
  },
  {
    "objectID": "layout/css/overflow.html#clip-vs-hidden",
    "href": "layout/css/overflow.html#clip-vs-hidden",
    "title": "Overflow",
    "section": "clip vs hidden",
    "text": "clip vs hidden\n\nhidden does not show any content that lies outside the element;\nclip does not show any content that lies ouside the elemnt + extra space described by overflow-clip-margin property.\n\nSo in the following example, I have created two divs, the first with overflow: hidden, the second with overflow: clip, and both have overflow-clip-margin: 1cm. As a result, only the content of the second div leaves the box for 1 cm.\n\n\n%%HTML\n&lt;div style=\"display:flex;height:5cm\"&gt;\n    &lt;div style=\"border:solid;width:5cm;height:3cm;overflow-clip-margin: 1cm;overflow: hidden\"&gt;Michaelmas term lately over, and the Lord Chancellor sitting in Lincoln's Inn Hall. Implacable November weather. As much mud in the streets as if the waters had but newly retired from the face of the earth.&lt;/div&gt;\n    &lt;div style=\"border:solid;width:5cm;height:3cm;overflow-clip-margin: 1cm;overflow: clip\"&gt;Michaelmas term lately over, and the Lord Chancellor sitting in Lincoln's Inn Hall. Implacable November weather. As much mud in the streets as if the waters had but newly retired from the face of the earth.&lt;/div&gt;\n&lt;/div&gt;\n\n\n    Michaelmas term lately over, and the Lord Chancellor sitting in Lincoln's Inn Hall. Implacable November weather. As much mud in the streets as if the waters had but newly retired from the face of the earth.\n    Michaelmas term lately over, and the Lord Chancellor sitting in Lincoln's Inn Hall. Implacable November weather. As much mud in the streets as if the waters had but newly retired from the face of the earth."
  },
  {
    "objectID": "layout/css/overflow.html#scroll-vs-auto",
    "href": "layout/css/overflow.html#scroll-vs-auto",
    "title": "Overflow",
    "section": "scroll vs auto",
    "text": "scroll vs auto\nIf you are using the scroll option, this means that scrollbars will always be displayed, but if you are using auto, it will only add scrollbars when needed.\nThe following example shows the difference.\nNote Some browsers display the scroll bar only when scrolling, but in a box that uses overflow: scroll less characters fit in one line than in a box that uses overflow: auto because there is some reserved space for the scroll bar.\n\n\n%%HTML\n&lt;div style=\"display:flex;height:5cm;width:10cm\"&gt;\n    &lt;div style=\"border:solid;width:5cm;height:5cm;overflow-clip-margin: 1cm;overflow: scroll\"&gt;Scroll&lt;br&gt; A B C D E F G H I J K L M N O P Q R S T U V W X Y &lt;/div&gt;\n    &lt;div style=\"border:solid;width:5cm;height:5cm;overflow-clip-margin: 1cm;overflow: auto\"&gt;Auto&lt;br&gt; A B C D E F G H I J K L M N O P Q R S T U V W X Y &lt;/div&gt;\n&lt;/div&gt;\n\n\n    Scroll A B C D E F G H I J K L M N O P Q R S T U V W X Y \n    Auto A B C D E F G H I J K L M N O P Q R S T U V W X Y"
  },
  {
    "objectID": "layout/css/other.html",
    "href": "layout/css/other.html",
    "title": "Other",
    "section": "",
    "text": "Оn this page I describe some things that cannot be attributed to any other topic. But I haven’t decided to make a separate page for them yet.\nfrom IPython.display import HTML"
  },
  {
    "objectID": "layout/css/other.html#properties",
    "href": "layout/css/other.html#properties",
    "title": "Other",
    "section": "Properties",
    "text": "Properties\n\ncolor - text color\n\n\n%%HTML\n&lt;text style=\"color:aqua\"&gt;aqua&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"color:black\"&gt;black&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"color:blue\"&gt;blue&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"color:fuchsia\"&gt;fuchsia&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"color:gray\"&gt;gray&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"color:green\"&gt;green&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"color:lime\"&gt;lime&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"color:maroon\"&gt;maroon&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"color:navy\"&gt;navy&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"color:olive\"&gt;olive&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"color:purple\"&gt;purple&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"color:red\"&gt;red&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"color:silver\"&gt;silver&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"color:teal\"&gt;teal&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"color:white\"&gt;white&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"color:yellow\"&gt;yellow&lt;/text&gt;&lt;br&gt;\n\naqua\nblack\nblue\nfuchsia\ngray\ngreen\nlime\nmaroon\nnavy\nolive\npurple\nred\nsilver\nteal\nwhite\nyellow\n\n\n\n\nbackground - background color \nNote background-color make the same.\n\n\n%%HTML\n&lt;text style=\"background:aqua\"&gt;aqua&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"background:black\"&gt;black&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"background:blue\"&gt;blue&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"background:fuchsia\"&gt;fuchsia&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"background:gray\"&gt;gray&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"background:green\"&gt;green&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"background:lime\"&gt;lime&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"background:maroon\"&gt;maroon&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"background:navy\"&gt;navy&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"background:olive\"&gt;olive&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"background:purple\"&gt;purple&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"background:red\"&gt;red&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"background:silver\"&gt;silver&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"background:teal\"&gt;teal&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"background:white\"&gt;white&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"background:yellow\"&gt;yellow&lt;/text&gt;&lt;br&gt;\n\naqua\nblack\nblue\nfuchsia\ngray\ngreen\nlime\nmaroon\nnavy\nolive\npurple\nred\nsilver\nteal\nwhite\nyellow"
  },
  {
    "objectID": "layout/css/other.html#ways-of-determining",
    "href": "layout/css/other.html#ways-of-determining",
    "title": "Other",
    "section": "Ways of determining",
    "text": "Ways of determining\n\nColor name\n\n%%HTML\n&lt;text style=\"color:red\"&gt;red text&lt;/text&gt;\n\nred text\n\n\n\n\nRGB\nUse the following syntax rgb(R, G, B). \\[R,G,B \\in (0,1,2,...,255)\\]\n\n%%HTML\n&lt;text style=\"background-color:rgb(60, 179, 113);\"&gt;rgb(60, 179, 113)&lt;/text&gt;\n\nrgb(60, 179, 113)\n\n\n\n\nRGBA\nUse the following syntax rgba(R, G, B, A). \\[R,G,B \\in (0,1,2,...,255);\\] \\[A \\in [0,1].\\]\n\n\n%%HTML\n&lt;text style=\"background-color:rgba(60, 179, 113, 0.0);\"&gt;rgba(60, 179, 113, 0.0)&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"background-color:rgba(60, 179, 113, 0.1);\"&gt;rgba(60, 179, 113, 0.1)&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"background-color:rgba(60, 179, 113, 0.2);\"&gt;rgba(60, 179, 113, 0.2)&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"background-color:rgba(60, 179, 113, 0.3);\"&gt;rgba(60, 179, 113, 0.3)&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"background-color:rgba(60, 179, 113, 0.4);\"&gt;rgba(60, 179, 113, 0.4)&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"background-color:rgba(60, 179, 113, 0.5);\"&gt;rgba(60, 179, 113, 0.5)&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"background-color:rgba(60, 179, 113, 0.6);\"&gt;rgba(60, 179, 113, 0.6)&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"background-color:rgba(60, 179, 113, 0.7);\"&gt;rgba(60, 179, 113, 0.7)&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"background-color:rgba(60, 179, 113, 0.8);\"&gt;rgba(60, 179, 113, 0.8)&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"background-color:rgba(60, 179, 113, 0.9);\"&gt;rgba(60, 179, 113, 0.9)&lt;/text&gt;&lt;br&gt;\n&lt;text style=\"background-color:rgba(60, 179, 113, 1.0);\"&gt;rgba(60, 179, 113, 1.0)&lt;/text&gt;&lt;br&gt;\n\nrgba(60, 179, 113, 0.0)\nrgba(60, 179, 113, 0.1)\nrgba(60, 179, 113, 0.2)\nrgba(60, 179, 113, 0.3)\nrgba(60, 179, 113, 0.4)\nrgba(60, 179, 113, 0.5)\nrgba(60, 179, 113, 0.6)\nrgba(60, 179, 113, 0.7)\nrgba(60, 179, 113, 0.8)\nrgba(60, 179, 113, 0.9)\nrgba(60, 179, 113, 1.0)\n\n\n\n\nHEX\nUse the following syntax #rrggbb.\n\n%%HTML\n&lt;text style=\"background-color:#afc445;\"&gt;#afc445&lt;/text&gt;\n\n#afc445\n\n\n\n\nHSL\nHue Saturation Lightness.\nUse the following syntax HSL(H,S,L).\nWhere: \\[H \\in [0,360);\\] \\[S, L \\in [0,100].\\]\n\n%%HTML\n&lt;text style=\"background-color:HSL(150, 60%, 50%);\"&gt;HSL(150, 60%, 50%)&lt;/text&gt;\n\nHSL(150, 60%, 50%)\n\n\n\n\nHSLA\nHue Saturation Lightness Alpha.\nUse the following syntax HSL(H,S,L).\nWhere: \\[H \\in [0,360);\\] \\[S, L \\in [0,100];\\] \\[A \\in [0,1].\\]\n\n%%HTML\n&lt;text style=\"background-color:HSL(150, 60%, 50%, 0.3);\"&gt;HSL(150, 60%, 50%, 0.3)&lt;/text&gt;\n\nHSL(150, 60%, 50%, 0.3)"
  },
  {
    "objectID": "layout/css/other.html#background-color",
    "href": "layout/css/other.html#background-color",
    "title": "Other",
    "section": "background-color",
    "text": "background-color\nThis is the property when you can use any color as page background. I have describet it here."
  },
  {
    "objectID": "layout/css/other.html#background-image",
    "href": "layout/css/other.html#background-image",
    "title": "Other",
    "section": "background-image",
    "text": "background-image\nProvide way to fullfill any html element with picture. Use the following syntax:\nbackground-image: url(&lt;path to image&gt;)\nNote: Whether it’s a link or an image on a computer, you have to wrap it in url() in any case.\nAs an example look the following code:\n\ndisplay(HTML(\n    '''\n    &lt;hr&gt;&lt;text style=\\\"font-size:16px\\\"&gt;HTML&lt;/text&gt;\n    '''\n))\nwith open(\"css_files/background_image.html\") as file:\n    print(file.read())\ndisplay(HTML('''&lt;hr&gt;'''))\n\n\n    HTML\n    \n\n\n&lt;div style=\"background-image: url('fractal_css.gif')\"&gt;\n&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;\n&lt;/div&gt;\n\n&lt;div style=\"background-image: url('https://content.codecademy.com/courses/web-101/web101-image_brownbear.jpg')\"&gt;\n&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;\n&lt;/div&gt;\n\n\n\n\n\nYou can check the result here."
  },
  {
    "objectID": "layout/css/other.html#background-repeat",
    "href": "layout/css/other.html#background-repeat",
    "title": "Other",
    "section": "background-repeat",
    "text": "background-repeat\nDescribes the way in which the image represented by the background-image will fill the area (repeat).\nCan take values:\n\nno-repeat - without repeats;\nrepeat - the image repeats horizontally and vertically;\nrepeat-x - the image repeats only horizontally;\nrepeat-y - the image repeats only vertically;\nspace - an integer number of repeats will be used, but if it’s not possible to fullfill area with an integer number of images then spaces will be added;\nround - the whole area must be filled with an integer number of images, the size of the image will be adjusted.\n\nIn the following example, I try all the diggerent options. You can see the result here.\n\ntypes = [\n    \"no-repeat\", \n    \"repate\", \n    \"repeat-x\", \n    \"repeat-y\", \n    \"space\", \n    \"round\"\n]\n\npattern_line = '''\n&lt;h2&gt;{type}&lt;/h2&gt;\n&lt;div style=\"background-image: url('mkpic.webp'); background-repeat: {type};\"&gt;\n    &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;\n&lt;/div&gt;\n'''\nhtml_res = \"\"\n\nfor type in types:\n    html_res += pattern_line.format(type = type)\n\n\n\nwith open(\"css_files/background-repeate.html\", \"w\") as file:\n    file.write(html_res)\n    \ndisplay(HTML('''&lt;hr&gt;'''))\nprint(html_res)\ndisplay(HTML('''&lt;hr&gt;'''))\n\n\n\n\n\n&lt;h2&gt;no-repeat&lt;/h2&gt;\n&lt;div style=\"background-image: url('mkpic.webp'); background-repeat: no-repeat;\"&gt;\n    &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;\n&lt;/div&gt;\n\n&lt;h2&gt;repate&lt;/h2&gt;\n&lt;div style=\"background-image: url('mkpic.webp'); background-repeat: repate;\"&gt;\n    &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;\n&lt;/div&gt;\n\n&lt;h2&gt;repeat-x&lt;/h2&gt;\n&lt;div style=\"background-image: url('mkpic.webp'); background-repeat: repeat-x;\"&gt;\n    &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;\n&lt;/div&gt;\n\n&lt;h2&gt;repeat-y&lt;/h2&gt;\n&lt;div style=\"background-image: url('mkpic.webp'); background-repeat: repeat-y;\"&gt;\n    &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;\n&lt;/div&gt;\n\n&lt;h2&gt;space&lt;/h2&gt;\n&lt;div style=\"background-image: url('mkpic.webp'); background-repeat: space;\"&gt;\n    &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;\n&lt;/div&gt;\n\n&lt;h2&gt;round&lt;/h2&gt;\n&lt;div style=\"background-image: url('mkpic.webp'); background-repeat: round;\"&gt;\n    &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;\n&lt;/div&gt;"
  },
  {
    "objectID": "layout/css/other.html#basic",
    "href": "layout/css/other.html#basic",
    "title": "Other",
    "section": "Basic",
    "text": "Basic\nAllows you to set borders for the hmlt element. Use the following syntax:\nstyle=\"border: &lt;width&gt; &lt;style&gt; &lt;color&gt;\"\nOr separetely:\nborder-width: &lt;width&gt;;\nborder-style: &lt;style&gt;;\nborder-color: &lt;color&gt;;\n\n\n%%HTML\n&lt;p style=\"border:red solid 5px\"&gt;red solid 5px&lt;/text&gt;&lt;br&gt;\n&lt;p style=\"border:red solid 10px\"&gt;red solid 10px&lt;/text&gt;&lt;br&gt;\n&lt;p style=\"border:red dotted 5px\"&gt;red dotted 5px&lt;/text&gt;&lt;br&gt;\n&lt;p style=\"border:red dotted 10px\"&gt;red dotted 10px&lt;/text&gt;&lt;br&gt;\n&lt;p style=\"border:yellow solid 5px\"&gt;yellow solid 5px&lt;/text&gt;&lt;br&gt;\n&lt;p style=\"border:yellow solid 10px\"&gt;yellow solid 10px&lt;/text&gt;&lt;br&gt;\n&lt;p style=\"border:yellow dotted 5px\"&gt;yellow dotted 5px&lt;/text&gt;&lt;br&gt;\n&lt;p style=\"border:yellow dotted 10px\"&gt;yellow dotted 10px&lt;/text&gt;&lt;br&gt;\n&lt;p style=\"border:green solid 5px\"&gt;green solid 5px&lt;/text&gt;&lt;br&gt;\n&lt;p style=\"border:green solid 10px\"&gt;green solid 10px&lt;/text&gt;&lt;br&gt;\n&lt;p style=\"border:green dotted 5px\"&gt;green dotted 5px&lt;/text&gt;&lt;br&gt;\n&lt;p style=\"border:green dotted 10px\"&gt;green dotted 10px&lt;/text&gt;&lt;br&gt;\n\nred solid 5px\nred solid 10px\nred dotted 5px\nred dotted 10px\nyellow solid 5px\nyellow solid 10px\nyellow dotted 5px\nyellow dotted 10px\ngreen solid 5px\ngreen solid 10px\ngreen dotted 5px\ngreen dotted 10px\n\n\nBy using syntax border:... you can use any sequence of properties - browser should recogise them.\n\n%%HTML\n&lt;p style=\"border:red 3px solid\"&gt;red 3px solid&lt;/p&gt;\n&lt;p style=\"border:3px solid red\"&gt;3px solid red&lt;/p&gt;\n\nred 3px solid\n3px solid red"
  },
  {
    "objectID": "layout/css/other.html#styles",
    "href": "layout/css/other.html#styles",
    "title": "Other",
    "section": "Styles",
    "text": "Styles\nIn following example I mentioned all possible border styles:\n\n\n%%HTML\n&lt;p style=\"border-style:solid\"&gt;solid&lt;/p&gt;\n&lt;p style=\"border-style:dotted\"&gt;dotted&lt;/p&gt;\n&lt;p style=\"border-style:dashed\"&gt;dashed&lt;/p&gt;\n&lt;p style=\"border-style:double\"&gt;double&lt;/p&gt;\n&lt;p style=\"border-style:groove\"&gt;groove&lt;/p&gt;\n&lt;p style=\"border-style:ridge\"&gt;ridge&lt;/p&gt;\n&lt;p style=\"border-style:outset\"&gt;outset&lt;/p&gt;\n&lt;p style=\"border-style:inset\"&gt;inset&lt;/p&gt;\n\nsolid\ndotted\ndashed\ndouble\ngroove\nridge\noutset\ninset"
  },
  {
    "objectID": "layout/css/other.html#width",
    "href": "layout/css/other.html#width",
    "title": "Other",
    "section": "Width",
    "text": "Width\nYou can set border widht: - In some measures: px, pt, cm, em etc.; - Using keywords: thin, medium, thick.\n\n\n%%HTML\n&lt;p style=\"border-width: top_bottom left_right;\"&gt;fsdf&lt;/p&gt;\n\nfsdf\n\n\n\n%%HTML\n&lt;p style=\"border:3px solid\"&gt;3px solid&lt;/p&gt;\n&lt;p style=\"border:3pt solid\"&gt;3pt solid&lt;/p&gt;\n&lt;p style=\"border:3cm solid\"&gt;3cm solid&lt;/p&gt;\n&lt;p style=\"border:3em solid\"&gt;3em solid&lt;/p&gt;\n&lt;p style=\"border:thin solid\"&gt;thin solid&lt;/p&gt;\n&lt;p style=\"border:medium solid\"&gt;medium solid&lt;/p&gt;\n&lt;p style=\"border:thick solid\"&gt;thick solid&lt;/p&gt;\n\n3px solid\n3pt solid\n3cm solid\n3em solid\nthin solid\nmedium solid\nthick solid"
  },
  {
    "objectID": "layout/css/other.html#different-borders",
    "href": "layout/css/other.html#different-borders",
    "title": "Other",
    "section": "Different borders",
    "text": "Different borders\n\nSyntax 1\nYou can specify different settings for different borders using syntax:\n\nborder-&lt;width/color/style&gt;: &lt;top and bottom&gt; &lt;left and right&gt;;\nborder-&lt;width/color/style&gt;: &lt;top&gt; &lt;left and right&gt; &lt;bottom&gt;;\nborder-&lt;width/color/style&gt;: &lt;top&gt; &lt;right&gt; &lt;bottom&gt; &lt;left&gt;.\n\nYou can check it by following example:\n\n\n%%HTML\n&lt;p style=\"border-width:3px ; border-style:solid ; border-color:red \"&gt;widthes:3px |colors:red |styles:solid &lt;/p&gt;\n&lt;p style=\"border-width:3px 9px ; border-style:solid dotted ; border-color:red yellow \"&gt;widthes:3px 9px |colors:red yellow |styles:solid dotted &lt;/p&gt;\n&lt;p style=\"border-width:3px 9px 15px ; border-style:solid dotted dashed ; border-color:red yellow green \"&gt;widthes:3px 9px 15px |colors:red yellow green |styles:solid dotted dashed &lt;/p&gt;\n&lt;p style=\"border-width:3px 9px 15px 21px ; border-style:solid dotted dashed double ; border-color:red yellow green purple \"&gt;widthes:3px 9px 15px 21px |colors:red yellow green purple |styles:solid dotted dashed double &lt;/p&gt;\n\nwidthes:3px |colors:red |styles:solid \nwidthes:3px 9px |colors:red yellow |styles:solid dotted \nwidthes:3px 9px 15px |colors:red yellow green |styles:solid dotted dashed \nwidthes:3px 9px 15px 21px |colors:red yellow green purple |styles:solid dotted dashed double \n\n\n\n\nSyntax 2\nYou can specify different settings for different borders using syntax border-&lt;\"\"/top/bottom/left/right&gt;-&lt;\"\"/color/style/width&gt;.\n\n%%HTML\n&lt;p style=\"border-top-style:dotted\"&gt;border-top-style:dotted&lt;/p&gt;\n&lt;p style=\"border-bottom:dashed 10px blue\"&gt;border-bottom:dashed 10px blue&lt;/p&gt;\n\nborder-top-style:dotted\nborder-bottom:dashed 10px blue"
  },
  {
    "objectID": "layout/css/other.html#corner-rounding",
    "href": "layout/css/other.html#corner-rounding",
    "title": "Other",
    "section": "Corner rounding",
    "text": "Corner rounding\nYou can set borders rounding by using the following syntax:\n\nborder-&lt;top/bottom&gt;-&lt;left/right&gt;-radius: &lt;value&gt;;\nborder-radius: &lt;all egtes value&gt;;\nborder-radius: &lt;top-left and bottom-right&gt; &lt;bottom-left and top-right&gt;;\nborder-radius: &lt;top-left&gt; &lt;bottom-left and top-right&gt; &lt;bottom-right&gt;;\nborder-radius: &lt;top-left&gt; &lt;top-right&gt; &lt;bottom-right&gt; &lt;bottom-left&gt;;\n\nYou can check all these options in the following example:\n\n%%HTML\n&lt;p style=\"border-top-left-radius:10px; border: solid\"&gt;border-top-left-radius:10px&lt;/p&gt;\n&lt;p style=\"border-radius: 10px; border: solid\"&gt;border-radius: 10px&lt;/p&gt;\n&lt;p style=\"border-radius: 10px 30px; border: solid\"&gt;border-radius: 10px 30px&lt;/p&gt;\n&lt;p style=\"border-radius: 10px 0px 20px; border: solid\"&gt;border-radius: 10px 0px 20px&lt;/p&gt;\n&lt;p style=\"border-radius: 10px 0px 20px 30px; border: solid\"&gt;border-radius: 10px 0px 20px 30px&lt;/p&gt;\n\nborder-top-left-radius:10px\nborder-radius: 10px\nborder-radius: 10px 30px\nborder-radius: 10px 0px 20px\nborder-radius: 10px 0px 20px 30px"
  },
  {
    "objectID": "layout/css/other.html#width-1",
    "href": "layout/css/other.html#width-1",
    "title": "Other",
    "section": "width",
    "text": "width\nNot to be confused with the width attribute\nYou can set width using:\n\npx, cm, em;\npercentage relative to parent size;\ninherit - using same values as parent;\nauto.\n\n\n%%HTML\n&lt;strong&gt;No width option&lt;/strong&gt;\n&lt;p&gt;\nLorem ipsum dolor sit amet, consectetur adipiscing elit. \nEtiam semper diam at erat pulvinar, at pulvinar felis blandit. \nVestibulum volutpat tellus diam, consequat gravida libero rhoncus ut.\nMaecenas imperdiet felis nisi, fringilla luctus felis hendrerit\nsit amet. Pellentesque interdum, nisl nec interdum maximus, augue\ndiam porttitor lorem, et sollicitudin felis neque sit amet erat.\n&lt;/p&gt;\n\n&lt;strong&gt;Width 200px&lt;/strong&gt;\n&lt;p style=\"width:200px\"&gt;\nLorem ipsum dolor sit amet, consectetur adipiscing elit. \nEtiam semper diam at erat pulvinar, at pulvinar felis blandit. \nVestibulum volutpat tellus diam, consequat gravida libero rhoncus ut.\nMaecenas imperdiet felis nisi, fringilla luctus felis hendrerit\nsit amet. Pellentesque interdum, nisl nec interdum maximus, augue\ndiam porttitor lorem, et sollicitudin felis neque sit amet erat.\n&lt;/p&gt;\n\n&lt;strong&gt;Width 50%&lt;/strong&gt;\n&lt;p style=\"width:50%\"&gt;\nLorem ipsum dolor sit amet, consectetur adipiscing elit. \nEtiam semper diam at erat pulvinar, at pulvinar felis blandit. \nVestibulum volutpat tellus diam, consequat gravida libero rhoncus ut.\nMaecenas imperdiet felis nisi, fringilla luctus felis hendrerit\nsit amet. Pellentesque interdum, nisl nec interdum maximus, augue\ndiam porttitor lorem, et sollicitudin felis neque sit amet erat.\n&lt;/p&gt;\n\nNo width option\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. \nEtiam semper diam at erat pulvinar, at pulvinar felis blandit. \nVestibulum volutpat tellus diam, consequat gravida libero rhoncus ut.\nMaecenas imperdiet felis nisi, fringilla luctus felis hendrerit\nsit amet. Pellentesque interdum, nisl nec interdum maximus, augue\ndiam porttitor lorem, et sollicitudin felis neque sit amet erat.\n\n\nWidth 200px\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. \nEtiam semper diam at erat pulvinar, at pulvinar felis blandit. \nVestibulum volutpat tellus diam, consequat gravida libero rhoncus ut.\nMaecenas imperdiet felis nisi, fringilla luctus felis hendrerit\nsit amet. Pellentesque interdum, nisl nec interdum maximus, augue\ndiam porttitor lorem, et sollicitudin felis neque sit amet erat.\n\n\nWidth 50%\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. \nEtiam semper diam at erat pulvinar, at pulvinar felis blandit. \nVestibulum volutpat tellus diam, consequat gravida libero rhoncus ut.\nMaecenas imperdiet felis nisi, fringilla luctus felis hendrerit\nsit amet. Pellentesque interdum, nisl nec interdum maximus, augue\ndiam porttitor lorem, et sollicitudin felis neque sit amet erat."
  },
  {
    "objectID": "layout/css/other.html#max-width",
    "href": "layout/css/other.html#max-width",
    "title": "Other",
    "section": "max-width",
    "text": "max-width\nThe width of the element cannot be exceeded. In the following example, I have created some paris of parent/child divs, each child has max-width:200px property, but the parent is gradually expanding. The feature here is that the child width follows the parent width only before 200px.\n\n\n%%HTML\n&lt;div style=\"width:170px;border:solid green\"&gt;\n    &lt;div style=\"max-width:200px; border:solid red\"&gt; parent width 170&lt;/div&gt;\n&lt;/div&gt;\n\n&lt;div style=\"width:180px;border:solid green\"&gt;\n    &lt;div style=\"max-width:200px; border:solid red\"&gt; parent width 180&lt;/div&gt;\n&lt;/div&gt;\n\n&lt;div style=\"width:190px;border:solid green\"&gt;\n    &lt;div style=\"max-width:200px; border:solid red\"&gt; parent width 190&lt;/div&gt;\n&lt;/div&gt;\n\n&lt;div style=\"width:200px;border:solid green\"&gt;\n    &lt;div style=\"max-width:200px; border:solid red\"&gt; parent width 200&lt;/div&gt;\n&lt;/div&gt;\n\n&lt;div style=\"width:210px;border:solid green\"&gt;\n    &lt;div style=\"max-width:200px; border:solid red\"&gt; parent width 210&lt;/div&gt;\n&lt;/div&gt;\n\n&lt;div style=\"width:220px;border:solid green\"&gt;\n    &lt;div style=\"max-width:200px; border:solid red\"&gt; parent width 220&lt;/div&gt;\n&lt;/div&gt;\n\n&lt;div style=\"width:230px;border:solid green\"&gt;\n    &lt;div style=\"max-width:200px; border:solid red\"&gt; parent width 230&lt;/div&gt;\n&lt;/div&gt;\n\n\n     parent width 170\n\n\n\n     parent width 180\n\n\n\n     parent width 190\n\n\n\n     parent width 200\n\n\n\n     parent width 210\n\n\n\n     parent width 220\n\n\n\n     parent width 230"
  },
  {
    "objectID": "layout/css/other.html#min-width",
    "href": "layout/css/other.html#min-width",
    "title": "Other",
    "section": "min-width",
    "text": "min-width\nAn item cannot take a value shorter than this length. In the following example, I have created some paris of parent/child divs, each child has min-width:200px property, but the parent is gradually expanding. The feature here is that the child width follows the parent width only after 200px.\n\n\n%%HTML\n&lt;div style=\"width:170px;border:solid green\"&gt;\n    &lt;div style=\"min-width:200px; border:solid red\"&gt; parent width 170&lt;/div&gt;\n&lt;/div&gt;\n\n&lt;div style=\"width:180px;border:solid green\"&gt;\n    &lt;div style=\"min-width:200px; border:solid red\"&gt; parent width 180&lt;/div&gt;\n&lt;/div&gt;\n\n&lt;div style=\"width:190px;border:solid green\"&gt;\n    &lt;div style=\"min-width:200px; border:solid red\"&gt; parent width 190&lt;/div&gt;\n&lt;/div&gt;\n\n&lt;div style=\"width:200px;border:solid green\"&gt;\n    &lt;div style=\"min-width:200px; border:solid red\"&gt; parent width 200&lt;/div&gt;\n&lt;/div&gt;\n\n&lt;div style=\"width:210px;border:solid green\"&gt;\n    &lt;div style=\"min-width:200px; border:solid red\"&gt; parent width 210&lt;/div&gt;\n&lt;/div&gt;\n\n&lt;div style=\"width:220px;border:solid green\"&gt;\n    &lt;div style=\"min-width:200px; border:solid red\"&gt; parent width 220&lt;/div&gt;\n&lt;/div&gt;\n\n&lt;div style=\"width:230px;border:solid green\"&gt;\n    &lt;div style=\"min-width:200px; border:solid red\"&gt; parent width 230&lt;/div&gt;\n&lt;/div&gt;\n\n\n     parent width 170\n\n\n\n     parent width 180\n\n\n\n     parent width 190\n\n\n\n     parent width 200\n\n\n\n     parent width 210\n\n\n\n     parent width 220\n\n\n\n     parent width 230"
  },
  {
    "objectID": "layout/css/other.html#font-size---font-size",
    "href": "layout/css/other.html#font-size---font-size",
    "title": "Other",
    "section": "font-size - font size",
    "text": "font-size - font size\nmeasures\n\n%%HTML\n&lt;p style=\"font-size:20px\"&gt;20px font size&lt;/p&gt;\n&lt;p style=\"font-size:10px\"&gt;10px font size&lt;/p&gt;\n&lt;p style=\"font-size:100%\"&gt;100% font size&lt;/p&gt;\n&lt;p style=\"font-size:120%\"&gt;120% font size&lt;/p&gt;\n\n20px font size\n10px font size\n100% font size\n120% font size"
  },
  {
    "objectID": "layout/css/other.html#font-weight---boldthin-text",
    "href": "layout/css/other.html#font-weight---boldthin-text",
    "title": "Other",
    "section": "font-weight - bold/thin text",
    "text": "font-weight - bold/thin text\nShapes the weight (saturation) of the font.\n\nnormal - Sets normal font saturation (default value).\nbold - Sets the font to bold.\nbolder Increases font saturation by one level relative to the parent element.\nlighter - Decreases the boldness of the font by one level relative to the parent element.\nSetting values between 100 and 900: These values define the specific font style, where 400 is equal to normal bold and 700 is equal to bold. Some browsers may not support all values in this range.\n\n\n\n%%HTML\n&lt;text style=\"font-weight:normal\"&gt;normal&lt;/text&gt;\n&lt;text style=\"font-weight:bold\"&gt;bold&lt;/text&gt;\n&lt;text style=\"font-weight:bolder\"&gt;bolder&lt;/text&gt;\n&lt;text style=\"font-weight:lighter\"&gt;lighter&lt;/text&gt;\n&lt;text style=\"font-weight:200\"&gt;200&lt;/text&gt;\n&lt;text style=\"font-weight:250\"&gt;250&lt;/text&gt;\n&lt;text style=\"font-weight:300\"&gt;300&lt;/text&gt;\n&lt;text style=\"font-weight:350\"&gt;350&lt;/text&gt;\n&lt;text style=\"font-weight:400\"&gt;400&lt;/text&gt;\n&lt;text style=\"font-weight:450\"&gt;450&lt;/text&gt;\n&lt;text style=\"font-weight:500\"&gt;500&lt;/text&gt;\n&lt;text style=\"font-weight:550\"&gt;550&lt;/text&gt;\n&lt;text style=\"font-weight:600\"&gt;600&lt;/text&gt;\n&lt;text style=\"font-weight:650\"&gt;650&lt;/text&gt;\n&lt;text style=\"font-weight:700\"&gt;700&lt;/text&gt;\n&lt;text style=\"font-weight:750\"&gt;750&lt;/text&gt;\n&lt;text style=\"font-weight:800\"&gt;800&lt;/text&gt;\n\nnormal\nbold\nbolder\nlighter\n200\n250\n300\n350\n400\n450\n500\n550\n600\n650\n700\n750\n800"
  },
  {
    "objectID": "layout/css/other.html#text-align---position-of-the-text",
    "href": "layout/css/other.html#text-align---position-of-the-text",
    "title": "Other",
    "section": "text-align - position of the text",
    "text": "text-align - position of the text\n\n%%HTML\n&lt;p style=\"text-align:center\"&gt;Centered text&lt;/p&gt;\n&lt;p style=\"text-align:right\"&gt;Rigth side text&lt;/p&gt;\n&lt;p style=\"text-align:left\"&gt;Left side text&lt;/p&gt;\n\nCentered text\nRigth side text\nLeft side text"
  },
  {
    "objectID": "layout/css/other.html#float---float-position",
    "href": "layout/css/other.html#float---float-position",
    "title": "Other",
    "section": "float - float position",
    "text": "float - float position\nThe float CSS property places an element on the left or right side of its container, allowing text and inline elements to wrap around it."
  },
  {
    "objectID": "layout/css/elements_positions.html",
    "href": "layout/css/elements_positions.html",
    "title": "Elements position (display)",
    "section": "",
    "text": "By default, HTML elements are usually displayed one by one from the top of the page to the bottom of the page, but sometimes you need to change this. So this page discusses some features associated with options to change this.\nYou can regulate it with display css option. Read more about this property here. Long story short, you need to specify the `display’ option for the elemnt to make it’s children have position in a specific way.\nSo in following example you can compare elements where display doesn’t specified and elements where display: flex:\n\nBy default, elements goes one by one verticaly and took all wide of the page;\nWith display: flex elements move one at a time from the left border, taking up only the space they need.\n\n\n%%HTML\n&lt;div&gt;\n    &lt;div style=\"border : black solid\"&gt;display doesn't specified&lt;/div&gt;\n    &lt;div style=\"border: black solid\"&gt;display doesn't specified&lt;/div&gt;\n&lt;/div&gt;\n&lt;hr&gt;\n&lt;div style=\"display: flex;\"&gt;\n    &lt;div style=\"border : black solid\"&gt;display: flex&lt;/div&gt;\n    &lt;div style=\"border : black solid\"&gt;display: flex&lt;/div&gt;  \n&lt;/div&gt;\n\n\n    display doesn't specified\n    display doesn't specified\n\n\n\n    display: flex\n    display: flex"
  },
  {
    "objectID": "layout/forms.html",
    "href": "layout/forms.html",
    "title": "Forms",
    "section": "",
    "text": "Forms allow the user to interact with the html page. There are a few special html tags to work with forms, these are covered on this page."
  },
  {
    "objectID": "layout/forms.html#type-atribute",
    "href": "layout/forms.html#type-atribute",
    "title": "Forms",
    "section": "type atribute",
    "text": "type atribute\nDescribe a purpose of the input. Can take the following values:\n\ntext;\npassword;\nemail;\ntel;\nnumber;\nrange;\nsubmit.\n\nThe values are discussed in more detail in other sub-sections."
  },
  {
    "objectID": "layout/forms.html#typetext",
    "href": "layout/forms.html#typetext",
    "title": "Forms",
    "section": "type=\"text\"",
    "text": "type=\"text\"\nJust basic input."
  },
  {
    "objectID": "layout/forms.html#typepassword",
    "href": "layout/forms.html#typepassword",
    "title": "Forms",
    "section": "type=\"password\"",
    "text": "type=\"password\"\nUsed to enter passwords. Symbols change to dots as you type.\n\n%%HTML\n&lt;input type=\"password\"&gt;"
  },
  {
    "objectID": "layout/forms.html#typeemail",
    "href": "layout/forms.html#typeemail",
    "title": "Forms",
    "section": "type=\"email\"",
    "text": "type=\"email\"\nUsed to enter an email address. The value is validated to ensure it is a real email address when sumbit button will be pressed.\n\n%%HTML\n&lt;form&gt;\n    &lt;input type=\"email\"&gt;\n    &lt;input type=\"submit\"&gt;\n&lt;/form&gt;\n\n\n    \n    \n\n\n\n\ntype=\"tel\"\nUsed to enter an phone numter. The value is validated ito ensure that it is a telephone number that satisfies the given pattern atribute. The following example dispays using of &lt;input type=\"tel\"&gt;. You can check the result here.\n\ndisplay(HTML(\"&lt;hr&gt;\"))\nwith open(\"forms_files/input_type_tel.html\") as file:\n    print(file.read())\ndisplay(HTML(\"&lt;hr&gt;\"))\n\n\n\n\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n\n&lt;h2&gt;Input for phone&lt;/h2&gt;\n\n&lt;form&gt;\n    &lt;label for=\"phone\"&gt;Phone number:&lt;/label&gt;&lt;br&gt;\n    &lt;input type=\"tel\" id=\"phone\" \n           pattern=\"[0-9]{3}-[0-9]{2}-[0-9]{3}\"&gt;&lt;br&gt;&lt;br&gt;\n           \n    &lt;small&gt;Fomat: 123-45-678&lt;/small&gt;\n    &lt;br&gt;&lt;br&gt;\n    &lt;input type=\"submit\"&gt;\n&lt;/form&gt;\n\n&lt;/body&gt;\n&lt;/html&gt;"
  },
  {
    "objectID": "layout/forms.html#typenumber",
    "href": "layout/forms.html#typenumber",
    "title": "Forms",
    "section": "type=\"number\"",
    "text": "type=\"number\"\nUsed to enter the number.\nAttributes:\n\nmin minimum value;\nmax maximum value.\n\nIn the following example you can play such an element.\n\n%%HTML\n&lt;form&gt;\n    &lt;input type=\"number\" min=3 max=10&gt;\n&lt;/form&gt;"
  },
  {
    "objectID": "layout/forms.html#typerange",
    "href": "layout/forms.html#typerange",
    "title": "Forms",
    "section": "type=\"range\"",
    "text": "type=\"range\"\nCreates a slider.\nAttributes:\n\nmin - describe a minimal value;\nmax - describe a maximum value;\nstep - describe a step of the slider.\n\n\n%%HTML\n    &lt;label for=\"range1\"&gt;Range1&lt;/label&gt;&lt;input id=\"range1\" type=\"range\" min=0 max=100 step=0.5&gt;&lt;br&gt;\n    &lt;label for=\"range2\"&gt;Range2&lt;/label&gt;&lt;input id=\"range2\" type=\"range\" min=0 max=5 step = 2&gt;\n\n    Range1\n    Range2"
  },
  {
    "objectID": "Docker/docker_volumes.html",
    "href": "Docker/docker_volumes.html",
    "title": "Volumes",
    "section": "",
    "text": "In order to connect a folder on the host to the container, use the -v option of the docker run command. The full syntax is as follows:\ndocker run \\\n    -v &lt;older on host1&gt;:&lt;folder in container1&gt; \\\n    -v &lt;older on host2&gt;:&lt;folder in container2&gt; \\\n    ...\nBy &lt;folder on hosti&gt; it can be understood as:\n\nA path to some folder on the host, this is commonly referred to as bind mount, and is used more commonly to pass something specific to that host into the container;\nA volume, this is the preferred mount method, and is used to store information that is “created” by the container; it is essentially the same as a folder on the host but controlled by a docker.\n\n\nBind mount\nIn the following example I create folder temp_folder, put it into container named temp_folder_inc_count. From container create file there, exited from container and even deleted it, I can get file from host folder.\n\n%%bash\ncd filesystem_example\nmkdir temp_folder\n\ndocker run \\\n    -v $(pwd)/temp_folder:/temp_folder_in_cont \\\n    --rm -itd --name temp_example \\\n    ubuntu &&gt; /dev/null\ndocker exec temp_example bash -c \"echo \\'hello from container\\' &gt;&gt; temp_folder_in_cont/hello\"\ndocker stop temp_example &&gt; /dev/null\n\ncat temp_folder/hello\nrm -r temp_folder\n\n'hello from container'\n\n\n\n\nVolume\nIs a file that resides on the host and doesn’t depend on an image, but depends on Docker. Its primary purpose is to store data.\n\ndocker volume service\ndocker volume is a separate service for managing volumes.\n\nls - show volumes;\ncreate - create volume;\nrm - remove volume;\nprune - remove volumes not used in any container;\nisnpect - allows you to retrieve information about volume (including where it lies on the host).\n\nThe following example shows how to use volumes. Step-by-step description: - temp_volume is created; - ubuntu container named example_container running, and volume created earlier attached to container as temp_volume_cont folder; - file writes to the temp_volume_cont from container; - example_container stops and removes automatically (because the --rm' option was setted); - next,inspectshows where you can find the volume on the host (Mountpointdirectory), but I can't get access from Jupyter - you should have root privileges for this folder; -example_container` was started in the same way as before; - and in the folder associated with the container, you can still find the file created in the previous steps.\n\n%%bash\ndocker volume create temp_volume &&gt; /dev/null\n\ndocker run \\\n    -v temp_volume:/temp_volume_cont\\\n    --rm --name example_container -itd\\\n    ubuntu &&gt; /dev/null\ndocker exec example_container bash -c \"echo \\'hello from volume\\' &gt;&gt; temp_volume_cont/hello\"\ndocker stop example_container &&gt; /dev/null\n\ndocker volume inspect temp_volume\n\ndocker run \\\n    -v temp_volume:/temp_volume_cont\\\n    --rm --name example_container -itd\\\n    ubuntu &&gt; /dev/null\ndocker exec example_container cat temp_volume_cont/hello\ndocker stop example_container &&gt; /dev/null\n\ndocker volume rm temp_volume &&gt; /dev/null\n\n[\n    {\n        \"CreatedAt\": \"2023-06-03T19:15:02+03:00\",\n        \"Driver\": \"local\",\n        \"Labels\": null,\n        \"Mountpoint\": \"/var/lib/docker/volumes/temp_volume/_data\",\n        \"Name\": \"temp_volume\",\n        \"Options\": null,\n        \"Scope\": \"local\"\n    }\n]\n'hello from volume'\n\n\n\n\n\nAccess\n\nContainers always have root\nEven if host has root access to some folder/file, container always works under root. So if you mount a folder/file in this way, it may lead to unauthorised changes. The following cell contains the example.\n\n%%bash\ncd filesystem_example\n\n# creating fodler and file with super secret message\nmkdir secret_dir\ntouch secret_dir/secret_file\necho \"super secret info\" &gt; secret_dir/secret_file\n# Close access to the folder\nchmod 000 secret_dir\n\n\n# make sure we can't access or delte the file\necho \"=====From host=====\"\ncat secret_dir/secret_file\nrm secret_dir/secret_file\n\n\n# run container and mount created folder\ndocker run --rm -itd --name perm_ex\\\n    -v $(pwd)/secret_dir:/experimental/secret_dir \\\n    ubuntu &&gt; /dev/null\n# and voila ealily extract secret info\necho \"=====From docker====\"\ndocker exec perm_ex cat experimental/secret_dir/secret_file\n# or even can delete secret file\ndocker exec perm_ex rm experimental/secret_dir/secret_file\ndocker exec perm_ex ls experimental/secret_dir\n\ndocker stop perm_ex &&gt; /dev/null\n\n=====From host=====\n=====From docker====\nsuper secret info\n\n\ncat: secret_dir/secret_file: Permission denied\nrm: cannot remove 'secret_dir/secret_file': Permission denied\n\n\n\n\nro (read only) option\nContinuing from the previous section, note that when you move the folder to the container, you can set the ro option, which will prevent the container from modifying the file.\n\n%%bash\necho \"some data\" &gt; ro_ex\n# running container with ro option\ndocker run --rm -idt --name ro_ex\\\n    -v $(pwd)/ro_ex:/experimental/ro_ex:ro\\\n    ubuntu &&gt; /dev/null\n# change the file\ndocker exec ro_ex bash -c \"echo \\\"new some data\\\" &gt; ro_ex\"\n# print new file\ncat ro_ex\n\ndocker stop ro_ex &&gt; /dev/null\nrm -r ro_ex\n\nsome data\n\n\nSo I tried to change file from container, but even after operation file sill have initial message.\n\n\nRunnig with setting user -u{sec-run_u_option}\nThe problem with access can be solved by setting the user when starting the container (-u option). Using the example from the section “Containers always have root”, you can do this.\n\n%%bash\ncd filesystem_example\n# создаем папку и в ней файл и даже в него записываем\n# сверхсекретное сообщение\nmkdir secret_dir\ntouch secret_dir/secret_file\necho \"super secret info\" &gt; secret_dir/secret_file\n# закрываю доступ в папку\nchmod 000 secret_dir\n\n# поднимаем контейнер и монтируем в него данную папку\ndocker run --rm -itd --name perm_ex -u=1000\\\n    -v $(pwd)/secret_dir:/experimental/secret_dir \\\n    ubuntu &&gt; /dev/null\necho \"=====Trying to access from a container=====\"\ndocker exec perm_ex cat secret_dir/secret_file\n\ndocker stop perm_ex &&gt; /dev/null\n\nmkdir: cannot create directory ‘secret_dir’: File exists\ntouch: cannot touch 'secret_dir/secret_file': Permission denied\nbash: line 6: secret_dir/secret_file: Permission denied\ncat: secret_dir/secret_file: No such file or directory\n\n\n=====Trying to access from a container=====\n\n\n\n\nMounting .dockerignore files\nEven if you mount the file described in .dockerignore, we will still have it in the container.\nIn the following example, I create app/ignore_file.txt and mention it in dockerignore. Build image using this .dockerignore, but in container based on this image I mount app folder. And as a result I can see contents of ignore_file.txt regardless of what I specified in the .dockerignore.\n\n%%bash\ncd filesystem_example\nmkdir app\necho \"message in ignore_file.txt\" &gt; app/ignore_file.txt\necho \"=====.dockerignore=====\"\necho \"app/ignore_file.txt\" &gt; .dockerignore\ncat .dockerignore\necho \"=====dockerfile=====\"\necho \"FROM ubuntu\" &gt; dockerfile\ncat dockerfile\n\n# build image with setted .dockerignore\ndocker build -t test_image &&gt; /dev/null\n\n# start container mountig file mentioned in .dockerignore\ndocker run --rm -itd --name ignore_ex\\\n    -v $(pwd)/app:/app\\\n    ubuntu &&gt; /dev/null\n\necho \"=====ignore-file from container=====\"\n# make sure that this secret file is in the container\ndocker exec ignore_ex cat app/ignore_file.txt\n\ndocker stop ignore_ex &&gt; /dev/null\ndocker rmi test_image &&gt; /dev/null\nrm -r app\nrm .dockerignore\nrm dockerfile\n\n=====.dockerignore=====\napp/ignore_file.txt\n=====dockerfile=====\nFROM ubuntu\n=====ignore-file from container=====\nmessage in ignore_file.txt\n\n\n\n\n\nVolume by default\nSome containers create their own volumes by default when they run, so you may find that your entire hard drive is flooded.\nFor example yandex/clickhouse-server. In the following cell I have some containers yandex/clickhouse-server running and show that each container has created a volume.\n\n%%bash\n\necho \"=====docker volume ls before=====\"\ndocker volume ls\n\n# run clickhouse\ndocker run -d --name db_1 --rm yandex/clickhouse-server &&gt; /dev/null\ndocker run -d --name db_2 --rm yandex/clickhouse-server &&gt; /dev/null\ndocker run -d --name db_3 --rm yandex/clickhouse-server &&gt; /dev/null\n\n# list volumes\necho \"=====docker volume ls after=====\"\ndocker volume ls\ndocker stop db_1 db_2 db_3 &&gt; /dev/null\n\n=====docker volume ls before=====\nDRIVER    VOLUME NAME\n=====docker volume ls after=====\nDRIVER    VOLUME NAME\nlocal     66e881449c54eb455e1a0e16b6b1cfff0aac6fbe31739ea59d307d5631c04fa2\nlocal     b6d2ba76cf901665684319a3197166a61d8da5d75b8804945a8f0368de0c89ec\nlocal     d3ba9de2668e8e3f68d43ccfb02db72738f2c2b7048ed2bfc94b3134d00f53f1\n\n\n\n\nSeveral volumes in one command\nYou can repeat -v option in docker run many times as you need.\nBasic example.\n\n%%bash\ncd filesystem_example\n\nmkdir test1 test2\necho \"message in test1\" &gt; test1/my_file\necho \"message in test2\" &gt; test2/my_file\n\ndocker run --rm --name example_cont -itd\\\n    -v $(pwd)/test1:/test1\\\n    -v $(pwd)/test2:/test2\\\n    ubuntu &&gt; /dev/null\n\necho \"=====files from container=====\"\ndocker exec example_cont cat test1/my_file\ndocker exec example_cont cat test2/my_file\n\ndocker stop example_cont &&gt; /dev/null\nrm -r test1 test2\n\n=====files from container=====\nmessage in test1\nmessage in test2\n\n\nBut you can’t mount a directory on the container twice.\n\n%%bash\ncd filesystem_example\n\nmkdir test1 test2\necho \"message in test1\" &gt; test1/my_file\necho \"message in test2\" &gt; test2/my_file\n\ndocker run --rm --name example_cont -itd\\\n    -v $(pwd)/test1:/test\\\n    -v $(pwd)/test2:/test\\\n    ubuntu\n\nrm -r test1 test2\n\ndocker: Error response from daemon: Duplicate mount point: /test.\nSee 'docker run --help'.\n\n\nA directory from host to different folders in container is available. But you need to know that they are actually the same directory with different names - any change in one will happen in the other.\nIn the following example, we mount test in host to test1 and test2 in container. Then we add the file new_file to test1, but it will appear not only in test1 but also in test2 and test.\n\n%%bash\ncd filesystem_example\n\nmkdir test\n\ndocker run --rm --name example_cont -itd\\\n    -v $(pwd)/test:/test1\\\n    -v $(pwd)/test:/test2\\\n    ubuntu &&gt; /dev/null\n\ndocker exec example_cont bash -c \"echo \\\"hello from container\\\" &gt; test1/new_file\"\necho \"=====ls test1 from container=====\"\ndocker exec example_cont ls test1\necho \"=====ls test2 from container=====\"\ndocker exec example_cont ls test2\n\ndocker stop example_cont &&gt; /dev/null\n\n\necho \"=====ls test=====\"\nls test\nrm -r test\n\n=====ls test1 from container=====\nnew_file\n=====ls test2 from container=====\nnew_file\n=====ls test=====\nnew_file"
  },
  {
    "objectID": "Docker/docker_file_instructions.html",
    "href": "Docker/docker_file_instructions.html",
    "title": "Dockerfile instructions",
    "section": "",
    "text": "Here are the most basic options for the dockerfile."
  },
  {
    "objectID": "Docker/docker_file_instructions.html#entrypoint",
    "href": "Docker/docker_file_instructions.html#entrypoint",
    "title": "Dockerfile instructions",
    "section": "ENTRYPOINT",
    "text": "ENTRYPOINT\nAllow command to run when container based on image is running. The feature of this instruction is that you can add something at the end of the docker run command, and it will be added to command described in ENTRYPOINT.\nSuch dockerfile:\nFROM ubuntu:20.04\nENTRYPOINT [ \"echo\" ]\nCreates a container that will echo the line declared at the end of the docker run command. For exmaple:\n\n%%bash\ndocker build \\\n    -f basic_instructions/entrypoint_example \\\n    -t test_ubuntu \\\n    basic_instructions &&gt; /dev/null\n\ndocker run --rm test_ubuntu \"hello world\"\ndocker run --rm test_ubuntu ls\ndocker rmi test_ubuntu &&gt; /dev/null\n\nhello world\nls\n\n\nSo the lines I passed as arguments were just printed - just like echo does."
  },
  {
    "objectID": "Docker/docker_file_instructions.html#cmd",
    "href": "Docker/docker_file_instructions.html#cmd",
    "title": "Dockerfile instructions",
    "section": "CMD",
    "text": "CMD\nAllow command to run when container based on image is running. If using this instruction you set any command for docker run, the command described in CMD will be completely removed with new commad.\nConsider containers based on dockerfile:\nFROM ubuntu:20.04\nCMD [ \"echo\" ]\nThe same command docker run --rm test_ubuntu ls like it was for the dockerfile from ENTRYPOINT example, desn’t print “ls” but executes the ls command.\n\n%%bash\ndocker build \\\n    -f basic_instructions/cmd_example \\\n    -t test_ubuntu \\\n    basic_instructions &&gt; /dev/null\ndocker run --rm test_ubuntu ls \ndocker rmi test_ubuntu &&gt; /dev/null\n\nbin\nboot\ndev\netc\nhome\nlib\nlib32\nlib64\nlibx32\nmedia\nmnt\nopt\nproc\nroot\nrun\nsbin\nsrv\nsys\ntmp\nusr\nvar"
  },
  {
    "objectID": "Docker/docker_file_instructions.html#entrypoint-cmd",
    "href": "Docker/docker_file_instructions.html#entrypoint-cmd",
    "title": "Dockerfile instructions",
    "section": "ENTRYPOINT + CMD",
    "text": "ENTRYPOINT + CMD\nBy combining ENTRYPOINT and CMD you can do this: - in ENTRYPOIN set the necessary part of the command; - in CMD set the part of the command that can be changed.\ndocker file like:\nFROM ubuntu:20.04\nENTRYPOINT [\"echo\"]\nCMD [\"default message\"]\nBy default, “default message” is printed, but if you add a line to `docker run’ it will be printed instead. The following cell show how it works:\n\n%%bash\ndocker build \\\n    -f basic_instructions/entrypoint_cmd \\\n    -t test_ubuntu \\\n    basic_instructions &&gt; /dev/null\ndocker run --rm test_ubuntu\ndocker run --rm test_ubuntu \"other message\"\ndocker rmi test_ubuntu &&gt; /dev/null\n\ndefault message\nother message\n\n\nConsequence of instractions doesn’t matter. So docker file:\nFROM ubuntu:20.04\nCMD [\"default message\"]\nENTRYPOINT [\"echo\"]\nWill work the same way. The folowing cell show this scenario.\n\n%%bash\ndocker build \\\n    -f basic_instructions/entrypoint_cmd2 \\\n    -t test_ubuntu \\\n    basic_instructions &&gt; /dev/null\ndocker run --rm test_ubuntu\ndocker run --rm test_ubuntu \"other message\"\ndocker rmi test_ubuntu &&gt; /dev/null\n\ndefault message\nother message\n\n\n\nSeveral CMD/ENTRYPOIN\nLooks like, docker only takes the last mention of CMD/ENTRY POINT. Which makes sense in the case of ‘inheriting’ containers. So at the following example I’m using docker file:\nFROM ubuntu:20.04\nCMD [\"echo\", \"default message1\"]\nCMD [\"echo\", \"default message2\"]\n\n%%bash\ndocker build \\\n    -f basic_instructions/several_cmd \\\n    -t test_ubuntu \\\n    basic_instructions &&gt; /dev/null\ndocker run --rm test_ubuntu\ndocker rmi test_ubuntu &&gt; /dev/null\n\ndefault message2\n\n\nSo it looks like docker just ignored CMD [\"echo\", \"default message1\"]"
  },
  {
    "objectID": "Docker/image_creation.html",
    "href": "Docker/image_creation.html",
    "title": "Image creation",
    "section": "",
    "text": "build - build image\nThis is the command used to create a new docker image.\nBasic arguments:\n\nthe last mandatory argument sets the build directory - you should use . to set the current directory.\n-t - set the name and tag of the image.\n\n\n%%bash\ncd build_command_files\necho \"=====building process=====\"\ndocker build -t custom_ubuntu:test .\necho \"=====check image=====\"\ndocker images | grep custom_ubuntu\ndocker rmi custom_ubuntu:test &&gt; /dev/null\n\n=====building process=====\nSending build context to Docker daemon  18.94kB\nStep 1/1 : FROM ubuntu\n ---&gt; 99284ca6cea0\nSuccessfully built 99284ca6cea0\nSuccessfully tagged custom_ubuntu:test\n=====check image=====\ncustom_ubuntu   test      99284ca6cea0   2 weeks ago     77.8MB\n\n\n\n-f - allow to chose the dockerfile\nYou should see a difference between building directory and dockerfile. The dockerfile is only taken from the building directory if it is not specified in the -f option. But the files used for the build will in any case come from the build directory.\nSo let’s try some experiments with these details. I have prepared some folders for experiments with these options, in the following cell I show tree and file contents.\n\n%%bash\ncd build_command_files/f_option_examples\ntree\n\necho\necho\necho\n\nfor file in $(find -type f | grep -v \"\\/\\.\"); do\n    echo \"\"\n    echo \"=====File: $file=====\"\n    cat $file\n    echo \"\"\ndone\n\n.\n├── dockerfile\n├── script.sh\n├── specialdockerfile\n└── test_folder\n    ├── dockerfile\n    └── script.sh\n\n1 directory, 5 files\n\n\n\n\n=====File: ./dockerfile=====\nFROM ubuntu:latest\nCOPY script.sh script.sh\nRUN echo \"\\necho message from basic dockerfile\" &gt;&gt; script.sh\n\n=====File: ./script.sh=====\necho \"It's a script.sh from the run folder\"\n\n=====File: ./specialdockerfile=====\nFROM ubuntu:latest\nCOPY script.sh script.sh\nRUN echo \"\\necho message from specialdockerfile\" &gt;&gt; script.sh\n\n=====File: ./test_folder/dockerfile=====\nFROM ubuntu:latest\nCOPY script.sh script.sh\nRUN echo \"\\necho message from test_folder/dockerfile\" &gt;&gt; script.sh\n\n=====File: ./test_folder/script.sh=====\necho \"It's a script from test_folder\"\n\n\nWe have:\n\nscripts:\n\nfrom run folder - prints \"It's a script.sh from the run folder\";\nfrom test_folder - prints \"It's a script from test_folder\";\n\ndockerfiles:\n\nbasic dockerfile from run directory - adds to printing script line \"message from basic dockerfile\";\nspecisaldocker file from run dicrecotry but with special name - adds to printing script line \"message from specialdockerfile\";\ndockerfile from test_folder directory - adds to printing script line \"message from test_folder/dockerfile\";\nspecialdockerfile from test_folder directory - adds to printing script line \"message from test_folder/specialdockerfile\";\n\n\nThis way we can always tell from the output of script.sh from the container in which folder and from which dockerfile the image was built. Let’s try different options:\n\nThe most basic case no -f option and . as build directory - will lead to using docker file from run directory and script.sh was copiet from run dicrecory as well;\n\n\n%%bash\ncd build_command_files/f_option_examples\ndocker build -t test_ubuntu . &&gt; /dev/null\ndocker run --rm -itd \\\n    --name test_ubuntu \\\n    test_ubuntu &&gt; /dev/null\necho \"=====message from container=====\"\ndocker exec test_ubuntu bash script.sh\n\ndocker stop test_ubuntu &&gt; /dev/null\ndocker rmi test_ubuntu &&gt; /dev/null\n\n=====message from container=====\nIt's a script.sh from the run folder\nmessage from basic dockerfile\n\n\n\nIf you do not set the -f option but specify the build folder as test_folder everythig will be taken from test_folder;\n\n\n%%bash\ncd build_command_files/f_option_examples\ndocker build -t test_ubuntu test_folder &&gt; /dev/null\ndocker run --rm -itd \\\n    --name test_ubuntu \\\n    test_ubuntu &&gt; /dev/null\necho \"=====message from container=====\"\ndocker exec test_ubuntu bash script.sh\n\ndocker stop test_ubuntu &&gt; /dev/null\ndocker rmi test_ubuntu &&gt; /dev/null\n\n=====message from container=====\nIt's a script from test_folder\nmessage from test_folder/dockerfile\n\n\n\nIf you set -f specialdockerfile and specify the build folder as test_folder - everything is obvious;\n\n\n%%bash\ncd build_command_files/f_option_examples\ndocker build \\\n    -t test_ubuntu \\\n    -f specialdockerfile \\\n    test_folder &&gt; /dev/null\ndocker run --rm -itd \\\n    --name test_ubuntu \\\n    test_ubuntu &&gt; /dev/null\necho \"=====message from container=====\"\ndocker exec test_ubuntu bash script.sh\n\ndocker stop test_ubuntu &&gt; /dev/null\ndocker rmi test_ubuntu &&gt; /dev/null\n\n=====message from container=====\nIt's a script from test_folder\nmessage from specialdockerfile\n\n\n\nBit exotic case - dockerfile from test_folder, but build directory is run directory;\n\n\n%%bash\ncd build_command_files/f_option_examples\ndocker build \\\n    -t test_ubuntu \\\n    -f test_folder/dockerfile \\\n    . &&gt; /dev/null\ndocker run --rm -itd \\\n    --name test_ubuntu \\\n    test_ubuntu &&gt; /dev/null\necho \"=====message from container=====\"\ndocker exec test_ubuntu bash script.sh\n\ndocker stop test_ubuntu &&gt; /dev/null\ndocker rmi test_ubuntu &&gt; /dev/null\n\n=====message from container=====\nIt's a script.sh from the run folder\nmessage from test_folder/dockerfile\n\n\nNote even if you have specified a folder as some assembly folder, you must still pass in the -f parameter the path relative to the run folder.\nThe next two points show the difference:\n\nHere I set building directory as test_folder but -f specialdockerfile - specialdockerfile from run folder will be used;\n\n\n%%bash\ncd build_command_files/f_option_examples\ndocker build \\\n    -t test_ubuntu \\\n    -f specialdockerfile \\\n    test_folder &&gt; /dev/null\ndocker run --rm -itd \\\n    --name test_ubuntu \\\n    test_ubuntu &&gt; /dev/null\necho \"=====message from container=====\"\ndocker exec test_ubuntu bash script.sh\n\ndocker stop test_ubuntu &&gt; /dev/null\ndocker rmi test_ubuntu &&gt; /dev/null\n\n=====message from container=====\nIt's a script from test_folder\nmessage from specialdockerfile\n\n\n\nHere I set building directory as test_folder but -f test_folder/specialdockerfile - specialdockerfile from test_folder will be used;\n\n\n%%bash\ncd build_command_files/f_option_examples\ndocker build \\\n    -t test_ubuntu \\\n    -f test_folder/specialdockerfile \\\n    test_folder &&gt; /dev/null\ndocker run --rm -itd \\\n    --name test_ubuntu \\\n    test_ubuntu &&gt; /dev/null\necho \"=====message from container=====\"\ndocker exec test_ubuntu bash script.sh\n\ndocker stop test_ubuntu &&gt; /dev/null\ndocker rmi test_ubuntu &&gt; /dev/null\n\n=====message from container=====\nIt's a script from test_folder\nmessage from test_folder/specialdockerfile\n\n\n\n\n\n.dockerignore - select files ignored during image building\nIn docker ignore, you should specify files to be ignored when building the image.\nFor example lets try to build image with followig paremeters:\n\n%%bash\ncd dockerignore\necho \"=====dockerfile=====\"\ncat dockerfile1\necho\necho \"=====.dockerignore=====\"\ncat .dockerignore\n\n=====dockerfile=====\nFROM ubuntu\nCOPY test_file test_file\n=====.dockerignore=====\ntest_file\ntest_folder/banned_file\n\n\nSo in dockerfile I try to copy test_file into image, but in docker ignore I bun this file. At the next cell, I’m trying to build such image - and getting error.\n\n%%bash\ncd dockerignore\ndocker build -t test_image -f dockerfile1 .\n\n#1 [internal] load build definition from dockerfile1\n#1 transferring dockerfile: 74B done\n#1 DONE 0.0s\n\n#2 [internal] load .dockerignore\n#2 transferring context: 50B done\n#2 DONE 0.0s\n\n#3 [internal] load metadata for docker.io/library/ubuntu:latest\n#3 DONE 0.0s\n\n#4 [1/2] FROM docker.io/library/ubuntu\n#4 DONE 0.0s\n\n#5 [internal] load build context\n#5 transferring context: 2B done\n#5 DONE 0.0s\n\n#6 [2/2] COPY test_file test_file\n#6 ERROR: failed to calculate checksum of ref 40880b12-160a-424d-a82b-d59b9594f64c::7cmhtk0ygxakortpvzlpd8pwp: \"/test_file\": not found\n------\n &gt; [2/2] COPY test_file test_file:\n------\ndockerfile1:2\n--------------------\n   1 |     FROM ubuntu\n   2 | &gt;&gt;&gt; COPY test_file test_file\n--------------------\nERROR: failed to solve: failed to compute cache key: failed to calculate checksum of ref 40880b12-160a-424d-a82b-d59b9594f64c::7cmhtk0ygxakortpvzlpd8pwp: \"/test_file\": not found\n\n\nCalledProcessError: Command 'b'cd dockerignore\\ndocker build -t test_image -f dockerfile1 .\\n'' returned non-zero exit status 1.\n\n\nOn the other hand, if I copy an entire folder and only a few files are blocked in the folder, the folder will be copied without those files. The following example show such configuration:\n\n%%bash\ncd dockerignore\n\necho \"=====dockerfile=====\"\ncat dockerfile2\necho\necho \"=====.dockerignore=====\"\ncat .dockerignore\n\necho\necho \"=====test_folder in host=====\"\nls test_folder\n\ndocker build -t test_image -f dockerfile2 . &&gt; /dev/null\n\ndocker run --rm --name test_container -itd test_image &&gt; /dev/null\necho \"=====test_folder in container=====\"\ndocker exec test_container ls test_folder\n\ndocker stop test_container &&gt; /dev/null\ndocker rmi test_image &&gt; /dev/null\n\n=====dockerfile=====\nFROM ubuntu\nCOPY test_folder test_folder\n=====.dockerignore=====\ntest_file\ntest_folder/banned_file\n\n=====test_folder in host=====\naccepted_file\nbanned_file\n=====test_folder in container=====\naccepted_file\n\n\nHere I copy the whole folder, but the file banned_file was mentioned in .dockerignore. So when we ls temp_folder from container we see only accepted_file in folder contents.\n\n\nFile in container forever\nIf you copied a file into the container, but literally deleted it by the next instruciton - it will still affect the size of the container.\nFollowing exampe show that fature: - First I create an image - just copy Ubuntu; - Second, add the COPY instruction for a large file - so the size of the container will be much larger than in the first step; - Last image add RUN rm for the file copied in the previous step, the point is that the image size hasn’t changed.\n\n%%bash\n\ndd if=/dev/zero of=file.txt bs=1M count=1000 &&gt; /dev/null\n\necho \"=====just ubuntu copy=====\"\necho \"FROM ubuntu\" &gt; test_dockerfile\ndocker build -f test_dockerfile -t test_image . &&gt; /dev/null\ndocker images | grep test_image\ndocker rmi test_image &&gt; /dev/null\n\necho \"=====ubuntu with big copied file=====\"\necho \"COPY file.txt file.txt\" &gt;&gt; test_dockerfile\ndocker build -f test_dockerfile -t test_image . &&gt; /dev/null\ndocker images | grep test_image\ndocker rmi test_image &&gt; /dev/null\n\necho \"=====ubuntu with deleted file=====\"\necho \"RUN rm file.txt\" &gt;&gt; test_dockerfile\ndocker build -f test_dockerfile -t test_image . &&gt; /dev/null\ndocker images | grep test_image\ndocker rmi test_image &&gt; /dev/null\n\n\nrm file.txt\nrm test_dockerfile\n\n=====just ubuntu copy=====\ntest_image                 latest    8173c3801863   3 months ago    77.8MB\n=====ubuntu with big copied file=====\ntest_image                 latest    86892a85e309   2 minutes ago   1.13GB\n=====ubuntu with deleted file=====\ntest_image                 latest    97524578efc6   52 seconds ago   1.13GB\n\n\n\n\ncommit - save state of the container\nAllows you to save the current state of any container as a new image. So if you have created something new in a container, you can save it as a new image. You have to follow such sintaksis:\ndocker commit &lt;container name&gt; &lt;new image name&gt;\nSo in follwing example I: - Running the container in which create file with message; - Create file in this container; - Commit this container as a new image; - Run the container based on the new image; - Check the message in the container based on the new image.\n\n%%bash\ndocker run --rm --name test_container -itd ubuntu &&gt; /dev/null\ndocker exec test_container bash -c \"echo \\\"I'm a new file in container\\\" &gt; new_file\"\n\ndocker commit test_container new_test_image &&gt; /dev/null\n\necho \"=====check new image=====\"\ndocker images | grep new_test_image\necho \"=====message from commited image=====\"\ndocker run --rm --name new_test_container -itd new_test_image &&gt; /dev/null\ndocker exec new_test_container cat new_file\n\ndocker stop new_test_container &&gt; /dev/null\ndocker stop test_container &&gt; /dev/null\ndocker rmi new_test_image &&gt; /dev/null\n\n=====check new image=====\nnew_test_image             latest    56c5c624ed38   Less than a second ago   77.8MB\n=====message from commited image=====\nI'm a new file in container"
  },
  {
    "objectID": "Docker/network/local_network.html",
    "href": "Docker/network/local_network.html",
    "title": "Local network",
    "section": "",
    "text": "It’s very interesting that you can access the containers you’ve created from other devices on your local network. It may not be associated with Docker exactly, but with networks in general, but still I am only familiar with the Throw Docker feature.\nSo we start a container with nginx on port 80 on host.\n\n%%bash\ndocker run --rm -itd --name test_nginx -p 80:80 nginx\n\n4a4a152a0d163ff988b14cc27953c435cd3acdc1328d941d40acbc9b4a4f2122\n\n\nUsing the ifconfig Linux utility, I can check some information about the network. The most interesting part is the section starting with w. In the line inet &lt;ip&gt; you can find your ip address in the wifi network.\nNote in your run ip can be different.\n\n%%bash\nifconfig\n\ndocker0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500\n        inet 172.17.0.1  netmask 255.255.0.0  broadcast 172.17.255.255\n        inet6 fe80::42:e8ff:fefa:8cee  prefixlen 64  scopeid 0x20&lt;link&gt;\n        ether 02:42:e8:fa:8c:ee  txqueuelen 0  (Ethernet)\n        RX packets 29  bytes 5741 (5.7 KB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 87  bytes 12097 (12.0 KB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\nlo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536\n        inet 127.0.0.1  netmask 255.0.0.0\n        inet6 ::1  prefixlen 128  scopeid 0x10&lt;host&gt;\n        loop  txqueuelen 1000  (Local Loopback)\n        RX packets 31340  bytes 9448607 (9.4 MB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 31340  bytes 9448607 (9.4 MB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\nvethd50493c: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500\n        inet6 fe80::9c62:edff:fe27:8803  prefixlen 64  scopeid 0x20&lt;link&gt;\n        ether 9e:62:ed:27:88:03  txqueuelen 0  (Ethernet)\n        RX packets 0  bytes 0 (0.0 B)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 26  bytes 5168 (5.1 KB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\nwlp0s20f3: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500\n        inet 192.168.100.10  netmask 255.255.255.0  broadcast 192.168.100.255\n        inet6 fe80::735b:b9a:6447:646d  prefixlen 64  scopeid 0x20&lt;link&gt;\n        ether dc:41:a9:2e:4c:6c  txqueuelen 1000  (Ethernet)\n        RX packets 283983  bytes 382835053 (382.8 MB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 46994  bytes 11436293 (11.4 MB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\n\n\nYou can connect any other device to this network and you’ll find that when you enter a higher ip you’ll get a nginx “welcome” page.\n\n\n%%bash\ndocker stop test_nginx &&gt; /dev/null"
  },
  {
    "objectID": "Docker/network/network_interfaces.html",
    "href": "Docker/network/network_interfaces.html",
    "title": "Network interfaces",
    "section": "",
    "text": "Each container creates it’s own network interface. To list information about network interfaces in linux you have to use ifconfig command.\nSo in the following cell we first printed the ifconfig result without specifying any containers, then started a few containers and ran ifconfig again. So the second ifconfig we got more network interfaces.\n\n%%bash\necho \"=====before creating a container=====\"\nifconfig -s | awk '{print $1}'\nfor i in {1..4}; do docker run --rm -d --name test_nginx$i nginx; done &&gt; /dev/null\necho \"=====after creating a container=====\"\nifconfig -s | awk '{print $1}'\n\ndocker stop test_nginx{1..4} &&gt; /dev/null\n\n=====before creating a container=====\nIface\ndocker0\neno1\nlo\nwlp0s20f\n=====after creating a container=====\nIface\ndocker0\neno1\nlo\nveth0ac7\nveth5c1d\nvethc055\nvethd659\nwlp0s20f"
  },
  {
    "objectID": "Docker/network/ports.html",
    "href": "Docker/network/ports.html",
    "title": "Ports",
    "section": "",
    "text": "You can set port using the following syntax run ... -p &lt;port on host&gt;:&lt;port on container&gt; ....\nTo show how it works, I need some specific Python libraries defined in the following cell.\n\nimport requests\nfrom requests import ConnectionError\nfrom IPython.display import HTML\n\nTwo following cells almost the same:\n\nStart the nginx container;\nTry to connect to the http://localhost:80;\nCatch ConnectionError if it occurs;\nIf connection goes fine, then display anwer as HTML.\n\nBut in the first example the -p 80:80 option was not used and in the second example it was. It is a connection error in the first cell and a welcome message from nginx in the second.\n\n!docker run --rm -d \\\n    --name test_nginx \\\n    nginx &&gt; /dev/null\n\ntry:\n    req_res = requests.get(\"http://localhost:80\")\n    display(HTML(req_res.content.decode(\"utf-8\")))\nexcept ConnectionError:\n    print(\"Connection error!!!\")\n    \n\n!docker stop test_nginx &&gt; /dev/null\n\nConnection error!!!\n\n\n\n!docker run --rm -d \\\n    --name test_nginx \\\n    -p 80:80 \\\n    nginx &&gt; /dev/null\n\ntry:\n    req_res = requests.get(\"http://localhost:80\")\n    display(HTML(req_res.content.decode(\"utf-8\")))\nexcept ConnectionError:\n    print(\"Connection error!!!\")\n    \n\n!docker stop test_nginx &&gt; /dev/null\n\n\n\n\nWelcome to nginx!\n\n\n\n\nIf you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.\n\nFor online documentation and support please refer to\nnginx.org.\nCommercial support is available at\nnginx.com.\n\nThank you for using nginx."
  },
  {
    "objectID": "Docker/network/ports.html#set-port-for-container",
    "href": "Docker/network/ports.html#set-port-for-container",
    "title": "Ports",
    "section": "",
    "text": "You can set port using the following syntax run ... -p &lt;port on host&gt;:&lt;port on container&gt; ....\nTo show how it works, I need some specific Python libraries defined in the following cell.\n\nimport requests\nfrom requests import ConnectionError\nfrom IPython.display import HTML\n\nTwo following cells almost the same:\n\nStart the nginx container;\nTry to connect to the http://localhost:80;\nCatch ConnectionError if it occurs;\nIf connection goes fine, then display anwer as HTML.\n\nBut in the first example the -p 80:80 option was not used and in the second example it was. It is a connection error in the first cell and a welcome message from nginx in the second.\n\n!docker run --rm -d \\\n    --name test_nginx \\\n    nginx &&gt; /dev/null\n\ntry:\n    req_res = requests.get(\"http://localhost:80\")\n    display(HTML(req_res.content.decode(\"utf-8\")))\nexcept ConnectionError:\n    print(\"Connection error!!!\")\n    \n\n!docker stop test_nginx &&gt; /dev/null\n\nConnection error!!!\n\n\n\n!docker run --rm -d \\\n    --name test_nginx \\\n    -p 80:80 \\\n    nginx &&gt; /dev/null\n\ntry:\n    req_res = requests.get(\"http://localhost:80\")\n    display(HTML(req_res.content.decode(\"utf-8\")))\nexcept ConnectionError:\n    print(\"Connection error!!!\")\n    \n\n!docker stop test_nginx &&gt; /dev/null\n\n\n\n\nWelcome to nginx!\n\n\n\n\nIf you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.\n\nFor online documentation and support please refer to\nnginx.org.\nCommercial support is available at\nnginx.com.\n\nThank you for using nginx."
  },
  {
    "objectID": "Docker/network/ports.html#several-ports",
    "href": "Docker/network/ports.html#several-ports",
    "title": "Ports",
    "section": "Several ports",
    "text": "Several ports\nYou can use the -p option to the docker run command as many times as you need. The nginx container was started with two ports. So requests to both of the ports won’t throw any errors.\n\n!docker run --rm -d \\\n    --name test_nginx \\\n    -p 123:80 -p 987:80\\\n    nginx &&gt; /dev/null\n\ntry:\n    requests.get(\"http://localhost:123\")\n    requests.get(\"http://localhost:987\")\nexcept ConnectionError:\n    print(\"Connection error!!!\")\n\n!docker stop test_nginx &&gt; /dev/null"
  },
  {
    "objectID": "Docker/network/ports.html#expose",
    "href": "Docker/network/ports.html#expose",
    "title": "Ports",
    "section": "EXPOSE",
    "text": "EXPOSE\nIs a keyword for dockerfiles that allows you to specify the port to be used by the container based on the corresponding image.\n\nExample on other image\nLet’s take nginx. If we start the container with it and then check the PORTS field - it’ll display the ports that should be used by this container.\n\n%%bash\ndocker run --rm -d \\\n    --name test_nginx \\\n    nginx &&gt; /dev/null\ndocker ps\ndocker stop test_nginx &&gt; /dev/null\n\nCONTAINER ID   IMAGE     COMMAND                  CREATED                  STATUS                  PORTS     NAMES\nd8469eb982e6   nginx     \"/docker-entrypoint.…\"   Less than a second ago   Up Less than a second   80/tcp    test_nginx\n\n\nAnd even if you want to check the history of the images, you can find the EXPOSE &lt;port&gt; layer there.\n\n%%bash\ndocker history nginx | grep EXPOSE\n\n&lt;missing&gt;      7 months ago   /bin/sh -c #(nop)  EXPOSE 80                    0B        \n\n\n\n\nUsing EXPOSE\nHere I want to focus on using the EXPOSE command to create your own docker files. So in the following cell I have created a docker based on ununtu, but added EXPOSE 1050 to it.\n\n%%writefile ports_files/test_dockerfile\nFROM ubuntu\nEXPOSE 1050\n\nOverwriting ports_files/test_dockerfile\n\n\nNow let us try to run the container based on the image we just created. The main difference is that in docker ps' in thePORTS` column you can find information about the port you mentioned in the docker file.\n\n%%bash\ndocker build -f ports_files/test_dockerfile -t test_expose . &&gt; /dev/null\n\ndocker run -itd --name test_expose --rm test_expose &&gt; /dev/null\ndocker ps\n\ndocker stop test_expose &&gt; /dev/null\ndocker rmi test_expose &&gt; /dev/null\n\nCONTAINER ID   IMAGE         COMMAND       CREATED        STATUS                  PORTS      NAMES\n55ceb23fccea   test_expose   \"/bin/bash\"   1 second ago   Up Less than a second   1050/tcp   test_expose"
  },
  {
    "objectID": "Docker/network/container_connection.html",
    "href": "Docker/network/container_connection.html",
    "title": "Container connection",
    "section": "",
    "text": "You need to use the --net=&lt;network id/name&gt; parameter for the docker run command. As in the following cell, --net host has been used, and the command that prints the name of the network container it is connected to shows host.\n\n%%bash\ndocker run --rm -itd --name just_net --net host ubuntu &&gt; /dev/null\ndocker inspect -f '{{range $key, $value := .NetworkSettings.Networks}}{{$key}} {{end}}' just_net\ndocker stop just_net &&gt; /dev/null\n\nhost"
  },
  {
    "objectID": "Docker/network/container_connection.html#during-container-creation",
    "href": "Docker/network/container_connection.html#during-container-creation",
    "title": "Container connection",
    "section": "",
    "text": "You need to use the --net=&lt;network id/name&gt; parameter for the docker run command. As in the following cell, --net host has been used, and the command that prints the name of the network container it is connected to shows host.\n\n%%bash\ndocker run --rm -itd --name just_net --net host ubuntu &&gt; /dev/null\ndocker inspect -f '{{range $key, $value := .NetworkSettings.Networks}}{{$key}} {{end}}' just_net\ndocker stop just_net &&gt; /dev/null\n\nhost"
  },
  {
    "objectID": "Docker/network/container_connection.html#existing-container",
    "href": "Docker/network/container_connection.html#existing-container",
    "title": "Container connection",
    "section": "Existing container",
    "text": "Existing container\nYou need to know commands:\n\ndocker network connect &lt;net name/id&gt; &lt;container name/id&gt; to connect any container to the selected network;\ndocker network disconnect &lt;net name/id&gt; &lt;container name/id&gt; to disconnect any container from the selected network.\n\nThe manoeuvre of disconnecting a container from the selected network in which it was created by default and reconnecting it to another is shown in the next cell.\n\n%%bash\ndocker run --rm -itd --name just_net ubuntu &&gt; /dev/null\necho \"=====Just created container=====\"\ndocker inspect -f '{{range $key, $value := .NetworkSettings.Networks}}{{$key}} {{end}}' just_net\n\ndocker network disconnect bridge just_net\necho \"=====Network disconnected=====\"\ndocker inspect -f '{{range $key, $value := .NetworkSettings.Networks}}{{$key}} {{end}}' just_net\n\ndocker network connect none just_net\necho \"=====Container connect to the host network=====\"\ndocker inspect -f '{{range $key, $value := .NetworkSettings.Networks}}{{$key}} {{end}}' just_net\n\ndocker stop just_net &&gt; /dev/null\n\n=====Just created container=====\nbridge \n=====Network disconnected=====\n\n=====Container connect to the host network=====\nnone \n\n\nNote you cannot connect the container to different networks at the same time. The following cell shows warning that docker prints if you’ll try.\n\n%%bash\ndocker run --rm -itd --name just_net ubuntu &&gt; /dev/null\ndocker network connect none just_net\ndocker stop just_net &&gt; /dev/null\n\nError response from daemon: container cannot be connected to multiple networks with one of the networks in private (none) mode"
  },
  {
    "objectID": "Docker/multistage_build/multistage_build.html",
    "href": "Docker/multistage_build/multistage_build.html",
    "title": "База",
    "section": "",
    "text": "Многоэтапная сборка в docker\nВ docker есть особый тип сборки, который позволяет оставить в финальном образе только те элементы, что нам нужны. Это, в некоторых случаях, позволит сэкономить размер docker образа.\nФормально это делается следующим образом:\nFROM &lt;название образа&gt; AS &lt;название сборки1&gt;\n\nFROM &lt;название образа&gt;\nCOPY --from=&lt;название сборки1&gt; &lt;файл из перовой сборки&gt; &lt;файл из второй сборки&gt;\n\nПример\nТак допуским ситуацию, где нам в финальной сборке надо иметь результаты обработки некоторого большого файла example.csv проводимой программой generation.py (в данном случае это просто взятие средних).\nЕсли делать это по простому, то мы сделаем dockerfile вида (simple_dockerfile):\n# простой dockerfile\nFROM python:3.10 AS BUILD\n\nWORKDIR some_dir\nCOPY example.csv example.csv\nCOPY requirements.txt requirements.txt\nCOPY generation.py generation.py\n\nRUN python -m pip install --upgrade pip && pip install -r requirements.txt\nRUN python generation.py\n\nCMD [\"cat\", \"means.csv\"]\nДалее: - Собираю образ; - Поднимаю контейнер на его основе, чтобы показать, что все работает номрмально; - Показываю размер образа.\n\n%%bash\ndocker build -q -t simple -f simple_dockerfile .\necho ==============================\ndocker run --rm simple\necho ==============================\ndocker images simple\necho ==============================\ndocker rmi simple\n\nsha256:daea387ec3b5bb08725927e3cf8a8274d8c535fbdebd7905760ebeeb6aec0156\n==============================\n,0\n0,0.49801512771615053\n1,0.5005384992657755\n2,0.5016440913126519\n3,0.49893742832936905\n4,0.5006416292879786\n5,0.500604403582076\n6,0.5024090926131595\n7,0.500360289940759\n8,0.4995687156844078\n9,0.4993301862968266\n10,0.4996320601371099\n11,0.49936364688453744\n12,0.4996892437983095\n13,0.5003040557416716\n14,0.5008896760787612\n15,0.49834042413261304\n16,0.5002565939707307\n17,0.499441058229968\n18,0.4994494362971878\n19,0.5000038702075555\n==============================\nREPOSITORY   TAG       IMAGE ID       CREATED          SIZE\nsimple       latest    daea387ec3b5   43 minutes ago   1.11GB\n==============================\nUntagged: simple:latest\nDeleted: sha256:daea387ec3b5bb08725927e3cf8a8274d8c535fbdebd7905760ebeeb6aec0156\n\n\nВ итоге мы вывели результат расчета. Ну и размер образа составляет титанические 1.11GB.\nА теперь сделаем тоже самое, только правильно. dockerfile в случае использоваиня многоэтапной сборке примет вид (multistage_dockerfile):\nFROM python:3.10 AS BUILDER\n\nWORKDIR some_dir\nCOPY example.csv example.csv\nCOPY requirements.txt requirements.txt\nCOPY generation.py generation.py\n\nRUN python -m pip install --upgrade pip && pip install -r requirements.txt\nRUN python generation.py\n\nFROM ubuntu:22.04\nWORKDIR some_dir\nCOPY --from=BUILDER /some_dir/means.csv /some_dir/means.csv\nCMD [\"cat\", \"means.csv\"]\nБуквально повторяю процедуры, которые проделал для предыдущего образа.\n\n%%bash\ndocker build -q -t multistage -f multistage_dockerfile .\necho ==============================\ndocker run --rm multistage\necho ==============================\ndocker images multistage\necho ==============================\ndocker rmi multistage\n\nsha256:15096f1d81baa6c46dadf24558dfa0c39ffcebb248495c779f1334b4b75a2eca\n==============================\n,0\n0,0.49801512771615053\n1,0.5005384992657755\n2,0.5016440913126519\n3,0.49893742832936905\n4,0.5006416292879786\n5,0.500604403582076\n6,0.5024090926131595\n7,0.500360289940759\n8,0.4995687156844078\n9,0.4993301862968266\n10,0.4996320601371099\n11,0.49936364688453744\n12,0.4996892437983095\n13,0.5003040557416716\n14,0.5008896760787612\n15,0.49834042413261304\n16,0.5002565939707307\n17,0.499441058229968\n18,0.4994494362971878\n19,0.5000038702075555\n==============================\nREPOSITORY   TAG       IMAGE ID       CREATED             SIZE\nmultistage   latest    15096f1d81ba   About an hour ago   77.8MB\n==============================\nUntagged: multistage:latest\nDeleted: sha256:15096f1d81baa6c46dadf24558dfa0c39ffcebb248495c779f1334b4b75a2eca\n\n\nИ так, в результате, все тоже самое но размером в 77.8MB.\n\n\nОстановка на нужном этапе\nПри многоэтапной сборке, например, для отладки, может понадобиться отсновится в сборке образа. Это делается с помощью опции --target &lt;название этапа&gt;. Так, в примере далее, я останавливаю сборку описанного выше образа multistage на этапе BUILDER и убеждаюсь, что это именно тот этап (файлы то его).\n\n%%bash\ndocker build -q -t multistage -f multistage_dockerfile --target BUILDER .\ndocker run --rm --name multistage -itd multistage\necho ==============================\ndocker exec multistage ls\necho ==============================\ndocker stop multistage\ndocker rmi multistage\n\nsha256:19ec4683d03994e2f466696a18fbd3db8deda7f1f251185feddb1d568b116a02\n0e9309e419e5c47647beb7005ecd0bb2d6c8c771eb96c216fcb24294781c4ff1\n==============================\nexample.csv\ngeneration.py\nmeans.csv\nrequirements.txt\n==============================\nmultistage\nUntagged: multistage:latest\nDeleted: sha256:19ec4683d03994e2f466696a18fbd3db8deda7f1f251185feddb1d568b116a02"
  },
  {
    "objectID": "other/python_apache.html",
    "href": "other/python_apache.html",
    "title": "Python with apache",
    "section": "",
    "text": "For examples in this notebook I use special docker containers - docker file for container shown in python_apache_files folder. So to run examples from this section you should build image by running command docker build -t my_apache . from python_apache_files directory. This docker image will contain:"
  },
  {
    "objectID": "other/python_apache.html#ports.conf",
    "href": "other/python_apache.html#ports.conf",
    "title": "Python with apache",
    "section": "ports.conf",
    "text": "ports.conf\nConfig that describes which ports to listen on. By default, apache2 ports.conf only uses ports 80 and 443. But for some examples I need port 8050, so I use it as well.\n\n%%bash\ncat python_apache_files/ports.conf\n\n# If you just change the port or add more ports here, you will likely also\n# have to change the VirtualHost statement in\n# /etc/apache2/sites-enabled/000-default.conf\n\nListen 80\nListen 8050\n\n&lt;IfModule ssl_module&gt;\n        Listen 443\n&lt;/IfModule&gt;\n\n&lt;IfModule mod_gnutls.c&gt;\n        Listen 443\n&lt;/IfModule&gt;"
  },
  {
    "objectID": "other/python_apache.html#launching",
    "href": "other/python_apache.html#launching",
    "title": "Python with apache",
    "section": "Launching",
    "text": "Launching\nBy running this section on our local host, a number of sites will be hosted:\n\n%%bash\ncd python_apache_files\n\ndocker run --rm --name test_apache -d -p 81:80 -p 82:8050 my_apache &&gt; /dev/null\n\n\ndocker cp ports.conf test_apache:/etc/apache2/ports.conf\n\n# wsgi basics\ndocker cp wsgi_example/wsgi_basic.wsgi test_apache:/var/www/application/wsgi_basic.wsgi\ndocker cp wsgi_example/suburl.wsgi test_apache:/var/www/application/suburl.wsgi\ndocker cp wsgi_example/wsgi_basic.conf test_apache:/etc/apache2/sites-available/wsgi_basic.conf\ndocker exec test_apache a2ensite wsgi_basic\n\n# specific port\ndocker cp use_specific_port/spec_port.wsgi test_apache:/var/www/application/spec_port.wsgi\ndocker cp use_specific_port/spec_port.conf test_apache:/etc/apache2/sites-available/spec_port.conf\ndocker exec test_apache a2ensite spec_port\n\n# dash application\n# docker exec test_apache mkdir /var/www/dash\ndocker cp dash/dash.wsgi test_apache:/var/www/application/dash.wsgi\ndocker cp dash/app.py test_apache:/var/www/application/app.py\n# docker cp dash/dash.wsgi test_apache:/var/www/dash/dash.wsgi\n# docker cp dash/app.py test_apache:/var/www/dash/app.py\n# docker cp dash/dash.conf test_apache:/etc/apache2/sites-available/dash.conf\n# docker exec test_apache a2ensite dash\n\n# docker exec test_apache chown -R :www-data /var/www/application\n# docker exec test_apache chmod -R o-rw /var/www/application\n\n\ndocker exec test_apache a2dissite 000-default\ndocker exec test_apache service apache2 reload\n\nEnabling site wsgi_basic.\nTo activate the new configuration, you need to run:\n  service apache2 reload\nEnabling site spec_port.\nTo activate the new configuration, you need to run:\n  service apache2 reload\nSite 000-default disabled.\nTo activate the new configuration, you need to run:\n  service apache2 reload\n * Reloading Apache httpd web server apache2\n * \n\n\nNote: for the site to work for any reason, it’s extremely important to stop apache2 default page with a2dissite 000-default command.\nBy runnig the following cell - sites will be stopped:\n\n%%bash\ndocker stop test_apache\n\ntest_apache"
  },
  {
    "objectID": "other/python_apache.html#wsgi-application",
    "href": "other/python_apache.html#wsgi-application",
    "title": "Python with apache",
    "section": "wsgi application",
    "text": "wsgi application\nWe need a WSGI application - this is a Python program that is used when you ask for a specific URL. It should have essance with name application, in basic option it can be just function. So in the following example I show exactly this option:\n\n%%bash\ncat python_apache_files/wsgi_example/wsgi_basic.wsgi\n\ndef application(environ, start_response):\n    status = '200 OK'\n    output = b\"I'm application from root. Basic example of wsgi\"\n\n    response_headers = [('Content-type', 'text/plain'),\n                        ('Content-Length', str(len(output)))]\n    start_response(status, response_headers)\n\n    return [output]"
  },
  {
    "objectID": "other/python_apache.html#site-config",
    "href": "other/python_apache.html#site-config",
    "title": "Python with apache",
    "section": "Site config",
    "text": "Site config\nReally important file that describe site behavior. Crusial information:\n\nShould be placed in /ect/apache2/apailible-sites;\nWSGIScriptAlias directive to describe which Python file will be used as the entry point, and which suburl of the site it will use;\nWSGIDaemonProcess -&gt; python-path describe which Python interpreter should be used.;\n\n\n%%bash\ncat python_apache_files/wsgi_example/wsgi_basic.conf\n\n&lt;VirtualHost *:80&gt;\n        ServerName application\n        ServerAdmin webmaster@localhost\n        DocumentRoot /var/www/application\n\n        ErrorLog ${APACHE_LOG_DIR}/error.log\n        CustomLog ${APACHE_LOG_DIR}/access.log combined\n\n        WSGIDaemonProcess application threads=5 user=www-data group=www-data python-path=/var/www/application/venv/lib/python3.11/site-packages python-home=/var/www/application/venv\n\n        WSGIScriptAlias /suburl /var/www/application/suburl.wsgi\n        WSGIScriptAlias / /var/www/application/wsgi_basic.wsgi\n        \n        &lt;Directory /var/www/application&gt;\n                Order deny,allow\n                Allow from all\n        &lt;/Directory&gt;\n\n&lt;/VirtualHost&gt;"
  },
  {
    "objectID": "other/python_apache.html#dash-application-1",
    "href": "other/python_apache.html#dash-application-1",
    "title": "Python with apache",
    "section": "dash application",
    "text": "dash application\n\n%%bash\ncat python_apache_files/dash/app.py\n\nfrom dash import Dash, html\n\napp = Dash(__name__, requests_pathname_prefix='/dash/')\napp.layout = html.Div(\"Hello world\")\n\nserver = app.server\n\nif __name__ == \"__main__\":\n    app.run_server(debug=True, port=8051)\n\n\nNote: - server = app.server is very important - it is loaded in wsgi; - requests_pathname_prefix='/dash/' in Dash constructor will allow to run application with suburl dash."
  },
  {
    "objectID": "other/python_apache.html#wsgi-file",
    "href": "other/python_apache.html#wsgi-file",
    "title": "Python with apache",
    "section": "wsgi file",
    "text": "wsgi file\nThe main purpose of the wsgi file here is to load dash.Dash as an application essence for wsgi.\n\n%%bash\ncat python_apache_files/dash/dash.wsgi\n\nimport sys\nsys.path.insert(0,\"/var/www/application/\")\nfrom app import server as application"
  },
  {
    "objectID": "other/basic_apache2.html",
    "href": "other/basic_apache2.html",
    "title": "Basic work with apache2 web server",
    "section": "",
    "text": "Sources\n\nInstalation https://httpd.apache.org/docs/2.4/install.html;\n\n\n\nRun/stop server\nTo start the Apache web server you can use service apache2 start. After successful execution of this command you should get response from localhost:80 - in browser it would be Apache2 Default Page. service apache2 stop therefore stops the server.\nIn fllowing example: - Before the server starts, I access the web server via curl - but get no response; - I start the server with service apache2 start, after starting curl return apache start page; - I stop the server with service apache2 stop - this stops the apache welcome page from appearing.\n\n%%bash\necho \"=====Curl before server start=====\"\ncurl -s localhost:80 | head -n 50\n\nservice apache2 start\necho \"=====Curl after server start=====\"\ncurl -s localhost:80 | head -n 50\n\nservice apache2 stop\necho \"=====Curl after server stop=====\"\ncurl -s localhost:80 | head -n 50\n\n=====Curl before server start=====\n&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\"&gt;\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\"&gt;\n  &lt;!--\n    Modified from the Debian original for Ubuntu\n    Last updated: 2022-03-22\n    See: https://launchpad.net/bugs/1966004\n  --&gt;\n  &lt;head&gt;\n    &lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" /&gt;\n    &lt;title&gt;Apache2 Ubuntu Default Page: It works&lt;/title&gt;\n    &lt;style type=\"text/css\" media=\"screen\"&gt;\n  * {\n    margin: 0px 0px 0px 0px;\n    padding: 0px 0px 0px 0px;\n  }\n\n  body, html {\n    padding: 3px 3px 3px 3px;\n\n    background-color: #D8DBE2;\n\n    font-family: Ubuntu, Verdana, sans-serif;\n    font-size: 11pt;\n    text-align: center;\n  }\n\n  div.main_page {\n    position: relative;\n    display: table;\n\n    width: 800px;\n\n    margin-bottom: 3px;\n    margin-left: auto;\n    margin-right: auto;\n    padding: 0px 0px 0px 0px;\n\n    border-width: 2px;\n    border-color: #212738;\n    border-style: solid;\n\n    background-color: #FFFFFF;\n\n    text-align: center;\n  }\n\n  div.page_header {\n    height: 180px;\n    width: 100%;\n\n=====Curl after server start=====\n&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\"&gt;\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\"&gt;\n  &lt;!--\n    Modified from the Debian original for Ubuntu\n    Last updated: 2022-03-22\n    See: https://launchpad.net/bugs/1966004\n  --&gt;\n  &lt;head&gt;\n    &lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" /&gt;\n    &lt;title&gt;Apache2 Ubuntu Default Page: It works&lt;/title&gt;\n    &lt;style type=\"text/css\" media=\"screen\"&gt;\n  * {\n    margin: 0px 0px 0px 0px;\n    padding: 0px 0px 0px 0px;\n  }\n\n  body, html {\n    padding: 3px 3px 3px 3px;\n\n    background-color: #D8DBE2;\n\n    font-family: Ubuntu, Verdana, sans-serif;\n    font-size: 11pt;\n    text-align: center;\n  }\n\n  div.main_page {\n    position: relative;\n    display: table;\n\n    width: 800px;\n\n    margin-bottom: 3px;\n    margin-left: auto;\n    margin-right: auto;\n    padding: 0px 0px 0px 0px;\n\n    border-width: 2px;\n    border-color: #212738;\n    border-style: solid;\n\n    background-color: #FFFFFF;\n\n    text-align: center;\n  }\n\n  div.page_header {\n    height: 180px;\n    width: 100%;\n\n=====Curl after server stop====="
  },
  {
    "objectID": "machine_learning/models_selection_methods/scaling_and_regularisation.html",
    "href": "machine_learning/models_selection_methods/scaling_and_regularisation.html",
    "title": "Scaling and regularisation",
    "section": "",
    "text": "It is recommended to scale the data before performing regularisation. In this page I want to show why.\nThe reason is quite simple - popular components of the target function responsible for regularisation of the model look like this.\n\n\\(\\sum{\\beta_i^2}\\);\n\\(\\sum{\\left|\\beta_i\\right|}\\).\n\nAll coefficients have the same contribution to the target function, regardless of their magnitude, so the optimisation algorithm naturally benefits from reducing first those coefficients that make a greater contribution to the target function, regardless of the economic/physical sense of the variables.\nThus regularisation may compress the coefficients too much at large scales without any intelligible reason for doing so. It is to counteract these phenomena that it is recommended to bring the data to a uniform scale by any available means.\nBelow is a small experiment that confirms this idea.\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge\n\nLet’s say you have a data frame with two features and their scaling is dramatically different.\nSo in the following cell I reproduce such a case. There are two functions and the numbers in the first function are usually 100 times smaller than in the second. But on the target variable, they out of order have about the same effect - each unit in the first variable contributes 100 times more to the value of the explained variable.\n\nnp.random.seed(10)\nsample_size = 200\n\nX = np.concatenate(\n    [\n        np.random.normal(1, 0.5, (sample_size, 1)),\n        np.random.normal(100, 50, (sample_size, 1))\n    ],\n    axis = 1\n)\n\ny = np.dot(X, np.array([500, 5])) + np.random.normal(0, 10, sample_size)\n\nWe will now gradually increase the scaling. Note that the higher scaling coefficient decreases much faster than the second one. Even in relative units:\n\n\\(\\beta_1(\\alpha=0) \\approx 2*\\beta_1(\\alpha=45)\\) coefficient with higher scaling became twice as low;\n\\(\\beta_1(\\alpha=0) \\approx \\beta_1(\\alpha=45)\\) coefficient with lower scaling will decrease a little, but not twice as much as the first coefficient.\n\n\ndisplay_frame = pd.DataFrame(\n    {alpha:Ridge(alpha = alpha).fit(X,y).coef_ for alpha in np.arange(0, 50, 5)}\n).T\n\ndisplay_frame.index.name = \"$\\\\alpha$\"\ndisplay_frame.columns = [\"$\\\\beta_1$\", \"$\\\\beta_2$\"]\ndisplay(display_frame)\n\n\n\n\n\n\n\n\n$\\beta_1$\n$\\beta_2$\n\n\n$\\alpha$\n\n\n\n\n\n\n0\n499.305569\n5.029773\n\n\n5\n451.840445\n5.012651\n\n\n10\n412.616182\n4.998492\n\n\n15\n379.658049\n4.986585\n\n\n20\n351.575574\n4.976432\n\n\n25\n327.361354\n4.967669\n\n\n30\n306.267632\n4.960028\n\n\n35\n287.727714\n4.953306\n\n\n40\n271.304290\n4.947344\n\n\n45\n256.654507\n4.942020\n\n\n\n\n\n\n\nNow we want to do the same operation, but with a standardised feature matrix. Display the coefficients you need to multiply with the standardised and original data in different columns.\nAs the result:\n\nThe coefficients on the standardized data decrease uniformly even in absolute terms;\nIf you transform coefficients to be used directly with the original data, because of the difference in scaling, the absolute decrease is greater for the features with higher scaling, but relatively both coefficients decreases ~ 20%.\n\n\nmeans = X.mean(axis=0)\nstd = X.std(axis = 0)\n\nX_stand = (X-means)/std\n\n\ndisplay_frame = pd.DataFrame(\n    {alpha:Ridge(alpha = alpha).fit(X_stand,y).coef_ for alpha in  np.arange(0, 50, 5)}\n).T\n\ndisplay_frame.index.name = \"$\\\\alpha$\"\ndisplay_frame.columns = [\"$\\\\beta_1$\", \"$\\\\beta_2$\"]\npd.concat(\n    [\n        display_frame,\n        (display_frame+means)/std\n    ],\n    keys=[\"Standardised data\", \"Original data\"],\n    axis = 1\n)\n\n\n\n\n\n\n\n\nStandardised data\nOriginal data\n\n\n\n$\\beta_1$\n$\\beta_2$\n$\\beta_1$\n$\\beta_2$\n\n\n$\\alpha$\n\n\n\n\n\n\n\n\n0\n243.721886\n232.585223\n501.430354\n7.222430\n\n\n5\n237.582185\n226.708414\n488.852137\n7.095341\n\n\n10\n231.744246\n221.121240\n476.892129\n6.974515\n\n\n15\n226.186353\n215.802807\n465.505843\n6.859501\n\n\n20\n220.888825\n210.734182\n454.652961\n6.749890\n\n\n25\n215.833785\n205.898170\n444.296855\n6.645309\n\n\n30\n211.004954\n201.279119\n434.404177\n6.545419\n\n\n35\n206.387481\n196.862749\n424.944500\n6.449913\n\n\n40\n201.967783\n192.636007\n415.890002\n6.358508\n\n\n45\n197.733422\n188.586938\n407.215194\n6.270944"
  },
  {
    "objectID": "machine_learning/metrics/GINI.html",
    "href": "machine_learning/metrics/GINI.html",
    "title": "GINI coeficient",
    "section": "",
    "text": "Metric used to evaluate the quality of classification algorithms.\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.metrics import roc_curve, auc\n\nimport matplotlib.pyplot as plt\n\nfrom statsmodels.distributions.empirical_distribution import ECDF\nnp.random.seed(10)"
  },
  {
    "objectID": "machine_learning/metrics/GINI.html#content",
    "href": "machine_learning/metrics/GINI.html#content",
    "title": "GINI coeficient",
    "section": "Content",
    "text": "Content\n\nBasic GINI definition;\n\\(GINI\\) as formula;\n\\(GINI\\) & \\(AUC_{roc}\\) relation."
  },
  {
    "objectID": "machine_learning/metrics/GINI.html#basic-gini-definition",
    "href": "machine_learning/metrics/GINI.html#basic-gini-definition",
    "title": "GINI coeficient",
    "section": "Basic GINI definition",
    "text": "Basic GINI definition\n\nToday, GINI is usually expressed through the ROC AUC, but in fact GINI originally had a separate definition through CAP curve.\nGINI for some model, is the ratio of the areas between the CAP curve of the model and the random CAP curve to the area between the ideal CAP and the random CAP.\n\nGraphical representation of GINI\n\nplot_ss = 10000\n\nnp.random.seed(3)\nrandom_range = np.random.rand(plot_ss)\n\nplot_data = pd.DataFrame({\n    \"p_hat\" : random_range,\n    \"y\" : map(\n        lambda r_val: np.random.choice(\n            [0, 1], p = [1 - r_val, r_val]\n        ), \n        random_range\n    )\n})\n\nplot_data.sort_values(\"y\",inplace = True, ascending = False)\nplot_data[\"p_hat_ideal\"] = np.linspace(1,0, plot_data.shape[0])\n\n\nfpr, or_tpr, t = roc_curve(\n    plot_data[\"y\"], plot_data[\"p_hat\"],\n    drop_intermediate = False\n)\nfpr, id_tpr, t = roc_curve(\n    plot_data[\"y\"], plot_data[\"p_hat_ideal\"],\n    drop_intermediate = False\n)\n\nCAP_x = np.arange(len(or_tpr))/len(or_tpr)\n\n\nplt.figure(figsize = [10,7])\nplt.plot(CAP_x, or_tpr, linewidth = 5)\nplt.plot(CAP_x, id_tpr, linewidth = 5)\nplt.plot(\n    [0,1], [0,1], color = \"red\", \n    linestyle = \"dashed\",\n    linewidth = 5\n)\n\nplt.fill_between(\n    np.arange(len(or_tpr))/len(or_tpr),\n    or_tpr,\n    np.arange(len(or_tpr))/len(or_tpr),\n    hatch = \"//\",\n    alpha = 0\n)\nplt.fill_between(\n    np.arange(len(or_tpr))/len(or_tpr),\n    id_tpr,\n    np.arange(len(or_tpr))/len(or_tpr),\n    hatch = \"\\\\\\\\\",\n    alpha = 0\n)\n\nplt.xlabel(\"Percentage of observations\", fontsize = 15)\nplt.ylabel(\"$TPR$\", fontsize = 15)\n\nplt.xlim([0,1])\nplt.ylim([-0.005,1.005])\n\nplt.legend(\n    [\n        \"Model's CAP\", \"Ideal CAP\", \n        \"Random CAP\", \"A\", \"B\"\n    ],\n    fontsize = 14\n)\nplt.show()\n\n\n\n\nFollowing the notations of the areas in the figure, we obtain:\n\\[GINI = \\frac{A}{B}\\]\nOr by using the alternative designation of areas (for reasons not yet known, especially popular)\n\nplt.figure(figsize = [10,7])\nplt.plot(CAP_x, or_tpr, linewidth = 5)\nplt.plot(CAP_x, id_tpr, linewidth = 5)\nplt.plot(\n    [0,1], [0,1], color = \"red\", \n    linestyle = \"dashed\",\n    linewidth = 5\n)\n\nplt.fill_between(\n    np.arange(len(or_tpr))/len(or_tpr),\n    or_tpr,\n    np.arange(len(or_tpr))/len(or_tpr),\n    hatch = \"//\",\n    alpha = 0\n)\nplt.fill_between(\n    np.arange(len(or_tpr))/len(or_tpr),\n    id_tpr,\n    or_tpr,\n    hatch = \"\\\\\\\\\",\n    alpha = 0\n)\n\nplt.xlabel(\"Percentage of observations\", fontsize = 15)\nplt.ylabel(\"$TPR$\", fontsize = 15)\n\nplt.xlim([0,1])\nplt.ylim([-0.005,1.005])\n\nplt.legend(\n    [\n        \"Model's CAP\", \"Ideal CAP\", \n        \"Random CAP\", \"A\", \"B\"\n    ],\n    fontsize = 14\n)\nplt.show()\n\n\n\n\n\\[GINI = \\frac{A}{B+A}\\]"
  },
  {
    "objectID": "machine_learning/metrics/GINI.html#gini-as-formula",
    "href": "machine_learning/metrics/GINI.html#gini-as-formula",
    "title": "GINI coeficient",
    "section": "\\(GINI\\) as formula",
    "text": "\\(GINI\\) as formula\n\nWe have already given a geometric definition of \\(GINI\\), but for software implementation of computations or for rigorous proofs we would rather use the analytical notation of this quantity.\n\nArea under model \\(CUP\\)\nFirst, let’s deal with the numerator of the GINI. We need to express the area under the CUP curve, which we denote by \\(AUC_{cap}\\).\nLet for some set of objects numbered \\(i \\in \\overline{1,n}\\) we have obtained some variable \\(p_i\\) which is the greater the higher the probability of occurrence of the predicted feature \\(y_i\\). Let objects have been sorted in descending order, so \\(p_i \\geq p_{i+1}, i \\in \\overline{1,n-1}\\). For example, it might look like the table below.\n\n\n\n\\(i\\)\n\\(p_i\\)\n\\(y_i\\)\n\\(i/n\\)\n\\(TPR_i\\)\n\\(FPR_i\\)\n\n\n\n\n1\n0.8\n1\n0.2\n1/3\n0\n\n\n2\n0.7\n1\n0.4\n2/3\n0\n\n\n3\n0.6\n0\n0.6\n2/3\n1/2\n\n\n4\n0.4\n0\n0.8\n2/3\n1\n\n\n5\n0.2\n1\n1\n1\n1\n\n\n\nThe area under an arbitrary curve \\(AUC\\) which is the set of points \\((x_i,y_i)\\) connected by lines can be written as:\n\\[AUC = \\sum_{i=1}^{n-1} (x_{i+1}-x_i)(y_{i} + y_{i+1})/2.\\]\nIn \\(CAP\\) curve case on the abscissa is the fraction of observations for which \\(p_i\\) is greater than some threshold or \\(i/n\\), on the ordinate \\(TPR_i\\). So assuming that \\(TPR_0 = 0\\) we obtain the analytical notation \\(AUC_{cap}\\):\n\\[AUC_{cap} = \\sum_{i=0}^{n-1}([i+1]/n - i/n)(TPR_{i+1} + TPR_i)/2 = \\sum_{i=0}^{n-1}(TPR_{i+1} + TPR_i)/2n.\\]\nObviously, the area between the \\(CUP\\) curve of the estimated model and the \\(CUP\\) curve of the random classifier is then expressed:\n\\[AUC_{cap}' = \\sum_{i=0}^{n-1}(TPR_{i+1} + TPR_i)/2n - 0.5.\\].\n\n\nArea under ideal \\(CUP\\)\nNow let’s deal with the denominator of the \\(CAP\\) curve.\nIn the general case, the ideal \\(CAP\\) curve takes the form displayed by the following cell:\n\ny_rel_ideal = [0, 1/3, 2/3, 1, 1, 1]\nx_rel = [i/5 for i in range(6)]\n\nplt.figure(figsize = [10, 7])\n\nplt.plot(x_rel, y_rel_ideal)\nplt.fill_between(\n    [0, 0.6], [0, 1], [0,0],\n    alpha = 0, hatch = \"//\"\n)\nplt.fill_between(\n    [0.6, 1], [0, 0], [1,1],\n    alpha = 0, hatch = \"\\\\\\\\\"\n)\n\nplt.yticks([0, 1])\nplt.xticks(\n    [0, 0.6, 1],\n    [\"0\", \"$\\gamma$\", \"1\"],\n    fontsize = 14\n)\n\nplt.title(\n    \"Ideal CAP curve\",\n    fontsize = 15\n)\n\nplt.grid()\nplt.xlabel(\"Percentage of observations\", fontsize = 15)\nplt.show()\n\n\n\n\nAll observations with feature manifestation have greater \\(p_i\\) than without feature manifestation. Therefore, the \\(CAP\\) curve grows until all objects in which the manifestation of the investigated trait is observed are finished. So in finishes it growth in point:\n\\[\\gamma = \\frac{\\sum_{i=1}^ny_i}{n}.\\]\nIt turns out that this area decomposes into 2 figures:\n\nA triangle, highlighted by a hatching slanted to the left;\nA rectangle marked by a hatching slanted to the right.\n\nAnd can be written:\n\\[AUC^I = \\gamma/2 + (1-\\gamma) = 1 - \\gamma/2.\\]\nThen the area between the ideal CAP curve and the random CAP curve is:\n\\[AUC'^I = 1 - \\gamma/2 - 0.5=0.5 - \\gamma/2.\\]\nOr by substituting \\(\\gamma\\):\n\\[AUC'^I = 0.5 - \\frac{\\sum_{i=1}^{n}y_i}{2n}.\\].\n\n\n\\(GINI\\)\nFinally, combine the previous two steps to write \\(GINI\\):\n\\[GINI=\\frac{AUC'_{cap}}{AUC'^I}=\\frac{\\sum_{i=0}^{n-1}(TPR_{i+1} + TPR_i)/2n - 0.5}{0.5 - \\frac{\\sum_{j=1}^{n}y_j}{2n}}=\\] \\[=\\frac{\\left[\\sum_{i=0}^{n-1}(TPR_{i+1} + TPR_i)-n\\right]/2n}{\\frac{n - \\sum_{j=1}^{n}y_j}{2n}}=\\frac{\\sum_{i=0}^{n-1}(TPR_{i+1} + TPR_i)-n}{n - \\sum_{j=1}^{n}y_j}.\\]\nOr by omitting conversions:\n\\[GINI=\\frac{\\sum_{i=0}^{n-1}(TPR_{i+1} + TPR_i)-n}{n - \\sum_{j=1}^{n}y_j}.\\]"
  },
  {
    "objectID": "machine_learning/metrics/GINI.html#gini-auc_roc-relation",
    "href": "machine_learning/metrics/GINI.html#gini-auc_roc-relation",
    "title": "GINI coeficient",
    "section": "\\(GINI\\) & \\(AUC_{roc}\\) relation",
    "text": "\\(GINI\\) & \\(AUC_{roc}\\) relation\n\nAll of the above refers to the basic definition of the \\(GINI\\) coefficient. But today in kaggle and in the industry, \\(GINI\\) is more often expressed through \\(AUC_{roc}\\):\n\\[GINI = 2(AUC_{roc} - 0.5) = 2AUC_{roc} - 1 \\tag{3.1}\\]\nIn my practice I encountered a situation when I was counting GINI using the formula familiar today, and my colleagues used a more classical formula, as a result of which there were conflicts in which I was accused of using the wrong formula. Therefore, I will further prove that both approaches to calculating GINI always lead to the same result.\nSo we need to prove the statement \\((3.1)\\).\nLet us denote at once that \\(FPR_0=TPR_0=0\\) - values that meet the maximum threshold where none of the values can be assigned to a positive class.\nLet’s write the area under the ROC of the curve, which will be the exponent \\(AUC_{roc}\\):\n\\[AUC_{roc} = \\sum_{i=0}^{n-1} (FPR_{i+1} - FPR_i)(TPR_{i+1} + TPR_i)/2.\\]\n\\[ 2\\left[\\sum_{i=0}^{n-1} (FPR_{i+1} - FPR_i)(TPR_{i+1} + TPR_i)/2\\right] -1 =\n\\frac{\\sum_{i=0}^{n-1}(TPR_{i+1} + TPR_i)-n}{n - \\sum_{j=1}^{n}y_j} \\tag{3.2}.\\]\nWe will work with the left part of the identity:\n\\[\n\\left[\\sum_{i=0}^{n-1} (FPR_{i+1} - FPR_i)(TPR_{i+1} + TPR_i)\\right] -1 \\tag{3.3}\n\\]\nLet’s discuss the properties of expression:\n\\[\n(FPR_{i+1} - FPR_i)\n\\]\n\\(FPR\\) (the fraction of false positive predictions) increases only for predictions without the manifestation of the trait. And, for an observation without the manifestation of the trait, it increases by the fraction that is occupied by one observation without manifestation of the trait:\n\\[\n(FPR_{i+1} - FPR_i)=\\begin{cases}\n    0, y_i=1;\\\\\n    \\frac{1}{n-\\sum_j^n y_j}, y_i=0.\n\\end{cases}\n\\]\nWhere \\((n-\\sum_j^n y_j)\\) - number of observations without the trait, тогда \\((\\frac{1}{n-\\sum_j^n y_j})\\) - the proportion of one observation in observations with a manifestation of the trait.\nThen the expression \\((3.3)\\) can be rewritten as follows:\n\\[\n\\left[\\sum_{i|y_{i+1}=0} \\frac{1}{n-\\sum_{j=1}^n y_j}(TPR_{i+1} + TPR_i)\\right] -1\n\\tag{3.4}\n\\]\nThat is, summation can be performed only for terms for which \\(y_{i+1}=0\\), all other terms will be zero. Moreover, in non-zero terms one of the multipliers is a constant with respect to the summation operator.\nLet us return to expression \\((3.2)\\) and put the results \\((3.4)\\) into it.\n\\[\n\\left[\\sum_{i|y_{i+1}=0} \\frac{1}{n-\\sum_{j=1}^n y_j}(TPR_{i+1} + TPR_i)\\right] -1 = \\frac{\\sum_{i=0}^{n-1}(TPR_{i+1} + TPR_i)-n}{n - \\sum_{j=1}^{n}y_j}\n\\Leftrightarrow\n\\]\n\\[\n\\Leftrightarrow\n\\left[\\sum_{i|y_{i+1}=0} \\frac{1}{n-\\sum_{j=1}^n y_j}(TPR_{i+1} + TPR_i)\\right] -1 =\n\\left[\\sum^{n-1}_{i=0}\n\\frac{1}{n - \\sum_{j=1}^{n}y_j}(TPR_{i+1}+TPR_i)\n\\right] - \\frac{n}{n - \\sum_{j=1}^{n}y_j}\n\\Leftrightarrow\n\\]\n\\[\n\\Leftrightarrow\n\\left[\\sum^{n-1}_{i=0}\n    \\frac{1}{n - \\sum_{j=1}^{n}y_j}(TPR_{i+1}+TPR_i)\n    \\right] -\n    \\left[\\sum_{i|y_{i+1}=0} \\frac{1}{n-\\sum_{j=1}^n y_j}(TPR_{i+1} + TPR_i)\\right] -\n    \\frac{n}{n - \\sum_{j=1}^{n}y_j} + 1\n= 0\n\\]\nPay attention to the expressions in square brackets - they completely coincide, only the number of summation components differs, i.e. after the subtraction only those components that are not in the subtractor will remain:\n\\[\n\\left[\n    \\sum_{i|y_{i+1}=1} \\frac{1}{n-\\sum_{j=1}^n y_j}(TPR_{i+1} + TPR_i)\n\\right] -\n    \\frac{n}{n - \\sum_{j=1}^{n}y_j} + 1\n= 0\n\\tag{3.5}\\]\nLet us discuss the interim results. We have shown that expression \\((3.2)\\) which we need to prove is equivalent to expression \\((3.5)\\), so by proving expression \\((3.5)\\) we arrive at the truth of expression \\((3.2)\\).\nLet us perform a number of transformations on the expression \\((3.5)\\).\n\\[\n\\left[\n    \\sum_{i|y_{i+1}=1} \\frac{1}{n-\\sum_{i=1}^n y_i}(TPR_{i+1} + TPR_i)\n\\right] -\n    \\frac{n}{n - \\sum_{i=1}^{n}y_i} + 1\n= 0 \\Leftrightarrow\n\\]\n\\[\n\\Leftrightarrow\n\\left[\n    \\sum_{i|y_{i+1}=1} \\frac{1}{n-\\sum_{i=1}^n y_i}(TPR_{i+1} + TPR_i)\n\\right] -\n    \\frac{\\sum_{i=1}^{n}y_i}{n - \\sum_{i=1}^{n}y_i}\n= 0\n\\Leftrightarrow\n\\]\n\\[\n\\Leftrightarrow\n\\frac{1}{n-\\sum_{i=1}^n y_i}\n\\left\\{\n    \\left[\n        \\sum_{i|y_{i+1}=1} (TPR_{i+1} + TPR_i)\n    \\right] -\n       \\sum_{i=1}^{n}y_i\n\\right\\}\n= 0\n\\]\nGiven that the expression \\(\\frac{1}{n-\\sum_{i=1}^n y_i}\\) is non-negative. To fulfil the last identity it is necessary that:\n\\[\n\\left[\n    \\sum_{i|y_{i+1}=1} (TPR_{i+1} + TPR_i)\n\\right] -\n   \\sum_{i=1}^{n}y_i\n=0 \\tag{3.6}\\]\nConsider the sum in square brackets:\n\\[\\sum_{i|y_{i+1}=1} (TPR_{i+1} + TPR_i)\\]\nLet’s rewrite it in a simpler way, but keeping in mind that the summation is carried out only on observations with the manifestation of the trait:\n\\[\\sum_{i=0}^{m-1} (TPR_{i+1} + TPR_i)\\]\nWhere \\[m=\\sum_{i=1}^{n}y_i \\tag{3.7}\\]\nNow recall that \\(TPR\\) is the proportion of customers, with manifestation, of the trait for whom the manifestation of the trait was predicted. And it can be written as:\n\\[TPR_i=\\frac{i}{m} \\tag{3.8}\\]\nReturning to the identity \\((3.6)\\) and using the results \\((3.7)\\) and \\((3.8)\\) we obtain:\n\\[\n\\left[\n\\sum_{i=0}^{m-1} \\left(\\frac{i+1}{m} + \\frac{i}{m}\\right)\n\\right]\n-m=0\n\\Leftrightarrow\n\\sum_{i=0}^{m-1} \\left(\\frac{2i+1}{m}\\right)=m\n\\]\nFinally:\n\\[\n\\sum_{i=0}^{m-1} 2i+1 = m^2\n\\]\nBy proving this identity we prove that the whole chain of identities above is fulfilled. There is already a very similar proof presented here. Let us give a similar proof for our example.\nLet’s spell out the expression on the left:\n\\[\\sum_{i=0}^{m-1} 2i+1 = 1 + 3 + 5 + ... + 2(m-1)+1.\\]\nThere are not enough even numbers in the summation add and subtract them:\n\\[\\sum_{i=0}^{m-1} 2i+1 = \\{1 + 3 + 5 + ... + [2(m-1)+1]\\} + \\{2 + 4 + 6 + ... + 2(m-1)\\} - \\{2 + 4 + 6 + ... + 2(m-1)\\}.\\]\nWe combine and conjugate the components of the first and second curly brackets and take 2 out of the second brackets:\n\\[\\sum_{i=0}^{m-1} 2i+1 = \\{1 + 2 + 3 + 4 + ... + 2(m-1) + [2(m-1)+1]\\} - 2\\{1 + 2 + 3 + ... + (m-1)\\}.\\]\nReturning to the summation operator we obtain:\n\\[\\sum_{i=0}^{m-1} 2i+1 = \\left[\\sum_{i=1}^{2(m-1)+1}i\\right] - 2\\left[\\sum_{i=1}^{m-1}i\\right]. \\tag{3.9}\\]\nIn the resulting expression, an expression of the form occurs twice:\n\\[\\sum_{i=1}^\\nu i.\\]\nHere you can find that:\n\\[2\\sum_{i=1}^\\nu i = \\sum_{i=1}^\\nu i + \\sum_{i=1}^\\nu i = [1 + 2 + ... + (\\nu-1) + \\nu] + [\\nu + (\\nu-1) + ... + 2 + 1]=\\] \\[=(\\nu+1) + (\\nu-1+2) + ... + (2 + \\nu - 1) + (\\nu+1)=\\] \\[=(\\nu+1) + (\\nu+1) + ... + (\\nu+1)=\\] \\[=\\sum_{i=1}^{\\nu} (\\nu+1) = \\nu(\\nu+1)\\]\nSo:\n\\[2\\sum_{i=1}^\\nu i = \\nu(\\nu+1) \\Leftrightarrow \\sum_{i=1}^\\nu i = \\frac{\\nu(\\nu +1)}{2}\\]\nUsing the last results in \\((3.9)\\): \\[\n\\sum_{i=0}^{m-1} 2i+1 =\n\\] \\[\n=\\frac{[2(m-1)+1]([2(m-1)+1] + 1)}{2} - 2\\frac{[m-1]([m-1]+1)}{2}=.\n\\] \\[\n=\\frac{2m[2m+1]}{2} - [m-1]([m-1]+1)=\n\\] \\[\n=m[2m+1] - m[m-1] =\n\\] \\[\n= 2m^2+m-m^2-m=\n\\] \\[\n= m^2\n\\]\nFinally:\n\\[\\sum_{i=0}^{m-1} 2i+1 = m^2\\]\nThus the identity \\((3.6)\\) is fulfilled, followed by the correctness of the identity \\((3.5)\\) and followed by the correctness of the identity \\((3.1)\\) \\(\\boxtimes\\)."
  },
  {
    "objectID": "machine_learning/metrics/chisquare.html",
    "href": "machine_learning/metrics/chisquare.html",
    "title": "Knowledge",
    "section": "",
    "text": "Разбор статистики \\(\\chi^2\\)\nИсточники: - расстояние Пирсона от Анатолия Карпова; - анализ таблиц сопряженности от Анатолия Карпова."
  },
  {
    "objectID": "machine_learning/data_transformations/tf_idf.html",
    "href": "machine_learning/data_transformations/tf_idf.html",
    "title": "TF-IDF",
    "section": "",
    "text": "TF-IDF is a method for extracting features for machine learning models from textual information.\nfrom math import log\n\nfrom collections import Counter\nfrom IPython.display import HTML\nimport latex2mathml.converter\n\n\n# set of phrases that I'll be using as the example at this page\nphrases = [\n    'a penny saved is a penny earned',\n    'the quick brown fox jumps over the lazy dog',\n    'beauty is in the eye of the beholder',\n    'early to bed and early to rise makes a man healthy wealthy and wise',\n    'give credit where credit is due',\n    \"if at first you don't succeed try try again\",\n    'justice delayed is justice denied',\n    'keep your friends close and your enemies closer',\n    'no pain no gain',\n    'quickly come quickly go',\n    'united we stand divided we fall',\n    'when in rome do as the romans do'\n]"
  },
  {
    "objectID": "machine_learning/data_transformations/tf_idf.html#tf---term-frequency",
    "href": "machine_learning/data_transformations/tf_idf.html#tf---term-frequency",
    "title": "TF-IDF",
    "section": "TF - term frequency",
    "text": "TF - term frequency\nTerm frequency is a metric for words in any next. It can be calculated using a formula:\n\\[tf(t,d)=\\frac{n_t}{\\sum_i n_i}\\]\nWhere:\n\n\\(t\\) - some word;\n\\(d\\) - some text;\n\\(n_t\\) - number of occurrences of word \\(t\\) in document \\(d\\);\n\\(\\sum_i n_i\\) - number of words in text \\(d\\).\n\nSo in the following cell I calculate the term frequencies of the words for some phrases. The result here is a table that contains original phrase and Term frequency, which for each word from the Original phrase corresponds to \\(tf\\) in the form &lt;word&gt;-&lt;tf&gt;.\nSo let’s take the logic of the first phrase - “a penny saved is a penny earned” - one step at a time:\n\nTotal count of words - \\(\\sum_i n_i = 7\\);\nYou can find the word “a” twice in the phrase so - \\(n_{'a'} = 2 \\Rightarrow tf('a')=\\frac{2}{7} \\approx 0.29\\);\nYou can find the word “penny” twice in the phrase so - \\(n_{'penny'}=2 \\Rightarrow tf('penny')= \\frac{2}{7} \\approx 0.29\\);\nAll other words occur once so \\(tf\\) for them can me computed as \\(\\frac{1}{7} \\approx 0.14\\).\n\n\nhtml_table = \"&lt;tr&gt;&lt;th&gt;Original phrase&lt;/th&gt;&lt;th&gt;Terms frequency&lt;/th&gt;&lt;/tr&gt;\"\ntf_dict = {}\n\nfor p in phrases:\n\n    words_in_phrase = dict(Counter(p.split()))\n    words_count = sum(words_in_phrase.values())\n    phrase_tfs = {word:number/words_count for word, number in words_in_phrase.items()}\n    tf_dict[p] = phrase_tfs\n    \n    tf_dict\n    counts_line = \"&lt;br&gt;\".join(\n        [\n            key + \" - \" + str(round(value, 2)) \n            for key, value in phrase_tfs.items()\n        ]\n    )\n    html_table += f\"&lt;tr&gt;&lt;td&gt;{p}&lt;/td&gt;&lt;td&gt;{counts_line}&lt;/td&gt;&lt;/tr&gt;\"\n\nHTML(\"&lt;table&gt;\" + html_table + \"&lt;/table&gt;\")\n\n\n\n\n\n\n\n\nOriginal phrase\nTerms frequency\n\n\na penny saved is a penny earned\na - 0.29\npenny - 0.29\nsaved - 0.14\nis - 0.14\nearned - 0.14\n\n\nthe quick brown fox jumps over the lazy dog\nthe - 0.22\nquick - 0.11\nbrown - 0.11\nfox - 0.11\njumps - 0.11\nover - 0.11\nlazy - 0.11\ndog - 0.11\n\n\nbeauty is in the eye of the beholder\nbeauty - 0.12\nis - 0.12\nin - 0.12\nthe - 0.25\neye - 0.12\nof - 0.12\nbeholder - 0.12\n\n\nearly to bed and early to rise makes a man healthy wealthy and wise\nearly - 0.14\nto - 0.14\nbed - 0.07\nand - 0.14\nrise - 0.07\nmakes - 0.07\na - 0.07\nman - 0.07\nhealthy - 0.07\nwealthy - 0.07\nwise - 0.07\n\n\ngive credit where credit is due\ngive - 0.17\ncredit - 0.33\nwhere - 0.17\nis - 0.17\ndue - 0.17\n\n\nif at first you don't succeed try try again\nif - 0.11\nat - 0.11\nfirst - 0.11\nyou - 0.11\ndon't - 0.11\nsucceed - 0.11\ntry - 0.22\nagain - 0.11\n\n\njustice delayed is justice denied\njustice - 0.4\ndelayed - 0.2\nis - 0.2\ndenied - 0.2\n\n\nkeep your friends close and your enemies closer\nkeep - 0.12\nyour - 0.25\nfriends - 0.12\nclose - 0.12\nand - 0.12\nenemies - 0.12\ncloser - 0.12\n\n\nno pain no gain\nno - 0.5\npain - 0.25\ngain - 0.25\n\n\nquickly come quickly go\nquickly - 0.5\ncome - 0.25\ngo - 0.25\n\n\nunited we stand divided we fall\nunited - 0.17\nwe - 0.33\nstand - 0.17\ndivided - 0.17\nfall - 0.17\n\n\nwhen in rome do as the romans do\nwhen - 0.12\nin - 0.12\nrome - 0.12\ndo - 0.25\nas - 0.12\nthe - 0.12\nromans - 0.12"
  },
  {
    "objectID": "machine_learning/data_transformations/tf_idf.html#idf---inverse-document-frequency",
    "href": "machine_learning/data_transformations/tf_idf.html#idf---inverse-document-frequency",
    "title": "TF-IDF",
    "section": "IDF - inverse document frequency",
    "text": "IDF - inverse document frequency\nFor each word of a text from the given set of texts. It can be calculated using fromula:\n\\[idf(t,D)=log \\frac{\\left| D \\right|}{\\left| \\left\\{ d_i \\in D | t \\in d_i \\right\\} \\right|};\\]\nWhere:\n\n\\(D\\) - set of texts;\n\\(\\left| A \\right|\\) - number of elements in the set A;\n\\(\\left| \\left\\{ d_i \\in D | t \\in d_i \\right\\} \\right|\\) - number of documents \\(d_i\\) from set \\(D\\) that contains word \\(t\\);\nNote the denominator of the formula contains exactly the number of documents in which the word is included, not the number of occurrences of the word in any documents it’s always true \\(\\left| D \\right| \\geq \\left| \\left\\{ d_i \\in D | t \\in d_i \\right\\} \\right| \\Rightarrow idf(t,D) \\geq 0\\).\n\nSo in the following cell there is an example of calculating \\(idf\\) for a set of texts. It’s displayed like a table that contains all the words from a set of texts and for each word it calculates the occurrences of the word in the set of texts and it’s \\(idf\\).\nLet’s take word “the” for example it occurs in 3 of 12 texts so it’s \\(idf=log(\\frac{12}{5}) \\approx 1.39\\).\n\nphrases_number = len(phrases)\nword_in_documents = Counter([w for p in phrases for w in set(p.split())])\nwords_idf = {}\n\n# we need to transform directry to MathJax here.\n# because quarto doesn't recognise $$ patterns in\n# output cells\nmath_jax_expression = latex2mathml.converter.convert(\n    \"\\left|\\left\\{d_i \\in D | t \\in d_i\\\\right\\}\\\\right|\"\n)\n\nhtml_table = (\n    \"&lt;tr&gt;&lt;th&gt;Word&lt;/th&gt;\"\n    f\"&lt;th&gt;{math_jax_expression}&lt;/th&gt;\"\n    \"&lt;th&gt;Inverse document frequency&lt;/th&gt;&lt;/tr&gt;\"\n)\n\nfor word, number in word_in_documents.items():\n\n    idf = log(phrases_number/number)\n    words_idf[word] = idf\n    \n    html_table += (\n        f\"&lt;tr&gt;&lt;td&gt;{word}&lt;/td&gt;\"\n        f\"&lt;td&gt;{number}&lt;/td&gt;\"\n        f\"&lt;td&gt;{round(idf, 2)}&lt;/td&gt;&lt;/tr&gt;\"\n    )\n\nHTML(\"&lt;table&gt;\" + html_table + \"&lt;/table&gt;\")\n\n\n\n\nWord\n\\(\\left| \\left\\{ d_{i} \\in D|t \\in d_{i} \\right\\} \\right|\\)\nInverse document frequency\n\n\nsaved\n1\n2.48\n\n\na\n2\n1.79\n\n\npenny\n1\n2.48\n\n\nearned\n1\n2.48\n\n\nis\n4\n1.1\n\n\nover\n1\n2.48\n\n\njumps\n1\n2.48\n\n\nbrown\n1\n2.48\n\n\ndog\n1\n2.48\n\n\nfox\n1\n2.48\n\n\nquick\n1\n2.48\n\n\nlazy\n1\n2.48\n\n\nthe\n3\n1.39\n\n\nbeauty\n1\n2.48\n\n\nbeholder\n1\n2.48\n\n\nin\n2\n1.79\n\n\nof\n1\n2.48\n\n\neye\n1\n2.48\n\n\nearly\n1\n2.48\n\n\nmakes\n1\n2.48\n\n\nman\n1\n2.48\n\n\nhealthy\n1\n2.48\n\n\nwealthy\n1\n2.48\n\n\nrise\n1\n2.48\n\n\nbed\n1\n2.48\n\n\nto\n1\n2.48\n\n\nand\n2\n1.79\n\n\nwise\n1\n2.48\n\n\nwhere\n1\n2.48\n\n\ndue\n1\n2.48\n\n\ncredit\n1\n2.48\n\n\ngive\n1\n2.48\n\n\nyou\n1\n2.48\n\n\nagain\n1\n2.48\n\n\ndon't\n1\n2.48\n\n\nsucceed\n1\n2.48\n\n\nif\n1\n2.48\n\n\ntry\n1\n2.48\n\n\nfirst\n1\n2.48\n\n\nat\n1\n2.48\n\n\ndenied\n1\n2.48\n\n\ndelayed\n1\n2.48\n\n\njustice\n1\n2.48\n\n\nkeep\n1\n2.48\n\n\nclose\n1\n2.48\n\n\nyour\n1\n2.48\n\n\nenemies\n1\n2.48\n\n\ncloser\n1\n2.48\n\n\nfriends\n1\n2.48\n\n\npain\n1\n2.48\n\n\ngain\n1\n2.48\n\n\nno\n1\n2.48\n\n\ncome\n1\n2.48\n\n\nquickly\n1\n2.48\n\n\ngo\n1\n2.48\n\n\nfall\n1\n2.48\n\n\nunited\n1\n2.48\n\n\nwe\n1\n2.48\n\n\ndivided\n1\n2.48\n\n\nstand\n1\n2.48\n\n\nromans\n1\n2.48\n\n\nas\n1\n2.48\n\n\ndo\n1\n2.48\n\n\nwhen\n1\n2.48\n\n\nrome\n1\n2.48\n\n\n\n\n\nThis metric shows how often a word occurs in general across all texts. If the word is really common, there’s a high probability that it’s just an article, pretext or something like that - it doesn’t make much sense in the sentence. For example, let’s consider the extreme value, if you find any word \\(t'\\) in any text, it means that \\(\\frac{\\left| D \\right|}{\\left| \\left\\{ d_i \\in D | t \\in d_i \\right\\} \\right|} = 1 \\Rightarrow log \\frac{\\left| D \\right|}{\\left| \\left\\{ d_i \\in D | t \\in d_i \\right\\} \\right|} = 0\\) - the presence of the word \\(t'\\) in no way makes it possible to distinguish one entry from another."
  },
  {
    "objectID": "machine_learning/data_transformations/tf_idf.html#tf-idf",
    "href": "machine_learning/data_transformations/tf_idf.html#tf-idf",
    "title": "TF-IDF",
    "section": "TF-IDF",
    "text": "TF-IDF\n\\(ft_{idf}\\) is a final metric of the TF-IDF analysis and can be calculated as the product of TF and IDF. So for each word in each text from the set of texts we compute it’s own value \\(tf_{idf}\\). So in the following cell I combine results from two previous sections to compute \\(tf_{idf}\\). For example, for word “a” in phrase “a penny saved is a penny earned” \\(tf_{idf} = 0.29*1.79 \\approx 0.51\\).\n\nhtml_table = (\n    f\"\"\"&lt;tr&gt;\n        &lt;th&gt;Phrase&lt;/th&gt;\n        &lt;th&gt;{latex2mathml.converter.convert(\"tf_{idf}\")}&lt;/th&gt;\n    &lt;/tr&gt;\"\"\"\n)\n\nfor phrase, tfs in tf_dict.items():\n    phrase_tf_idf_line = \"&lt;br&gt;\".join([\n        (\n            word + \" - \" + \n            str(round(words_idf[word]*tf,2))\n        )\n        for word, tf in tfs.items()\n    ])\n\n    html_table += f\"&lt;tr&gt;&lt;td&gt;{phrase}&lt;/td&gt;&lt;td&gt;{phrase_tf_idf_line}&lt;/td&gt;&lt;/tr&gt;\"\nHTML(\"&lt;table&gt;\" + html_table + \"&lt;/table&gt;\")       \n\n\n\n\n\n\n\n\nPhrase\n\\(tf_{idf}\\)\n\n\na penny saved is a penny earned\na - 0.51\npenny - 0.71\nsaved - 0.35\nis - 0.16\nearned - 0.35\n\n\nthe quick brown fox jumps over the lazy dog\nthe - 0.31\nquick - 0.28\nbrown - 0.28\nfox - 0.28\njumps - 0.28\nover - 0.28\nlazy - 0.28\ndog - 0.28\n\n\nbeauty is in the eye of the beholder\nbeauty - 0.31\nis - 0.14\nin - 0.22\nthe - 0.35\neye - 0.31\nof - 0.31\nbeholder - 0.31\n\n\nearly to bed and early to rise makes a man healthy wealthy and wise\nearly - 0.35\nto - 0.35\nbed - 0.18\nand - 0.26\nrise - 0.18\nmakes - 0.18\na - 0.13\nman - 0.18\nhealthy - 0.18\nwealthy - 0.18\nwise - 0.18\n\n\ngive credit where credit is due\ngive - 0.41\ncredit - 0.83\nwhere - 0.41\nis - 0.18\ndue - 0.41\n\n\nif at first you don't succeed try try again\nif - 0.28\nat - 0.28\nfirst - 0.28\nyou - 0.28\ndon't - 0.28\nsucceed - 0.28\ntry - 0.55\nagain - 0.28\n\n\njustice delayed is justice denied\njustice - 0.99\ndelayed - 0.5\nis - 0.22\ndenied - 0.5\n\n\nkeep your friends close and your enemies closer\nkeep - 0.31\nyour - 0.62\nfriends - 0.31\nclose - 0.31\nand - 0.22\nenemies - 0.31\ncloser - 0.31\n\n\nno pain no gain\nno - 1.24\npain - 0.62\ngain - 0.62\n\n\nquickly come quickly go\nquickly - 1.24\ncome - 0.62\ngo - 0.62\n\n\nunited we stand divided we fall\nunited - 0.41\nwe - 0.83\nstand - 0.41\ndivided - 0.41\nfall - 0.41\n\n\nwhen in rome do as the romans do\nwhen - 0.31\nin - 0.22\nrome - 0.31\ndo - 0.62\nas - 0.31\nthe - 0.17\nromans - 0.31\n\n\n\n\n\nFor each record, you can perform some aggregations on the numbers received. Common aggregations are maximum and average."
  },
  {
    "objectID": "machine_learning/tree_based_methods/ensemble copy.html",
    "href": "machine_learning/tree_based_methods/ensemble copy.html",
    "title": "Баггинг",
    "section": "",
    "text": "Изучение ансамблей решающих деревьев\nИспользьвание алгоритма - решающее дерево может приводить к высокой дисперисии предсказаний. Для того, чтобы это избежать исползуются методы, которые обощают под названием ансамбли решающих деревьев. Выделяют следующие методы:\n\nBagging;\nRandom Forests;\nBoosting;\nBayesian Additive Regression Trees.\n\nИсточники\n\nISLR;\nСобрание возможностей sklearn посвященных ансамблям;\n\n\nfrom copy import copy\nimport warnings\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.ensemble import BaggingRegressor, BaggingClassifier\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n\nfrom sklearn.model_selection import cross_val_score\n\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\n\nПредполагает бутстрапирование обучающей выборки и подгонку множестава деревьев. Для получания предсказаний результаты каждого дерева аггрегируют (обычно усредняют). Вообще говоря, баггинг можно ложить не только на деревья но и на KNN модели.\n\nСравнение с единичным деревом\nСоздам выборку и на ней попробую оценить обычное дерево и баггинг на дереве. Результаты сравним.\n\nsample_size = 500\nnp.random.seed(10)\n\ndata = pd.DataFrame({\n    \"x\" : np.random.rand(sample_size)\n})\n\nf_x = lambda x: 15*x**3 - 15*x**2 + x \n\ndata[\"y\"] = f_x(data[\"x\"]) + np.random.uniform(0, 1, sample_size)\n\nДля контроля глубины дерева будет использоватся алгоритм обрезки с учетом сложности затрат. Для того, надо подобрать его парамерт \\(\\alpha\\).\n\nccp_alphas = DecisionTreeRegressor().\\\ncost_complexity_pruning_path(\n    data[[\"x\"]], data[\"y\"]\n)[\"ccp_alphas\"]\n\nclf = DecisionTreeRegressor()\nMSEs = {}\n\nfor alpha in tqdm(ccp_alphas):\n    clf.set_params(ccp_alpha = alpha)\n    MSE = cross_val_score(\n        clf, data[[\"x\"]], \n        data[\"y\"], \n        cv = 20,\n        scoring = \"neg_mean_squared_error\"\n    )\n    MSEs[alpha] = MSE\n\nMSEs = pd.DataFrame(MSEs)\n\n100%|██████████| 345/345 [00:21&lt;00:00, 15.69it/s]\n\n\n\nplt.figure(figsize = [15, 5])\n\ndef set_params():\n    plt.xlabel(\"$\\\\alpha$\")\n    plt.ylabel(\"$MSE$\")\n\nbordres = [-0.0001, 0.0035]\n\nplt.subplot(121)\nplt.plot(MSEs.columns, MSEs.mean())\nset_params()\nfor b in bordres:\n    plt.axvline(b, color = \"gray\", linestyle = \"dashed\")\nplt.title(\"Общий график\", fontsize = 14)\n\n\ncond = (MSEs.columns &gt; bordres[0]) & (MSEs.columns &lt; bordres[1])\n\nplt.subplot(122)\nplt.errorbar(MSEs.columns[cond], MSEs.mean()[cond], MSEs.std()[cond])\nplt.axvline(\n    MSEs.mean().idxmax(), \n    color = \"gray\", \n    linestyle = \"dashed\"\n)\nplt.xlim(bordres)\nset_params()\nplt.title(\"Максимум - подробно\", fontsize = 14)\nplt.xticks([MSEs.mean().idxmax()])\n\ndel set_params\n\n\n\n\nТеперь подгоням которые будем сравнивать.\n\nx_range = np.arange(0, 1, 0.005)\n\nclf = DecisionTreeRegressor(ccp_alpha=0.0076)\nclf.fit(data[[\"x\"]].to_numpy(), data[\"y\"])\ntree_predict = clf.predict(x_range[:, np.newaxis])\n\nbclf = BaggingRegressor(clf, n_estimators=200).fit(\n    data[[\"x\"]].to_numpy(), data[\"y\"]\n)\nbagging_predict = bclf.predict(x_range[:, np.newaxis])\n\nplt.figure(figsize = [15, 5])\n\nplt.subplot(121)\nplt.scatter(data[\"x\"], data[\"y\"])\nfor pred, col in zip(\n    [tree_predict, bagging_predict],\n    [\"red\", \"green\"]\n):\n    plt.plot(\n        x_range, pred,\n        color = col, linewidth = 3\n    )\n\nplt.legend([\"Наблюдения\", \"Дерево\", \"Баггинг\"])\nplt.title(\"Предсказания моделей\")\nplt.xlabel(\"x\")\n\nplt.subplot(122)\nplt.boxplot((\n    (f_x(x_range) - tree_predict)**2, \n    (f_x(x_range) - bagging_predict)**2)\n)\n\nplt.title(\"Отклонениние модели от реального закона распределения\")\nplt.ylabel(\"$RS$\")\nans = plt.xticks([1,2], [\"Дерево\", \"Баггинг\"])\n\n\n\n\nВсе складывается так, что баггированная модель “сглаживает” неготорые скачки, которые проявляются в одиночном дереве. В результате квадраты остатков (которые описывает правый график) более тучные для баггинга - меньше болших оклонений вверх но и нижняя граница выше.\n\n\nСлучайный лес (Random Forest)\nИдея как у баггинга, но, кроме того, что каждая подвыборка содержит только \\(m\\) из \\(p\\) случайных показателей. Обычно берут \\(m = \\sqrt{p}\\). А в остальном особой разницы нет.\nЕсли в выборке есть очень сильные показатели, то первое деление дерева производится часто именно по нему. В результате простой баггинг от подвыборки к подвыборке имеет очень похожие результаты. Случайное дерево справляется с этой ситуацией лучше.\n\n\nOOB\nOOB - out of bag. Способ оценки качества ансамблей моделей, при котором, оценка метрики качества модели производится на том “кусочке” выборке который не был задейсвован при обучении конкретной модели. Затем полученные величины, собранные со всей выборки агрегируют - полученное число и становится результатом.\n\nПример на котором будем экспериментировать\n\n# делаю пример классификационной задачи\nnp.random.seed(50)\n\n# vector of coefs of the discriminating curve (as a polynom)\ncoefs = np.array([0.40740741, -4.44444444, 13.88888889, -9.25925926])\n\ndef compute_poly_for_np_array(array, coefs):\n    '''\n        Calculation of the polynomial for an numpy array.\n        Inputs:\n            array - an array of any dimension to be used as an argument of the polynom;\n            coefs - polynomial coefficients (i-th for i degree);\n        Output numpy.array with gived same dimentions as array.\n    '''\n    \n    poly_degree = len(coefs)\n    return np.sum(\n        [coefs[i]*(array**i) for i in range(poly_degree)], \n        axis = 0\n    )\n\n# transition speed from one class to another\ntrans_speed = 4\ndef prob_for_bayes(x1, x2):\n    '''\n        This function describes a Bayesian \n        law that is used to divide into classes.\n        Inputs:\n            x1 - coordinate of first variable;\n            x2 - coordiante of second variable;\n        Output:\n            p - probability that observation takes class 1 \n                for given coordinates.\n    '''\n    return 1/(1 + np.exp(trans_speed*(compute_poly_for_np_array(x1, coefs) - x2)))\n\n\n\ndef create_sample():\n    '''\n        Создать случайную выборку, в соответсвии с\n        заданным законом распределения\n    '''\n\n    df = pd.DataFrame(\n        np.random.rand(1000, 2),\n        columns = [\"x1\", \"x2\"]\n    )\n    df[\"prob\"] = prob_for_bayes(df[\"x1\"], df[\"x2\"])\n    df[\"class\"] = df[\"prob\"].apply(\n        lambda p: np.random.choice([0, 1], p = [1-p, p])\n    )\n\n    return df\n\n\n# создание выборки - примера\nnp.random.seed(50)\ndf = create_sample()\n\n# создание сетки\nx1_range = np.linspace(0, 1, 500)\nx2_range = np.linspace(0, 1, 500)\n\nx1_mesh, x2_mesh = np.meshgrid(x1_range, x2_range)\nmesh_df = pd.DataFrame({\n    \"x1\" : x1_mesh.ravel(), \"x2\" : x2_mesh.ravel()\n})\n\nreal_prob = prob_for_bayes(mesh_df[\"x1\"], mesh_df[\"x2\"]).to_numpy()\nreal_prob = real_prob.reshape(x1_mesh.shape)\n\n\nplt.figure(figsize = [15, 7])\n\nplt.subplot(121)\nplt.imshow(real_prob, origin = \"lower\", extent = [0, 1, 0, 1])\nplt.title(\"Дейсвительное распредлеление\")\nplt.xlabel(\"$x_1$\"); plt.ylabel(\"$x_2$\");\n\nplt.subplot(122)\nplt.scatter(df[\"x1\"], df[\"x2\"], c=df[\"class\"])\nplt.xlabel(\"$x_1$\"); plt.ylabel(\"$x_2$\");\nans = plt.title(\"Реализация распределения\")\n\n\n\n\n\n\nРеализация в sklearn\nsklearn умеет OOB, но из метрик доступна только accuracy (доля правильно предсказанных), это тупо захардкожено тут. Например, можно провести oob для баггинга используя параметр oob_score = True констуктора класса BaggingClassifier. Ну а после обучения получить поле oob_score_ полученного объекта.\n\nclf = BaggingClassifier(\n    DecisionTreeClassifier(ccp_alpha = 0.01),\n    n_estimators=30,\n    oob_score=True,\n    random_state=50,\n).fit(df[[\"x1\", \"x2\"]], df[\"class\"])\n\nclf.oob_score_\n\n0.747\n\n\nИнтересное поле - oob_decision_function_. В исходние получается, что это среднее предсказание для каждого наблюдения, из всех его попаданий вне обучающей выборки. На нем при желании можно оценивать AUC.\n\nclf.oob_decision_function_\n\narray([[0.47801078, 0.52198922],\n       [0.35901744, 0.64098256],\n       [0.12722208, 0.87277792],\n       ...,\n       [0.15777634, 0.84222366],\n       [0.89753659, 0.10246341],\n       [0.86331456, 0.13668544]])\n\n\nИнтерестно, что если для каждого наблюдения сформировать предсказание и сравнить среднюю точность с oob_score_, то всегда будет получаться одно и тоже число. Хотя в коде sklearn oob_score_ вычисляется подругому. Если будет совсем нечего делать, можно попробовать разобраться почему так происходит.\n\n# частный случай пример \n(df[\"class\"].to_numpy() == np.argmax(clf.oob_decision_function_, axis=1)).mean()\n\n0.747\n\n\n\n# 20 других частных случаев\nlst = []\n\nfor i in range(20):\n    temp_df = create_sample()\n\n    temp_clf = BaggingClassifier(\n        DecisionTreeClassifier(ccp_alpha = 0.01),\n        n_estimators=30,\n        oob_score=True,\n        random_state=50,\n    ).fit(temp_df[[\"x1\", \"x2\"]], temp_df[\"class\"])\n\n    comp = (\n        temp_df[\"class\"].to_numpy() == \\\n        np.argmax(temp_clf.oob_decision_function_, axis=1)\n    ).mean()\n\n    lst.append(comp == temp_clf.oob_score_)\n\nall(lst)\n\nTrue\n\n\n\n\nСравнение cv и oob\n\ndef fit_estimate(X, y, ccp, n_estimators):\n\n    # приходится отключить warnings потому, что bagging\n    # ругается в случае малого числа оценщиков\n    warnings.filterwarnings(\"ignore\", category=UserWarning)\n    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n    clf = BaggingClassifier(\n        DecisionTreeClassifier(ccp_alpha = ccp),\n        n_estimators=n_estimators,\n        oob_score=True,\n        random_state=50,\n    )\n\n    cv_score = cross_val_score(\n        clf, X, y, scoring=\"accuracy\", cv = 20\n    )\n    res = cv_score.mean(), clf.fit(X,y).oob_score_\n    warnings.filterwarnings(\"ignore\", category=UserWarning)\n    warnings.filterwarnings(\"ignore\", category=RuntimeWarning\n\n    return res\n\ndef get_best_model(scores):\n    '''\n        Получить лучшую модель.\n        Под лучшей понимается модель, с самым высоким \n        значением целевой метрики, но при этом с \n        минимальными возможными числом оценщиков \n        и глубиной дерева.\n\n        Input:\n            scores - pandas.Series который содержит занчения целевой\n                     метрики, а в колонках у него:\n                        \"n_estimators\" - число оценщиков соответсвующей модели;\n                        \"ccp\" - параметр обрезки дерева.\n        Output - та строка из scores в которой заданы параметры\n                 наилучшей модели.\n    '''\n\n    main_val = scores.name\n    return scores.reset_index().sort_values(\n        [main_val, \"n_estimators\", \"ccp\"], \n        ascending=[False, True, False]\n    ).iloc[0]\n\n# создание тех комбинаций\n# параметров модели которое нужно\n# осчитать\nccp_alphas = DecisionTreeRegressor().\\\ncost_complexity_pruning_path(\n    df[[\"x1\", \"x2\"]], df[\"class\"]\n)[\"ccp_alphas\"]\n\nccp_alphas = np.linspace(ccp_alphas.min(), ccp_alphas.max(), 10)\nestimator_counts = range(30, 70)\n\ncombinations = pd.DataFrame(\n    [col.ravel() for col in np.meshgrid(ccp_alphas, estimator_counts)],\n    index=[\"ccp\", \"n_estimators\"]\n).T\n\nSyntaxError: '(' was never closed (1082649026.py, line 19)\n\n\nval_ress = combinations.apply( lambda x: fit_estimate( df[[“x1”, “x2”]], df[“class”], x[“ccp”], int(x[“n_estimators”]) ), axis = 1 )\nval_ress = val_ress.apply( lambda row: pd.Series( row, index = [“cross validation”, “OOB”] ) ) val_ress.index = pd.MultiIndex.from_frame(combinations)\nval_ress.to_csv(“cv_vs_oob.csv”)\n\nval_ress = pd.read_csv(\"cv_vs_oob.csv\", index_col=[0, 1])\n\n\nplt.scatter(val_ress[\"cross validation\"], val_ress[\"OOB\"])\nplt.xlabel(\"CV accuracy\")\nplt.title(\"OOB против CV\")\nans = plt.ylabel(\"OOB accuracy\")\n\n\n\n\n\npd.concat(\n    [\n        get_best_model(val_ress[\"cross validation\"]),\n        get_best_model(val_ress[\"OOB\"])\n    ],\n    axis=1\n)\n\n\n\n\n\n\n\n\n281\n201\n\n\n\n\nccp\n0.006929\n0.006929\n\n\nn_estimators\n58.000000\n50.000000\n\n\ncross validation\n0.763000\nNaN\n\n\nOOB\nNaN\n0.756000\n\n\n\n\n\n\n\n\n\n\nООб\n\nheart = pd.read_csv(\"Heart.csv\", index_col=0)\n\ny = (heart[\"AHD\"] == \"Yes\").astype(\"int32\")\nX = pd.get_dummies(heart.drop(\"AHD\", axis=1))\n\nX_train = X.iloc[:152]\nX_test = X.loc[~X.index.isin(X_train.index)]\ny_train = y.loc[X_train.index]\ny_test = y.loc[X_test.index]\n\nclf = RandomForestClassifier()\nfor tree_count in range(1,301):\n    clf.set_params()\n\n\n\n\n\n\n\n\nAge\nSex\nRestBP\nChol\nFbs\nRestECG\nMaxHR\nExAng\nOldpeak\nSlope\nCa\nChestPain_asymptomatic\nChestPain_nonanginal\nChestPain_nontypical\nChestPain_typical\nThal_fixed\nThal_normal\nThal_reversable\n\n\n\n\n153\n67\n0\n115\n564\n0\n2\n160\n0\n1.6\n2\n0.0\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n154\n55\n1\n160\n289\n0\n2\n145\n1\n0.8\n2\n1.0\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n155\n64\n1\n120\n246\n0\n2\n96\n1\n2.2\n3\n1.0\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n156\n70\n1\n130\n322\n0\n2\n109\n0\n2.4\n2\n3.0\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n157\n51\n1\n140\n299\n0\n0\n173\n1\n1.6\n1\n0.0\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n299\n45\n1\n110\n264\n0\n0\n132\n0\n1.2\n2\n0.0\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\n\n\n300\n68\n1\n144\n193\n1\n0\n141\n0\n3.4\n2\n2.0\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n301\n57\n1\n130\n131\n0\n0\n115\n1\n1.2\n2\n1.0\nTrue\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n302\n57\n0\n130\n236\n0\n2\n174\n0\n0.0\n2\n1.0\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n303\n38\n1\n138\n175\n0\n0\n173\n0\n0.0\n1\nNaN\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n151 rows × 18 columns"
  },
  {
    "objectID": "machine_learning/var_vs_bias/var_vs_bias.html",
    "href": "machine_learning/var_vs_bias/var_vs_bias.html",
    "title": "Введение",
    "section": "",
    "text": "Компромисс дисперсии и смещения\nВ этом notebook я надеюсь подробно разобрать вопрос компромиса между смещением (bias) и дисперсией (variance) в машинном обучении.\nВдохновлено соотсветсвующим разделом в ISLR.\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\n\nimport scipy.interpolate as inter\n\nn = 1000\n\nУтверждается что средне квадратическую ошибку модели (Mean Square Error, MSE) можно разложить на три составляющие: - Дисперсию; - Квардрат смещения; - Неустранимую ошибку.\nИли записывая серез формулу:\n\\[\\mathbb{E}(y_0 - \\hat{f}(x_0))^2 = Var(\\hat{f}(x_0)) + [Bias(\\hat{f}(x_0))]^2 + Var(\\varepsilon). \\tag{1}\\]\nГде: - \\(\\hat{f}(x)\\) - некоторая модель описывающая данные; - \\(x_0\\) - некоторое произвольное контрольное значение предикторов; - \\(y_0\\) - некоторое произвольное контрольное значение оклика; - \\(\\varepsilon\\) - неустранимая ошибка модели (irreducible error); в нее вкладывается та информация о описываемом процессе, которой нет в полученной выборке; - \\(Var(\\hat{f}(x_0))\\) - дисперсия полученной модели; эта величина должна быть тем больше, чем сильнее предстказания модели будут меняться от измениния выбоки; - \\(Bias(\\hat{f}(x_0))\\) - смещение модели эта величина тем меньше, чем точнее модель подогнанна к описываемому просцессу; - \\(Var(\\varepsilon)\\) - дисперсия неустранимой ошибки; чем полнее и точнее наши данные, тем меньше эта величина; - \\(\\mathbb{E}(y_0 - \\hat{f}(x_0))^2\\) - математическое ожидание ошибки полученной модели.\nВ различных источниках к этой теме очень часто прикладывают график подобный этому:\n\nx_plot_range = np.arange(0,0.8, 0.05)\nvariance = 3*(x_plot_range)**2 + x_plot_range\nbias = 3*(x_plot_range)**2 - 4.8*x_plot_range + 2\nirrecible_error = 1\nmodel_error = variance + bias + irrecible_error\n\nplt.figure(figsize = [10, 6])\nplt.plot(x_plot_range, variance)\nplt.plot(x_plot_range, bias)\nplt.axhline(irrecible_error, color = \"gray\", linestyle = \"dashed\")\nplt.plot(x_plot_range, model_error)\n\nplt.xticks([])\nplt.yticks([])\n\nplt.xlabel(\"Сложность модели\", fontsize = 15)\nplt.ylabel(\"Ошибка\", fontsize = 15)\n\nans = plt.legend([\n    \"$Var(\\hat{f}(x_0))$\",\n    \"$Bias(\\hat{f}(x_0))$\",\n    \"$Var(\\\\varepsilon)$\",\n    \"$\\mathbb{E}(y_0 - \\hat{f}(x_0))^2$\"\n])\n\n\n\n\nДалее я на примере простой задачи регрессии попытаюсь провести такой вычислитеньный эксперимент, который приведет именно к такому графику.\n\nИдея эксперимента\nПусть имеется некоторе уравнение которое обисывает некоторых достаточно сложный полином. В следующей ячейке происходит его генерация.\n\nx_p = np.array([0, 1, 2, 3, 4])\ny_p = np.array([2, 4, 3, 5, 4])\npoly = inter.lagrange(x_p, y_p)\n\nИтак, полученный полином записывается в форме:\n\n\\[f(x)=-0.0002+0.0105x^{1}-0.2263x^{2}+2.7223x^{3}-20.1388x^{4}+94.3996x^{5}-278.1615x^{6}+491.6866x^{7}-466.4732x^{8}+178.181x^{9}+2.0x^{10}\\]\n\ndef f(x):\n    X = np.concatenate(\n        [(x**a)[:, np.newaxis] for a in range(len(poly.coef))],\n        axis = 1\n    )\n    return np.dot(X, poly.coef[::-1][:, np.newaxis]).ravel()\n    \nx_range = np.arange(0, 4.01, 0.01)\nans = plt.plot(x_range, f(x_range))\n\n\n\n\nПусть зачение объясняемой переменной объясняется так:\n\\(y = f(x) + \\varepsilon\\)\nПусть для нашего примера \\(\\varepsilon \\sim N(0, 0.5)\\). То есть, по определениею нормального распределения \\(Var(\\varepsilon) = 0.5^2=0.25\\). Таким образом мы можем сгенерировать множество выборок подобных \\((x_i,y_i), i \\in \\overline{1,n}\\). Некторые из них предствлены на рисунке:\n\ny = lambda x: f(x) + np.random.normal(0, 0.5, x.shape)\n\n\nnp.random.seed(20)\nsample_size = 100\nexamples_count = 3\n\nX_samples = np.random.uniform(0, 4, [sample_size, examples_count])\nY_samples = np.concatenate(\n    [\n        y(X_samples[:, col_i])[:, np.newaxis] for col_i in range(examples_count)\n    ],\n    axis = 1\n)\n\nplt.figure(figsize = [5, 10])\nfor i in range(examples_count):\n    plt.subplot(examples_count, 1, i+1)\n    plt.scatter(\n        X_samples[:, i], Y_samples[:,i]\n    )\n    plt.xlabel(\"x\");ans = plt.ylabel(\"y\")\n\n\n\n\nОбычно об \\(f(x)\\) нам ничего, кроме выборки неизвестно. Потому задачу по формированию модели можно поставить следующим образом - найти такое \\(\\hat{f}(x)\\) что-бы оно максимально походило на дейсвительное \\(f(x)\\) располагая только выборкой. Методы подгонки моделей решают задачу оптимальным (или близким к оптимальному) образом при условии, что исследователь определился с идентификационной формой модели. Таким образом, задача, обычно, сводится именно к определению идентификационной формы модели.\n\ndef get_poly_matrix(X, p = 2):\n    return np.concatenate(\n        [np.array(X)[:, np.newaxis]**(i) for i in range(p+1)],\n        axis = 1\n    )\n\ndef get_poly_model(X, y, p = 2):\n    \n    X_matr = get_poly_matrix(X, p)\n    \n    return LinearRegression(\n        fit_intercept=False\n    ).fit(X_matr, y)\n\ndef get_poly_predict(X, y, p = 2):\n    \n    X_matr = get_poly_matrix(X, p)\n    return LinearRegression(\n        fit_intercept=False\n    ).fit(X_matr, y).predict(X_matr)\n\nnp.random.seed(21)\nsample_size = 100\npoly_max = 5\n\nX_sample = np.sort(np.random.uniform(0, 4, sample_size))\nY_sample = y(X_sample)\n\nplt.figure(figsize = [10, 8])\n\nlegend_line = \"\"\nlegend_list = []\n\nfor i in range(poly_max):\n    \n    X_matr = get_poly_matrix(X_sample, p = i)\n    model = LinearRegression(\n        fit_intercept=False\n    ).fit(X_matr, Y_sample)\n    pred = model.predict(X_matr)\n    plt.plot(X_sample, pred, linewidth = 5)\n    \n    eq = \"$f(x)=\"\n    for i, c in enumerate(model.coef_.round(3)):\n        if c != 0:\n            eq += \\\n                (\"+\" if c &gt; 0 and i != 0 else \"\") + \\\n                str(c) + \\\n                (\"x^{{{}}}\".format(i) if i != 0 else \"\")\n    eq += \"$\"\n    legend_list += [eq]\n\n    \nans = plt.scatter(X_sample, Y_sample, color = \"black\")\nplt.legend(legend_list)\n\nplt.xlabel(\"x\", fontsize = 13);ans = plt.ylabel(\"y\", fontsize = 13)\n\n\n\n\n\nplt.figure(figsize = [15, 7])\n\nsample_size = 30\n\nfor i in range(3):\n    for j, p in enumerate([0,4, 10]):\n        \n        plt.subplot(3,3,i+1+3*j)\n        X_sample = np.sort(np.random.uniform(0, 4, sample_size))\n        Y_sample = y(X_sample)\n        X_matrix = np.zeros([sample_size, poly_max])\n\n        plt.scatter(X_sample, Y_sample)\n        plt.plot(\n            X_sample,\n            get_poly_predict(X_sample, Y_sample, p),\n            color = \"black\"\n        )\n\n\n\n\n\nnp.random.seed(30)\nexp_count = 2000\nsample_size = 30\npoly_max = 14\n\nX_matrix = np.zeros([sample_size, poly_max])\n# x_ij - результат i-го эксперимента для j-го полинома\nprediction = np.zeros([exp_count, poly_max])\nresidual = np.zeros([exp_count, poly_max])\n\nfor i in range(exp_count):\n    # генерирую выборку актуальную на этой итерации\n    X_sample = np.random.uniform(0, 4, sample_size)\n    Y_sample = y(X_sample)\n    X_matrix = np.zeros([sample_size, poly_max])\n    \n    # индекс того наблюдения которое будет использоваться для проверки\n    i_0 = 0\n    \n    # пробегаюсь по возможным коэффициентам полинома\n    # подгонаяю соответсвующие модели\n    for j in range(poly_max):\n        pred = get_poly_predict(X_sample, Y_sample, j)\n        prediction[i,j] = pred[i_0]\n        residual[i,j] = (pred[i_0] - Y_sample[i_0])\n\n\nplt.plot(np.var(prediction, axis = 0))\nplt.plot(np.mean(residual**2, axis = 0))\nplt.plot(np.var(prediction, axis = 0) + np.mean(residual**2, axis = 0))\n\n\n\n\n\nnp.argsort(\n    (np.var(prediction, axis = 0) + np.mean(residual**2, axis = 0))\n)\n\narray([ 0,  1, 12, 13, 11,  9, 10,  8,  3,  4,  2,  6,  7,  5])\n\n\nnp.random.seed(30) exp_count = 2000 sample_size = 100 poly_max = 14\nX_matrix = np.zeros([sample_size, poly_max]) # x_ij - результат i-го эксперимента для j-го полинома prediction = np.zeros([exp_count, poly_max]) residual = np.zeros([exp_count, poly_max])"
  },
  {
    "objectID": "machine_learning/gradient_descent.html",
    "href": "machine_learning/gradient_descent.html",
    "title": "Gradient descent",
    "section": "",
    "text": "Is a cursial method for machine learning. But in general it’s a way of optimising - finding values where the function depends on those values taking the minimum/maximum value. Here I’ll try to implement a basic variation of gradient descent using only numpy. I’ll explain each step to make everything clear.\nimport numpy as np\nimport plotly as pltly\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots"
  },
  {
    "objectID": "machine_learning/gradient_descent.html#task",
    "href": "machine_learning/gradient_descent.html#task",
    "title": "Gradient descent",
    "section": "Task",
    "text": "Task\nSuppose we have classic linear regression task - we have array of point \\((x_i, y_i)\\) just like generated and visualised in the cell below:\n\nsample_size = 200\n\nreal_alpha = 3; real_beta = 2;\n\nX = np.random.uniform(-3 , 3, sample_size)\ny = X*real_alpha + real_beta + np.random.normal(0, 3, sample_size)\n\n\ndef pltly_plot_line(alpha, beta, **kwargs):\n    line_X = np.linspace(-3,3, 10)\n    line_y = alpha*line_X + beta\n    \n    return go.Scatter(\n        x=line_X, y=line_y, \n        mode='lines', **kwargs\n    )\n\ndef get_my_scatter(X, y):\n    return go.Scatter(\n        x=X, y=y, mode='markers', \n        marker_size=10, name=\"Observations\",\n        marker_color = \"blue\"\n    )\n\ngo.Figure(\n    data = [\n        get_my_scatter(X, y),\n        pltly_plot_line(\n            real_alpha, real_beta, name=\"Real line\",\n            line=dict(width=5)\n        )\n    ],\n    layout={\n        \"xaxis_title\" : \"X\",\n        \"yaxis_title\" : \"y\",\n        \"width\" : 1000,\n        \"height\" : 500,\n        \"dragmode\" : False\n    }\n)\n\n\n                                                \n\n\nWe need to find such a function \\(y=x\\alpha + \\beta\\) that produces the closest line to the array of points. More precisely to find \\(\\alpha\\) and \\(\\beta\\) that corresponds closes line.\nWe will use \\(MSE\\) as a loss function:\n\\[L(\\alpha, \\beta)=\\frac{\\sum_{i=1}^n \\left[(x_i\\alpha + \\beta) - y_i\\right]^2}{n}\\]\nBut the optimisation result will be the same with \\(SSE\\) (Sum Square Errors):\n\\[L(\\alpha, \\beta)=\\sum_{i=1}^n \\left[(x_i\\alpha + \\beta) - y_i\\right]^2\\]\nSo we will use \\(SSE\\) because it requires fewer operations.\nThe following code displays \\(SSE\\) values for different values of \\(\\alpha\\) and \\(\\beta\\). You can choose any combination of \\((\\alpha, \\beta)\\). The points on the \\(L\\) plot represent the selected SSE combinations. On the scatter plot there are lines that correspond by colour to those combinations of \\((\\alpha,\\beta)\\). As you can see best line have green color and it’s corresponds to minimal value of \\(L\\).\n\ndef sse_fun(y_true, y_pred):\n    return ((y_true - y_pred)**2).sum()\n\ndef pltly_get_sse_surface(alphas, betas):\n    \n    grid = np.array([\n        [sse_fun(y, alpha*X + beta) for beta in betas] \n        for alpha in alphas\n    ]).T\n\n    return go.Surface(\n        z=grid, x=alphas, y=betas,\n        hovertemplate=\"alpha:%{x}&lt;br&gt;beta:%{y}&lt;br&gt;Loss:%{z}&lt;extra&gt;&lt;/extra&gt;\",\n        showscale=False\n    )\n\nloss_scene_layout = dict(\n    title='Loss', autosize=False,\n    width=600, height=800,\n    margin=dict(l=65, r=50, b=65, t=90),\n    scene=dict(\n        xaxis_title='alpha',\n        yaxis_title='beta',\n        zaxis_title='L'\n    )\n)\ndef pltly_get_sse_fig(data):\n    fig = go.Figure(data=data)\n    fig.update_layout(**loss_scene_layout)\n    return fig\n\n\nfig = make_subplots(\n    rows=1, cols=2,\n    specs=[[{\"type\": \"scene\"}, {\"type\": \"xy\"}]]\n)\n\nalphas_grid = np.linspace(real_alpha-3, real_alpha+3, 100)\nbetas_grid = np.linspace(real_beta-3, real_beta+3, 100)\n\nfig.add_trace(\n    pltly_get_sse_surface(alphas_grid, betas_grid), \n    row=1, col=1\n)\nfig.add_trace(get_my_scatter(X, y), row=1, col=2)\nloss_scatter_plot_layout = dict(\n    width=1000, height=500,\n    scene=dict(\n        xaxis_title='alpha',\n        yaxis_title='beta',\n        zaxis_title='L'\n    ),\n    xaxis1_title = \"x\",\n    yaxis1_title = \"y\",\n    dragmode = False\n)\nfig.update_layout(**loss_scatter_plot_layout\n)\n\n# visualisation of points and lines corresponding to them\npoints_to_visualise = [\n    (real_alpha, real_beta, \"green\"),\n    (real_alpha-2, real_beta+1, \"red\"),\n    (real_alpha+2, real_beta-2, \"orange\")\n]\n\nfor i, (alpha, beta, col) in enumerate(points_to_visualise):\n    loss_val = sse_fun(alpha*X + beta, y)\n    fig.add_trace(\n        go.Scatter3d(\n            x=[alpha], y=[beta], \n            z=[loss_val + loss_val/10], \n            marker_color=col,\n            showlegend=False\n        ),\n        row=1, col=1\n    )\n    fig.add_trace(\n        pltly_plot_line(\n            alpha, beta, \n            line=dict(color=col, width=5),\n            name=f\"line{i+1}\",\n        ),\n        row=1, col=2\n    )\nfig.show()"
  },
  {
    "objectID": "machine_learning/gradient_descent.html#gradient",
    "href": "machine_learning/gradient_descent.html#gradient",
    "title": "Gradient descent",
    "section": "Gradient",
    "text": "Gradient\nThe idea is really simple - we need to pick a random point where we want to optimise and move in the direction of the fastest decreasing of function. Ok, let’s say we start by picking point \\((\\alpha_0, \\beta_0)\\), then move to point \\((\\alpha_1, \\beta_1)\\) and so on. All we need for the optional point \\((\\alpha_i, \\beta_i)\\) is to determine the direction of the fastest decreasing of the \\(L\\) function - antigradient of the function:\n\\[-\\nabla L(\\alpha, \\beta)= -(\\frac{\\partial L}{\\partial \\alpha}, \\frac{\\partial L}{\\partial \\beta})\\]\nSo we need some math with partial derivatives:\n\\[\\frac{\\partial L}{\\partial \\alpha}(\\alpha, \\beta)=2\\sum_{i=1}^n (\\alpha x_i+\\beta - y_i)x_i;\\] \\[\\frac{\\partial L}{\\partial \\beta}(\\alpha, \\beta)=2\\sum_{i=1}^n \\alpha x_i +\\beta - y_i.\\]\nSo from each \\((\\alpha_i,\\beta_i)\\) we can substitute the values into \\(\\nabla L(\\alpha, \\beta)\\) and get the direction we need to move. A good way to check if you have done everything right is to visualise the gradients at different points. The following cell defines a function to calculate gradint at a point and uses it to visualise the directions of the gradient. It would be cool to point cones tangential to the error graph, but plotly has some issues with cones pointing in case of large axis dimensionality, so I didn’t get it right. But you can still see that the gradient realisation works well and all the cones point in the direction of minimisation.\n\ndef sse_gradient(alpha, beta, X, y):\n    n = len(X)\n    \n    expand_X = X[np.newaxis,:]\n    expand_y = y[np.newaxis,:]\n    expand_alpha = alpha[:, np.newaxis]\n    expand_beta = beta[:, np.newaxis]\n\n    alpha_grad = 2*(\n        (np.dot(expand_alpha, expand_X) + expand_beta - expand_y)*expand_X\n    ).sum(axis=1)/n\n    beta_grad = 2*(expand_alpha*expand_X + expand_beta - expand_y).sum(axis=1)/n\n    \n    return (alpha_grad, beta_grad)\n\ncones_a = np.linspace(real_alpha - 2, real_alpha + 2, 10)\ncones_b = np.linspace(real_beta - 2, real_beta + 2, 10)\ncones_a, cones_b = np.meshgrid(cones_a, cones_b)\ncones_a = cones_a.ravel(); cones_b = cones_b.ravel()\ncons_preds = np.dot(\n    cones_a[:, np.newaxis], X[np.newaxis, :]\n) + cones_b[:, np.newaxis]\ncones_sse = ((cons_preds - y[np.newaxis, :])**2).sum(axis=1)\n\ngrad_a, grad_b = sse_gradient(cones_a, cones_b, X, y)\ngrad_a = -grad_a;grad_b = -grad_b\ngrad_L = np.zeros(grad_a.size)\n\npltly_get_sse_fig(\n    data = [\n        pltly_get_sse_surface(alphas_grid, betas_grid),\n        go.Cone(\n            x=cones_a, y=cones_b, z=cones_sse, \n            u=grad_a, v=grad_b, w=grad_L,\n            showscale = False,\n            colorscale=[\n                'rgba(255, 0, 0, 0.2)', \n                'rgba(255, 0, 0, 0.2)'\n            ]\n        )\n    ]\n)"
  },
  {
    "objectID": "machine_learning/gradient_descent.html#algorithm",
    "href": "machine_learning/gradient_descent.html#algorithm",
    "title": "Gradient descent",
    "section": "Algorithm",
    "text": "Algorithm\nNow, finally, to implement the optimisation algorithm. So we need\n\nSelect the point \\((\\alpha_0, \\beta_0)\\) (sets \\(i=0\\));\nMake an optimisation step like \\((\\alpha_{i+1}, \\beta_{i+1}) = (\\alpha_{i}, \\beta_{i}) - lr*\\nabla L(\\alpha, \\beta)\\);\nCheck the stop criteria:\n\nIf it completes, stop the optimisation and select \\((\\alpha_i, \\beta_i)\\) as the result;\nIf it does not finish, \\(i:=i+1\\) and return to step 2.\n\n\nWhere \\(lr\\) - learning rate, parameter that determines the speed of the optimisation.\nJust look how easy it is to get rid of unnecessary things - the following cell represents a really simple reasilation. Of course, solutions in libraries like sklearn are universal, much more optimised and have many features - that requires a lot of code. But the idea can be shown in just 10 lines of code.\nAs the result showen sequence of coefficient changes during optimisation.\n\nlr = 10e-2\n\n# here is defining of initial point\ncoefs = [(real_alpha+2.5, real_beta-2.5)]\n\n# we don't use any complex stop criteria\n# just 20 steps\nfor i in range(10):\n    grad_a, grad_b = sse_gradient(\n        np.array([coefs[-1][0]]), \n        np.array([coefs[-1][1]]), X, y\n    )\n\n    coefs.append((\n        coefs[-1][0] - lr*grad_a[0], \n        coefs[-1][1] - lr*grad_b[0]\n    ))\n\ncoefs\n\n[(5.5, -0.5),\n (3.9349832493396333, -0.057139502353940996),\n (3.4195523053914667, 0.2959285356499043),\n (3.2499591159509156, 0.5779810461599597),\n (3.194286908040883, 0.8034908101279732),\n (3.1761150891780643, 0.9838552095357639),\n (3.1702669194859174, 1.1281325591415075),\n (3.1684520655189967, 1.2435498785726589),\n (3.1679439508768503, 1.3358823189407831),\n (3.1678484920582104, 1.4097478750204548),\n (3.167874630554446, 1.4688402454478422)]\n\n\nLet’s visualise the results of the previous cell. So on the left plot you can see how the optimisation looks in terms of the loss function, but on the right you can see how the optimisation works in terms of the regression line obtained.\n\nfig = make_subplots(\n    rows=1, cols=2,\n    specs=[[{\"type\": \"scene\"}, {\"type\": \"xy\"}]]\n)\n\ncolorscale = pltly.colors.get_colorscale('RdYlGn')\nsample_colorscale = pltly.colors.sample_colorscale(\n    colorscale, np.linspace(0, 1, len(coefs))\n)\n\npath_data = {\n    \"x\" : [], \"y\" : [], \"z\" : [], \n    \"marker_color\" : []\n}\nlines = []\nfor (alpha, beta), col in zip(coefs, sample_colorscale):\n    \n    sse_val = sse_fun(X*alpha + beta, y)\n    path_data[\"x\"].append(alpha)\n    path_data[\"y\"].append(beta)\n    path_data[\"z\"].append(sse_val + sse_val/30)\n    path_data[\"marker_color\"].append(col)\n\n    lines.append(pltly_plot_line(\n        alpha, beta,\n        line=dict(color=col),\n        showlegend=False\n    ))\n\nfig.add_trace(\n    pltly_get_sse_surface(alphas_grid, betas_grid),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Scatter3d(\n        **path_data,\n        line=dict(width=4, color=\"yellow\"),\n        marker=dict(size=4),\n        showlegend=False\n    ),\n    row=1, col=1\n)\n\nfig.add_trace(\n    get_my_scatter(X,y)\n)\nfor l in lines:\n    fig.add_trace(l, row=1, col=2)\n\nfig.update_layout(**loss_scatter_plot_layout)"
  },
  {
    "objectID": "git/merge.html",
    "href": "git/merge.html",
    "title": "Merge",
    "section": "",
    "text": "Branches merge is opertion that allows you to get changes from intependend branches in one common commit. The git merge command is crucial for this page.\nTo merge branches you need to git checkout to brunch to which we merge all the changes, and execute command git merge &lt;branch to be merged&gt;."
  },
  {
    "objectID": "git/merge.html#sec-to_ancestor",
    "href": "git/merge.html#sec-to_ancestor",
    "title": "Merge",
    "section": "To ancestor",
    "text": "To ancestor\nThe simplest case is when you create a branch, make a few commits, and want to add those changes from an ancestor branch that hasn’t changed.\nSimply put, you just tell the parent branch to just start referencing the last commit of the branch you want to merge into the parent branch.\nIn the next cell:\n\nCreated repository where there is basic commit;\nFrom basic commit branch example_branch was created;\nbranch_commit was commited to example_branch, first git log display exactly this state of the repository;\nThen example_branch was merged to master - on git log it just desplayed as master moved to same commit as example_branch.\n\n\n%%bash\nmkdir merge_example\ncd merge_example\ngit init &&gt; /dev/null\n\necho \"content\" &gt; test_file\ngit add --all\ngit commit -m \"basic commit\" &&gt; /dev/null\n\necho\necho \"=====creating branch=====\"\ngit checkout -b example_branch 2&gt;&1\necho \"content2\" &gt; test_file\ngit commit -am \"branch commit\" &&gt; /dev/null\ngit log --decorate --graph\n\necho\necho \"=====merging to master=====\"\ngit checkout master 2&gt;&1\n\necho \"-----git merge-----\"\ngit merge example_branch\necho \"-----git log-----\"\ngit log --decorate --graph\n\ncd ..\nrm -r merge_example\n\n\n=====creating branch=====\nSwitched to a new branch 'example_branch'\n* commit bbd3463120a7fc513429a75ca2ebfff325571f84 (HEAD -&gt; example_branch)\n| Author: Dranikf &lt;kobfedsur@gmail.com&gt;\n| Date:   Sun Sep 10 16:20:47 2023 +0300\n| \n|     branch commit\n| \n* commit c42d7deed4d71b0f5c9036816fe65e7295a5e227 (master)\n  Author: Dranikf &lt;kobfedsur@gmail.com&gt;\n  Date:   Sun Sep 10 16:20:47 2023 +0300\n  \n      basic commit\n\n=====merging to master=====\nSwitched to branch 'master'\n-----git merge-----\nUpdating c42d7de..bbd3463\nFast-forward\n test_file | 2 +-\n 1 file changed, 1 insertion(+), 1 deletion(-)\n-----git log-----\n* commit bbd3463120a7fc513429a75ca2ebfff325571f84 (HEAD -&gt; master, example_branch)\n| Author: Dranikf &lt;kobfedsur@gmail.com&gt;\n| Date:   Sun Sep 10 16:20:47 2023 +0300\n| \n|     branch commit\n| \n* commit c42d7deed4d71b0f5c9036816fe65e7295a5e227\n  Author: Dranikf &lt;kobfedsur@gmail.com&gt;\n  Date:   Sun Sep 10 16:20:47 2023 +0300\n  \n      basic commit"
  },
  {
    "objectID": "git/merge.html#sec-basic_case",
    "href": "git/merge.html#sec-basic_case",
    "title": "Merge",
    "section": "Basic case",
    "text": "Basic case\nHere I’ll play with the case when you have to branches but one doesn’t refer to any ancestor of other (like it was in To ancestor) section, so we have two separate branches.\nNote in this example I don’t pay any attention ot conflicts, the example is made in such a way as to avoid conflicts.\nIn the example:\n\nI created a repository where there are two branches that have independent commits in them, this is shown in the first git log output;\nThen I merge example_branch into master;\nIn the final git log output:\n\nThere is a new commit where the two branches merge;\nThe message of the commit is Merge branch 'example_branch' - automatically generated by git, in practice you’ll be able to set the message you want.\n\n\n\n%%bash\nmkdir merge_example\ncd merge_example\ngit init &&gt; /dev/null\n\necho \"content\" &gt; file\ngit add --all\ngit commit -m \"basic commit\" &&gt; /dev/null\n\ngit checkout -b example_branch &&gt; /dev/null\necho \"content\" &gt; branch_file\ngit add --all\ngit commit -m \"branch commit\" &&gt; /dev/null\n\ngit checkout master &&gt; /dev/null\necho \"master content\" &gt; file\ngit commit -am \"master commit\" &&gt; /dev/null\n\necho\necho \"=====log=====\"\ngit log --decorate --graph --all\n\necho\necho \"=====merge=====\"\ngit merge example_branch\n\necho\necho \"=====log=====\"\ngit log --decorate --graph --all\n\ncd ..\nrm -r merge_example\n\n\n=====log=====\n* commit 6ea69b66f6c2a49ea60a3a6b5b3de66181274234 (example_branch)\n| Author: Fedor Kobak &lt;kobfedsur@gmail.com&gt;\n| Date:   Sun Sep 10 13:00:54 2023 +0300\n| \n|     branch commit\n|   \n| * commit 8b9f875b80b9d44020dedf40a39b1894f4a406d6 (HEAD -&gt; master)\n|/  Author: Fedor Kobak &lt;kobfedsur@gmail.com&gt;\n|   Date:   Sun Sep 10 13:00:54 2023 +0300\n|   \n|       master commit\n| \n* commit 15149be74a6d219dac74624c94bd7aeda0691e9d\n  Author: Fedor Kobak &lt;kobfedsur@gmail.com&gt;\n  Date:   Sun Sep 10 13:00:54 2023 +0300\n  \n      basic commit\n\n=====merge=====\nMerge made by the 'ort' strategy.\n branch_file | 1 +\n 1 file changed, 1 insertion(+)\n create mode 100644 branch_file\n\n=====log=====\n*   commit 96f925a2adbada606ba4678c71460cf697fd8590 (HEAD -&gt; master)\n|\\  Merge: 8b9f875 6ea69b6\n| | Author: Fedor Kobak &lt;kobfedsur@gmail.com&gt;\n| | Date:   Sun Sep 10 13:00:54 2023 +0300\n| | \n| |     Merge branch 'example_branch'\n| | \n| * commit 6ea69b66f6c2a49ea60a3a6b5b3de66181274234 (example_branch)\n| | Author: Fedor Kobak &lt;kobfedsur@gmail.com&gt;\n| | Date:   Sun Sep 10 13:00:54 2023 +0300\n| | \n| |     branch commit\n| | \n* | commit 8b9f875b80b9d44020dedf40a39b1894f4a406d6\n|/  Author: Fedor Kobak &lt;kobfedsur@gmail.com&gt;\n|   Date:   Sun Sep 10 13:00:54 2023 +0300\n|   \n|       master commit\n| \n* commit 15149be74a6d219dac74624c94bd7aeda0691e9d\n  Author: Fedor Kobak &lt;kobfedsur@gmail.com&gt;\n  Date:   Sun Sep 10 13:00:54 2023 +0300\n  \n      basic commit"
  },
  {
    "objectID": "git/merge.html#solve-conflict",
    "href": "git/merge.html#solve-conflict",
    "title": "Merge",
    "section": "Solve conflict",
    "text": "Solve conflict\nIn the section basic case the example shows the case where the brances to be merged modify different files. But how does git deal with the case where branches being merged have changes in the same files?\nGit will enter a special state - a merge conflict. It will be necessary to edit the files that caused the conflict, add changes to the stage, and commit the changes.\nWhen there is a conflict, git will make some changes to the conflicting files. It will record where there is a conflict:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n&lt;content of the branch we are merging into&gt;\n===========\n&lt;content of the branch we merge into another&gt;\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; &lt;branch ot be merged name&gt;\n\nManual example\nWe can put any content there: we can put content from one of the branches, or we can put completely different content.\nSo in the following example\n\nCreate a repository;\nCommit with the message basic commit;\nFrom basic commit commit created example branch commit of which file has content for example branch;\nThe master branch continues it’s way with a commit which has file with content for example branch;\nThe first git log output shows the state of the repository at this step, so for now we have two branches where file has different contents;\nTried to merge example_branch into the master branch and got a message that there was a merge conflict;\nShows what the git status of the repository looks like in this state:\n\nPrints possible solutions;\nPrints the files that caused the merge conflict;\n\nSuppose we want to fix a conflict. Our actions are to write into the conflict files how they should be fixed as a result, add them to the stage, and then commit;\nInterestingly, git will overwrite the files involved in the conflict. The example shows what our file will look like, see =====conflict file=====;\nI set the file to after merge content and just commit the changes - this commit will become the merge point for the conflicting branches.\n\n\n%%bash\nmkdir merge_example\ncd merge_example\ngit init &&gt; /dev/null\n\necho \"content for basic commit\" &gt; file\ngit add --all\ngit commit -m \"basic commit\" &&gt; /dev/null\n\ngit checkout -b example_branch &&gt; /dev/null\necho \"content for example branch\" &gt; file\ngit commit -am \"commit in example_branch\" &&gt; /dev/null\n\ngit checkout master &&gt; /dev/null\necho \"content for master branch\" &gt; file\ngit commit -am \"commit in master\" &&gt; /dev/null\n\necho \"=====git log=====\"\ngit log --decorate --graph --all\n\necho\necho \"=====merge try=====\"\ngit merge example_branch\necho\necho \"=====git status=====\"\ngit status\necho\necho \"=====conflict file=====\"\ncat file\n\necho \"after merge content\" &gt; file\ngit add file\ngit commit -am \"my after merge commit\" &&gt; /dev/null\n\necho\necho \"=====git log=====\"\ngit log --all --decorate --graph\n\n\ncd ..\nrm -r merge_example\n\n=====git log=====\n* commit 69a666cd3dc1c3f0b436a1ecbeb58ea32eb080c1 (example_branch)\n| Author: Dranikf &lt;kobfedsur@gmail.com&gt;\n| Date:   Sun Sep 10 16:43:44 2023 +0300\n| \n|     commit in example_branch\n|   \n| * commit 51377f0e8c2f305bfcab9c4a97a6a36d8e44c99f (HEAD -&gt; master)\n|/  Author: Dranikf &lt;kobfedsur@gmail.com&gt;\n|   Date:   Sun Sep 10 16:43:44 2023 +0300\n|   \n|       commit in master\n| \n* commit 7307ad26c4cc29bbb0eac4b9ad2420aeab716f0b\n  Author: Dranikf &lt;kobfedsur@gmail.com&gt;\n  Date:   Sun Sep 10 16:43:44 2023 +0300\n  \n      basic commit\n\n=====merge try=====\nAuto-merging file\nCONFLICT (content): Merge conflict in file\nAutomatic merge failed; fix conflicts and then commit the result.\n\n=====git status=====\nOn branch master\nYou have unmerged paths.\n  (fix conflicts and run \"git commit\")\n  (use \"git merge --abort\" to abort the merge)\n\nUnmerged paths:\n  (use \"git add &lt;file&gt;...\" to mark resolution)\n    both modified:   file\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\n=====conflict file=====\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\ncontent for master branch\n=======\ncontent for example branch\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; example_branch\n\n=====git log=====\n*   commit acb33c07dd270ac0ff04f29c37303d10ecb2ed0c (HEAD -&gt; master)\n|\\  Merge: 51377f0 69a666c\n| | Author: Dranikf &lt;kobfedsur@gmail.com&gt;\n| | Date:   Sun Sep 10 16:43:44 2023 +0300\n| | \n| |     my after merge commit\n| | \n| * commit 69a666cd3dc1c3f0b436a1ecbeb58ea32eb080c1 (example_branch)\n| | Author: Dranikf &lt;kobfedsur@gmail.com&gt;\n| | Date:   Sun Sep 10 16:43:44 2023 +0300\n| | \n| |     commit in example_branch\n| | \n* | commit 51377f0e8c2f305bfcab9c4a97a6a36d8e44c99f\n|/  Author: Dranikf &lt;kobfedsur@gmail.com&gt;\n|   Date:   Sun Sep 10 16:43:44 2023 +0300\n|   \n|       commit in master\n| \n* commit 7307ad26c4cc29bbb0eac4b9ad2420aeab716f0b\n  Author: Dranikf &lt;kobfedsur@gmail.com&gt;\n  Date:   Sun Sep 10 16:43:44 2023 +0300\n  \n      basic commit\n\n\n\n\nSelect version\nSometimes files can differ significantly between branches, so it can be difficult to edit files in all the difference entries. You may want to consider specifying which branch file should be taken for the merge commit.\nYou can do this with the syntax git checkout --theirs/ours &lt;list of files&gt;. If you use ours it will take the version of the file from the current branch, if you use theirs it will take the version from the merging branch.\nThe following example shows the difference:\n\nThere are two files added to the git repository;\nIn new_branch the files change one way in master the other, so you’ll have conflicts when merging;\nUse git checkout to resolve the conflict:\n\nFor test_file1 we use the --ours option;\nFor test_file2 we use the --theirs option;\n\nAs a result we have the version from master for test_file1 and the version from example_branch for test_file2;\n\n\n%%bash\nmkdir merge_example\ncd merge_example\ngit init &&gt; /dev/null\n\necho -e \"version\\n\"\\\n\"of the test_file1\\n\"\\\n\"from initian commit\"\\\n&gt; test_file1\necho -e \"version\\n\"\\\n\"of the test_file2\\n\"\\\n\"from initian commit\"\\\n&gt; test_file2\n\ngit add --all\ngit commit -m \"initial commit\" &&gt; /dev/null\n\ngit checkout -b new_branch &&gt; /dev/null\necho -e \"version\\n\"\\\n\"of the test_file1\\n\"\\\n\"from new_branch\"\\\n&gt; test_file1\necho -e \"version\\n\"\\\n\"of the test_file2\\n\"\\\n\"from new_branch\"\\\n&gt; test_file2\ngit commit -am \"commit from new_branch\" &&gt; /dev/null\n\ngit checkout master &&gt; /dev/null\necho -e \"version\\n\"\\\n\"of the test_file1\\n\"\\\n\"from master\"\\\n&gt; test_file1\necho -e \"version\\n\"\\\n\"of the test_file2\\n\"\\\n\"from master\"\\\n&gt; test_file2\ngit commit -am \"commit from master\" &&gt; /dev/null\n\ngit log --graph --decorate --all\n\ngit merge new_branch &&gt; /dev/null\n\necho\necho\necho \"=====Files before checkout=====\"\necho \"-----test_file1-----\"\ncat test_file1\necho \"-----test_file2-----\"\ncat test_file2\n\n\necho\necho \"=====Files after checkout=====\"\ngit checkout --ours test_file1 &&gt; /dev/null\ngit checkout --theirs test_file2 &&gt; /dev/null\necho \"-----test_file1-----\"\ncat test_file1\necho \"-----test_file1-----\"\ncat test_file2\n\ncd ..\nrm -r merge_example\n\n* commit 7d17539a009082402162cd98502bc9e3e0032aca (HEAD -&gt; master)\n| Author: Fedor Kobak &lt;kobfedsur@gmail.com&gt;\n| Date:   Mon Oct 30 15:50:33 2023 +0300\n| \n|     commit from master\n|   \n| * commit 40934888b03ac7bb5e4d2df298f6eb294a881cce (new_branch)\n|/  Author: Fedor Kobak &lt;kobfedsur@gmail.com&gt;\n|   Date:   Mon Oct 30 15:50:33 2023 +0300\n|   \n|       commit from new_branch\n| \n* commit c830f9d4e47bb64137721c148926432b922365ed\n  Author: Fedor Kobak &lt;kobfedsur@gmail.com&gt;\n  Date:   Mon Oct 30 15:50:33 2023 +0300\n  \n      initial commit\n\n\n=====Files for merge=====\n-----test_file1-----\nversion\nof the test_file1\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nfrom master\n=======\nfrom new_branch\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; new_branch\n-----test_file2-----\nversion\nof the test_file2\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nfrom master\n=======\nfrom new_branch\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; new_branch\n\n=====Files before checkout=====\n-----test_file1-----\nversion\nof the test_file1\nfrom master\n-----test_file1-----\nversion\nof the test_file2\nfrom new_branch"
  },
  {
    "objectID": "git/commit.html",
    "href": "git/commit.html",
    "title": "Commit",
    "section": "",
    "text": "git commit is central command for git that allows to save staged memories for ever."
  },
  {
    "objectID": "git/commit.html#add-files--a",
    "href": "git/commit.html#add-files--a",
    "title": "Commit",
    "section": "Add files (-a)",
    "text": "Add files (-a)\nThis option makes git to stage file that wasn’t staged before the commit. It’s like to do commit for edited files without execution of git add before.\n\nBasic example\nSo in the following example:\n\nCreate a repository containing a modified file;\nFirst attempt to commit without additional options - fails, log doesn’t show control commit just because there was nothing to commit;\nSecond attempt to commit -a - it works, new commit appears in git log results.\n\n\n%%bash\nmkdir commit_example\ncd commit_example\n\ngit init &&gt; /dev/null\necho \"some text\" &gt; test_file\ngit add --all\ngit commit -m \"innitial commit\"&&gt; /dev/null\necho \"some other text\" &gt; test_file\n\necho \"=====Just commit=====\"\necho \"-----commit-----\"\ngit commit -m \"control commit\"\necho \"-----log-----\"\ngit log --oneline\n\necho\necho \"=====Commit -a=====\"\necho \"-----commit-----\"\ngit commit -am \"control commit\"\necho \"-----log-----\"\ngit log --oneline\n\n\ncd ..\nrm -r commit_example\n\n=====Just commit=====\n-----commit-----\nOn branch master\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n    modified:   test_file\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n-----log-----\nb492f00 innitial commit\n\n=====Commit -a=====\n-----commit-----\n[master 1b938f2] control commit\n 1 file changed, 1 insertion(+), 1 deletion(-)\n-----log-----\n1b938f2 control commit\nb492f00 innitial commit\n\n\n\n\nUntracked files\nThis option won’t work for untracked files. You have to add it before.\nSo in the following example, I’m trying to use commit -a for a repository with only one untracked file, and getting messages that I need to add it first.\n\n%%bash\nmkdir commit_example\ncd commit_example\n\ngit init &&gt; /dev/null\necho \"some text\" &gt; test_file\n# git add --all don't add untracked file\necho \"=====commit=====\"\ngit commit -am \"innitial commit\"\necho \"=====log=====\"\ngit log\n\ncd ..\nrm -r commit_example\n\n=====commit=====\nOn branch master\n\nInitial commit\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    test_file\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n=====log=====\n\n\nfatal: your current branch 'master' does not have any commits yet"
  },
  {
    "objectID": "git/commit.html#update-tip-commit---amend",
    "href": "git/commit.html#update-tip-commit---amend",
    "title": "Commit",
    "section": "Update tip commit (--amend)",
    "text": "Update tip commit (--amend)\nYou can replace tip commit of the current branch with new commit. You can think about it like you added new changes to the last commit.\nTo perform it you have to use --amend option for git commit command.\nSo in the following example:\n\nNew repository has been initiated and first commit created;\nSome changes have been made - this is shown in the git status output;\nBut by using git add and git commit --amend changes have been committed;\nBecause of the --amend option, there is still only one commit in the git log output.\n\n\n%%bash\nmkdir commit_amend\ncd commit_amend\n\ngit init &&gt; /dev/null\n\n# initial commit\necho \"first line\" &gt; new_file\ngit add --all\ngit commit -m \"test commit\" &&gt; /dev/null\n\necho \"new line\" &gt;&gt; new_file\necho \"=====file changed=====\"\ngit status\n\ngit add --all\ngit commit --amend -m \"test commit change\" &&gt; /dev/null\n\necho\necho \"======git status=====\"\ngit status\necho\necho \"=====git log=====\"\ngit log --all --graph --oneline\n\ncd ..\nrm -r commit_amend\n\n=====file changed=====\nOn branch master\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n    modified:   new_file\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\n======git status=====\nOn branch master\nnothing to commit, working tree clean\n\n=====git log=====\n* fc660b7 test commit change"
  },
  {
    "objectID": "git/log.html",
    "href": "git/log.html",
    "title": "Log",
    "section": "",
    "text": "git log is a command that allows you to check some information about:"
  },
  {
    "objectID": "git/log.html#oneline",
    "href": "git/log.html#oneline",
    "title": "Log",
    "section": "oneline",
    "text": "oneline\ngit log --oneline allows you to get information about each commit in just one line, without any extra information.\nSo the following example shows the difference - git log without --oneline and with it, used for repositories with few commits.\n\n%%bash\nmkdir log_examples\ncd log_examples\ngit init &&gt; /dev/null\n\n\nfor i in {1..5}\ndo\n  echo \"Line $i\" &gt;&gt; file\n  git add file\n  git commit -m \"Line $i added\" &&gt; /dev/null\ndone\n\necho \"=====git log=====\"\ngit log\necho\necho \"=====git log --oneline=====\"\ngit log --oneline\n\ncd ..\nrm -r log_examples\n\n=====git log=====\ncommit 0ce330290ccbf23709c21b9ab2f954735ecd03b3\nAuthor: Fedor Kobak &lt;kobfedsur@gmail.com&gt;\nDate:   Sat Sep 9 14:15:46 2023 +0300\n\n    Line 5 added\n\ncommit 80e85aa88db21f3f50baf377614b9eb485f258bb\nAuthor: Fedor Kobak &lt;kobfedsur@gmail.com&gt;\nDate:   Sat Sep 9 14:15:46 2023 +0300\n\n    Line 4 added\n\ncommit 65a9790a5234f3fc4d0f4d954c76b60db30fc8ce\nAuthor: Fedor Kobak &lt;kobfedsur@gmail.com&gt;\nDate:   Sat Sep 9 14:15:46 2023 +0300\n\n    Line 3 added\n\ncommit 4df96a83e43be23415c0b0023d10b2e0282188ff\nAuthor: Fedor Kobak &lt;kobfedsur@gmail.com&gt;\nDate:   Sat Sep 9 14:15:46 2023 +0300\n\n    Line 2 added\n\ncommit 7bdb94f91398e7cd4bcb431dc4a98557629ad202\nAuthor: Fedor Kobak &lt;kobfedsur@gmail.com&gt;\nDate:   Sat Sep 9 14:15:46 2023 +0300\n\n    Line 1 added\n\n=====git log --oneline=====\n0ce3302 Line 5 added\n80e85aa Line 4 added\n65a9790 Line 3 added\n4df96a8 Line 2 added\n7bdb94f Line 1 added"
  },
  {
    "objectID": "git/file_stages.html",
    "href": "git/file_stages.html",
    "title": "File statuses",
    "section": "",
    "text": "In git, files can take on different states with respect to the current commit. The following picture mention central idea."
  },
  {
    "objectID": "git/file_stages.html#get-status",
    "href": "git/file_stages.html#get-status",
    "title": "File statuses",
    "section": "Get status",
    "text": "Get status\nThe git status command allows you to get some information about the current status of the git repository. This command is very common on this site, so you can find many different examples of how to use it."
  },
  {
    "objectID": "git/file_stages.html#untracked-files",
    "href": "git/file_stages.html#untracked-files",
    "title": "File statuses",
    "section": "Untracked files",
    "text": "Untracked files\nIf git hasn’t seen a file before, it will correspond to this category. You can find files on this stage in Untracked files section of git status command.\nIn the following example, I am simply trying to add a some text file to the freshly created git repo, and when I call git status, I can find it in the untracked files section.\n\n%%bash\nmkdir status_test\ncd status_test\ngit init &&gt; /dev/null\n\necho \"=====Empty repo=====\"\ngit status\necho \"some text\" &gt; test_file\necho\necho \"=====One file added=====\"\ngit status\n\ncd ..\nrm -r  status_test\n\n=====Empty repo=====\nOn branch master\n\nNo commits yet\n\nnothing to commit (create/copy files and use \"git add\" to track)\n\n=====One file added=====\nOn branch master\n\nNo commits yet\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    test_file\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n\n\nTo make git track some file you should use command git add &lt;filemane&gt;."
  },
  {
    "objectID": "git/file_stages.html#staged",
    "href": "git/file_stages.html#staged",
    "title": "File statuses",
    "section": "Staged",
    "text": "Staged\nThese are the files for the next commit.\nIn git status they’re shown in the Changes to be committed section.\nSo in the following example, I have created and added a file to be tracked in the git repo, and show the result of git status for such a repository.\n\n%%bash\nmkdir status_test\ncd status_test\ngit init &&gt; /dev/null\n\necho \"some text\" &gt; test_file\ngit add test_file\necho\necho \"=====File to be commited=====\"\ngit status\n\ncd ..\nrm -r  status_test\n\n\n=====File to be commited=====\nOn branch master\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n    new file:   test_file\n\n\n\nYou can commit changes by using git commit command."
  },
  {
    "objectID": "git/file_stages.html#commited-files",
    "href": "git/file_stages.html#commited-files",
    "title": "File statuses",
    "section": "Commited files",
    "text": "Commited files\nIf you just commit all the files and check git status, you will always get the same message.\nYou can also check the list of commits using git log.\nSo in the following example, I will try git commit and git log on the repository with a single commit and no changes since that commit.\n\n%%bash\nmkdir status_test\ncd status_test\ngit init &&gt; /dev/null\n\necho \"some text\" &gt; test_file\ngit add test_file\ngit commit -m \"my message\" &&gt; /dev/null\necho\necho \"=====git status=====\"\ngit status\necho \"=====git log=====\"\ngit log\n\n\ncd ..\nrm -r  status_test\n\n\n=====git status=====\nOn branch master\nnothing to commit, working tree clean\n=====git log=====\ncommit 6e0339d191892a011e66351179c1b7cb9b08b292\nAuthor: Fedor Kobak &lt;kobfedsur@gmail.com&gt;\nDate:   Wed Sep 6 15:02:05 2023 +0300\n\n    my message"
  },
  {
    "objectID": "git/file_stages.html#changed",
    "href": "git/file_stages.html#changed",
    "title": "File statuses",
    "section": "Changed",
    "text": "Changed\nAny file that has been changed since the last commit will appear in the Changes not staged to commit section, so if you try to commit in this momed, any changes made to these files won’t be committed.\nTo commit these changes you need to use git add or git commit -a.\nThe following example shows the difference:\n\nThe first print shows test_file as modified in the Changes not staged for commit block;\nSecond block corresponds to case if you’re trying to commit unstaged files. Interestingly, if you’re trying to commit without any changes, it won’t commit and will print as for git status;\nLast block is same as previous, but before committing I did git add &lt;filename&gt; - so new commit was added, which is displayed in the git status and git log results.\n\n\n%%bash\nmkdir status_test\ncd status_test\ngit init &&gt; /dev/null\n\necho \"some text\" &gt; test_file\ngit add test_file &&gt; /dev/null\ngit commit -m \"first commit\" &&gt; /dev/null\necho \"some other text\" &gt; test_file\n\necho \"=====Changed file=====\"\ngit status\n\necho\necho \"=====Not added commit=====\"\necho \"-----git commit-----\"\ngit commit -m \"unsuccessful commit\"\necho \"-----git log-----\"\ngit log --oneline\n\necho\necho \"=====Add file=====\"\ngit add test_file\necho \"-----git commit-----\"\ngit commit -m \"second commit\"\necho \"-----git status-----\"\ngit status\necho \"-----git log-----\"\ngit log --oneline\n\ncd ..\nrm -r  status_test\n\n=====Changed file=====\nOn branch master\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n    modified:   test_file\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\n=====Not added commit=====\n-----git commit-----\nOn branch master\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n    modified:   test_file\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n-----git log-----\naaad9bc first commit\n\n=====Add file=====\n-----git commit-----\n[master 4dba109] second commit\n 1 file changed, 1 insertion(+), 1 deletion(-)\n-----git status-----\nOn branch master\nnothing to commit, working tree clean\n-----git log-----\n4dba109 second commit\naaad9bc first commit"
  },
  {
    "objectID": "git/diff.html",
    "href": "git/diff.html",
    "title": "Difference",
    "section": "",
    "text": "The git diff command allows you to chech difference between condition of repository."
  },
  {
    "objectID": "git/diff.html#untracked-files",
    "href": "git/diff.html#untracked-files",
    "title": "Difference",
    "section": "Untracked files",
    "text": "Untracked files\nUntracked files wont be displayed in git diff.\nThe following example just init git repository and add a file there and the git diff command doesn’t print anything in this case.\n\n%%bash\nmkdir diff_example\ncd diff_example\n\ngit init &&gt; /dev/null\necho \"some text\" &gt; test_file\ngit diff\n\ncd ..\nrm -r diff_example"
  },
  {
    "objectID": "git/diff.html#changed-files",
    "href": "git/diff.html#changed-files",
    "title": "Difference",
    "section": "Changed files",
    "text": "Changed files\nBy default git diff shows unstaged changes. In fact, any change to the file can be described as an insert/delete. So the following subsections show what it’s like to have insert/delete lines in the git diff results.\n\nInsert line\nMain point of the next cell is to add line \"second line\" to the file commited before. In git diff results it’ll look like +second line.\n\n%%bash\nmkdir diff_example\ncd diff_example\n\ngit init &&gt; /dev/null\necho \"first line\" &gt; test_file\ngit add test_file &&gt; /dev/null\ngit commit -m \"initial commit\" &&gt; /dev/null\n\n\necho \"second line\" &gt;&gt; test_file\ngit diff\n\ncd ..\nrm -r diff_example\n\ndiff --git a/test_file b/test_file\nindex 08fe272..06fcdd7 100644\n--- a/test_file\n+++ b/test_file\n@@ -1 +1,2 @@\n first line\n+second line\n\n\n\n\nDelete line\nIn this case I created files with two lines \"first line\" and \"second line\". After committing, I save the new version of the file without the second line. Gid diff shows it as -second line.\n\n%%bash\nmkdir diff_example\ncd diff_example\ngit init &&gt; /dev/null\n\ncat &gt; test_file &lt;&lt; EOF\nfirst line\nsecond line\nEOF\ngit add test_file\ngit commit -m \"initial commit\" &&gt; /dev/null\n\ncat &gt; test_file &lt;&lt; EOF\nfirst line\nEOF\ngit diff\n\ncd ..\nrm -r diff_example\n\ndiff --git a/test_file b/test_file\nindex 06fcdd7..08fe272 100644\n--- a/test_file\n+++ b/test_file\n@@ -1,2 +1 @@\n first line\n-second line"
  },
  {
    "objectID": "git/diff.html#new-file",
    "href": "git/diff.html#new-file",
    "title": "Difference",
    "section": "New file",
    "text": "New file\nIn this section, I’ll show you what a newly added file looks like in git diff.\n\nEmpty file\nIn the following example, I’ve added an empty second_file and compared it with the state of the repository in the previous commit, if there is no second_file' line, thenew file mode` will signal that this file has been created.\n\n%%bash\nmkdir diff_example\ncd diff_example\ngit init &&gt; /dev/null\n\ntouch first_file\ngit add first_file\ngit commit -m \"first commit\" &&gt; /dev/null\n\ntouch second_file\ngit add second_file &&gt; /dev/null\n\ngit diff --staged\ncat second_file\n\ncd ..\nrm -r diff_example\n\ndiff --git a/second_file b/second_file\nnew file mode 100644\nindex 0000000..e69de29\n\n\n\n\nFile with content\nIn this example I have created a file second_file and added some content to it. It’ll share all the details of adding empty file option - new file mode 100644 signal that new file has been created. But there is also a section with changes to the file below. Note that the source file from the commit to which we are comparing the state is marked as --- /dev/null (in Linux this directory is used for information that should simply be deleted).\n\n%%bash\nmkdir diff_example\ncd diff_example\ngit init &&gt; /dev/null\n\ntouch first_file\ngit add first_file\ngit commit -m \"first commit\" &&gt; /dev/null\n\necho \"some content\" &gt; second_file\ngit add second_file &&gt; /dev/null\n\ngit diff --staged\ncat second_file\n\ncd ..\nrm -r diff_example\n\ndiff --git a/second_file b/second_file\nnew file mode 100644\nindex 0000000..2ef267e\n--- /dev/null\n+++ b/second_file\n@@ -0,0 +1 @@\n+some content\nsome content"
  },
  {
    "objectID": "git/diff.html#staged-option",
    "href": "git/diff.html#staged-option",
    "title": "Difference",
    "section": "–staged option",
    "text": "–staged option\nBy default git diff shows only changed but not staged files. To get information about staged files use --staged option.\n\nNew file\nBy using the --staged option, you can even see the difference between a newly created (but added) file.\nSo in the following example I have created and added to the repo file and compared the results with and without the --staged option.\n\n%%bash\nmkdir diff_example\ncd diff_example\n\ngit init &&gt; /dev/null\necho \"some text\" &gt; staged_file\ngit add staged_file\n\necho \"=====just diff====\"\ngit diff\n\necho\necho \"=====diff staged=====\"\ngit diff --staged\n\ncd ..\nrm -r diff_example\n\n=====just diff====\n\n=====diff staged=====\ndiff --git a/staged_file b/staged_file\nnew file mode 100644\nindex 0000000..7b57bd2\n--- /dev/null\n+++ b/staged_file\n@@ -0,0 +1 @@\n+some text\n\n\n\n\nUnstaged changes\nIf you use the --staged option, you won’t see any difference when unstaging files. In the example:\n\nIn the first commit I just added first_file;\nI added second_file and staged it;\nI changed first_file but did not stage it;\nUsing git diff' without the–staged’ option I only got changes in first_file because it wasn’t staged;\nUsing git diff --staged I only got information about second_file but not about first_file.\n\n\n%%bash\nmkdir diff_example\ncd diff_example\ngit init &&gt; /dev/null\n\necho \"first text\" &gt; first_file\ngit add first_file\ngit commit -m \"first file add\" &&gt; /dev/null\n\necho \"sefond text\" &gt; second_file\ngit add second_file\necho \"changed first text\" &gt; first_file\n\necho \"=====just diff====\"\ngit diff\n\necho\necho \"=====diff staged=====\"\ngit diff --staged\n\ncd ..\nrm -r diff_example\n\n=====just diff====\ndiff --git a/first_file b/first_file\nindex 38181e5..fc9cf92 100644\n--- a/first_file\n+++ b/first_file\n@@ -1 +1 @@\n-first text\n+changed first text\n\n=====diff staged=====\ndiff --git a/second_file b/second_file\nnew file mode 100644\nindex 0000000..422845a\n--- /dev/null\n+++ b/second_file\n@@ -0,0 +1 @@\n+sefond text"
  },
  {
    "objectID": "git/diff.html#compare-two-commits",
    "href": "git/diff.html#compare-two-commits",
    "title": "Difference",
    "section": "Compare two commits",
    "text": "Compare two commits\nYou can compare two arbitrary commits using the syntax git diff &lt;basic commit&gt; &lt;comparison commit&gt;.\nNote basic commit not necessary to be earlier that comparison commit.\nSo in the following example:\n\nIn cycle created few commits each add some line to file:\n\nNote line val \"hash$i=$(git rev-parse HEAD)\" just saves hash of last commit to variable hash&lt;i&gt;, so, for example, you can get hash of first commit from variable $hash1;\n\ngit log just help to understand current state of the repository;\nThen I copare commits Line 1 added and Line 2 added in both options:\n\nLine 1 added as basic commit and Line 2 added as comparison commit - +Line 2show that it was added in comparison commit relatively basic commit;\nLine 2 added as basic commit and Line 1 added as comparison commit - -Line 2show that it was deleted in comparison commit relatively basic commit.\n\n\n\n%%bash\nmkdir diff_example\ncd diff_example\ngit init &&gt; /dev/null\n\nfor i in {1..3}\ndo\n  echo \"Line $i\" &gt;&gt; file\n  git add file\n  git commit -m \"Line $i added\" &&gt; /dev/null\n  eval \"hash$i=$(git rev-parse HEAD)\"\ndone\n\necho \"=====git log=====\"\ngit log --oneline --decorate\n\necho\necho \"=====git diff \\$hash1 \\$hash2=======\"\ngit diff $hash1 $hash2\necho\necho \"=====git diff \\$hash2 \\$hash1=======\"\ngit diff $hash2 $hash1\n\ncd ..\nrm -r diff_example\n\n=====git log=====\nc9578cb (HEAD -&gt; master) Line 3 added\nff19e36 Line 2 added\nbd6fd11 Line 1 added\n\n=====git diff $hash1 $hash2=======\ndiff --git a/file b/file\nindex 3be9c81..c82de6a 100644\n--- a/file\n+++ b/file\n@@ -1 +1,2 @@\n Line 1\n+Line 2\n\n=====git diff $hash2 $hash1=======\ndiff --git a/file b/file\nindex c82de6a..3be9c81 100644\n--- a/file\n+++ b/file\n@@ -1,2 +1 @@\n Line 1\n-Line 2"
  },
  {
    "objectID": "git/previous_commit.html",
    "href": "git/previous_commit.html",
    "title": "Previous commit ~",
    "section": "",
    "text": "You can use the &lt;commit&gt;~n construction to reference a commit that is n steps back in the commit chain."
  },
  {
    "objectID": "git/previous_commit.html#basic-example",
    "href": "git/previous_commit.html#basic-example",
    "title": "Previous commit ~",
    "section": "Basic example",
    "text": "Basic example\nIn the following example:\n\nTwo commits were created;\ngit log shows that HEAD refer to second commit;\ngit show HEAD~1 show the idea:\n\ngit show is a command that displays information about a passed commit;\nHEAD~1 is passed as an argument to git show, which means that a commit back from commit HEAD refers to;\nSo we got information about first commit, which is correct, it is a commit back from second commit.\n\n\n\n%%bash\nmkdir operations_examples\ncd operations_examples\ngit init &&gt; /dev/null\n\ntouch file1\ngit add --all\ngit commit -am \"first commit\" &&gt; /dev/null\n\ntouch file2\ngit add --all\ngit commit -am \"second commit\" &&gt; /dev/null\n\necho \"=====git log=====\"\ngit log --oneline --decorate\necho \"=====git show=====\"\ngit show HEAD~1\n\ncd ..\nrm -r operations_examples\n\n=====git log=====\ne94d28e (HEAD -&gt; master) second commit\n73e5ca8 first commit\n=====git show=====\ncommit 73e5ca80b04925b1f03d6b9e0b164a796439ff71\nAuthor: Fedor Kobak &lt;kobfedsur@gmail.com&gt;\nDate:   Sun Sep 10 10:25:44 2023 +0300\n\n    first commit\n\ndiff --git a/file1 b/file1\nnew file mode 100644\nindex 0000000..e69de29"
  },
  {
    "objectID": "git/previous_commit.html#use-hash",
    "href": "git/previous_commit.html#use-hash",
    "title": "Previous commit ~",
    "section": "Use hash",
    "text": "Use hash\nI was wondering if it is possible to use the commit hash in this construct, i.e. &lt;hash&gt;~1? Yes, you can! The following example confirms it:\n\nCreated two commits, first commit and second commit;\nStored hash of second commit in variable\nI used variable with syntax ~1;\nAll is well - I got info on first commit, which is one before second commit.\n\n\n%%bash\nmkdir operations_examples\ncd operations_examples\ngit init &&gt; /dev/null\n\ntouch file1\ngit add --all\ngit commit -am \"first commit\" &&gt; /dev/null\n\ntouch file2\ngit add --all\ngit commit -am \"second commit\" &&gt; /dev/null\nsecond_hash=$(git rev-parse HEAD)\n\necho \"=====git log=====\"\ngit log --oneline --decorate\necho \"=====git show=====\"\ngit show $second_hash~1\n\ncd ..\nrm -r operations_examples\n\n=====git log=====\ne79a082 (HEAD -&gt; master) second commit\n3d2a63f first commit\n=====git show=====\ncommit 3d2a63fcbd20eb9730ae250cef9ab6133e60ef19\nAuthor: Fedor Kobak &lt;kobfedsur@gmail.com&gt;\nDate:   Sun Sep 10 10:35:54 2023 +0300\n\n    first commit\n\ndiff --git a/file1 b/file1\nnew file mode 100644\nindex 0000000..e69de29"
  },
  {
    "objectID": "git/init.html",
    "href": "git/init.html",
    "title": "Init repository",
    "section": "",
    "text": "This page is about how to initialise a git repository in a folder.\nYou should use the git init command to create a new git repository. It will just add a `.git’ folder to the folder you’re init as the git repository - this folder is a marker for git that this is the git repository.\nIn the following example I just - Create a new folder; - Run git init there; - Show that .git folder appears there.\n\n%%bash\nmkdir git_init\ncd git_init\n\necho \"=====new folder is empty=====\"\nls -a\n\necho \"=====creating git repo=====\"\ngit init\necho \"=====check what appears in git repo=====\"\nls -a\n\ncd ..\nrm -r git_init\n\n=====new folder is empty=====\n.\n..\n=====creating git repo=====\nInitialized empty Git repository in /home/fedor/Documents/knowledge/git/git_init/.git/\n=====check what appears in git repo=====\n.\n..\n.git\n\n\nhint: Using 'master' as the name for the initial branch. This default branch name\nhint: is subject to change. To configure the initial branch name to use in all\nhint: of your new repositories, which will suppress this warning, call:\nhint: \nhint:   git config --global init.defaultBranch &lt;name&gt;\nhint: \nhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\nhint: 'development'. The just-created branch can be renamed via this command:\nhint: \nhint:   git branch -m &lt;name&gt;"
  },
  {
    "objectID": "git/checkout.html",
    "href": "git/checkout.html",
    "title": "Checkout",
    "section": "",
    "text": "git checkout is a command that allows you to swich current state of project to the another commit."
  },
  {
    "objectID": "git/checkout.html#back-up-commit",
    "href": "git/checkout.html#back-up-commit",
    "title": "Checkout",
    "section": "Back up commit",
    "text": "Back up commit\nIn the next example:\n\nTwo commits have been created:\n\nfirst commit add first_file.\nsecond commit edits first file and adds second file;\n\nI used git log to show these commits:\n\nNote the (HEAD-&gt;master) near second commit, this means you are staying on second commit and the master branch is pointing to it;\nNote To print HEAD-&gt;master in jupyter notebook output you have to use the --decorate option for some reason, in terminal this is not necessary;\n\nI stored the hash of the first commit in the first_hash variable and printed it;\nUsing git checkout $first_hash I moved HEAD to first commit, which means it’s now an active commit:\n\nNote In real work you can use the hash of the commit and don’t use a variable to store the commit hash;\nNote by default git checkout prints it’s message to the error stream, so I used 2&gt;&1 to redirect it to the out stream;\n\nBy calling git log again, I showed:\n\nThat (HEAD) is now next to the first commit message;\nThe second commit isn’t even shown now - git’s default behaviour for git log is to show the history of the commit that created the head commit. Use the --all option to show all messages;\n\nls will show that there is no second file in the project folder now - it’s ok, we’ve moved to a commit where there is no such file;\ncat first_file shows that the first file is still the same as it was in the second commit:\n\nThis is because we reverted git to first_commit but not the working directory. In git status you can see that first_file is marked as modified, it is indeed different from the commit that HEAD is currently referring to;\nTo return a file to the state as in first commit you simply need to do a git restore for that file, which is what I am doing and demonstrating the first_file from first commit.\n\n\n\n%%bash\nmkdir checkout_example\ncd checkout_example\n\ngit init &&gt; /dev/null\n\necho \"some text\" &gt; first_file\ngit add first_file\ngit commit -m \"first commit\" &&gt; /dev/null\nfirst_hash=$(git rev-parse HEAD)\n\necho \"some text\" &gt; second_file\necho \"this is the text for second commit\" &gt; first_file\ngit add second_file\ngit commit -m \"second commit\" &&gt; /dev/null\n\necho \"=====log=====\"\ngit log --decorate\n\necho\necho \"=====first commit hash=====\"\necho $first_hash\n\necho\necho \"=====git chechout to first commit=====\"\ngit checkout $first_hash 2&gt;&1\n\necho\necho \"=====log=====\"\ngit log --decorate\necho \"=====ls=====\"\nls\necho \"=====first_file=====\"\ncat first_file\n\necho\necho \"====status=====\"\ngit status\ngit restore first_file\necho \"=====first_file after restore=====\"\ncat first_file\n\n\ncd ..\nrm -r checkout_example\n\n=====log=====\ncommit 23e858c0a0924f75ef4f1a9eddf0c357f9d865b5 (HEAD -&gt; master)\nAuthor: Dranikf &lt;kobfedsur@gmail.com&gt;\nDate:   Thu Sep 7 22:45:41 2023 +0300\n\n    second commit\n\ncommit f42455672912f0347c9e29165583ddc5accf9b25\nAuthor: Dranikf &lt;kobfedsur@gmail.com&gt;\nDate:   Thu Sep 7 22:45:41 2023 +0300\n\n    first commit\n\n=====first commit hash=====\nf42455672912f0347c9e29165583ddc5accf9b25\n\n=====git chechout to first commit=====\nNote: switching to 'f42455672912f0347c9e29165583ddc5accf9b25'.\n\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by switching back to a branch.\n\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -c with the switch command. Example:\n\n  git switch -c &lt;new-branch-name&gt;\n\nOr undo this operation with:\n\n  git switch -\n\nTurn off this advice by setting config variable advice.detachedHead to false\n\nHEAD is now at f424556 first commit\nM   first_file\n\n=====log=====\ncommit f42455672912f0347c9e29165583ddc5accf9b25 (HEAD)\nAuthor: Dranikf &lt;kobfedsur@gmail.com&gt;\nDate:   Thu Sep 7 22:45:41 2023 +0300\n\n    first commit\n=====ls=====\nfirst_file\n=====first_file=====\nthis is the text for second commit\n\n====status=====\nHEAD detached at f424556\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n    modified:   first_file\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n=====first_file after restore=====\nsome text"
  },
  {
    "objectID": "git/branches.html",
    "href": "git/branches.html",
    "title": "Branches",
    "section": "",
    "text": "A branch is a reference to a commit which has the following property - it can be shifted to some next commit."
  },
  {
    "objectID": "git/branches.html#master",
    "href": "git/branches.html#master",
    "title": "Branches",
    "section": "Master",
    "text": "Master\nmaster is the name of the default git brunch that’s created when the repository is initialised.\nIn the example below, I show that in the git log output there is the word master in branches next to the hash of the first commit module just created.\n\n%%bash\nmkdir branches_example\ncd branches_example\ngit init &&gt; /dev/null\n\necho \"some text\" &&gt; some_file\ngit add --all\ngit commit -m \"first commit\" &&gt; /dev/null\ngit log --decorate\n\ncd ..\nrm -r branches_example\n\ncommit 2a72b456a3dbdfcdff5ad1dfdb65abed55b8a54b (HEAD -&gt; master)\nAuthor: Fedor Kobak &lt;kobfedsur@gmail.com&gt;\nDate:   Sat Sep 9 13:21:48 2023 +0300\n\n    first commit"
  },
  {
    "objectID": "git/branches.html#create-branch",
    "href": "git/branches.html#create-branch",
    "title": "Branches",
    "section": "Create branch",
    "text": "Create branch\nThe git branch \"&lt;branch name&gt;\" command creates a new branch on the commit that HEAD refers to.\nIn the next cell I used this command to create new_branch:\n\nIn the git log output you can see that there are now two branch names in the parentheses next to the commit hash;\nNote that by default git does not move HEAD to the created branch. The message HEAD -&gt; master in the git log output and the message On branch master in the git status output will indicate this. To create brunch an immediately move to it use git checkout with -b option.\n\n\n%%bash\nmkdir branches_example\ncd branches_example\ngit init &&gt; /dev/null\n\necho \"some text\" &&gt; some_file\ngit add --all\ngit commit -m \"first commit\" &&gt; /dev/null\ngit branch \"new_branch\"\n\n\necho \"=====git log=====\"\ngit log --decorate --oneline\necho \"=====git status=====\"\ngit status\n\ncd ..\nrm -r branches_example\n\n=====git log=====\n54231d2 (HEAD -&gt; master, new_branch) first commit\n=====git status=====\nOn branch master\nnothing to commit, working tree clean"
  },
  {
    "objectID": "git/branches.html#list-branches",
    "href": "git/branches.html#list-branches",
    "title": "Branches",
    "section": "List branches",
    "text": "List branches\nTo list all available branches, use git branch (without the positional argument, which refers to the name of the new branch).\nThe branch to which HEAD will be linked is indicated by an asterisk (*).\n\nBasic example\nIn the following example\n\nI create a few branches;\nThen use the git branch' command to print out a list of branches - all the branches I created and the automatically generatedmaster’ branch are shown. * indicates master;\nThen I switch HEAD to another branch;\nNow git branch prints almost the same, except that * indicates the branch I have selected.\n\n\n%%bash\nmkdir branches_example\ncd branches_example\ngit init &&gt; /dev/null\n\nfor i in {1..5}\ndo\n  echo \"Line $i\" &gt;&gt; file\n  git add file\n  git commit -m \"Branch $i add.\" &&gt; /dev/null\n  git branch \"branch_$i\"\ndone\n\necho \"=====git branch=====\"\ngit branch\n\necho\ngit checkout branch_3 2&gt;&1\necho \"=====git branch=====\"\ngit branch\n\ncd ..\nrm -r branches_example\n\n=====git branch=====\n  branch_1\n  branch_2\n  branch_3\n  branch_4\n  branch_5\n* master\n\nSwitched to branch 'branch_3'\n=====git branch=====\n  branch_1\n  branch_2\n* branch_3\n  branch_4\n  branch_5\n  master\n\n\n\n\nHEAD arbitrary\nYou can use git checkout for optional commits. This subsection shows how git branch will display the case where you have selected some commit (not branch) to be referenced by HEAD - which branch will * display?\nIn the following example:\n\nI generated few branches;\nI make a random commit and remember its hash. And to this hash I make git checkout - now HEAD refers to an arbitrary commit and not to some branch;\nThen I execute git branch. For * created special line * (HEAD detached at &lt;hash&gt;) which makes it clear that HEAD points to an arbitrary commit and not to any of the branches.\n\n\n%%bash\nmkdir branches_example\ncd branches_example\ngit init &&gt; /dev/null\n\nfor i in {1..5}\ndo\n  echo \"Line $i\" &gt;&gt; file\n  git add file\n  git commit -m \"Branch $i add.\" &&gt; /dev/null\n  git branch \"branch_$i\"\ndone\n\necho \"Temp line\" &gt;&gt; file\ngit add file\ngit commit -m \"Commit without branch.\" &&gt; /dev/null\nno_branch_commit_hash=$(git rev-parse HEAD)\n\necho\ngit checkout $no_branch_commit_hash 2&gt;&1\necho $no_branch_commit_hash\necho \"=====git branch=====\"\ngit branch\n\ncd ..\nrm -r branches_example\n\n\nNote: switching to '5b8059a0c1a47cdb0fcd6233a58ea7cf159ebf15'.\n\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by switching back to a branch.\n\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -c with the switch command. Example:\n\n  git switch -c &lt;new-branch-name&gt;\n\nOr undo this operation with:\n\n  git switch -\n\nTurn off this advice by setting config variable advice.detachedHead to false\n\nHEAD is now at 5b8059a Commit without branch.\n5b8059a0c1a47cdb0fcd6233a58ea7cf159ebf15\n=====git branch=====\n* (HEAD detached at 5b8059a)\n  branch_1\n  branch_2\n  branch_3\n  branch_4\n  branch_5\n  master"
  },
  {
    "objectID": "git/branches.html#commit-to-branch",
    "href": "git/branches.html#commit-to-branch",
    "title": "Branches",
    "section": "Commit “to” branch",
    "text": "Commit “to” branch\nIt is not correct to say commit “to” a branch, it is correct to say make a commit and move a branch into it. Every time a commit is made, it is written somewhere in git and the branch that HEAD is currently bound to is simply moved to that commit.\nThe following example shows how I switched to new_branch while HEAD was on the first commit. And after the second commit, the text HEAD-&gt;new_branch automatically moved to the second commit.\n\n%%bash\nmkdir branches_example\ncd branches_example\ngit init &&gt; /dev/null\n\necho \"first line\" &gt; some_file\ngit add --all\ngit commit -m \"first commit\" &&gt; /dev/null\ngit checkout -b \"new_branch\"\n\necho \"=====first commit=====\"\necho \"-----git log-----\"\ngit log --decorate --oneline\n\necho\necho \"second line\" &gt;&gt; some_file\ngit commit -am \"second commit\" &&gt; /dev/null\necho \"=====second commit=====\"\necho \"-----git log-----\"\ngit log --decorate --oneline\n\ncd ..\nrm -r branches_example\n\nSwitched to a new branch 'new_branch'\n\n\n=====first commit=====\n-----git log-----\n21bed3c (HEAD -&gt; new_branch, master) first commit\n\n=====second commit=====\n-----git log-----\naf9c0df (HEAD -&gt; new_branch) second commit\n21bed3c (master) first commit"
  },
  {
    "objectID": "git/branches.html#idea-of-branching",
    "href": "git/branches.html#idea-of-branching",
    "title": "Branches",
    "section": "Idea of branching",
    "text": "Idea of branching\nWhen you talk about branches, you assume that they can grow independently of each other; in git, that is the idea.\nCommits made to one branch are not included in the history of the other branches. So in the following example:\n\nIn the beginning:\n\nbasic commit was created;\nFrom basic commit two branches were created, first_branch and second_branch;\nA log of this state of the repository was printed after the message =====initial log=====;\n\nThen commits have been made to both branches;\nWith the command git log --all --graph --decorate I printed the log:\n\nBy default, git log only prints history for the commit that HEAD refers to, so we need the --all option to print all history;\nThe --graph option is a feature that allows us to visualise branches - so we have `basic commit’ and two brunches of it;\nThe --decocate option is just needed for correct dislpay in jupyter.\n\n\n\n%%bash\nmkdir branches_example\ncd branches_example\ngit init &&gt; /dev/null\n\necho \"basic content\" &gt; file\ngit add --all\ngit commit -m \"basic commit\" &&gt; /dev/null\n\ngit branch first_branch\ngit branch second_branch\n\necho \"=====initial log=====\"\ngit log --decorate --oneline\n\necho \"=====work with first_branch=====\"\ngit checkout first_branch\necho \"first branch content\" &gt; file\ngit commit -am \"first branch commit\" &&gt; /dev/null\necho \"=====work with second_branch=====\"\ngit checkout second_branch\necho \"second branch content\" &gt; file\ngit commit -am \"second branch commit\" &&gt; /dev/null\n\necho\necho \"=====git log=====\"\ngit log --graph --all --decorate\n\ncd ..\nrm -r branches_example\n\n=====initial log=====\ne544f85 (HEAD -&gt; master, second_branch, first_branch) basic commit\n=====work with first_branch=====\n=====work with second_branch=====\n\n=====git log=====\n* commit c5c73f3cf1903d3ff3360c3454fff0bca21c7616 (first_branch)\n| Author: Fedor Kobak &lt;kobfedsur@gmail.com&gt;\n| Date:   Sun Sep 10 11:27:18 2023 +0300\n| \n|     first branch commit\n|   \n| * commit a377c623cb616c02193a5f2ef6d8389ddd5cbc1a (HEAD -&gt; second_branch)\n|/  Author: Fedor Kobak &lt;kobfedsur@gmail.com&gt;\n|   Date:   Sun Sep 10 11:27:18 2023 +0300\n|   \n|       second branch commit\n| \n* commit e544f8579d51b9383defa2b83f31e58ffc614569 (master)\n  Author: Fedor Kobak &lt;kobfedsur@gmail.com&gt;\n  Date:   Sun Sep 10 11:27:18 2023 +0300\n  \n      basic commit\n\n\nSwitched to branch 'first_branch'\nSwitched to branch 'second_branch'"
  },
  {
    "objectID": "git/ignore.html",
    "href": "git/ignore.html",
    "title": "How git ignore works",
    "section": "",
    "text": "To list files that match any pattern in your .gitignore, use the --ignored option to the git status command. All files to be ignored will be listed in the Ignored files: section.\nIn the next cell I define a .gitignore' file, which tells git to ignore any file that matchesfile_to_be_ignored*’.\n\n%%writefile ignore_files/check_ignored_files/.gitignore\nfile_to_be_ignored*\n\nOverwriting ignore_files/check_ignored_files/.gitignore\n\n\nSo lets see how it looks. In the following cell I create some files, some matching file_to_be_ignored, some not, and then run git status with and without --ignored.\nWith the --ignored option, you can see the file_to_be_ignored* files in the Ignored files: section.\n\n%%bash\ncd ignore_files/check_ignored_files\ngit init &&gt; /dev/null\n\ntouch file_to_be_ignored{1..3}\ntouch other_file{1..3}\n\necho \"=====git status=====\"\ngit status\n\necho\necho \"=====git status --ignored=====\"\ngit status --ignored\n\nrm -r .git\n\n=====git status=====\nOn branch master\n\nNo commits yet\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    .gitignore\n    other_file1\n    other_file2\n    other_file3\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n\n=====git status --ignored=====\nOn branch master\n\nNo commits yet\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    .gitignore\n    other_file1\n    other_file2\n    other_file3\n\nIgnored files:\n  (use \"git add -f &lt;file&gt;...\" to include in what will be committed)\n    file_to_be_ignored1\n    file_to_be_ignored2\n    file_to_be_ignored3\n\nnothing added to commit but untracked files present (use \"git add\" to track)"
  },
  {
    "objectID": "git/ignore.html#check-ignored-files",
    "href": "git/ignore.html#check-ignored-files",
    "title": "How git ignore works",
    "section": "",
    "text": "To list files that match any pattern in your .gitignore, use the --ignored option to the git status command. All files to be ignored will be listed in the Ignored files: section.\nIn the next cell I define a .gitignore' file, which tells git to ignore any file that matchesfile_to_be_ignored*’.\n\n%%writefile ignore_files/check_ignored_files/.gitignore\nfile_to_be_ignored*\n\nOverwriting ignore_files/check_ignored_files/.gitignore\n\n\nSo lets see how it looks. In the following cell I create some files, some matching file_to_be_ignored, some not, and then run git status with and without --ignored.\nWith the --ignored option, you can see the file_to_be_ignored* files in the Ignored files: section.\n\n%%bash\ncd ignore_files/check_ignored_files\ngit init &&gt; /dev/null\n\ntouch file_to_be_ignored{1..3}\ntouch other_file{1..3}\n\necho \"=====git status=====\"\ngit status\n\necho\necho \"=====git status --ignored=====\"\ngit status --ignored\n\nrm -r .git\n\n=====git status=====\nOn branch master\n\nNo commits yet\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    .gitignore\n    other_file1\n    other_file2\n    other_file3\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n\n=====git status --ignored=====\nOn branch master\n\nNo commits yet\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    .gitignore\n    other_file1\n    other_file2\n    other_file3\n\nIgnored files:\n  (use \"git add -f &lt;file&gt;...\" to include in what will be committed)\n    file_to_be_ignored1\n    file_to_be_ignored2\n    file_to_be_ignored3\n\nnothing added to commit but untracked files present (use \"git add\" to track)"
  },
  {
    "objectID": "git/ignore.html#any-entry",
    "href": "git/ignore.html#any-entry",
    "title": "How git ignore works",
    "section": "Any entry",
    "text": "Any entry\nUsing the syntax *&lt;symbol_combination&gt;* you can specify a symbol combination and any file with that symbol combination in the relative filepath will be ignored by git.\nThe following example shows:\n\nFiles tree and .gitignore, which tells git to ignore any file with symb_comb in it’s filepath;\nThe results of the git status of a freshly created repository, which helps us understand exactly what files git is ignoring.\n\nThere are few important fatures:\n\nOf course, some_folder2 doesn’t have symb_comb in it’s filepath, but it only has one file and that file should be ignored by git, so to git this folder appears empty;\nGit ignores files in directories started or ended with symb_comb.\n\n\n%%bash\ncd ignore_files/any_entry\ngit init &&gt; /dev/null\n\necho \"=====FILES TREE=====\"\ntree\n\necho\necho\necho \"=====.GITIGNORE FILE=====\"\ncat .gitignore\n\n\necho\necho\necho \"=====GIT STATUS=====\"\ngit status\n\nrm -r .git\n\n=====FILES TREE=====\n.\n├── some_folder\n│   └── some_file\n└── some_random_file\n\n1 directory, 2 files\n\n\n=====.GITIGNORE FILE=====\n*symb_comb*\n\n\n=====GIT STATUS=====\nOn branch master\n\nNo commits yet\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    .gitignore\n    some_folder/\n    some_random_file\n\nnothing added to commit but untracked files present (use \"git add\" to track)"
  },
  {
    "objectID": "git/ignore.html#gitignore-in-subfolders",
    "href": "git/ignore.html#gitignore-in-subfolders",
    "title": "How git ignore works",
    "section": ".gitignore in subfolders",
    "text": ".gitignore in subfolders\nYou can add a .gitignore file in any subdirectory of your repository - git will interpret this and ignore all files, and you should describe the files relative to the subdirectory path.\nFor example, let’s look at creating a repository based on the ignore_files/subfolder folder. Suppose there is also a test_subfolder folder, and there is some special work being done there that requires the some_file_in_subfolder file to be ignored. So you can just add a .gitignore file to test_subfolder and git will automatically ignore any files mentioned in test_subfolder/.gitinore from test_subfolder.\nIn the following cells, this is what happens to the file test_subfolder/some_file_in_subfolder, and git status --ignored shows this file as ignored.\n\n%%writefile ignore_files/subfolders/test_subfolder/.gitignore\nsome_file_in_subfolder\n\nOverwriting ignore_files/subfolders/test_subfolder/.gitignore\n\n\n\n%%writefile ignore_files/subfolders/test_subfolder/some_file_in_subfolder\ntest file content\n\nWriting ignore_files/subfolders/test_subfolder/some_file_in_subfolder\n\n\n\n%%bash\ncd ignore_files/subfolders\ngit init &&gt; /dev/null\n\ngit add --all\n\ngit status --ignored\nrm -r .git\n\nOn branch master\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n    new file:   test_subfolder/.gitignore\n\nIgnored files:\n  (use \"git add -f &lt;file&gt;...\" to include in what will be committed)\n    test_subfolder/some_file_in_subfolder"
  },
  {
    "objectID": "machine_learning/var_vs_bias/quadratic.html",
    "href": "machine_learning/var_vs_bias/quadratic.html",
    "title": "Knowledge",
    "section": "",
    "text": "import numpy as np\nimport scipy.interpolate as inter\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\nnp.random.seed(4)\n\n# data generation\nx_p = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\ny_p = np.array([2, 4, 3, 5, 4, 6, 5, 7, 6, 8, 10])\npoly = inter.lagrange(x_p, y_p)\ndef f(x):\n    X = np.concatenate(\n        [(x**a)[:, np.newaxis] for a in range(len(poly.coef))],\n        axis = 1\n    )\n    return np.dot(X, poly.coef[::-1][:, np.newaxis]).ravel()\n\ny = lambda x: f(x) + np.random.normal(0, 1, x.shape)\n\nX_sample = np.sort(np.random.uniform(0, 10, 200)).astype(np.longdouble)\nY_sample = y(X_sample)\n\n\n# models\ndef get_poly_matrix(X, p = 2):\n    return np.concatenate(\n        [np.array(X)[:, np.newaxis]**(i) for i in range(p+1)],\n        axis = 1\n    )\ndef get_poly_predict(X, y, p = 2):\n    \n    X_matr = get_poly_matrix(X, p)\n    return LinearRegression(\n        fit_intercept=False\n    ).fit(X_matr, y).predict(X_matr)\n\n\nCHANGE_ME = 17\nplt.scatter(X_sample, Y_sample, color = \"black\")\nplt.title(\"polynomial degree \" + str(CHANGE_ME))\nplt.plot(\n    X_sample,\n    get_poly_predict(X_sample, Y_sample, CHANGE_ME),\n    linewidth = 4\n)\nplt.savefig(\"poly16.png\")"
  },
  {
    "objectID": "machine_learning/tree_based_methods/tree.html",
    "href": "machine_learning/tree_based_methods/tree.html",
    "title": "Источники",
    "section": "",
    "text": "Изучение алгоритма “Решающее дерево”\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport seaborn as sns\n\nimport sklearn.tree as tree\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.model_selection import cross_val_score,\\\n                                    train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.datasets import load_breast_cancer\n\nimport graphviz\n\n\nISLR страница 336;\nUser guide от sklearn;\nАлгоритм обрезки с учётом сложности затрат в sklearn;\nПример использования алгоритма обрезки с учётом сложности затрат от sklearn;\n\n\nИдея группы методов\nРешающим деревом можно решать как задачи регрессии так и классификации. Идея в том, что исходное пространтсво предикторов разбивается на обаласти (\\(R_1, R_2, ... , R_m\\)) и на каждой области \\(R_i\\) предсказывается значение зависящее от наблюдений в ней.\nНапример для задачи регрессии можно предсказывать среднее значение отклика для выбранной области. Так в следующем примере представлено как может быть сформировано решеющее дерево для данных соответсвующий параболоиду.\nГраф сверху описывает принцип принятия решений, рисунки снизу указывают как формировались предсказания.\n\nзвенья графа в которых происходит решение называются внутренние узлы (internal nodes);\nзвенья графа которые формирут конечный результат и соответсвуют \\(R_i\\) называеются конечные узлы или листики (terminal nodes).\n\n\nЗадача регрессии\n\n# подготовка обучающей выборки\nsample_size = 500\nnp.random.seed(30)\n\nx1_lim = [-5, 5]\nx2_lim = [-5, 5]\n\nsample_df = pd.DataFrame({\n    \"$x_1$\" : np.random.uniform(*x1_lim, sample_size),\n    \"$x_2$\" : np.random.uniform(*x2_lim, sample_size),\n})\n\nsample_df[\"$y$\"] = sample_df[\"$x_1$\"]**2 + sample_df[\"$x_2$\"]**2\n\n# формирование модели\nmy_first_tree = tree.DecisionTreeRegressor(\\\n    max_depth = 3\n).fit(\n    sample_df[[\"$x_1$\", \"$x_2$\"]],\n    sample_df[\"$y$\"]\n)\n\n# сетка предстказаний\nx1_range = np.arange(*x1_lim, 0.1)\nx2_range = np.arange(*x2_lim, 0.1)\nx1_mesh, x2_mesh = np.meshgrid(x1_range, x2_range)\n\nmesh_df = pd.DataFrame({\n    \"$x_1$\" : x1_mesh.ravel(),\n    \"$x_2$\" : x2_mesh.ravel()\n})\np_mesh = np.reshape(\n    my_first_tree.predict(mesh_df),\n    x1_mesh.shape\n)\n\n# визуализация\ndot_data = tree.export_graphviz(\n    my_first_tree, out_file=None,\n    feature_names = [\"x_1\", \"x_2\"],\n    filled=True, rounded=True,  \n    special_characters=True\n)\ngraph = graphviz.Source(dot_data)\ndisplay(graph)\n\nfig = plt.figure(figsize = [20, 10])\n\nax1 = fig.add_subplot(121)\nDecisionBoundaryDisplay.from_estimator(\n    my_first_tree,\n    mesh_df,\n    cmap=cm.coolwarm,\n    response_method=\"predict\",\n    ax = ax1,\n)\nsns.scatterplot(\n    data = sample_df,\n    x = \"$x_1$\", y = \"$x_2$\",\n    size = \"$y$\",\n    ax=ax1,\n    color = \"green\"\n)\nplt.xlabel(\"$x_1$\", fontsize = 14)\nplt.ylabel(\"$x_2$\", fontsize = 14)\n\nax2 = fig.add_subplot(122, projection='3d')\nax2.scatter(\n    sample_df[\"$x_1$\"],\n    sample_df[\"$x_2$\"],\n    sample_df[\"$y$\"],\n    color = \"green\"\n)\nsurf = ax2.plot_surface(\n    x1_mesh, x2_mesh, p_mesh,\n    cmap=cm.coolwarm\n)\n\nfig.colorbar(surf, shrink=0.5, aspect=5)\n\nplt.xlabel(\"$x_1$\"); plt.ylabel(\"$x_2$\");\n\n\n\n\n\n\n\n\n\nЗадача классификации\nИдея та же - бинарное деление пространтсва предикоторов максимизируя некоторую статистику. Отдельного внимания заслуживают эти статистики:\n\nИндекс GINI:\n\n\\[G = \\sum_{k =1}^K \\hat{p}_{mk}(1-\\hat{p}_{mk});\\]\nгде \\(\\hat{p}_{mk}\\) - доля наблюдений \\(k\\) класса принадлежащих региону \\(m\\);\nИнтерестно, что в данном случае получается что чем ближе доли \\(\\hat{p}_{mk}\\) к нулю или единице (что в данном случае хорошо), тем меньше будет индекс GINI. Пока не до конца понятно как именно происходит минимизация - похоже подбирается такое отсечение, чтобы GINI был миниматен в любой из получаемых после отсечения областей.\n\nЭнтропия\n\n\\[D = -\\sum_{k=1}^{K}\\hat{p}_{mk}log(\\hat{p}_{mk})\\]\nОбладает очень похожими свойствами с индексом GINI.\n\n\n\nОбрезка дерева\nОчевидно, что слишком глубокие деревья ведут к переобучению а недостаточно глубокие к низкой гибкости модели. В результате требуется подобрать такую глибину дерева, чтобы подобрать опитимальный компромисс между дисперсией и смещением. Для того можно пробовать:\n\nИдеальном случае следовало бы провести кроссвалидацию для дерева любой длинны и выбрать ту, что на кроссвалидации выдает наилучшие результаты;\nВ [1] предлагают использовать алгоритм “обрезки с учетом сложности затрат” (cost complexity pruning).\n\n\nАлгоритм “обрезки с учетом сложности затрат”\nАлгоритм предполагает использование целевой функции:\n\\[\\sum_{m=1}^{|T|}\\sum_{x_i \\in R_m}(y_i-\\hat{y}_{R_m})^2+\\alpha|T|\\rightarrow min\\]\nМодель кроссвалидируется при разных \\(\\alpha\\). Соответсвенно подбирается оптимальный параметр \\(\\alpha\\). По сути это эквивалентно регуляризации модели.\n\nSKLearn\nПример представленный в докумментации sklearn. Основная фишка в том, что sklearn может найти эффективные \\(\\alpha\\) для каждого звена. То есть такие \\(\\alpha\\), при которых отбрасывается следущее звено за ненадобностью (то есть эффект от \\(\\alpha|T|\\) выше чем влияние узла на результат на тренировочной выборке).\nВ sklearn предзожен метод tree.DecisionTreeClassifier.cost_complexity_pruning_path. Она возвращаяет те после которых отбрасывается листик. Кроме того, можно получить соответствующую суммарную примесь листьев на каждом этапе процесса обрезки (total impurity of leaves). Так далее представлен гарфик, на котором показано, как с ростом \\(\\alpha\\) все больше и больше лисьев оказываются отброщенными.\n\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nclf = tree.DecisionTreeClassifier(random_state=0)\npath = clf.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities\n\nfig, ax = plt.subplots()\nax.plot(ccp_alphas[:-1], impurities[:-1], marker=\"o\", drawstyle=\"steps-post\")\nax.set_xlabel(\"эффективные $\\\\alpha$\")\nax.set_ylabel(\"суммарная примесь листьев\")\nans = ax.set_title(\"Суммарная примесь листьев и эффективные $\\\\alpha$\")\n\n\n\n\nСледующий пример учит модели на эффективных \\(\\alpha\\) и представляет как с увеличением \\(\\alpha\\) уменьшаяется гибкость модели - мельше листьев и глубины. Вплоть до одного узла.\nЭтот пример я вставил больше потому, что полезно знать как извлекать длинну глубину и число узлов полученного дерева: - tree.DecisionTreeClassifier.tree_.max_depth; - tree.DecisionTreeClassifier.tree_.node_count.\n\nclfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = tree.DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    clf.fit(X_train, y_train)\n    clfs.append(clf)\nprint(\n    \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n        clfs[-1].tree_.node_count, ccp_alphas[-1]\n    )\n)\n\nclfs = clfs[:-1]\nccp_alphas = ccp_alphas[:-1]\n\nnode_counts = [clf.tree_.node_count for clf in clfs]\ndepthes = [clf.tree_.max_depth for clf in clfs]\nfig, ax = plt.subplots(2, 1, figsize = [14, 10])\nax[0].plot(ccp_alphas, node_counts, marker=\"o\", drawstyle=\"steps-pre\")\nax[0].set_xlabel(\"$\\\\alpha$\", fontsize = 14)\nax[0].set_ylabel(\"Число узлов\")\nax[0].set_title(\"Число узлов и $\\\\alpha$\")\nax[1].plot(ccp_alphas, depthes, marker=\"o\", drawstyle=\"steps-pre\")\nax[1].set_xlabel(\"$\\\\alpha$\", fontsize = 14)\nax[1].set_ylabel(\"Глупина дерева\")\nax[1].set_title(\"Глубина и $\\\\alpha$\")\nfig.tight_layout()\n\nfor i, (alpha, node_count, depth) in enumerate(zip(\n    ccp_alphas, node_counts, depthes\n)):\n    ax[0].text(alpha, node_count, i, fontsize = 14)\n    ax[0].axhline(node_count, color = \"gray\", linestyle = \"dashed\",alpha = 0.1)\n    ax[1].text(alpha, depth, i, fontsize = 14)\n    ax[1].axhline(depth, color = \"gray\", linestyle = \"dashed\",alpha = 0.1)\n\nax[0].set_yticks(node_counts)\nans = ax[1].set_yticks(depthes)\n\nNumber of nodes in the last tree is: 1 with ccp_alpha: 0.3272984419327777\n\n\n\n\n\nДля того, что-бы убедиться во всем описанно я решил нарисовать графы для деревьев соответсвующие 7-й и 8-й точкам. Чтобы проследить как исчезает лишний уровень глубины дерева.\n\n# визуализация\ndef plot_my_tree(clf):\n\n    dot_data = tree.export_graphviz(\n        clf, out_file=None,\n        filled=True, rounded=True,\n        special_characters=True\n    )\n    graph = graphviz.Source(dot_data)\n    display(graph)\n\nprint(\"Alpha =\", ccp_alphas[7])\nplot_my_tree(clfs[7])\nprint(\"Alpha =\", ccp_alphas[8])\nplot_my_tree(clfs[8])\n\nAlpha = 0.009114019793328328\nAlpha = 0.011443661971830986\n\n\n\n\n\n\n\n\nДалее продолжается исследование баланса между дисперсией и смещением. В следующем примере показано как растёт точность предсказаний на тестовой выборке с увеличением параметра \\(\\alpha\\).\n\ntrain_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]\n\nfig, ax = plt.subplots()\nax.set_xlabel(\"$\\\\alpha$\")\nax.set_ylabel(\"Точность\")\nax.set_title(\"Точность и $\\\\alpha$ для терениовочной и тестовой выборки\")\nax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"Тренировачная\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker=\"o\", label=\"Тестовая\", drawstyle=\"steps-post\")\nax.legend()\nplt.show()\n\n\n\n\n\n\nISLR\nПроведем на python эксперимент аналогичный представленному в [1]. В источнике не указано подробно как эксперимент проводился, потому проведу его как чуствую:\n\nВсе следующие шаги формируются в множество прогонов:\n\nФормируем train/test;\nВычиляем эффективные \\(\\alpha\\) для выбранного train;\nДля всех эффективных \\(\\alpha\\):\n\nОциниваем на train/test;\nКроссвалидируем на test.\n\n\n\nПо идее кроссвалидация тут не нужна, потому как прогоны разбиения train/test сами по себе эквиванентны кроссвалидации.\nfrom tqdm import tqdm\nnp.random.seed(25) fold_count = 6 Hitters = pd.read_csv(“Hitters.csv”).dropna()\ndef get_fit_data(f): return ( pd.get_dummies( f[[ “Years”, “RBI”, “Hits”, “PutOuts”, “Walks”, “Runs” ]] ), np.log(f[“Salary”]) )\ntree_regressor = tree.DecisionTreeRegressor()\nresults_frame = pd.DataFrame( columns = [ “alpha”, “Прогон экперимента”, “MSE тренировочное”, “MSE тестовое”, “Кроссвалидация”, “Глубина дерева”, “Число листов” ], )\nindexator = 0\n\n\n\n\nпрогоняем эксперимент\nfor i in tqdm(range(50)):\ntrain = Hitters.sample(132)\ntest = Hitters.loc[~Hitters.index.isin(train.index)]\n\ncpp_alphas = tree_regressor.cost_complexity_pruning_path(\n    *get_fit_data(train)\n)[\"ccp_alphas\"]\n\nfor alpha in cpp_alphas:\n\n    # подготовка\n    tree_regressor.set_params(ccp_alpha = alpha)\n    train_model = tree_regressor.fit(*get_fit_data(train))\n    train_X, train_y = get_fit_data(train)\n    test_X, test_y = get_fit_data(test)\n    train_model.fit(train_X, train_y)\n\n    train_pred = train_model.predict(train_X)\n    test_pred = train_model.predict(test_X)\n    cv_result = (\n        cross_val_score(\n            tree_regressor,\n            *get_fit_data(train),\n            cv = fold_count,\n            scoring=\"neg_mean_squared_error\"\n        )[:, np.newaxis]\n    )\n\n    results_frame.loc[indexator] = {\n        \"alpha\" : alpha,\n        \"Прогон экперимента\" : i,\n        \"MSE тренировочное\" : mean_squared_error(train_y, train_pred),\n        \"MSE тестовое\" : mean_squared_error(test_y, test_pred),\n        \"Кроссвалидация\" : -np.mean(cv_result),\n        \"Глубина дерева\" : train_model.tree_.max_depth,\n        \"Число листов\" : train_model.get_n_leaves()\n    }\n\n    indexator += 1\nresults_frame.to_csv(“ISLR_exeriment.csv”)\nДалее графичеки представим результаты. На следующем графике показана связь точности модели с числом листов. Палочками представлено стандарное отлоенние по различным прогонам.\n\nresults_frame = pd.read_csv(\"ISLR_exeriment.csv\")\n\ntree_depth_gb = results_frame.groupby(\"Число листов\")\n\nMSE_plots = (\n    tree_depth_gb[\"MSE тренировочное\"],\n    tree_depth_gb[\"MSE тестовое\"],\n    tree_depth_gb[\"Кроссвалидация\"]\n)\n\nfor MSE in MSE_plots:\n    plt.errorbar(tree_depth_gb.groups.keys(), MSE.mean(), MSE.std())\n\nans = plt.legend([\n    \"MSE тренировочное\",\n    \"MSE тестовое\",\n    \"Кроссвалидация\"\n])\nplt.xlabel(\"Число листов\", fontsize = 14)\nplt.ylabel(\"Точность модели\", fontsize = 14)\n\nans = plt.xlim([0,20])\n\n\n\n\nНе получается в точности воспроизвести эксперимент из ISLR. И в нашем случае лучшая модель исопльзует 4 листа. Но идея о бесконечном увеличении метрики качества на обучающей выборке понятна.\n\n\nДерево и пропуски\nВроде сказано, что дерево может по прежнему обучаться на тех наборах данных в которых присудствуют пропуски. Рассмотрим подробнее как это работатет.\nВ следующей ячейке создан набор данных на задачу классификации , в котром пропущено 100 значений для одной из переменных.\n\nnp.random.seed(20)\n\nd = pd.DataFrame({\n    \"x1\" : np.random.uniform(0, 10, 500),\n    \"x2\" : np.random.uniform(0, 10, 500)\n})\n\nr = d[\"x1\"] + d[\"x2\"]\n\nd[\"c\"] = ((r - r.min())/(r.max() - r.min())).apply(\n    lambda prob: np.random.choice([1,0], p = [prob**2, 1-(prob**2)])\n)\n\nd[\"x1 nans\"] = d[\"x1\"]\nd.loc[d.sample(100).index, \"x1 nans\"] = np.NaN\n\nans = sns.scatterplot(data=d, x=\"x1\", y=\"x2\", hue=\"c\", style=d[\"x1 nans\"].isna())\n\n\n\n\nПопробуем, такой набор данных скормить дереву и посмотрим, какое решение оно предложит:\nclf = tree.DecisionTreeClassifier(max_depth=3,random_state=0) clf.fit(d[[“x1 nans”, “x2”]], d[“c”])\ndot_data = tree.export_graphviz( clf, out_file=None, filled=True, rounded=True, special_characters=True, feature_names=[“x1”, “x2”] ) graph = graphviz.Source(dot_data) display(graph)\nВ общем, это не сработало - выдает ошибку оценщик."
  },
  {
    "objectID": "machine_learning/data_transformations/PCA.html",
    "href": "machine_learning/data_transformations/PCA.html",
    "title": "Первая главная компонента",
    "section": "",
    "text": "Метод главных компонент\nВсе особенности и тонкоссти исползования метода главных компонент.\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.decomposition import PCA"
  },
  {
    "objectID": "machine_learning/data_transformations/PCA.html#определение",
    "href": "machine_learning/data_transformations/PCA.html#определение",
    "title": "Первая главная компонента",
    "section": "Определение",
    "text": "Определение\nПусть имеюстся \\(n\\) наблюдений за \\(m\\) переменными - \\(x_{ij}, i\\in \\overline{1,n}, j \\in \\overline{1,m}\\). Тогда первой главной компонентой назвается функция:\n\\[Z_1(x_1, x_2, ... , x_m) = \\sum_{j=1}^m\\phi_{j}(x_j - \\bar{x}_j);\\]\nГде: - \\(\\bar{x}_j\\) - среднее выборочное \\(j\\)-й переменной \\(\\forall j\\).\nПри том \\(\\phi_j\\) подбираются так, чтобы:\n\\[(\\phi_1, \\phi_2, ... \\phi_m) = argmax_{\\varphi_1, \\varphi_2, ..., \\varphi_{m}}\\left\\{Var\\left[\\sum_{j=1}^m\\varphi_{j}(x_j - \\bar{x}_j)\\right]\\right\\};\\]\n\\[\\sum_{j=1}^m \\varphi_j^2 = 1.\\]’"
  },
  {
    "objectID": "machine_learning/data_transformations/PCA.html#пример",
    "href": "machine_learning/data_transformations/PCA.html#пример",
    "title": "Первая главная компонента",
    "section": "Пример",
    "text": "Пример\nДопустим имеются \\(5\\) наблюдений за 2-мя переменными.\n“\\(\\phi_{j}(x_{{i}, {j}} - \\\\bar{x}_{j})\\)”.format(i = 2,j = 3)\n\nfor i in range(1, len(x1) + 1):\n    formula = \"\"\n    for j in range(1,3):\n        formula += \"$\\phi_1(x_{\" + str(i) + str(j) + \"} - \\\\bar{x}_\" + str(j) + \")$\"\n    print(\"- \" + formula + \";\")\n\n- $\\phi_1(x_{11} - \\bar{x}_1)$$\\phi_1(x_{12} - \\bar{x}_2)$;\n- $\\phi_1(x_{21} - \\bar{x}_1)$$\\phi_1(x_{22} - \\bar{x}_2)$;\n- $\\phi_1(x_{31} - \\bar{x}_1)$$\\phi_1(x_{32} - \\bar{x}_2)$;\n- $\\phi_1(x_{41} - \\bar{x}_1)$$\\phi_1(x_{42} - \\bar{x}_2)$;\n\n\n\n\\(\\phi_1(x_{1,1} - \\bar{x}_1)\\)\\(\\phi_1(x_{1,2} - \\bar{x}_2)\\);\n\\(\\phi_1(x_{2,1} - \\bar{x}_1)\\)\\(\\phi_1(x_{2,2} - \\bar{x}_2)\\);\n\\(\\phi_1(x_{3,1} - \\bar{x}_1)\\)\\(\\phi_1(x_{3,2} - \\bar{x}_2)\\);\n\\(\\phi_1(x_{4,1} - \\bar{x}_1)\\)\\(\\phi_1(x_{4,2} - \\bar{x}_2)\\);\n\n\nx1 = np.array([2, 3, 5, 6])\nx2 = np.array([3, 2, 6, 5])\n\nplt.figure(figsize = [10, 7])\nplt.scatter(x1, x2, s = 100)\n\n\nfor i in range(len(x1)):\n    plt.annotate(\n        str(i+1), (x1[i], x2[i]), fontsize = 20\n    )\n\nplt.axhline(\n    np.mean(x2), color = 'red', linestyle = \"dashed\"\n)\nplt.axvline(\n    np.mean(x1), color = \"red\",linestyle = \"dashed\"\n)\n\ndef create_tiks(values, sub_index):\n    \n    ticks_vals = np.unique(np.concatenate(\n        [values, [np.mean(values)]]\n    ))\n    tick_labels = \\\n    [\n        str(tick_val) \n        if tick_val != np.mean(x1) \n        else \"$\\\\bar{x}_ \" + sub_index + \" = $\" + str(tick_val) \n        \\\n        for tick_val in ticks_vals\n    ]\n    \n    return [ticks_vals, tick_labels]\nplt.xticks(*create_tiks(x1, \"1\"), fontsize = 14)\nplt.yticks(*create_tiks(x2, \"2\"), fontsize = 14)\n\nplt.grid()\n\n\n\n\n\nx1 - np.mean(x1)\nx2 - np.mean(x2)\n\narray([-2.,  0., -1.,  2.,  1.])"
  },
  {
    "objectID": "machine_learning/data_transformations/standartisation.html",
    "href": "machine_learning/data_transformations/standartisation.html",
    "title": "1. Определение",
    "section": "",
    "text": "Процедура стандартизации данных\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom statsmodels.tools.tools import add_constant\nПусть имеется множество измерений определенного признака \\(x_i, i=\\overline{1,n}\\). Тогда стандартизацией такого ряда называется преобразование по формуле:\n\\[\\tilde{x}_{i} = \\frac{x_{i} - \\bar{x}}{\\sigma_x}. \\tag{1.1}\\]\nГде: - \\(\\bar{x}\\) - среднее арифметческое рассматртваемого ряда; - \\(\\sigma_x\\) - стандартное отклонение.\nПолучется, что выражение \\((1.1)\\) может быть переписано следующим образом:\n\\[\\tilde{x}_{i} = \\frac{x_{i} - \\bar{x}}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^n(x_i - \\bar{x})^2}}.\\]\nИногда предпочитают не отнимать среднее арифметическое в числителе, тогда формула \\((1.1)\\) принимает вид:\n\\[\\tilde{x}_i  = \\frac{x_i}{\\sigma}.\\]"
  },
  {
    "objectID": "machine_learning/data_transformations/standartisation.html#линейная-регрессия",
    "href": "machine_learning/data_transformations/standartisation.html#линейная-регрессия",
    "title": "1. Определение",
    "section": "Линейная регрессия",
    "text": "Линейная регрессия\n\nВведение\nМодель на исходнынх данных в матричных обозначениях примет вид:\n\\[\\hat{y} = b X. \\tag{3.1}\\]\nГде: - \\(X\\) - факторная матрица; - \\(\\hat{y}\\) - вектор столбец предсказаний модели; - \\(b\\) - вектор строка оценк коэффициентов модели.\nТогда модель на стандартизированных данных примет вид:\n\\[\\hat{y} = \\tilde{b} \\tilde{X}. \\tag{3.2}\\]\nГде: - \\(\\tilde{X}\\) - стандартизированная матрица предикоторов; - \\(\\tilde{b}\\) - оценки коэффициентов полученные при использовании стантатицованной фактороной матрицы.\nУточним, что факторную матрицу можно переписать: \\[\\tilde{X} = \\left(\\begin{array}\\\\\n    x_{11}/\\sigma_{x_1}&x_{12}/\\sigma_{x_2} & \\cdots & x_{1p}/\\sigma_{x_p}\\\\\n    x_{21}/\\sigma_{x_1}&x_{22}/\\sigma_{x_2} & \\cdots & x_{2p}/\\sigma_{x_p}\\\\\n    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n    x_{n1}/\\sigma_{x_1}&x_{n2}/\\sigma_{x_2} & \\cdots & x_{np}/\\sigma_{x_p}\\\\\n\\end{array}\\right)\\]\nГде: - \\(p\\) - число переменных модели; - \\(\\sigma_{x_j}\\) - стандартное отклоненение \\(j\\)-й переменной.\nТакая матрица раскладывается:\n\\[\\tilde{X} = X*\\Sigma'. \\tag{3.3}\\]\nГде: - \\(\\Sigma' = diag(1/\\sigma_{x_1}, 1/\\sigma_{x_2}, \\cdots, 1/\\sigma_{x_p})\\).\n\n\nВлияние стандартизации на коэффициенты\n\nТеория\nТеоритически можно доказать, что предсказания линейной регрессии на исходных данных и на стандартизированных данных не отличаются. Далее представлено доказательсво.\nПредсзакания модели \\((3.1)\\) для \\(i\\)-го наблюдения формируются так:\n\\[\\hat{y}_i = \\sum_{j=1}^p b_jx_{ij}\\]\nА модели \\((3.2)\\):\n\\[\\hat{y}_i = \\sum_{j=1}^p \\tilde{b}_j\\tilde{x}_{ij}\\]\nГлавный вопрос этого раздела, дают ли эти две модели одинаковое предсказание? Поработаем с последним выражением:\n\\[\\hat{y}_i = \\sum_{j=1}^p \\tilde{b}_j \\frac{x_{ij}}{\\sigma_j}.\\]\nТаким образом, если \\(b_j = \\tilde{b}_j/\\sigma_j, \\forall j\\) - то получается, что предсказания моделей одинаковые. Покажем это.\nДля нахождения коэффициентов в \\((2.1)\\) можно воспользоваться матричной формулой:\n\\[b=\\left[X^TX\\right]^{-1}X^TY\\tag{3.4}\\]\nДля нахождения коэффициентов в \\((2.2)\\) можно воспользоваться формулой:\n\\[\\tilde{b}=\\left[\\tilde{X}^T\\tilde{X}\\right]^{-1}\\tilde{X}^TY\\]\nИспользуя \\((3.3)\\):\n\\[\\tilde{b} =\n\\left[\\left(X\\Sigma'\\right)^T\\left(X\\Sigma'\\right)\\right]^{-1}\\left(X\\Sigma'\\right)^TY\n=\\left[\\Sigma^TX^TX\\Sigma'\\right]^{-1}\\Sigma'^TX^TY\\]\nДалее используя свойсва обращения произведения \\((AB)^{-1} = B^{-1}A^{-1}\\):\n\\[\\tilde{b} = \\left[X^TX\\Sigma'\\right]^{-1}\\left[\\Sigma^T\\right]^{-1}\\Sigma'^TX^TY\\]\n\\[\\tilde{b}=\\Sigma'^{-1}\\left[X^TX\\right]^{-1}X^TY\\]\nТогда:\n\\[\\Sigma'\\tilde{b} = \\left[X^TX\\right]^{-1}X^TY\\]\nПодставляя \\((3.4)\\) получим, что:\n\\[\\Sigma'^{-1}\\tilde{b}=b.\\]\nРасписывая выражение подробнее:\n\\[\\left(\\begin{array}\\\\\n    1/\\sigma_1 & 0 & \\cdots & 0 \\\\\n    0 & 1/\\sigma_2 & \\cdots & 0 \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    0 & 0 & \\cdots & 1/\\sigma_p\n\\end{array}\\right)\n\\left(\\begin{array}\\\\\n    \\tilde{b}_1 \\\\\n    \\tilde{b}_2 \\\\\n    \\vdots \\\\\n    \\tilde{b}_p\n\\end{array}\\right) =\n\\left(\\begin{array}\\\\\n    b_1 \\\\\n    b_2 \\\\\n    \\vdots \\\\\n    b_p\n\\end{array}\\right)\n\\]\nОт куда следует, что:\n\\[\n\\left(\\begin{array}\\\\\n    \\tilde{b}_1/\\sigma_1 \\\\\n    \\tilde{b}_2/\\sigma_2 \\\\\n    \\vdots \\\\\n    \\tilde{b}_p/\\sigma_p\n\\end{array}\\right) =\n\\left(\\begin{array}\\\\\n    b_1 \\\\\n    b_2 \\\\\n    \\vdots \\\\\n    b_p\n\\end{array}\\right)\n\\]\nТогда \\(b_j = \\tilde{b}_j/\\sigma_j, \\forall j\\), что и требовалось доказать \\(\\boxtimes\\).\n\n\nЧисленный эксперимент\nОднако, на числах, рассуждения, представленные выше, не выполняются. Далее представлен эксперимент это подтверждающий.\nФормирование данных.\n\nn = 200\nnp.random.seed(15)\n\nx = pd.DataFrame({\n    \"x1\":np.random.normal(0, 0.3, n),\n    \"x2\":np.random.normal(0, 3, n)\n})\n\ny = x[\"x1\"]*3 + x[\"x2\"]*2 + (np.random.rand(n)-0.5) + 3\n\nКоэффициенты на исходных данных примут вид:\n\nbasic_model = LinearRegression(fit_intercept = False).fit(\n    add_constant(x), y)\nbasic_model.coef_\n\narray([2.95566332, 3.13373587, 2.00359775])\n\n\nКоэффициент при использованнии стандатризированных данных примет вид:\n\n# стандартизованная модель\nx_stand = (x-x.mean())/np.std(x)\n\nstand_model = LinearRegression(fit_intercept = False).fit(\n    add_constant(x_stand), y\n)\n\nstand_model.coef_\n\narray([2.60710742, 0.95731566, 5.8421477 ])\n\n\nИли, приводя коэффициент к использованию на исходных данных.\n\nstand_model.coef_/np.concatenate([[1], x.std().to_numpy()])\n\narray([2.60710742, 3.12589172, 1.99858248])\n\n\nКак видно, коэффициенты на стандартизорованных данных достаточно заметно отличаются от коэффициентов на исходных данных. Следовательно и предсказания должны отличаться в чем мы удостоверимся.\n\npd.set_option(\"display.precision\", 50)\npred_df = pd.DataFrame({\n    \"basic predict\" : basic_model.predict(add_constant(x)),\n    \"stand predict\" : stand_model.predict(add_constant(x_stand))\n})\n\npred_df\n\n\n\n\n\n\n\n\nbasic predict\nstand predict\n\n\n\n\n0\n3.53475496739942940394030301831662654876708984...\n3.53475496739942940394030301831662654876708984...\n\n\n1\n-0.47040250288071661088906694203615188598632812...\n-0.47040250288071527862143739184830337762832641...\n\n\n2\n1.19065003861016061037503277475479990243911743...\n1.19065003861016149855345247488003224134445190...\n\n\n3\n-5.00032926914154529640654800459742546081542968...\n-5.00032926914154263187128890422172844409942626...\n\n\n4\n7.54930929882019441379270574543625116348266601...\n7.54930929882019441379270574543625116348266601...\n\n\n...\n...\n...\n\n\n195\n5.73269150865232113289948756573721766471862792...\n5.73269150865232024472106786561198532581329345...\n\n\n196\n-2.20031285504599605218345459434203803539276123...\n-2.20031285504599383173740534402895718812942504...\n\n\n197\n-4.44552414243254645498382160440087318420410156...\n-4.44552414243254467862698220415040850639343261...\n\n\n198\n2.16478039072416539312371241976507008075714111...\n2.16478039072416583721292226982768625020980834...\n\n\n199\n-0.80340280563815458236831545946188271045684814...\n-0.80340280563815325010068590927403420209884643...\n\n\n\n\n200 rows × 2 columns\n\n\n\nНо оказалось, предсказания почити отличаются - различие в рамках погрешности!!!\nПоявлялась мысль, что данное различие обусровлено особенностями sklearn. Но при использовании формул привычной матричной алгебры, результат тот-же самый.\nОбычные данные\n\nnp_y = y.to_numpy().reshape([n,1])\nnp_x = add_constant(x).to_numpy()\nnp.dot(\n    np.linalg.inv(\n        np.dot(np.transpose(np_x), np_x)\n    ),\n    np.dot(np.transpose(np_x), np_y)\n).ravel()\n\narray([2.95566332, 3.13373587, 2.00359775])\n\n\nСтандартизированные данные\n\nnp_x_stand = add_constant(x_stand).to_numpy()\nnp.dot(\n    np.linalg.inv(\n        np.dot(np.transpose(np_x_stand), np_x_stand)\n    ),\n    np.dot(np.transpose(np_x_stand), np_y)\n).ravel()\n\narray([2.60710742, 0.95731566, 5.8421477 ])"
  },
  {
    "objectID": "machine_learning/metrics/jaccard_index.html",
    "href": "machine_learning/metrics/jaccard_index.html",
    "title": "Jaccard index",
    "section": "",
    "text": "A popular measure of the similarity of objects. I have encountered it as a quality measure in computer vision, more precisely, in the task of segmenting objects in a picture. So this page, for now, is purely an application of Jaccard’s measure in a pixel-by-pixel comparison of pictures."
  },
  {
    "objectID": "machine_learning/metrics/jaccard_index.html#definition",
    "href": "machine_learning/metrics/jaccard_index.html#definition",
    "title": "Jaccard index",
    "section": "Definition",
    "text": "Definition\nLet’s say we have a set of classes \\(K\\), a set of objects taking classes \\(y = \\{y_i\\}, y_i \\in K\\), and a set of model predictions \\(y' = \\{y'_i\\}, y'_i \\in K\\). So the Jaccard index that measures the similarity of \\(y\\) and \\(y'\\) for the class \\(k\\) can be written as\n\\[J_k(y,y')=\\frac{\\sum_{i=1}^{n} \\left( \\left[y_i=k\\right]\\left[y'_i=k\\right]\\right)}{\\sum_{i=1}^n max\\left(\\left[y_i=k\\right],\\left[y'_i=k\\right]\\right)}.\\]\nWhere:\n\n\\([a]=\\begin{cases}  1 - \\text{if a statement true};\\\\  0 - \\text{if a statement false}.  \\end{cases}\\)\n\nLet us then speculate about the interpretation of the components of the presented formula: - Numenator \\(\\sum_{i=1}^{n} \\left( \\left[y_i=k\\right]\\left[y'_i=k\\right]\\right)\\) - number of objects corresponding to class \\(k\\) that were predicted to be members of class \\(k\\); - Denominator \\(\\sum_{i=1}^n max\\left(\\left[y_i=k\\right],\\left[y'_i=k\\right]\\right)\\) - number of object that was classified as \\(k\\) class or actually was.\nSo the sense of the formula is really simple - it’s part of the intersection in union."
  },
  {
    "objectID": "machine_learning/metrics/jaccard_index.html#example",
    "href": "machine_learning/metrics/jaccard_index.html#example",
    "title": "Jaccard index",
    "section": "Example",
    "text": "Example\nHere is a simple example of calculating the Jaccard index for two dimensional arrays, which in our case simulate pictures:\n\nimport numpy as np\n\ny_pred = np.array([\n    [1, 1, 2, 2, 2],\n    [1, 1, 2, 1, 2],\n    [1, 0, 0, 0, 0],\n    [2, 2, 2, 0, 0],\n    [2, 1, 1, 1, 2]\n])\ny_true = np.array([\n    [1, 1, 1, 2, 2],\n    [1, 1, 1, 2, 2],\n    [1, 1, 1, 2, 2],\n    [0, 0, 0, 2, 2],\n    [0, 0, 0, 2, 2]\n])\n\nunique_classes = np.intersect1d(np.unique(y_pred), np.unique(y_true))\n\nresult = {}\nfor val in unique_classes:\n    pred_class_pixels = (y_pred == val)\n    true_class_pixels = (y_true == val)\n    intersection = (pred_class_pixels & true_class_pixels)\n    union = (pred_class_pixels | true_class_pixels)\n    result[val] = intersection.sum()/union.sum()\n\nresult\n\n{0: 0.0, 1: 0.38461538461538464, 2: 0.25}"
  },
  {
    "objectID": "machine_learning/metrics/CAP.html",
    "href": "machine_learning/metrics/CAP.html",
    "title": "CAP curve",
    "section": "",
    "text": "The CAP curve is a tool to graphically assess the quality of the classification algorithm. Actually, just like ROC curve, which is much more popular in modern machine learning. I haven’t found any advantages of using CAP curve instead of ROC curve (ROC doesn’t have any advantages either). But you can face it if you are dealing with old literature or specific field, so this page may be useful for you.\nSources:\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import accuracy_score, auc\n\nfrom statsmodels.distributions.empirical_distribution import ECDF"
  },
  {
    "objectID": "machine_learning/metrics/CAP.html#content",
    "href": "machine_learning/metrics/CAP.html#content",
    "title": "CAP curve",
    "section": "Content",
    "text": "Content\n\nCAP curve definition;\nIdeal CAP curve;\nRandom CAP curve;\n\\(TPR\\) on the ordinate axis at the CAP curve."
  },
  {
    "objectID": "machine_learning/metrics/CAP.html#cap-curve-definition",
    "href": "machine_learning/metrics/CAP.html#cap-curve-definition",
    "title": "CAP curve",
    "section": "CAP curve definition",
    "text": "CAP curve definition\nCAP curve represents the cumulative number of positive outcomes on the ordinate axis relative to the corresponding cumulative number of the classifying parameter on the abscissa axis.\n\nBasic understanding of CAP curve by example\nFor example, let some model predict the probabilities \\(\\hat{p}_i\\) that \\(y_i=1\\) (the \\(i\\)-th client has a positive outcome, manifestation of a trait). Suppose we have 5 observations in the test sample, we have predictions for them and a real class:\n\n\n\n\\(\\hat{p}_i\\)\n\\(y_i\\)\n\n\n\n\n0.2\n1\n\n\n0.6\n0\n\n\n0.8\n1\n\n\n0.7\n1\n\n\n0.4\n0\n\n\n\nIn order to construct a CAP curve we need to:\n\nSort the observations by decreasing \\(\\hat{p}_i\\);\n\n\n\n\n\\(\\hat{p}_i\\)\n\\(y_i\\)\n\n\n\n\n0.8\n1\n\n\n0.7\n1\n\n\n0.6\n0\n\n\n0.4\n0\n\n\n0.2\n1\n\n\n\n\nNumber each observation starting from 1;\n\n\n\n\n\\(i\\)\n\\(\\hat{p}_i\\)\n\\(y_i\\)\n\n\n\n\n1\n0.8\n1\n\n\n2\n0.7\n1\n\n\n3\n0.6\n0\n\n\n4\n0.4\n0\n\n\n5\n0.2\n1\n\n\n\n\nCalculate the cumulative sum \\(y_i\\) (\\(\\hat{S}_{\\hat{y}}\\));\n\n\n\n\n\\(i\\)\n\\(\\hat{p}_i\\)\n\\(y_i\\)\n\\(\\hat{S}_{\\hat{y}}\\)\n\n\n\n\n1\n0.8\n1\n1\n\n\n2\n0.7\n1\n2\n\n\n3\n0.6\n0\n2\n\n\n4\n0.4\n0\n2\n\n\n5\n0.2\n1\n3\n\n\n\nStarting at point (0,0) and continuing with the variables \\(i\\) and \\(\\hat{S}_{\\hat{y}}\\), a CAP curve is plotted.\n\nplt.figure(figsize = [10,7])\n\nx = list(range(6))\ny = [0,1,2,2,2,3]\n\nplt.plot(x, y, marker = \".\")\n\nplt.xlabel(\"Number of observations\", fontsize = 15)\nplt.ylabel(\"$\\hat{S}_{\\hat{y}}$\", fontsize = 15)\n\nfor x_val, y_val in zip(x, y):\n    plt.text(x_val, y_val + 0.05, x_val, fontsize = 14)\n\nplt.yticks(y)\nplt.grid()\n\n\n\n\nInterpretation for \\(i\\)-th point is: in \\(i\\) worthest, according to the model, the observations are \\(\\hat{S}_{\\hat{y}_i}\\) manifestations of the trait. Or for each point from the example:\n\nIn the 1 worst, according to the model, observations lie 1 manifestations of the trait;\nIn the 2 worst observations, according to the model, there are 2 manifestations of the trait;\nIn the 3 worst, according to the model, observations lie 2 manifestations of the trait;\nIn the 4 worst, according to the model, observations lie 2 manifestations of the trait;\nIn the 5 worst, according to the model, observations lie 3 manifestations of the trait.\n\n\n\nRelative CAP curve\nIf on the abscissa axis we plot not the number of observations \\(i\\) but \\(i/n\\) (where \\(n\\) is the number of observations on which the CUP curve is calculated) and on the ordinate axis we plot not the cumulative sum but the cumulative percentage (the value coinciding with \\(TPR\\), detail), then we get what I call a relative CAP curve, the same in shape but bounded in the unit square: \n\n\n\n\\(i\\)\n\\(\\hat{p}_i\\)\n\\(y_i\\)\n\\(\\hat{S}_{\\hat{y}}\\)\n\\(i/n\\)\n\\(TPR_i\\)\n\n\n\n\n1\n0.8\n1\n1\n0.2\n1/3\n\n\n2\n0.7\n1\n2\n0.4\n2/3\n\n\n3\n0.6\n0\n2\n0.6\n2/3\n\n\n4\n0.4\n0\n2\n0.8\n2/3\n\n\n5\n0.2\n1\n3\n1\n1\n\n\n\n\nplt.figure(figsize = [10,7])\n\ny_rel = [0, 1/3, 2/3, 2/3, 2/3, 1]\nx_rel = [i/5 for i in range(6)]\n\nplt.plot(\n    x_rel, y_rel, marker = \".\"\n)\n\nplt.xlabel(\"Proportion of observations\", fontsize = 15)\n\nplt.ylabel(\"$TPR$\", fontsize = 15)\n\nfor i, (x_val, y_val) in enumerate(zip(x_rel, y_rel)):\n    plt.text(x_val, y_val + 0.02, i, fontsize = 14)\n    \nplt.yticks(y_rel)\nplt.grid()\n\n\n\n\nThe interpretation of the \\(i\\)th point would be as follows: in \\(i/n*100\\\\%\\) of the worst observations, according to the model, there are \\(TPR_i*100\\%\\) manifestations of the trait. Or for each point in the example under consideration:\n\nThe 0% worst observations, according to the model, contain 0% of the manifestations of the trait;\nIn 20% of the worst observations, according to the model, lies 33% of the manifestations of the trait;\nIn the 40% worst observations, according to the model, lies 67% of the manifestations of the trait;\nIn the 60% worst observations, according to the model, lies 67% of the manifestations of the trait;\nIn the 80% worst observations, according to the model, lies 67% of the manifestations of the trait;\nIn the 100% worst observations, according to the model, lies 100% of the manifestations of the trait.\n\nFurther, by default, the relative CAP curve will be discussed, because I find it more applicable."
  },
  {
    "objectID": "machine_learning/metrics/CAP.html#ideal-cap-curve",
    "href": "machine_learning/metrics/CAP.html#ideal-cap-curve",
    "title": "CAP curve",
    "section": "Ideal CAP curve",
    "text": "Ideal CAP curve\nPlays an important role in understanding the CAP curve.\n\nDescription\nAn ideal classifier has the following property - it can make such a discriminant variable \\(p_i\\) from an input combination of observation descriptor variables \\(X_i\\) such that:\n\\[y_i=1,y_j=0 \\Rightarrow p_i&gt;p_j; \\forall i,j; i\\neq j\\]\nThat is, for any two observations (\\(i\\)th and \\(j\\)th), if one has a manifestation of the trait and the other does not, the prediction for the former should be greater.\nIn such conditions, after sorting by descending order, all observations with the manifestation of the trait will be higher than all observations without the manifestation of the trait. Consequently, the CAP curve will grow only at the beginning, while only observations with the manifestation of the trait are going on. So until it reaches 1 - at the point corresponding to the lowest prediction for the observation with the manifestation of the trait. Then it will be unchanged for all observations without the trait.\n\n\nDemonstration by example\nGoing back to the example from the last section, last table considered, in the case of a perfect classifier would have to be sorted as follows:\n\n\n\n\\(i\\)\n\\(\\hat{p}_i\\)\n\\(y_i\\)\n\\(\\hat{S}_{\\hat{y}}\\)\n\\(i/n\\)\n\\(TPR\\)\n\n\n\n\n1\n0.8\n1\n1\n0.2\n1/3\n\n\n2\n0.7\n1\n2\n0.4\n2/3\n\n\n3\n0.2\n1\n2\n0.6\n1\n\n\n4\n0.6\n0\n2\n0.8\n1\n\n\n5\n0.4\n0\n3\n1\n1\n\n\n\nAll observations with \\(y_i=1\\) is higher than observations with \\(y_i=0\\).\nLet’s add a perfect CAP curve to the past graph.\n\nplt.figure(figsize = [10,7])\n\ny_rel = [0, 1/3, 2/3, 2/3, 2/3, 1]\ny_rel_ideal = [0, 1/3, 2/3, 1, 1, 1]\nx_rel = [i/5 for i in range(6)]\n\nplt.plot(x_rel, y_rel, marker = \".\")\nplt.plot(x_rel, y_rel_ideal, marker = \".\", color = \"green\")\n\nplt.xlabel(\"Proportion of observations\", fontsize = 15)\nplt.ylabel(\"$TPR$\", fontsize = 15)\n\nplt.legend(\n    [\"Actual CAP\", \"Ideal CAP\"],\n    fontsize = 15\n)\n\nplt.yticks(y_rel)\nplt.grid()"
  },
  {
    "objectID": "machine_learning/metrics/CAP.html#random-cap-curve",
    "href": "machine_learning/metrics/CAP.html#random-cap-curve",
    "title": "CAP curve",
    "section": "Random CAP curve",
    "text": "Random CAP curve\nTogether with the ideal is used to understand how good or bad this or that model is.\n\nDescription\nFrom the reasoning proposed above, it is clear that the faster the CUP curve grows at the beginning, the better the classifier to be censored. It is clear that a random classifier will form a CAP that grows uniformly over any area of the fraction of observations considered. Therefore, in the case of the general population, it will simply be a straight line extending from point (0,0) to point (1,1). In the case of a sample, it will be a curve with very close (or, in some cases, overlapping) characteristics.\n\n\nComputational experiment\nConsider a computational experiment: let’s simulate a random classifier and post a CAP for it.\n\ndef plot_random_CAP(sample_size):\n    experimental_sample = pd.DataFrame({\n        \"p_hat\" : np.random.rand(sample_size),\n        \"y\" : np.random.choice([0,1], sample_size)\n    })\n\n    experimental_sample.sort_values(\n        \"p_hat\", ascending = False,\n        inplace = True\n    )\n\n    experimental_sample[\"$$\\hat{F}_{\\hat{y}}$$\"] = ECDF(\n        1 - experimental_sample.query(\"y == 1\")[\"p_hat\"]\n    )(1 - experimental_sample[\"p_hat\"])\n    experimental_sample.head()\n\n    experimental_sample[\"$i$\"] = range(1, sample_size + 1)\n    experimental_sample[\"$i/n$\"] = experimental_sample[\"$i$\"]/sample_size\n\n    plt.plot(\n        experimental_sample[\"$i/n$\"], \n        experimental_sample[\"$$\\hat{F}_{\\hat{y}}$$\"],\n        color = \"red\"\n    )\n    plt.xlim([0,1])\n    plt.ylim([0,1])\n    plt.xlabel(\"Proportion of observations\")\n    plt.ylabel(\"$FPR$\")\n    \n    \nsample_sizes = [500, 1000, 5000, 10000]\nplt.figure(figsize = [15, 10])\nfor i in range(4):\n    plt.subplot(2, 2, i+1)\n    plt.title(\"Sample size \" + str(sample_sizes[i]))\n    plot_random_CAP(sample_sizes[i])\n\n\n\n\nIt can be seen that such a curve really tends to a diagonal straight line with increasing sample size. Therefore, in applied research, it is assumed to be equal to a diagonal line.\n\n\nAdding to the example of the previous section\nThen the full CAP graph, for the example considered, will take the form:\n\nplt.figure(figsize = [10,7])\n\ny_rel = [0, 1/3, 2/3, 2/3, 2/3, 1]\ny_rel_ideal = [0, 1/3, 2/3, 1, 1, 1]\nx_rel = [i/5 for i in range(6)]\n\nplt.plot(x_rel, y_rel, marker = \".\")\nplt.plot(\n    x_rel, y_rel_ideal, \n    marker = \".\", color = \"green\"\n)\nplt.plot(\n    [0,1], [0,1],\n    color = \"red\", marker = \".\"\n)\n\nplt.xlabel(\"Proportion of observations\", fontsize = 15)\nplt.ylabel(\"$TPR$\", fontsize = 15)\n\nplt.legend(\n    [\n        \"Real CAP\", \n        \"Ideal CAP\",\n        \"Random CAP\"\n    ],\n    fontsize = 15\n)\n\nplt.yticks(y_rel)\nplt.grid()"
  },
  {
    "objectID": "machine_learning/metrics/CAP.html#tpr-on-the-ordinate-axis-of-cap",
    "href": "machine_learning/metrics/CAP.html#tpr-on-the-ordinate-axis-of-cap",
    "title": "CAP curve",
    "section": "\\(TPR\\) on the ordinate axis of CAP",
    "text": "\\(TPR\\) on the ordinate axis of CAP\n\\(TPR\\) is the proportion of correctly predicted observations of trait manifestations at the treshold \\(p'\\):\n\\[TPR_i(p')=\\sum_{i=1}^n\\frac{I(\\hat{p}_i \\geq p')}{n};\\]\n\\[I(\\hat{p}_i \\geq p')=\\begin{cases}\n    1, \\hat{p}_i \\geq p';\\\\\n    0, \\text{in other case}.\n\\end{cases}\\]\nThis value will be on the ordinate axis of the CAP curve.\n\nChecking by example\nQuite a tentative verification, but still, let’s calculate \\(TPR\\) as we did for the CAP curve and get it using sklearn.metrics.roc_curve and compare.\n\nfrom sklearn.metrics import roc_curve\n\nnp.random.seed(10)\n\nsample_size = 200\n\n\ntest_df = pd.DataFrame({\n    \"p_hat\" : np.random.rand(sample_size),\n    \"y\" : np.random.choice([0,1], sample_size)\n})\n\nCAT_TPR = np.concatenate([\n    [0],\n    np.sort(\n        ECDF(1 - test_df.query('y == 1')[\"p_hat\"])\\\n        (1 - test_df[\"p_hat\"])\n    )\n])\n\nfpr, tpr, t = roc_curve(\n    test_df[\"y\"],\n    test_df[\"p_hat\"],\n    drop_intermediate = False\n)\n\n# сравнение - c точностью до 4-ех знаков после запятой\nall(np.round(CAT_TPR,4) == np.round(tpr,4))\n\nTrue\n\n\nAll fine - values are the same."
  },
  {
    "objectID": "machine_learning/models_selection_methods/l2_regularisation.html",
    "href": "machine_learning/models_selection_methods/l2_regularisation.html",
    "title": "L2 (Ridge regularisation)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom IPython.display import clear_output"
  },
  {
    "objectID": "machine_learning/models_selection_methods/l2_regularisation.html#souces",
    "href": "machine_learning/models_selection_methods/l2_regularisation.html#souces",
    "title": "L2 (Ridge regularisation)",
    "section": "Souces",
    "text": "Souces\n\nhttps://www.statlearning.com/ 6 глава."
  },
  {
    "objectID": "machine_learning/models_selection_methods/l2_regularisation.html#description",
    "href": "machine_learning/models_selection_methods/l2_regularisation.html#description",
    "title": "L2 (Ridge regularisation)",
    "section": "Description",
    "text": "Description\nIn L2-regularisation, a component is added to the target function of the coefficient estimation method:\n\\[\\lambda\\sum_{j=1}^n\\beta^2_j\\]\nWhere: - \\(\\beta_j\\) - estimated coefficient; - \\(\\lambda\\) - parameter indicating how much the model should be regularised."
  },
  {
    "objectID": "machine_learning/models_selection_methods/l2_regularisation.html#regression",
    "href": "machine_learning/models_selection_methods/l2_regularisation.html#regression",
    "title": "L2 (Ridge regularisation)",
    "section": "Regression",
    "text": "Regression\nL2-regularisation combined with a regression model is called ridge regression.\nSo if we use MSE as a quality function, we will have a modifide function:\n\\[\\sum_{i=1}^n\\left(y_i - x_i\\beta\\right)^2 + \\lambda\\sum_{j=1}^p\\beta^2_j \\rightarrow min\\]\nWhere:\n\n\\(n\\) - sample size;\n\\(p\\) - data dimention;\n\\(x_i = (x_{i1}, x_{i2}, ..., x_{ip})\\) - vector describing the \\(i\\text{-}th\\) observation;\n\\(\\beta = (\\beta_1, \\beta_2, ..., \\beta_p)\\) - vector of coefficient estimates.\n\nNote To perform refularization to regression you need to ensure that your features have the same scaling. Check more here."
  },
  {
    "objectID": "machine_learning/models_selection_methods/l2_regularisation.html#compression-of-coefficients",
    "href": "machine_learning/models_selection_methods/l2_regularisation.html#compression-of-coefficients",
    "title": "L2 (Ridge regularisation)",
    "section": "Compression of coefficients",
    "text": "Compression of coefficients\nHere I reproduce the experiment from the ISLR.\nLoading Credit data.\n\nCredit = pd.read_csv(\"Credit.csv\", index_col = 0)\n\nnominal_names = [\n    \"Gender\", \"Student\", \"Married\", \"Ethnicity\"\n]\n\nohe = OneHotEncoder(\n    sparse_output = False, drop = \"first\"\n).fit(\n    Credit[nominal_names]\n)\n\nCredit = pd.concat(\n    [\n        pd.DataFrame(\n            ohe.transform(Credit[nominal_names]),\n            columns = ohe.get_feature_names_out(),\n            index= Credit.index\n        ),\n        Credit.loc[:,~Credit.columns.isin(nominal_names)]\n    ],\n    axis = 1\n)\n\nX = Credit.iloc[:,:-1]\ny = Credit.iloc[:, -1]\n\nCredit.head()\n\n\n\n\n\n\n\n\nGender_Female\nStudent_Yes\nMarried_Yes\nEthnicity_Asian\nEthnicity_Caucasian\nIncome\nLimit\nRating\nCards\nAge\nEducation\nBalance\n\n\nID\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0.0\n0.0\n1.0\n0.0\n1.0\n14.891\n3606\n283\n2\n34\n11\n333\n\n\n2\n1.0\n1.0\n1.0\n1.0\n0.0\n106.025\n6645\n483\n3\n82\n15\n903\n\n\n3\n0.0\n0.0\n0.0\n1.0\n0.0\n104.593\n7075\n514\n4\n71\n11\n580\n\n\n4\n1.0\n0.0\n0.0\n1.0\n0.0\n148.924\n9504\n681\n3\n36\n11\n964\n\n\n5\n0.0\n0.0\n1.0\n0.0\n1.0\n55.882\n4897\n357\n2\n68\n16\n331\n\n\n\n\n\n\n\nWe will increase the regularisation parameter and take the values of the coefficients. The procedure is rather long, so it is supposed to perform the calculation and put the results in a file.\n\n# coefs_frame = pd.DataFrame(columns = X.columns)\n\n# stand_X = X/np.sqrt(((X - X.mean())**2).sum()/X.shape[0])\n\n# alphas = np.arange(0, 2000, 0.01)\n# int_count = len(alphas)\n\n# for i, alpha in enumerate(alphas):\n#     clear_output(wait=True)\n#     print(\"{}/{}\".format(i, int_count))\n#     coefs_frame.loc[alpha] = pd.Series(\n#         Ridge(alpha = alpha).fit(stand_X,y).coef_,\n#         index = X.columns\n#     )\n    \n# coefs_frame.index.name = \"alpha\"\n# coefs_frame.to_csv(\"l2_regularisation_files/l2_reg_coefs.csv\")\n\nThe obtained values of coefficients are plotted on the graphs.\n\ncoefs_frame = pd.read_csv(\"l2_regularisation_files/l2_reg_coefs.csv\", index_col = 0)\n\nplot_var_names = [\"Limit\", \"Rating\", \"Student_Yes\", \"Income\"]\nline_styles = ['-', '--', '-.', ':']\n\nbeta_0 = np.sqrt(np.sum(coefs_frame.loc[0]**2))\ncoefs_frame[\"beta_i/beta_0\"] = coefs_frame.apply(\n    lambda row: np.sqrt(np.sum(row**2))/beta_0,\n    axis = 1\n)\n\n\nplt.figure(figsize = [15, 7])\nplt.subplot(121)\n\nfor i in range(len(plot_var_names)):\n    plt.plot(\n        coefs_frame.index, \n        coefs_frame[plot_var_names[i]],\n        linestyle = line_styles[i]\n    )\n    \nfor col in coefs_frame.loc[\n    :, ~coefs_frame.columns.isin(plot_var_names)\n]:\n    plt.plot(\n        coefs_frame.index, coefs_frame[col], \n        color = \"gray\", alpha = 0.5\n    )\n    \nplt.legend(plot_var_names)\nplt.xlabel(\"$\\\\lambda$\", fontsize = 14)\n    \nplt.gca().set_xscale(\"log\")\n\nplt.subplot(122)\n\nfor i in range(len(plot_var_names)):\n    plt.plot(\n        coefs_frame[\"beta_i/beta_0\"], \n        coefs_frame[plot_var_names[i]],\n        linestyle = line_styles[i]\n    )\n    \nfor col in coefs_frame.loc[\n    :, ~coefs_frame.columns.isin(plot_var_names)\n]:\n    plt.plot(\n        coefs_frame[\"beta_i/beta_0\"], coefs_frame[col], \n        color = \"gray\", alpha = 0.5\n    )\n\nans = plt.xlabel(\n    \"$\\\\frac{||\\\\hat{\\\\beta}_{\\\\lambda}^R||_2}{||\\\\hat{\\\\beta}||_2}$\",\n    fontsize = 15\n)\n\n\n\n\n\nThe graph on the left shows how the coefficients converge as the regularisation parameter increases. For clarity, a logarithmic scale for the regularisation parameter is taken. The most prominent coefficients are highlighted in colour and line style - the data are standardised, so the scale of the values does not matter;\nThe vergence is plotted to the right on the ordinate:\n\n\\[\\frac{||\\hat{\\beta}_{\\lambda}^R|_2}{||\\hat{\\beta}||_2}\\]\nWhere: - \\(||\\beta||_2 = \\sqrt{\\sum_{j=1}^p \\beta^2_j}\\) - is the Euclidean distance of the coefficients \\(\\beta\\) from the origin; - \\(\\hat{\\beta}\\) - coefficients obtained by the least squares method (equivalent to the coefficients obtained at \\(\\lambda = 0\\)); - \\(\\hat{\\beta}^R_{\\lambda}\\) - coefficients obtained using regularisation."
  },
  {
    "objectID": "other/linux_backup.html",
    "href": "other/linux_backup.html",
    "title": "How to linux backup",
    "section": "",
    "text": "rsync usage example;\nrsync documentation;\n backup with rsync;\n official ubuntu backup instructions;\noffical ubutntu backup instructions - section about tar.\n\n\ntar\nIn the following cell I just try to follow the instructions from official ubutntu backup instructions - section about tar, to make a backup - and test how it works in different cases.\nI want to try:\n\nstep 1 - backup creating:\n\nrun docker container with ubuntu;\ninstall curl, abache2;\nrun the apache2 service;\ncheck apache2 with curl;\nuse tar to make a backup of the current system;\nstop the container;\n\nstep2 - recovery from back up:\n\nrun a new empty ubuntu container;\ncopy the backup archive from the previous container;\nrestore the backup in empty ubuntu system;\ncheck curl and apache2.\n\n\nstep 1 - backup creating\n\n%%bash\nmkdir backups\ndocker run --rm --name ubun_cont -i -v $(pwd)/backups:/backups ubuntu\n\necho \"=====install=====\"\napt-get update &&gt; /dev/null\napt-get install curl apache2 -y &&gt; /dev/null\nservice apache2 start &&gt; /dev/null\n\n\necho\necho \"=====check=====\"\ncurl -s localhost:80 | head -n 20\n\necho\necho \"=====make backup=====\"\ntar \\\n    -cvpzf \\\n    /backups/backup.tar.gz \\\n    --exclude=/backups/backup.tar.gz \\\n    --one-file-system / &&gt; /dev/null\nexit\n\n=====install=====\n\n=====check=====\n&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\"&gt;\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\"&gt;\n  &lt;!--\n    Modified from the Debian original for Ubuntu\n    Last updated: 2022-03-22\n    See: https://launchpad.net/bugs/1966004\n  --&gt;\n  &lt;head&gt;\n    &lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" /&gt;\n    &lt;title&gt;Apache2 Ubuntu Default Page: It works&lt;/title&gt;\n    &lt;style type=\"text/css\" media=\"screen\"&gt;\n  * {\n    margin: 0px 0px 0px 0px;\n    padding: 0px 0px 0px 0px;\n  }\n\n  body, html {\n    padding: 3px 3px 3px 3px;\n\n    background-color: #D8DBE2;\n\n=====make backup=====\n\n\nThere is not much to say - it saves the backup in the “backups” folder.\nstep2 - recovery from backup\n\n%%bash\ndocker run --rm --name ubun_cont -i -v $(pwd)/backups:/backups ubuntu\ntar -xvpzf /backups/backup.tar.gz -C / --numeric-owner &&gt; /dev/null\n\necho \"=====culr --help=====\"\ncurl --help\necho \"=====curl localhost:80=====\"\nculr -s localhost:80\n\necho\necho \"=====curl localhost:80 after apache=====\"\nservice apache2 start\ncurl -s localhost:80 | head -n 20\n\nexit\n\n=====culr --help=====\nUsage: curl [options...] &lt;url&gt;\n -d, --data &lt;data&gt;          HTTP POST data\n -f, --fail                 Fail silently (no output at all) on HTTP errors\n -h, --help &lt;category&gt;      Get help for commands\n -i, --include              Include protocol response headers in the output\n -o, --output &lt;file&gt;        Write to file instead of stdout\n -O, --remote-name          Write output to a file named as the remote file\n -s, --silent               Silent mode\n -T, --upload-file &lt;file&gt;   Transfer local FILE to destination\n -u, --user &lt;user:password&gt; Server user and password\n -A, --user-agent &lt;name&gt;    Send User-Agent &lt;name&gt; to server\n -v, --verbose              Make the operation more talkative\n -V, --version              Show version number and quit\n\nThis is not the full help, this menu is stripped into categories.\nUse \"--help category\" to get an overview of all categories.\nFor all options use the manual or \"--help all\".\n=====curl localhost:80=====\n\n=====curl localhost:80 after apache=====\n * Starting Apache httpd web server apache2\n * \n&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\"&gt;\n&lt;html xmlns=\"http://www.w3.org/1999/xhtml\"&gt;\n  &lt;!--\n    Modified from the Debian original for Ubuntu\n    Last updated: 2022-03-22\n    See: https://launchpad.net/bugs/1966004\n  --&gt;\n  &lt;head&gt;\n    &lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" /&gt;\n    &lt;title&gt;Apache2 Ubuntu Default Page: It works&lt;/title&gt;\n    &lt;style type=\"text/css\" media=\"screen\"&gt;\n  * {\n    margin: 0px 0px 0px 0px;\n    padding: 0px 0px 0px 0px;\n  }\n\n  body, html {\n    padding: 3px 3px 3px 3px;\n\n    background-color: #D8DBE2;\n\n\n/bin/bash: line 6: culr: command not found\nAH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 172.17.0.2. Set the 'ServerName' directive globally to suppress this message\n\n\nNote Even without downloading curl and apache2 I can use it after unpacking. To use apache2 you had to start the corresponding service - but it was still really easy.\n\n\nrsync basics\nActually rsync is just a copy tool, but if you compare it with cp it has much more options and features.\n\nBasic syntax\nrsync &lt;copied file&gt; &lt;destination of copying&gt;\nExample follows\n\n%%bash\n# creating to directories\nmkdir dir1 dir2\n# creating file in dir1\necho \"hello new file\" &gt; dir1/test_file\n# rsynk file from dir1 to dir2\nrsync dir1/test_file dir2/test_file\n# check the message\ncat dir2/test_file\n\nrm -r dir1 dir2\n\nhello new file\n\n\n\n\nr - copy recursive\nAllow to copy a folder with all its contents.\nIn the following example I’m trying to copy contents from dir1 to dir2, as you can see I couldn’t do it without the r option.\n\n%%bash\n\nmkdir dir1 dir2\ntouch dir1/file{1..5}.txt\n\necho \"=====content of dir1=====\"\nls dir1\n\necho \"=====no r option=====\"\nrsync dir1/ dir2\nls dir2\n\necho \"=====r option=====\"\nrsync -r dir1/ dir2\nls dir2\n\nrm -r dir1 dir2\n\n=====content of dir1=====\nfile1.txt\nfile2.txt\nfile3.txt\nfile4.txt\nfile5.txt\n=====no r option=====\nskipping directory .\n=====r option=====\nfile1.txt\nfile2.txt\nfile3.txt\nfile4.txt\nfile5.txt\n\n\n\n\n-a - attributes\nIt works just like r, but also deals with file attributes (like creation time, user and so on). So in the following example:\n\nFew files was created in dir1;\nThe user for these files has been changed from root to user1;\nThen I use rsync twice:\n\nFirst with the r option, which puts the user back in root;\nSecond, with the a option, which saves the user attribute as user1 from source.\n\n\n\n%%bash\n# docker container for usperuser access\ndocker run --rm --name test_container -i ubuntu\napt-get update &&gt; /dev/null\napt-get install -y rsync &&gt; /dev/null\n# creating file with some specific user\nuseradd user1\nmkdir dir1 dir2\ntouch dir1/file{1..5}.txt\nchown user1 dir1/file*\n\necho \"=====dir1=====\"\nls -l dir1\n\necho\necho \"=====dir2 after rsync -r=====\"\nrsync -r dir1/ dir2\nls -l dir2\n\necho\necho \"=====dir2 after rsync -a=====\"\nrsync -a dir1/ dir2\nls -l dir2\n\nexit\n\n=====dir1=====\ntotal 0\n-rw-r--r-- 1 user1 root 0 Jun 17 15:31 file1.txt\n-rw-r--r-- 1 user1 root 0 Jun 17 15:31 file2.txt\n-rw-r--r-- 1 user1 root 0 Jun 17 15:31 file3.txt\n-rw-r--r-- 1 user1 root 0 Jun 17 15:31 file4.txt\n-rw-r--r-- 1 user1 root 0 Jun 17 15:31 file5.txt\n\n=====dir2 after rsync -r=====\ntotal 0\n-rw-r--r-- 1 root root 0 Jun 17 15:31 file1.txt\n-rw-r--r-- 1 root root 0 Jun 17 15:31 file2.txt\n-rw-r--r-- 1 root root 0 Jun 17 15:31 file3.txt\n-rw-r--r-- 1 root root 0 Jun 17 15:31 file4.txt\n-rw-r--r-- 1 root root 0 Jun 17 15:31 file5.txt\n\n=====dir2 after rsync -a=====\ntotal 0\n-rw-r--r-- 1 user1 root 0 Jun 17 15:31 file1.txt\n-rw-r--r-- 1 user1 root 0 Jun 17 15:31 file2.txt\n-rw-r--r-- 1 user1 root 0 Jun 17 15:31 file3.txt\n-rw-r--r-- 1 user1 root 0 Jun 17 15:31 file4.txt\n-rw-r--r-- 1 user1 root 0 Jun 17 15:31 file5.txt\n\n\n\n\n--delete - delete extraneous\nIf you don’t use this option rsync will save files wich are in destination directory. Оtherwise all files that are not in the source will be deleted in the destination folder. The following exampe show: - In dir2 I created file test.txt; - First I rsync files from dir1 to dir2 without the --delete option - test.txt is still in dir2; - Second I rsync files from dir1 to dir2 using the --delete option - test.txt disappears from dir2.\n\n%%bash\nmkdir dir1 dir2\ntouch dir1/file{1..5}.txt\ntouch dir2/test.txt\n\necho \"====initial dir2=====\"\nls dir2\n\necho \"=====dir2 after rsync no --delete=====\"\nrsync -r dir1/ dir2\nls dir2\n\necho \"=====dir2 after rsync with --delete=====\"\nrsync -r --delete dir1/ dir2\nls dir2\n\nrm -r dir1 dir2\n\n====initial dir2=====\ntest.txt\n=====dir2 after rsync no --delete=====\nfile1.txt\nfile2.txt\nfile3.txt\nfile4.txt\nfile5.txt\ntest.txt\n=====dir2 after rsync with --delete=====\nfile1.txt\nfile2.txt\nfile3.txt\nfile4.txt\nfile5.txt"
  },
  {
    "objectID": "Docker/docker_compose.html",
    "href": "Docker/docker_compose.html",
    "title": "Docker-compose",
    "section": "",
    "text": "Docker-Compose is a tool that is presented as the next step of abstraction under Docker - a tool that allows to organise interactions between containers.\n\nYAML\nThe language used to describe the behaviour of docker-compose.\nYAML allows you to describe data structures consisting of lists and dictionaries:\n\nEach item in the list starts with the symbol -;\nEach new key in the dictionary sets like &lt;key&gt;;\nTemplates (anchors) - the possibility to create the link to some essential and then use it in any place;\nThe nesting of the structure is formed by indents.\n\nFurther examples will make it clearer.\nPython has a library called “yaml” that allows you to convert “yaml” markup to the corresponding Python data structures.\n\nimport yaml\n\nIn the following example, I create the dictionary with the keys names, ages, which contains the corresponding lists:\n\nyaml_str = \\\n'''\nnames:\n    - peter\n    - olga\nages:\n    - 22\n    - 18\n'''\n\nyaml.full_load(yaml_str)\n\n{'names': ['peter', 'olga'], 'ages': [22, 18]}\n\n\nNote that there should be a space between &lt;key&gt; and &lt;value&gt;. Here, for comparison, is a correctly created dictionary under the key postgres and a dictionary under the key clickhouse which was created incorrectly."
  },
  {
    "objectID": "Docker/sources.html",
    "href": "Docker/sources.html",
    "title": "Sources",
    "section": "",
    "text": "This is where I put useful information about Docker. If any sources are mentioned in later sections, you should find them here.\n\nDocker instalation USE ONLY COMMANDS PROVIDED BY DOCKER DOCUMENTATION;\n May be useful after installing;\n\nHow not to always put sudo before the docker command;\nSomething still incomprehensible..\n\n Start daemon  sudo systemctl start docker;\n Docker cource by karpovcources."
  },
  {
    "objectID": "Docker/network/container_communication.html",
    "href": "Docker/network/container_communication.html",
    "title": "Containers communication",
    "section": "",
    "text": "In this page I want to focus on ways of organising container communication."
  },
  {
    "objectID": "Docker/network/container_communication.html#creating-containers",
    "href": "Docker/network/container_communication.html#creating-containers",
    "title": "Containers communication",
    "section": "Creating containers",
    "text": "Creating containers\nIn the following cell, create containers for experiments.\n\n%%bash\ndocker run --rm -d --name test_nginx1 nginx &&gt; /dev/null\ndocker run --rm -d --name test_nginx2 nginx &&gt; /dev/null\ndocker network create test_network &&gt; /dev/null\ndocker run --rm -d \\\n    --name test_nginx3 \\\n    --net test_network \\\n    nginx &&gt; /dev/null\n\nNote Don’t forget to stop the containers when you’ve finished playing with them.\n\n%%bash\ndocker stop test_nginx1 test_nginx2 test_nginx3 &&gt; /dev/null\ndocker test_network"
  },
  {
    "objectID": "Docker/network/container_communication.html#get-ip-of-the-containers",
    "href": "Docker/network/container_communication.html#get-ip-of-the-containers",
    "title": "Containers communication",
    "section": "Get ip of the containers",
    "text": "Get ip of the containers\n\nInspect network\nThere’s a lot of important information in the output of docker inspect for network that you need to organise interaction between containers.\nThe following cell shows the output of docker inspect for network bridge. There you will find a Containers dictionary that matches the container id with information about the container. Among other things, the IPv4Address field is there.\n\n%%bash\ndocker inspect bridge\n\n[\n    {\n        \"Name\": \"bridge\",\n        \"Id\": \"cb0be6f8e70135dc9f75a03d73c9a5b9c9dd6225f18103191f676596edc9bdd1\",\n        \"Created\": \"2023-10-30T07:56:28.229725692+03:00\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"EnableIPv6\": false,\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Options\": null,\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.17.0.0/16\",\n                    \"Gateway\": \"172.17.0.1\"\n                }\n            ]\n        },\n        \"Internal\": false,\n        \"Attachable\": false,\n        \"Ingress\": false,\n        \"ConfigFrom\": {\n            \"Network\": \"\"\n        },\n        \"ConfigOnly\": false,\n        \"Containers\": {\n            \"63dae9a4f564ad5a4aa8654a57410eb02353fea7d0339aebd362ce4d893c6f46\": {\n                \"Name\": \"test_nginx1\",\n                \"EndpointID\": \"cbbae9a540e6fb959efda6c9f72a721e61c02f042243df2df76139e1b43d9ebe\",\n                \"MacAddress\": \"02:42:ac:11:00:02\",\n                \"IPv4Address\": \"172.17.0.2/16\",\n                \"IPv6Address\": \"\"\n            },\n            \"9ffabf6ffaff150ea77827a86403dffce3fe9cbae6cb9df2a96972823c3e31fd\": {\n                \"Name\": \"test_nginx2\",\n                \"EndpointID\": \"05c50eea53052dd13258a5a55140d5fe12b840f8090fc270d81fc02d92c8c0c9\",\n                \"MacAddress\": \"02:42:ac:11:00:03\",\n                \"IPv4Address\": \"172.17.0.3/16\",\n                \"IPv6Address\": \"\"\n            }\n        },\n        \"Options\": {\n            \"com.docker.network.bridge.default_bridge\": \"true\",\n            \"com.docker.network.bridge.enable_icc\": \"true\",\n            \"com.docker.network.bridge.enable_ip_masquerade\": \"true\",\n            \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\",\n            \"com.docker.network.bridge.name\": \"docker0\",\n            \"com.docker.network.driver.mtu\": \"1500\"\n        },\n        \"Labels\": {}\n    }\n]\n\n\n\n\nInspect container\nThe docker inspect &lt;container name/id&gt; command will give you information about the container, and by following the `NetworkSettings.IPAddress’ path you can access the ip address of the container.\n\n%%bash\n\necho \"====nginx1 ip=====\"\necho\\\n    $(docker inspect\\\n    --format '{{ .NetworkSettings.IPAddress }}'\\\n    test_nginx1)\n\necho \"====nginx2 ip=====\"\necho\\\n    $(docker inspect\\\n    --format '{{ .NetworkSettings.IPAddress }}'\\\n    test_nginx2)\n\n====nginx1 ip=====\n172.17.0.2\n====nginx2 ip=====\n172.17.0.3"
  },
  {
    "objectID": "Docker/network/container_communication.html#sec-from_cont_to_cont",
    "href": "Docker/network/container_communication.html#sec-from_cont_to_cont",
    "title": "Containers communication",
    "section": "From container to container",
    "text": "From container to container\nBy using the IP address of the container, you can access it from the other container on the same network.\nIn the next cell, a commit is formed first, which will be called in the next step from the test_nginx2 container to make a curl in test_nginx1. As a result we get html code which nginx responds to in its welcome page.\n\n%%bash\n# getting nginx2 ip\nnginx2_ip=$(\n    docker inspect \\\n    --format '{{ .NetworkSettings.IPAddress }}'\\\n    test_nginx2\n)\ncommand=\"curl -s $nginx2_ip\"\necho \"=====command to be executed=====\"\necho $command\necho\necho \"=====execution result=====\"\ndocker exec test_nginx1 $command\n\n=====command to be executed=====\ncurl -s 172.17.0.3\n\n=====execution result=====\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n&lt;style&gt;\nhtml { color-scheme: light dark; }\nbody { width: 35em; margin: 0 auto;\nfont-family: Tahoma, Verdana, Arial, sans-serif; }\n&lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n&lt;p&gt;If you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.&lt;/p&gt;\n\n&lt;p&gt;For online documentation and support please refer to\n&lt;a href=\"http://nginx.org/\"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;\nCommercial support is available at\n&lt;a href=\"http://nginx.com/\"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;\n\n&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;"
  },
  {
    "objectID": "Docker/network/container_communication.html#isolation-example",
    "href": "Docker/network/container_communication.html#isolation-example",
    "title": "Containers communication",
    "section": "Isolation example",
    "text": "Isolation example\nThe idea of networking in Docker is to be able to build effective structures as combinations of different containers. But you don’t want containers that you don’t expect to interact with to be able to interact. So in this case, you create containers that you don’t want to interact with each other on a different network - you isolate them.\nSo in the following example, as in the container-to-container section, I’m trying to query from one container to another, but now I’m trying to access a container deployed on the other network and got an error.\n\n%%bash\nnginx2_ip=$(\n    docker inspect \\\n    --format '{{ .NetworkSettings.IPAddress }}'\\\n    test_nginx2\n)\n\ncommand=\"curl -s $nginx2_ip\"\necho \"=====command to be executed=====\"\necho $command\necho\necho \"=====execution result=====\"\ndocker exec test_nginx3 $command\n\n=====command to be executed=====\ncurl -s 172.17.0.3\n\n=====execution result=====\n\n\nCalledProcessError: Command 'b'nginx2_ip=$(\\n    docker inspect \\\\\\n    --format \\'{{ .NetworkSettings.IPAddress }}\\'\\\\\\n    test_nginx2\\n)\\n\\ncommand=\"curl -s $nginx2_ip\"\\necho \"=====command to be executed=====\"\\necho $command\\necho\\necho \"=====execution result=====\"\\ndocker exec test_nginx3 $command\\n'' returned non-zero exit status 28."
  },
  {
    "objectID": "Docker/network/small_features.html",
    "href": "Docker/network/small_features.html",
    "title": "Small features",
    "section": "",
    "text": "In this page I store topics related to the Docker networks that are too small to have their own page."
  },
  {
    "objectID": "Docker/network/small_features.html#ls-list-networks",
    "href": "Docker/network/small_features.html#ls-list-networks",
    "title": "Small features",
    "section": "ls list networks",
    "text": "ls list networks\n\n!docker network ls\n\nNETWORK ID     NAME      DRIVER    SCOPE\n4b609bad1d6c   bridge    bridge    local\n80c3e6772c8c   host      host      local\n45bb103d970f   none      null      local"
  },
  {
    "objectID": "Docker/network/small_features.html#name-of-network",
    "href": "Docker/network/small_features.html#name-of-network",
    "title": "Small features",
    "section": "Name of network",
    "text": "Name of network\nIf you need for some container to get network this container connected to, you need you command like this:\ndocker inspect -f '{{range $key, $value := .NetworkSettings.Networks}}{{$key}} {{end}}' &lt;container name&gt;\nI don’t really understand how it works yet. But the following cell shows that it’s working - by default every container is connected to the bridge network, so for just created container this command returns bridge.\n\n%%bash\ndocker run --rm -itd --name just_net ubuntu &&gt; /dev/null\ndocker inspect \\\n    -f '{{range $key, $value := .NetworkSettings.Networks}}{{$key}} {{end}}' \\\n    just_net\ndocker stop just_net &&gt; /dev/null\n\nbridge"
  },
  {
    "objectID": "Docker/network/default_networks.html",
    "href": "Docker/network/default_networks.html",
    "title": "Default networks",
    "section": "",
    "text": "Docker comes with three nets by default.\n!docker network ls\n\nNETWORK ID     NAME      DRIVER    SCOPE\n1726f609501a   bridge    bridge    local\n80c3e6772c8c   host      host      local\n45bb103d970f   none      null      local"
  },
  {
    "objectID": "Docker/network/default_networks.html#network-none",
    "href": "Docker/network/default_networks.html#network-none",
    "title": "Default networks",
    "section": "Network none",
    "text": "Network none\nBy connecting the container to this network, it is completely blocked from any interactions through the network.\nIn the following cell, there is a simple bash script that allows you to compare the behavior of the container to make it make a fucking bug?\n\nFrom host to container\nIn the following cell, there is a simple bash script that allows you to compare the behavior of a container connected to the bridge network and a container connected to the none network.\nAt the beginning, a container is started using the bridge network. When the command curl localhost:80 is executed, a basic nginx response is received.\nThen, the network for the container is changed to none. As a consequence, when the command curl localhost:80 is executed again, there is no response. This lack of response is a consequence of using the none network for the container.\nOverall, this script demonstrates the impact of network selection on the container’s ability to communicate and receive responses.\n\n%%bash\ndocker run --rm -itd\\\n    -p 80:80 \\\n    --name network_example\\\n    nginx &&gt; /dev/null\nsleep 2\n\necho \"=====DEFAULT NETWORK=====\"\necho \"-----network name-----\"\ndocker inspect -f \\\n    '{{range $key, $value := .NetworkSettings.Networks}}{{$key}} {{end}}' \\\n    network_example\necho \"-----curl to nginx-----\"\ncurl -s localhost:80 | head -n 10\n\ndocker network disconnect bridge network_example\ndocker network connect none network_example\n\necho\necho\necho \"=====none NETWORK=====\"\necho \"-----network name-----\"\ndocker inspect -f \\\n    '{{range $key, $value := .NetworkSettings.Networks}}{{$key}} {{end}}' \\\n    network_example\necho \"-----curl to nginx-----\"\ncurl -s localhost:80 | head -n 10\n\ndocker stop network_example &&gt; /dev/null\n\n=====DEFAULT NETWORK=====\n-----network name-----\nbridge \n-----curl to nginx-----\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n&lt;style&gt;\nhtml { color-scheme: light dark; }\nbody { width: 35em; margin: 0 auto;\nfont-family: Tahoma, Verdana, Arial, sans-serif; }\n&lt;/style&gt;\n&lt;/head&gt;\n\n\n=====none NETWORK=====\n-----network name-----\nnone \n-----curl to nginx-----\n\n\n\n\nFrom container to internet\nAnother feature of the none network is that it does not have access to the internet. for instance, you can use the curl command to fetch data from any website while the docker container is connected to the bridge network. however, if you switch the network to none, the curl command won’t work.\nThe script demonstrates this scenario. initially, the container is connected to the bridge network, and executing the curl example.com command within the container successfully fetches the webpage. but after switching the container to the none network, the same curl command does not produce any output.\n\n%%bash\ndocker run --rm -itd\\\n    --name network_example\\\n    nginx &&gt; /dev/null\nsleep 2\n\necho \"=====DEFAULT NETWORK=====\"\necho \"-----network name-----\"\ndocker inspect -f \\\n    '{{range $key, $value := .NetworkSettings.Networks}}{{$key}} {{end}}' \\\n    network_example\necho \"-----curl from bridge-----\"\ndocker exec network_example curl -s http://example.com/ | head -n 10\n\ndocker network disconnect bridge network_example\ndocker network connect none network_example\n\necho\necho\necho \"=====none NETWORK=====\"\necho \"-----network name-----\"\ndocker inspect -f \\\n    '{{range $key, $value := .NetworkSettings.Networks}}{{$key}} {{end}}' \\\n    network_example\necho \"-----curl from none-----\"\ndocker exec network_example curl -s http://example.com/ | head -n 10\n\ndocker stop network_example &&gt; /dev/null\n\n=====DEFAULT NETWORK=====\n-----network name-----\nbridge \n-----curl from bridge-----\n&lt;!doctype html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Example Domain&lt;/title&gt;\n\n    &lt;meta charset=\"utf-8\" /&gt;\n    &lt;meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" /&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" /&gt;\n    &lt;style type=\"text/css\"&gt;\n    body {\n\n\n=====none NETWORK=====\n-----network name-----\nnone \n-----curl from none-----"
  },
  {
    "objectID": "Docker/network/default_networks.html#network-host",
    "href": "Docker/network/default_networks.html#network-host",
    "title": "Default networks",
    "section": "Network host",
    "text": "Network host\nCompletely removes the container’s network isolation, that is, the container shares the network completely with the host.\n\nConnect to host\nNote you can’t connect or disconnect a container to the host network. It will cause an error. You can only create containers that are already connected to the host network.\nSo the following cell shows what message you will get when you try to connect or disconnect a container to the host network.\n\n%%bash\ndocker run --rm -itd\\\n    --name network_example\\\n    nginx &&gt; /dev/null\nsleep 2\n\ndocker network disconnect bridge network_example\ndocker network connect host network_example\n\ndocker stop network_example &&gt; /dev/null\n\nError response from daemon: container cannot be disconnected from host network or connected to host network\n\n\n\n\nPorts with host network\nIf you are using the host network, you don’t need to worry about setting ports for the container because it will share all ports with the host (the computer the container is running on).\nSo, the following code cell starts an nginx container twice without using any specified ports. In the first case, it doesn’t specify any network, but in the second case, the --net host parameter is used. When curl localhost:80 is executed in the first case, it doesn’t return anything, but in the second case, it returns the nginx welcome page\n\n%%bash\ndocker run --rm -itd\\\n    --name network_example\\\n    nginx &&gt; /dev/null\nsleep 2 \n\necho \"=====DEFAULT NETWORK=====\"\necho \"-----network name-----\"\ndocker inspect -f \\\n    '{{range $key, $value := .NetworkSettings.Networks}}{{$key}} {{end}}' \\\n    network_example\necho \"-----curl to bridge-----\"\ncurl -s localhost:80\n\ndocker stop network_example &&gt; /dev/null\nsleep 2 &&gt; /dev/null\n\ndocker run --rm -itd\\\n    --name network_example\\\n    --net host\\\n    nginx &&gt; /dev/null\nsleep 2\n\necho\necho\necho \"=====host NETWORK=====\"\necho \"-----network name-----\"\ndocker inspect -f \\\n    '{{range $key, $value := .NetworkSettings.Networks}}{{$key}} {{end}}' \\\n    network_example\necho \"-----curl to host-----\"\ncurl -s localhost:80 | head -n 10\n\ndocker stop network_example &&gt; /dev/null\n\n=====DEFAULT NETWORK=====\n-----network name-----\nbridge \n-----curl to bridge-----\n\n\n=====host NETWORK=====\n-----network name-----\nhost \n-----curl to host-----\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n&lt;style&gt;\nhtml { color-scheme: light dark; }\nbody { width: 35em; margin: 0 auto;\nfont-family: Tahoma, Verdana, Arial, sans-serif; }\n&lt;/style&gt;\n&lt;/head&gt;"
  },
  {
    "objectID": "Docker/docker_commands.html",
    "href": "Docker/docker_commands.html",
    "title": "Docker commands",
    "section": "",
    "text": "Here we describe those docker commands that do not merit a separate discussion."
  },
  {
    "objectID": "Docker/docker_commands.html#ps---show-containers",
    "href": "Docker/docker_commands.html#ps---show-containers",
    "title": "Docker commands",
    "section": "ps - show containers",
    "text": "ps - show containers\nTo view active containers, you should use the docker ps command.\n\n%%bash\ndocker run -itd --rm --name test_container python:3.10 &&gt; /dev/null\ndocker ps\ndocker stop test_container &&gt; /dev/null\n\nCONTAINER ID   IMAGE         COMMAND     CREATED        STATUS                  PORTS     NAMES\nb17984f2b882   python:3.10   \"python3\"   1 second ago   Up Less than a second             test_container\n\n\n\n-a - show all\nBy default, inactive containers are hidden.\n\n%%bash\ndocker run --name temp_container ubuntu &&gt; /dev/null\necho \"========no option -a==============\"\ndocker ps\necho \"=========with option -a=============\"\ndocker ps -a\ndocker rm temp_container &&gt; /dev/null\n\n========no option -a==============\nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n=========with option -a=============\nCONTAINER ID   IMAGE     COMMAND       CREATED                  STATUS                              PORTS     NAMES\nad0439c23847   ubuntu    \"/bin/bash\"   Less than a second ago   Exited (0) Less than a second ago             temp_container"
  },
  {
    "objectID": "Docker/docker_commands.html#history---show-layers-of-the-image",
    "href": "Docker/docker_commands.html#history---show-layers-of-the-image",
    "title": "Docker commands",
    "section": "history - show layers of the image",
    "text": "history - show layers of the image"
  },
  {
    "objectID": "Docker/docker_commands.html#start---activate-exited-container",
    "href": "Docker/docker_commands.html#start---activate-exited-container",
    "title": "Docker commands",
    "section": "start - activate exited container",
    "text": "start - activate exited container\nIn the next cell, I build a ubuntu container that prints “hello world” when it starts. Like all standard ubuntu containers it quits, which we can see in the STATUS of the ps command. Then by docker start the container is reactivated.\nFor some reason it only works when I use the -i option for docker start.\n\n%%bash\ncd containers_files\ndocker build -t test_ubuntu . &&gt; /dev/null\necho \"=====First start=====\"\ndocker run --name u1 test_ubuntu\necho \"=====List of all containers=====\"\ndocker ps -a\necho \"=====Restart=====\"\ndocker start -i u1\ndocker rm u1 &&gt; /dev/null\ndocker rmi test_ubuntu &&gt; /dev/null\n\n=====First start=====\nhello world\n=====List of all containers=====\nCONTAINER ID   IMAGE         COMMAND                CREATED        STATUS                              PORTS     NAMES\naf533b577c94   test_ubuntu   \"echo 'hello world'\"   1 second ago   Exited (0) Less than a second ago             u1\n=====Restart=====\nhello world"
  },
  {
    "objectID": "Docker/docker_commands.html#stop---stop-some-container",
    "href": "Docker/docker_commands.html#stop---stop-some-container",
    "title": "Docker commands",
    "section": "stop - stop some container",
    "text": "stop - stop some container\nIn the following example, I start a container with Ubuntu in such a way that it stays running, which was proven by using docker ps. But after using docker stop the same container is in the state “Exited”.\n\n%%bash\ndocker run -itd --name test_ubuntu ubuntu &&gt; /dev/null\necho \"=====Just started container=====\"\ndocker ps -a\ndocker stop test_ubuntu &&gt; /dev/null\necho \"=====Stoped container (STATUS \\\"Exited\\\")=====\"\ndocker ps -a\ndocker rm test_ubuntu &&gt; /dev/null\n\n=====Just started container=====\nCONTAINER ID   IMAGE     COMMAND       CREATED                  STATUS                  PORTS     NAMES\ne175ae336dcb   ubuntu    \"/bin/bash\"   Less than a second ago   Up Less than a second             test_ubuntu\n=====Stoped container (STATUS \"Exited\")=====\nCONTAINER ID   IMAGE     COMMAND       CREATED          STATUS                                PORTS     NAMES\ne175ae336dcb   ubuntu    \"/bin/bash\"   10 seconds ago   Exited (137) Less than a second ago             test_ubuntu"
  },
  {
    "objectID": "Docker/docker_commands.html#exec---run-in-the-container",
    "href": "Docker/docker_commands.html#exec---run-in-the-container",
    "title": "Docker commands",
    "section": "exec - run in the container",
    "text": "exec - run in the container\nIn the next command I start ubuntu conteiner. Then using docker exec' I run thels’ command in the container and get a typical Linux result for that command.\n\n%%bash\ndocker run -itd --rm --name test_ubuntu ubuntu &&gt; /dev/null\ndocker exec test_ubuntu ls\ndocker stop test_ubuntu &&gt; /dev/null\n\nbin\nboot\ndev\netc\nhome\nlib\nlib32\nlib64\nlibx32\nmedia\nmnt\nopt\nproc\nroot\nrun\nsbin\nsrv\nsys\ntmp\nusr\nvar\n\n\n\n-it - take control\nIt looks like the options individually have the same properties as in the docker run command. But the important thing is that you can take control of the detached container with this command. The following example shows how I can get control in detached ubuntu by running the bash command with named options."
  },
  {
    "objectID": "Docker/docker_commands.html#logs---get-container-console",
    "href": "Docker/docker_commands.html#logs---get-container-console",
    "title": "Docker commands",
    "section": "logs - get container console",
    "text": "logs - get container console\nDesrtiption of the command you can find here."
  },
  {
    "objectID": "Docker/docker_commands.html#rm---automatic-container-removal",
    "href": "Docker/docker_commands.html#rm---automatic-container-removal",
    "title": "Docker commands",
    "section": "-rm - automatic container removal",
    "text": "-rm - automatic container removal\nAfter stopping, the container is removed. In the following example there are two containers of ubuntu, one with the --rm option and the other without. Both containers stops by default - but only one stays with STATUS == Exited.\n\n%%bash\ndocker run --name no_rm_ubuntu ubuntu\ndocker run --rm --name ubuntu_with_rm ubuntu\ndocker ps -a\ndocker rm no_rm_ubuntu &&gt; /dev/null\n\nCONTAINER ID   IMAGE     COMMAND       CREATED        STATUS                              PORTS     NAMES\n7acc3cc0c61c   ubuntu    \"/bin/bash\"   1 second ago   Exited (0) Less than a second ago             no_rm_ubuntu"
  },
  {
    "objectID": "Docker/docker_commands.html#name---set-name-for-the-container",
    "href": "Docker/docker_commands.html#name---set-name-for-the-container",
    "title": "Docker commands",
    "section": "--name - set name for the container",
    "text": "--name - set name for the container\n\n%%bash\ndocker run --rm --name unbuntu_container -d ubuntu\ndocker ps\ndocker stop unbuntu_container\n\n7116dcdaa0a131a8507f9cbe149719498456fe356a23f1ddfc48f65080cfe660\nCONTAINER ID   IMAGE     COMMAND       CREATED        STATUS                  PORTS     NAMES\n7116dcdaa0a1   ubuntu    \"/bin/bash\"   1 second ago   Up Less than a second             unbuntu_container\nunbuntu_container"
  },
  {
    "objectID": "Docker/docker_commands.html#d---make-container-not-to-lock-cli",
    "href": "Docker/docker_commands.html#d---make-container-not-to-lock-cli",
    "title": "Docker commands",
    "section": "-d - make container not to lock CLI",
    "text": "-d - make container not to lock CLI\n\n%%bash\ndocker run --rm -d --name temp_nginx nginx &&gt; /dev/null\ndocker ps\ndocker stop temp_nginx &&gt; /dev/null\n\nCONTAINER ID   IMAGE     COMMAND                  CREATED                  STATUS                  PORTS     NAMES\n09175cb8c6d9   nginx     \"/docker-entrypoint.…\"   Less than a second ago   Up Less than a second   80/tcp    temp_nginx"
  },
  {
    "objectID": "Docker/docker_commands.html#i---interactive-mode",
    "href": "Docker/docker_commands.html#i---interactive-mode",
    "title": "Docker commands",
    "section": "-i - interactive mode",
    "text": "-i - interactive mode\nMode that allows to interapt with container.\nUnfortunately it is not possible to show how this mode will work in jupyter. So here is a gif of me experimenting with the following command:\ndocker run -i --rm --name test_ubuntu ubuntu\n\nIn the example, the ubuntu container has been started and is stopped by default. The -i option causes it to lock the terminal. Extremely interesting that you will be able to send messages to the container and get results (as I showed with ls and exit commands), but you won’t get the description of the user common for Linux terminal."
  },
  {
    "objectID": "Docker/docker_commands.html#t---turn-on-tty",
    "href": "Docker/docker_commands.html#t---turn-on-tty",
    "title": "Docker commands",
    "section": "-t - turn on TTY",
    "text": "-t - turn on TTY\nTTY is something on backend language. The point here is that in combination with the -i option you can make will show you the “description” of the user which was mentioned in the previous section. And the terminal will be blocked waiting for input. But this option don’t allow you to send anything to the container.\nThe following .gif show what you will get in case using command docker run -t --rm --name temp_ubuntu ubuntu."
  },
  {
    "objectID": "Docker/docker_commands.html#it---combination",
    "href": "Docker/docker_commands.html#it---combination",
    "title": "Docker commands",
    "section": "-it - combination",
    "text": "-it - combination\nBoth options were discovered above, but they are really often used in combination. So Ubuntu provides full CLI interface with this options."
  },
  {
    "objectID": "Docker/docker_commands.html#commands-from-jupyter",
    "href": "Docker/docker_commands.html#commands-from-jupyter",
    "title": "Docker commands",
    "section": "Commands from jupyter",
    "text": "Commands from jupyter\nUsing only the i option (without t), you can send commands to the container from the Jupyter cell.\nSo in the following example, I can easily run the ls program from the newly created ubuntu container.\n\n%%bash\ndocker run --rm -i --name test_container ubuntu\nls\nexit\n\nbin\nboot\ndev\netc\nhome\nlib\nlib32\nlib64\nlibx32\nmedia\nmnt\nopt\nproc\nroot\nrun\nsbin\nsrv\nsys\ntmp\nusr\nvar\n\n\nIt’s a pity I didn’t understand this before I wrote most of the Docker description. So don’t be surprised if you see something like this:\n\n\ndocker exec … docker exec … docker exec …\n\nIn some section of docker description."
  },
  {
    "objectID": "Docker/docker_commands.html#e---environment-variables",
    "href": "Docker/docker_commands.html#e---environment-variables",
    "title": "Docker commands",
    "section": "-e - environment variables",
    "text": "-e - environment variables\nYou can also pass the values of the variables of the system you are calling from.\n\n%%bash\ndocker run --rm -i --name e_example -e FEDOR_TEST_VAR=$HOME ubuntu\necho $FEDOR_TEST_VAR\nexit\n\n/home/fedor"
  },
  {
    "objectID": "Docker/docker_commands.html#sec-docker_run_com_on_start",
    "href": "Docker/docker_commands.html#sec-docker_run_com_on_start",
    "title": "Docker commands",
    "section": "Command on container start",
    "text": "Command on container start\nIn the docker run command, after the name of the base image, you can specify a command that will be executed when the container starts. So in the next example I will show you how to run echo \"hello world\" in a new ubuntu container.\n\n%%bash\ndocker run --rm ubuntu echo \"hello world\"\n\nhello world\n\n\n\nIf you want to run several commands you have to use bash -c \"&lt;command&gt;\", like in following example:\n\n\n%%bash\ndocker run --rm ubuntu bash -c \"\n    echo \\\"hello world\\\"; \n    echo \\\"hello world2\\\"\n\"\n\nhello world\nhello world2\n\n\n\nNote that when containers are in interactive mode, the command executed should take input, otherwise it will just leave container anyway, the follwing gif describe how it looks like: \n\nIn the example, I ran the echo command with container starts and showed that there were no containers created - because the container was stopped and removed just after it was started. But immediately I created a new container, but now with the command bash -c \"echo \\\"hello world\\\"; bash - in this case the last command is bash which waits for input, so I’m comming back into the container."
  },
  {
    "objectID": "Docker/docker_commands.html#v---map-folder-on-container-to-folder-on-host",
    "href": "Docker/docker_commands.html#v---map-folder-on-container-to-folder-on-host",
    "title": "Docker commands",
    "section": "-v - map folder on container to folder on host",
    "text": "-v - map folder on container to folder on host\nThis is wide topic detailed in this section;"
  },
  {
    "objectID": "Docker/docker_commands.html#u---set-user",
    "href": "Docker/docker_commands.html#u---set-user",
    "title": "Docker commands",
    "section": "-u - set user",
    "text": "-u - set user\nBy default, all docker containers have root privileges, but it can be useful to set user. For a more detailed description in the context of filesystem access, see here."
  },
  {
    "objectID": "Docker/docker_commands.html#p---use-port",
    "href": "Docker/docker_commands.html#p---use-port",
    "title": "Docker commands",
    "section": "-p - use port",
    "text": "-p - use port\nFor mode details see this."
  },
  {
    "objectID": "Docker/docker_commands.html#pull---download-image-from-dockerhub.com",
    "href": "Docker/docker_commands.html#pull---download-image-from-dockerhub.com",
    "title": "Docker commands",
    "section": "pull - download image from dockerhub.com",
    "text": "pull - download image from dockerhub.com\nIt is obligatory to pass on one of the:\n\n&lt;image name&gt;:&lt;tag&gt; (&lt;tag&gt; - by default is latest);\nIMAGE ID - image id.\n\n\n%%bash\ndocker images | grep hello-world\necho \"===============beforre pull======================\"\ndocker pull hello-world:latest &&gt; /dev/null\necho \"===============after pull======================\"\ndocker images | grep hello-world\ndocker rmi hello-world &&gt; /dev/null\n\n===============beforre pull======================\n===============after pull======================\nhello-world                latest    9c7a54a9a43c   2 weeks ago     13.3kB"
  },
  {
    "objectID": "Docker/docker_commands.html#images---show-list-of-available-images",
    "href": "Docker/docker_commands.html#images---show-list-of-available-images",
    "title": "Docker commands",
    "section": "images - show list of available images",
    "text": "images - show list of available images\n\n%%bash\ndocker images\n\nREPOSITORY    TAG       IMAGE ID       CREATED         SIZE\nhello-world   latest    feb5d9fea6a5   16 months ago   13.3kB\n\n\n\n--a,--all - show all images\nBy default, so called “intermediate”, images are hiden. Nowadays I don’t know what “intermediate” images are.\n\n%%bash\ndocker images --all\n\nREPOSITORY    TAG       IMAGE ID       CREATED         SIZE\nhello-world   latest    feb5d9fea6a5   16 months ago   13.3kB\n\n\n\n\n--digests - show digests\n\n%%bash\ndocker images --digests\n\nREPOSITORY    TAG       DIGEST                                                                    IMAGE ID       CREATED         SIZE\nhello-world   latest    sha256:aa0cc8055b82dc2509bed2e19b275c8f463506616377219d9642221ab53cf9fe   feb5d9fea6a5   16 months ago   13.3kB\n\n\n\n\n--format - format output\nAllows output to be formatted using GO templates.\n\n%%bash\ndocker images --format '{{.Repository}}:{{.Tag}}'\n\nhello-world:latest\n\n\n\n\n--no-trunc - full output\n\n%%bash\ndocker images --no-trunc\n\nREPOSITORY    TAG       IMAGE ID                                                                  CREATED         SIZE\nhello-world   latest    sha256:feb5d9fea6a5e9606aa995e879d862b825965ba48de054caab5ef356dc6b3412   16 months ago   13.3kB\n\n\n\n\n-q, --quiet - print only images ids\n\n%%bash\ndocker images -q\n\nfeb5d9fea6a5"
  },
  {
    "objectID": "Docker/docker_commands.html#rmi---delete-images",
    "href": "Docker/docker_commands.html#rmi---delete-images",
    "title": "Docker commands",
    "section": "rmi - delete images",
    "text": "rmi - delete images\nIt is obligatory to pass on one of the:\n\n&lt;image name&gt;:&lt;tag&gt; (&lt;tag&gt; - by default is latest);\nIMAGE ID - image id.\n\n\n%%bash\ndocker images\necho \"===========================\"\ndocker rmi hello-world:latest\necho \"===========================\"\ndocker images\n\nREPOSITORY    TAG       IMAGE ID       CREATED         SIZE\nhello-world   latest    feb5d9fea6a5   16 months ago   13.3kB\n===========================\nUntagged: hello-world:latest\nUntagged: hello-world@sha256:aa0cc8055b82dc2509bed2e19b275c8f463506616377219d9642221ab53cf9fe\nDeleted: sha256:feb5d9fea6a5e9606aa995e879d862b825965ba48de054caab5ef356dc6b3412\nDeleted: sha256:e07ee1baac5fae6a26f30cabfe54a36d3402f96afda318fe0a96cec4ca393359\n===========================\nREPOSITORY   TAG       IMAGE ID   CREATED   SIZE"
  },
  {
    "objectID": "Docker/docker_commands.html#build---build-image-from-dockerfile",
    "href": "Docker/docker_commands.html#build---build-image-from-dockerfile",
    "title": "Docker commands",
    "section": "build - build image from dockerfile",
    "text": "build - build image from dockerfile\nThis command has description here"
  },
  {
    "objectID": "Docker/docker_commands.html#commit---save-container-state",
    "href": "Docker/docker_commands.html#commit---save-container-state",
    "title": "Docker commands",
    "section": "commit - save container state",
    "text": "commit - save container state\nThis command has description here."
  },
  {
    "objectID": "Docker/docker_commands.html#host---container",
    "href": "Docker/docker_commands.html#host---container",
    "title": "Docker commands",
    "section": "host -> container",
    "text": "host -&gt; container\nThere is the following syntax:\ndocker cp &lt;host path&gt; &lt;cantainer name&gt;:&lt;container path&gt;\nThe following example shows how to copy a file into a container."
  },
  {
    "objectID": "Docker/docker_commands.html#container---host",
    "href": "Docker/docker_commands.html#container---host",
    "title": "Docker commands",
    "section": "container -> host",
    "text": "container -&gt; host\nThere is syntax for m\n\n%%bash\ncd filesystem_example\ndocker run --rm --name test_ubuntu -itd ubuntu &&gt; /dev/null\n\ndocker cp copied_message.txt test_ubuntu:copied_message.txt\n\necho \"=====copied to container=====\"\ndocker exec test_ubuntu cat copied_message.txt\necho \"\"\n\ndocker cp test_ubuntu:copied_message.txt new_message.txt\necho \"=====copied from container=====\"\ncat new_message.txt\n\nrm new_message.txt\ndocker stop test_ubuntu &&gt; /dev/null\n\n=====copied to container=====\nThis message is for checking the copy functions.\n=====copied from container=====\nThis message is for checking the copy functions."
  },
  {
    "objectID": "Docker/docker_commands.html#basic-examples",
    "href": "Docker/docker_commands.html#basic-examples",
    "title": "Docker commands",
    "section": "Basic examples",
    "text": "Basic examples\nThis command provide information about docker ojects in json format such as:\n\nimages:\n\n\n%%bash\ndocker inspect ubuntu\n\n[\n    {\n        \"Id\": \"sha256:74f2314a03de34a0a2d552b805411fc9553a02ea71c1291b815b2f645f565683\",\n        \"RepoTags\": [\n            \"ubuntu:latest\"\n        ],\n        \"RepoDigests\": [\n            \"ubuntu@sha256:2adf22367284330af9f832ffefb717c78239f6251d9d0f58de50b86229ed1427\"\n        ],\n        \"Parent\": \"\",\n        \"Comment\": \"\",\n        \"Created\": \"2023-03-01T04:38:49.239257335Z\",\n        \"Container\": \"298f60554671ae2f5bf43b9892526aaa221e8093c9cee1ca68ef65fc3ac67600\",\n        \"ContainerConfig\": {\n            \"Hostname\": \"298f60554671\",\n            \"Domainname\": \"\",\n            \"User\": \"\",\n            \"AttachStdin\": false,\n            \"AttachStdout\": false,\n            \"AttachStderr\": false,\n            \"Tty\": false,\n            \"OpenStdin\": false,\n            \"StdinOnce\": false,\n            \"Env\": [\n                \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n            ],\n            \"Cmd\": [\n                \"/bin/sh\",\n                \"-c\",\n                \"#(nop) \",\n                \"CMD [\\\"/bin/bash\\\"]\"\n            ],\n            \"Image\": \"sha256:6088cf91777e3b0190e579c7c7cab9c65626f5ff625373bcdb02ae877a9118d8\",\n            \"Volumes\": null,\n            \"WorkingDir\": \"\",\n            \"Entrypoint\": null,\n            \"OnBuild\": null,\n            \"Labels\": {\n                \"org.opencontainers.image.ref.name\": \"ubuntu\",\n                \"org.opencontainers.image.version\": \"22.04\"\n            }\n        },\n        \"DockerVersion\": \"20.10.12\",\n        \"Author\": \"\",\n        \"Config\": {\n            \"Hostname\": \"\",\n            \"Domainname\": \"\",\n            \"User\": \"\",\n            \"AttachStdin\": false,\n            \"AttachStdout\": false,\n            \"AttachStderr\": false,\n            \"Tty\": false,\n            \"OpenStdin\": false,\n            \"StdinOnce\": false,\n            \"Env\": [\n                \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n            ],\n            \"Cmd\": [\n                \"/bin/bash\"\n            ],\n            \"Image\": \"sha256:6088cf91777e3b0190e579c7c7cab9c65626f5ff625373bcdb02ae877a9118d8\",\n            \"Volumes\": null,\n            \"WorkingDir\": \"\",\n            \"Entrypoint\": null,\n            \"OnBuild\": null,\n            \"Labels\": {\n                \"org.opencontainers.image.ref.name\": \"ubuntu\",\n                \"org.opencontainers.image.version\": \"22.04\"\n            }\n        },\n        \"Architecture\": \"amd64\",\n        \"Os\": \"linux\",\n        \"Size\": 77810712,\n        \"VirtualSize\": 77810712,\n        \"GraphDriver\": {\n            \"Data\": {\n                \"MergedDir\": \"/var/lib/docker/overlay2/dfcfb531746a088aaf673a8bc04602c8c8e19b1114680ff8164a95abfef8a0e6/merged\",\n                \"UpperDir\": \"/var/lib/docker/overlay2/dfcfb531746a088aaf673a8bc04602c8c8e19b1114680ff8164a95abfef8a0e6/diff\",\n                \"WorkDir\": \"/var/lib/docker/overlay2/dfcfb531746a088aaf673a8bc04602c8c8e19b1114680ff8164a95abfef8a0e6/work\"\n            },\n            \"Name\": \"overlay2\"\n        },\n        \"RootFS\": {\n            \"Type\": \"layers\",\n            \"Layers\": [\n                \"sha256:202fe64c3ce39b94d8beda7d7506ccdfcf7a59f02f17c915078e4c62b5c2ed11\"\n            ]\n        },\n        \"Metadata\": {\n            \"LastTagTime\": \"0001-01-01T00:00:00Z\"\n        }\n    }\n]\n\n\n\ncontainers:\n\n\n%%bash\ndocker run --rm --name test -itd ubuntu &&gt; /dev/null\ndocker inspect test\ndocker stop test &&gt; /dev/null\n\n[\n    {\n        \"Id\": \"020c99a8fbdca844a78956c0f7b2aba4fcd53e4f299112894dd0c0b7aca246c5\",\n        \"Created\": \"2023-06-12T13:37:52.23307605Z\",\n        \"Path\": \"/bin/bash\",\n        \"Args\": [],\n        \"State\": {\n            \"Status\": \"running\",\n            \"Running\": true,\n            \"Paused\": false,\n            \"Restarting\": false,\n            \"OOMKilled\": false,\n            \"Dead\": false,\n            \"Pid\": 32562,\n            \"ExitCode\": 0,\n            \"Error\": \"\",\n            \"StartedAt\": \"2023-06-12T13:37:52.471660971Z\",\n            \"FinishedAt\": \"0001-01-01T00:00:00Z\"\n        },\n        \"Image\": \"sha256:74f2314a03de34a0a2d552b805411fc9553a02ea71c1291b815b2f645f565683\",\n        \"ResolvConfPath\": \"/var/lib/docker/containers/020c99a8fbdca844a78956c0f7b2aba4fcd53e4f299112894dd0c0b7aca246c5/resolv.conf\",\n        \"HostnamePath\": \"/var/lib/docker/containers/020c99a8fbdca844a78956c0f7b2aba4fcd53e4f299112894dd0c0b7aca246c5/hostname\",\n        \"HostsPath\": \"/var/lib/docker/containers/020c99a8fbdca844a78956c0f7b2aba4fcd53e4f299112894dd0c0b7aca246c5/hosts\",\n        \"LogPath\": \"/var/lib/docker/containers/020c99a8fbdca844a78956c0f7b2aba4fcd53e4f299112894dd0c0b7aca246c5/020c99a8fbdca844a78956c0f7b2aba4fcd53e4f299112894dd0c0b7aca246c5-json.log\",\n        \"Name\": \"/test\",\n        \"RestartCount\": 0,\n        \"Driver\": \"overlay2\",\n        \"Platform\": \"linux\",\n        \"MountLabel\": \"\",\n        \"ProcessLabel\": \"\",\n        \"AppArmorProfile\": \"docker-default\",\n        \"ExecIDs\": null,\n        \"HostConfig\": {\n            \"Binds\": null,\n            \"ContainerIDFile\": \"\",\n            \"LogConfig\": {\n                \"Type\": \"json-file\",\n                \"Config\": {}\n            },\n            \"NetworkMode\": \"default\",\n            \"PortBindings\": {},\n            \"RestartPolicy\": {\n                \"Name\": \"no\",\n                \"MaximumRetryCount\": 0\n            },\n            \"AutoRemove\": true,\n            \"VolumeDriver\": \"\",\n            \"VolumesFrom\": null,\n            \"ConsoleSize\": [\n                0,\n                0\n            ],\n            \"CapAdd\": null,\n            \"CapDrop\": null,\n            \"CgroupnsMode\": \"private\",\n            \"Dns\": [],\n            \"DnsOptions\": [],\n            \"DnsSearch\": [],\n            \"ExtraHosts\": null,\n            \"GroupAdd\": null,\n            \"IpcMode\": \"private\",\n            \"Cgroup\": \"\",\n            \"Links\": null,\n            \"OomScoreAdj\": 0,\n            \"PidMode\": \"\",\n            \"Privileged\": false,\n            \"PublishAllPorts\": false,\n            \"ReadonlyRootfs\": false,\n            \"SecurityOpt\": null,\n            \"UTSMode\": \"\",\n            \"UsernsMode\": \"\",\n            \"ShmSize\": 67108864,\n            \"Runtime\": \"runc\",\n            \"Isolation\": \"\",\n            \"CpuShares\": 0,\n            \"Memory\": 0,\n            \"NanoCpus\": 0,\n            \"CgroupParent\": \"\",\n            \"BlkioWeight\": 0,\n            \"BlkioWeightDevice\": [],\n            \"BlkioDeviceReadBps\": [],\n            \"BlkioDeviceWriteBps\": [],\n            \"BlkioDeviceReadIOps\": [],\n            \"BlkioDeviceWriteIOps\": [],\n            \"CpuPeriod\": 0,\n            \"CpuQuota\": 0,\n            \"CpuRealtimePeriod\": 0,\n            \"CpuRealtimeRuntime\": 0,\n            \"CpusetCpus\": \"\",\n            \"CpusetMems\": \"\",\n            \"Devices\": [],\n            \"DeviceCgroupRules\": null,\n            \"DeviceRequests\": null,\n            \"MemoryReservation\": 0,\n            \"MemorySwap\": 0,\n            \"MemorySwappiness\": null,\n            \"OomKillDisable\": null,\n            \"PidsLimit\": null,\n            \"Ulimits\": null,\n            \"CpuCount\": 0,\n            \"CpuPercent\": 0,\n            \"IOMaximumIOps\": 0,\n            \"IOMaximumBandwidth\": 0,\n            \"MaskedPaths\": [\n                \"/proc/asound\",\n                \"/proc/acpi\",\n                \"/proc/kcore\",\n                \"/proc/keys\",\n                \"/proc/latency_stats\",\n                \"/proc/timer_list\",\n                \"/proc/timer_stats\",\n                \"/proc/sched_debug\",\n                \"/proc/scsi\",\n                \"/sys/firmware\"\n            ],\n            \"ReadonlyPaths\": [\n                \"/proc/bus\",\n                \"/proc/fs\",\n                \"/proc/irq\",\n                \"/proc/sys\",\n                \"/proc/sysrq-trigger\"\n            ]\n        },\n        \"GraphDriver\": {\n            \"Data\": {\n                \"LowerDir\": \"/var/lib/docker/overlay2/06b3ab167c3c359841177a3bc998d8bc5c32c1c8893d7518e2df4193a1dc98ee-init/diff:/var/lib/docker/overlay2/dfcfb531746a088aaf673a8bc04602c8c8e19b1114680ff8164a95abfef8a0e6/diff\",\n                \"MergedDir\": \"/var/lib/docker/overlay2/06b3ab167c3c359841177a3bc998d8bc5c32c1c8893d7518e2df4193a1dc98ee/merged\",\n                \"UpperDir\": \"/var/lib/docker/overlay2/06b3ab167c3c359841177a3bc998d8bc5c32c1c8893d7518e2df4193a1dc98ee/diff\",\n                \"WorkDir\": \"/var/lib/docker/overlay2/06b3ab167c3c359841177a3bc998d8bc5c32c1c8893d7518e2df4193a1dc98ee/work\"\n            },\n            \"Name\": \"overlay2\"\n        },\n        \"Mounts\": [],\n        \"Config\": {\n            \"Hostname\": \"020c99a8fbdc\",\n            \"Domainname\": \"\",\n            \"User\": \"\",\n            \"AttachStdin\": false,\n            \"AttachStdout\": false,\n            \"AttachStderr\": false,\n            \"Tty\": true,\n            \"OpenStdin\": true,\n            \"StdinOnce\": false,\n            \"Env\": [\n                \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n            ],\n            \"Cmd\": [\n                \"/bin/bash\"\n            ],\n            \"Image\": \"ubuntu\",\n            \"Volumes\": null,\n            \"WorkingDir\": \"\",\n            \"Entrypoint\": null,\n            \"OnBuild\": null,\n            \"Labels\": {\n                \"org.opencontainers.image.ref.name\": \"ubuntu\",\n                \"org.opencontainers.image.version\": \"22.04\"\n            }\n        },\n        \"NetworkSettings\": {\n            \"Bridge\": \"\",\n            \"SandboxID\": \"422100341f6b603db62a0c616fa614091b66ac5a06796fa577503fd469b4ce56\",\n            \"HairpinMode\": false,\n            \"LinkLocalIPv6Address\": \"\",\n            \"LinkLocalIPv6PrefixLen\": 0,\n            \"Ports\": {},\n            \"SandboxKey\": \"/var/run/docker/netns/422100341f6b\",\n            \"SecondaryIPAddresses\": null,\n            \"SecondaryIPv6Addresses\": null,\n            \"EndpointID\": \"b2440d2f1d13674c667458fd051354ec32d2e11475db2308e20cd057e3b16037\",\n            \"Gateway\": \"172.17.0.1\",\n            \"GlobalIPv6Address\": \"\",\n            \"GlobalIPv6PrefixLen\": 0,\n            \"IPAddress\": \"172.17.0.2\",\n            \"IPPrefixLen\": 16,\n            \"IPv6Gateway\": \"\",\n            \"MacAddress\": \"02:42:ac:11:00:02\",\n            \"Networks\": {\n                \"bridge\": {\n                    \"IPAMConfig\": null,\n                    \"Links\": null,\n                    \"Aliases\": null,\n                    \"NetworkID\": \"cf037e1d3aeb308ae603f8028da3343134d92e79462fdcb5390b196a9e49c87d\",\n                    \"EndpointID\": \"b2440d2f1d13674c667458fd051354ec32d2e11475db2308e20cd057e3b16037\",\n                    \"Gateway\": \"172.17.0.1\",\n                    \"IPAddress\": \"172.17.0.2\",\n                    \"IPPrefixLen\": 16,\n                    \"IPv6Gateway\": \"\",\n                    \"GlobalIPv6Address\": \"\",\n                    \"GlobalIPv6PrefixLen\": 0,\n                    \"MacAddress\": \"02:42:ac:11:00:02\",\n                    \"DriverOpts\": null\n                }\n            }\n        }\n    }\n]\n\n\n\nnetworks:\n\n\n%%bash\ndocker inspect bridge\n\n[\n    {\n        \"Name\": \"bridge\",\n        \"Id\": \"cf037e1d3aeb308ae603f8028da3343134d92e79462fdcb5390b196a9e49c87d\",\n        \"Created\": \"2023-06-12T08:14:24.801403507+03:00\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"EnableIPv6\": false,\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Options\": null,\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.17.0.0/16\",\n                    \"Gateway\": \"172.17.0.1\"\n                }\n            ]\n        },\n        \"Internal\": false,\n        \"Attachable\": false,\n        \"Ingress\": false,\n        \"ConfigFrom\": {\n            \"Network\": \"\"\n        },\n        \"ConfigOnly\": false,\n        \"Containers\": {},\n        \"Options\": {\n            \"com.docker.network.bridge.default_bridge\": \"true\",\n            \"com.docker.network.bridge.enable_icc\": \"true\",\n            \"com.docker.network.bridge.enable_ip_masquerade\": \"true\",\n            \"com.docker.network.bridge.host_binding_ipv4\": \"0.0.0.0\",\n            \"com.docker.network.bridge.name\": \"docker0\",\n            \"com.docker.network.driver.mtu\": \"1500\"\n        },\n        \"Labels\": {}\n    }\n]"
  },
  {
    "objectID": "Docker/docker_commands.html#special-cases",
    "href": "Docker/docker_commands.html#special-cases",
    "title": "Docker commands",
    "section": "Special cases",
    "text": "Special cases\n\nFor more information about the path to container logs, you can look here."
  },
  {
    "objectID": "Docker/docker_compose/docker_compose.html",
    "href": "Docker/docker_compose/docker_compose.html",
    "title": "YAML",
    "section": "",
    "text": "Разбор инстумента docker-compose\nПозиционируется как инстумент, который следующая ступень абстрации над docker - инстумент который позволяет организовать взаимодейсвие контейнеров между собой.\nЯзык разметки, с помощью которого задается поведение docker-compose.\nYAML позволяет описывать структуры данных которые состоят из списков и словарей: - каждый элемент списка начинается с символа -; - каждый новый ключ в словаре задатся так &lt;ключ&gt;:; - шаблоны (якоря) - возможность создать ссылку на некоторую сущность и потом использовать её в произвольном месте структуры; - вложенность структуры формируется отступами.\nДалее на примерах станет понятнее.\nВ python существует библиотека yaml, которая позволяет пребразовывать yaml разметку в соответсвующие структуры данных python.\n\nimport yaml\n\nВ следующем примере создается ключ словарь с ключами names, ages под которыми скрываются соответсвующие списки.\n\nyamls_str = \\\n\"\"\"\nnames:\n    - peter\n    - olga\nages:\n    - 22\n    - 18\n\"\"\"\n\nyaml.full_load(yamls_str)\n\n{'names': ['peter', 'olga'], 'ages': [22, 18]}\n\n\nЗаметим, что между &lt;ключ&gt;: и &lt;значение&gt; обязательно должен быть пробел. Вот, для сравнения, правильно созданный словарь под ключом postgres и ошибочно созданный под ключом clickhouse.\n\nyamls_str = \\\n\"\"\"\npostgres:\n    user: postgres_app_user\n    password: postgres_app_password\n    host: postgres_host\n    port: 5432\nclickhouse:\n    host:clickhouse_host\n    user:clickhouse_app_user\n    db:clickhouse_app_db\n    password:clickhouse_app_password\n\"\"\"\n\nyaml.full_load(yamls_str)\n\n{'postgres': {'user': 'postgres_app_user',\n  'password': 'postgres_app_password',\n  'host': 'postgres_host',\n  'port': 5432},\n 'clickhouse': 'host:clickhouse_host user:clickhouse_app_user db:clickhouse_app_db password:clickhouse_app_password'}\n\n\nИнетестно то, что для того, что по умолчанию yaml игнорирует все переносы на новую строку. Для того, чтобы справиться с этим исполюзуется: - | после имени ключа заставил yaml “видеть” перевод строки; - &gt; после имени ключа воткнет перенос строки в конец занчения.\nТак в примене ниже test1 и test2 с точки срения программы читающей yaml не отличаются. А вот test3 и test4 получают в некоторых местах служебный \\n.\n\nyamls_str = \\\n\"\"\"\ntest1: peter olga\ntest2:\n    peter\n    olga\ntest3: |\n    peter\n    olga\ntest4: &gt;\n    peter olga\n\"\"\"\n\nyaml.full_load(yamls_str)\n\n{'test1': 'peter olga',\n 'test2': 'peter olga',\n 'test3': 'peter\\nolga\\n',\n 'test4': 'peter olga\\n'}\n\n\nНу и для примера покажем как сформировать список словарей:\n\nyamls_str = \\\n\"\"\"\n- name: peter\n  age: 22\n- name: olga\n  age: 18\n\"\"\"\n\nyaml.full_load(yamls_str)\n\n[{'name': 'peter', 'age': 22}, {'name': 'olga', 'age': 18}]\n\n\nЯкоря создаются следующим образом: - В сущности на которую ссылаются задают &&lt;обозначение ссылки&gt;; - Когда эту сущность надо вставить куда-то используется синтаксис &lt;&lt;: *&lt;обозначение ссылки&gt;.\nВ примере далее были описаны свойства junior-a некоторой компании, а затем созданы две сушности которым были переданы свойсва этих junior-ов.\n\nyamls_str = \\\n\"\"\"\njunior:\n    &junior\n    position: junior\n    salary: 55000\n\nPeter:\n    &lt;&lt;: *junior\nOlga:\n    &lt;&lt;: *junior\n\"\"\"\n\nyaml.full_load(yamls_str)\n\n{'junior': {'position': 'junior', 'salary': 55000},\n 'Peter': {'position': 'junior', 'salary': 55000},\n 'Olga': {'position': 'junior', 'salary': 55000}}\n\n\n\nКоманды docker-compose\nТут будут рассмотрены самые полезные случаи для работы с коммандой docker-compose. Для примера используется docker-compose.yaml приложенный в той-же папке, что и этот notebook.\n\nup - поднять приложение\nКоманда up используется для того, чтобы поднять приложение спользующее docker-compose. Выполняется обязательно в той папке в которой лежит yaml описывающий приложение.\nОпции:\n\nd - запустит в фоновом режиме - терминал останеться под управлением пользователя.\n\nТак, следующий пример показывает, что до вызова docker-comporse up в docker нет не контейнеров ни вольюмов, а после, все это появляется.\n\n%%bash\necho '=======запущенные контейнеры========='\ndocker ps\necho '===========доступные volume=========='\ndocker volume ls\n\necho '==========запускаю приложение==========='\ndocker-compose up -d &&gt; /dev/null\n\necho '=======запущенные контейнеры========='\ndocker ps --format '{{.Names}}'\necho '===========доступные volume=========='\ndocker volume ls\n\ndocker-compose down -v &&gt; /dev/null\n\n=======запущенные контейнеры=========\nCONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES\n===========доступные volume==========\nDRIVER    VOLUME NAME\n==========запускаю приложение===========\n=======запущенные контейнеры=========\ndocker_compose-db-1\n===========доступные volume==========\nDRIVER    VOLUME NAME\nlocal     docker_compose_ex_vol\n\n\n\n\ndown - положить приложение\nОпять же требуется вызывать из папки в которой лежит yaml описывающий приложение.\nОпции: - v - удалит все volume созданные при поднятии этого приложения.\n\n%%bash\ndocker-compose up -d &&gt; /dev/null\ndocker-compose down &&gt; /dev/null\n\necho '============без опции -v============'\ndocker volume ls\ndocker volume rm docker_compose_ex_vol &gt; /dev/null 2&gt;&1\n\n\ndocker-compose up -d &&gt; /dev/null\ndocker-compose down -v &&gt; /dev/null\necho '============опция -v============'\ndocker volume ls\n\n============без опции -v============\nDRIVER    VOLUME NAME\nlocal     docker_compose_ex_vol\n============опция -v============\nDRIVER    VOLUME NAME\n\n\n\n\n\nСоздание docker-compose файла\nЗдается с помощью описанного выше языка yaml. Далее будем обсуждать ключи которые могут быть использованы и для чего они могут быть использованы.\n\nКлюч services\nСкрывает под собой описание контейнеров, которые будут использоваться при развертывании приложения. Каждый ключ в словале под seriveces создает новый контенер, ключи задаются произвольно. То есть выглядеть это должно прмерно так:\nservices:\n  &lt;контейнер1&gt;:\n    &lt;инструкции&gt;\n  &lt;контейнер2&gt;:\n    &lt;инструкии&gt;\n  ...\nДалее рассмотрим различные интструкции которые может содржать каждый из контейнеров:\n\nБазовые интсрукции\nЕсть ряд интсрукции которые просто воспроизводят некторые комманды обычного docker. Не считаю, что они заслуживают отдельного разбора (пока не столкнулся с проблемами), потому просто покажу соответсвие с коммандами docker.\n\n\n\n\n\n\n\ndocker-compose.yaml\ndocker\n\n\n\n\nimage: &lt;образ&gt;\ndocker run &lt;образ&gt;\n\n\ncontainer_name: &lt;имя контейнера&gt;\ndocker run --name &lt;имя контейнера&gt;\n\n\nvolumes:  - &lt;volume/путь на хосте 1&gt;:&lt;путь в контейнере1&gt;  - &lt;volume/путь на хосте 2&gt;:&lt;путь в контейнере2&gt;  …\ndocker run \\-v &lt;путь на хосте/volume 1&gt;:&lt;путь в контейнере1&gt;\\  -v &lt;путь на хосте/volume 2&gt;:&lt;путь в контейнере2&gt;\\  …\n\n\nenvironment:&lt;имя переменной1&gt;: &lt;значение1&gt; &lt;имя переменной2&gt;: &lt;значение2&gt;  …\ndocker run \\  -e &lt;имя переменной1&gt;=&lt;значение переменной1&gt;  -e &lt;имя переменной2&gt;=&lt;значение переменной2&gt;  …\n\n\nnetworks:  - &lt;сеть 1&gt;  - &lt;сеть 2&gt;  …\ndocker run  --net &lt;сеть 1&gt;  --net &lt;сеть 2&gt;  …\n\n\nports:  &lt;порт на хосте1&gt;:&lt;порт в контейнере1&gt;  &lt;порт на хосте2&gt;:&lt;порт в контейнере2&gt;  …\ndocker run  -p &lt;порт на хосте1&gt;:&lt;порт в контейнере1&gt;  -p &lt;порт на хосте2&gt;:&lt;порт в контейнере2&gt;  …\n\n\n\n\n\nСборка образа - инструкция build\nСоответсвует комманде docker build. Соберет образ и запустит на его основе контейнер. заметим, что при опускании приложения, docker-compose остановит и удалит контейнер, но, не образ, поэтому удаление образа (в случае необходимости) ложится на админа.\n\n%%bash\ncd build_example\ndocker-compose up -d &&gt; /dev/null\necho '=====показываю что появился новый образ====='\ndocker images | grep build_example_my_small_ubuntu\necho '=====а на его основании контейнер====='\ndocker ps -a --format '{{.Names}}'\ndocker-compose down &&gt; /dev/null\necho '=====опустил приложение, но образ то остался====='\ndocker images | grep build_example_my_small_ubuntu\ndocker rmi build_example_my_small_ubuntu &&gt; /dev/null\n\n=====показываю что появился новый образ=====\nbuild_example_my_small_ubuntu   latest    098799dc601d   9 days ago      77.8MB\n=====а на его основании контейнер=====\nbuild_example-my_small_ubuntu-1\n=====опустил приложение, но образ то остался=====\nbuild_example_my_small_ubuntu   latest    098799dc601d   9 days ago      77.8MB\n\n\n\n\nИнтерактивный режим и подключение tty\nДелаются интрукциями:\ntty: true\nstdin_open: true\nДалее пример. Там поднимаются два контейнера на основе ubuntu один: - with_tty использует названные инструкции; - no_tty не использует названные инструкции.\nВ итоге при вызовер docker ps -a выявляюется обра образа, а при docker ps только образ with_tty, что говорит о том, что образ no_tty завершает свою работу.\n\n%%bash\n\ncd it_option\ndocker-compose up -d &&gt; /dev/null\nsleep 5\necho \"=====docker ps -a=====\"\ndocker ps -a --format '{{.Names}}'\necho \"=====docker ps=====\"\ndocker ps --format '{{.Names}}'\ndocker-compose down &&gt; /dev/null\n\n=====docker ps -a=====\nno_tty\nwith_tty\n=====docker ps=====\nwith_tty\n\n\n\n\nПерезапуск контейнера - интструкция restart\nВ случае, если некоторая программа при определенных обстоятельсвах будет вылетать с ошбкой её приходится перезапускать. В docker-compose для того предусмотренна инструация restart. Она, похоже, может принимать множество значений, но на сегоняшний день известна только always, которая приведет к тому, что контейнер будет перезапускаться пока не запустится.\nОбраз приведенный в папке restart сделан таким образом, чтобы программа останавливась всегда, когда файл check_file сорержит число меньшеее 5, но приращает число файле на 1. Если же программа видит число большее либо равное 5-ти в контейнере, то это приводит к тому, что она зависает в бесконечном цикле.\nЗапуская docker-compose.yaml с таким контейнером внутри и опцией restart: true получаем, что записанное в файле число дорастает ровно до 5.\n\n%%bash\n\ncd restart\necho \"1\" &gt; check_file\n\ndocker-compose up -d &&gt; /dev/null\nsleep 10\n\ndocker-compose down &&gt; /dev/null\ndocker rmi example_python &&gt; /dev/null\n\ncat check_file\n\n5\n\n\n\n\nПроверка работоспособности контейнера - инструкция healthcheck\nИногоа бывает так, что контейнер может провериться, по поводу того насколько правильно он запущен. Для того, чтобы вызвать команду проверки используется интсрукция healthy.\nhealthy имеет некоторые настройки, вот некоторые из них: - test - задает команду которая будет исопльзована для проверки контейнера; - interval; - timeout; - retries.\nВот, например, как инструкция может быть использована в postgresql:\n...\nhealthcheck: \n    test: [\"CMD-SHELL\", \"pg_isready\", \"-U\", \"docker_app\"]\n...\nТак в примере, представленном ниже, разворачивается два контейнера на основе postgresql (на остальные контейнеры в рамках этого примера можно не обращать внимания). Один с этой опицией, другой без. При отображении их через dokcer ps у одного в STATUS есть пририска (health: starting) у второго - нет.\n\n%%bash\ncd postgres_example\ndocker-compose up -d &&gt; /dev/null\ndocker ps --format '{{.Names}}'\ndocker-compose down -v &&gt; /dev/null\n\npostgres_example-ubuntu3-1\npostgres_example-ubuntu1-1\npostgres_example-db1-1\npostgres_example-db2-1\n\n\n\n\nЗависимость от других сревисов - depends_on\nЕсли требуется что-то делать в зависимости от другого сервиса в этом docker-compose.yaml то делается это так:\n...\ndepends_on:\n    &lt;имя обуславливающего сервиса&gt;:\n        &lt;инструкции&gt;\n...\nНапример, если требуется запускать контейнер, только в том случае, если другой сервис healthy, то это будет записано так:\n...\ndepends_on:\n    &lt;имя обуславливающего сервиса&gt;:\n        condition: service_healthy\n...\nНа данный момент известно о следующих возможностях этой инструкции:\n\ncondition: service_healthy - проверить проведена ли для обулавливающего контейнера проверка healthcheck;\ncondition: service_started - проверить поднят ли обуславливающий контейнер.\n\nДля примера представим “docker-compose.yaml” расположенный в папке “postgres_example”. В нем: - сервисы “ubuntu1” и “ubuntu2” поднимаются только в случае “здоровья” сервисов “db1” и “db2” соотсветсвенно - в результате поднимается только “ubuntu1” потому как проверка healthcheck предусмотрена только для “db1”; - сервисы “ubuntu3” и “ubuntu4” поднимаются только в тех случах если подняты “ubuntu1” и “ubuntu2” соотственно - так как на прошлом шаге “unbuntu2” не поднялся не поднимется и “ubuntu4”.\n\n%%bash\ncd postgres_example\ndocker-compose up -d &&gt; /dev/null\ndocker ps --format '{{.Names}}'\ndocker-compose down -v &&gt; /dev/null\n\npostgres_example-ubuntu3-1\npostgres_example-ubuntu1-1\npostgres_example-db2-1\npostgres_example-db1-1\n\n\n\n\nРеплики контейнера - интсрукция deploy-&gt;replicas\nДля интсрукции deploy, наверняка будет больше подинтсрукций. Но на сегдняшний день известно только об replicas.\ndeploy-&gt;replicas позволяет задать число копий поднимаемого контейнера.\nТак использование последовательности инструкций:\ndeploy:\n  replicas: 3\nПриводит к тому, что будет поднято три реплики сервиса для которого это записано.\nТак в примере ниже я одним махом поднимаю 3 контейнера ubuntu:\n\n%%bash\ncd replicas\ndocker-compose up -d &&gt; /dev/null\ndocker ps -a --format '{{.Names}}'\ndocker-compose down &&gt; /dev/null\n\nreplicas-my_ubuntu-1\nreplicas-my_ubuntu-3\nreplicas-my_ubuntu-2\n\n\n\n\nКоманды при запуске - инструкция command\nМожно указать команду, которая будет выполнена при запуске контейнера. Так “docker-file.yaml” в папке “command” содержить строку: command: bash -c \"echo \\\"hello world\\\" &gt; test_file; bash\"\nЭта строка в контейре создаёт файл “test_file” наличие и содержание, котого мы проверяем в примере ниже:\n\n%%bash\ncd command\ndocker-compose up -d &&gt; /dev/null\ndocker exec temp_ubuntu cat test_file\ndocker-compose down &&gt; /dev/null\n\nhello world\n\n\nИнтерестно, то что если написть command так:\ncommand echo \"hello world\" &gt; test_file\nКонтейнер завершает работу. Видимо это связано с тем, что последняя комманда должна бесконечно ожидать ввода, чтобы все работало - именно такую функцию выполняет комманда bash.\n\n\n\nКлюч volumes\nЗадает volumes.\nКаждый вложенный ключ создаст volume.\nvolumes:\n    &lt;volume1&gt;:\n        name: &lt;volume name&gt;\n    &lt;volume2&gt;:\n    ...\nУ каждого volume можно задать поле name (а можно и не задавать) которое укажет имя volume при поднятии приложения. Так, в примере ниже, из папки volume_example создается volume с именем example_name.\n\n%%bash\ncd volume_example\ndocker-compose up -d &&gt; /dev/null\n\necho \"=====список volume=====\"\ndocker volume ls\n\ndocker-compose down -v &&gt; /dev/null\n\n=====список volume=====\nDRIVER    VOLUME NAME\nlocal     example_name\n\n\n\n\nКлюч networks\nЗадает сети импользуемые в приложении.\nКаждый вложенный ключ создаст сеть.\nnetworks:\n    &lt;net1&gt;:\n        name: &lt;network name&gt;\n    &lt;net2&gt;:\n    ...\nКлюч name задает имя сети и не является обязательным. На в папке network-example лежит yaml файл, который описывает сеть с именем example_name.\n\n%%bash\ncd network_example\ndocker-compose up -d &&gt; /dev/null\ndocker network ls\ndocker-compose down -v &&gt; /dev/null\n\nNETWORK ID     NAME           DRIVER    SCOPE\n75034612bd8f   bridge         bridge    local\n14cb969f28d4   example_name   bridge    local\nf8b2503d0640   host           host      local\nb1d7e6bda275   none           null      local\n\n\n\n\n\nДетали\n\nСеть по умолчанию\nЛюбое приложение запущенное через docker-compose, в случае отсудсвия указанных сетей, создает себе сеть с названием по типу &lt;имя папки&gt;_default. Так в примере далее показано, что в списке, кроме базовых сетей, появляется docker_compose_default.\n\n%%bash\ndocker-compose up -d &&gt; /dev/null\necho \"=====созданные сети=====\"\ndocker network ls\ndocker-compose down -v &&gt; /dev/null\n\n=====созданные сети=====\nNETWORK ID     NAME                     DRIVER    SCOPE\n75034612bd8f   bridge                   bridge    local\n205bf876f99f   docker_compose_default   bridge    local\nf8b2503d0640   host                     host      local\nb1d7e6bda275   none                     null      local\n\n\n\n\nНазвание по умолчанию\nvolumes/сети, по умолчанию, получают некоторые названия по типу &lt;название папки&gt;_&lt;название ключа&gt;. Так, в следующем примере, в папке default_namimg указаны volume ex_vol и сеть ex_net, но для них не указано ключа name.\n\n%%bash\ncd default_naming\ndocker-compose up -d &&gt; /dev/null\necho '=====volumes====='\ndocker volume ls\necho '=====networks====='\ndocker network ls\ndocker-compose down -v &&gt; /dev/null\n\n=====volumes=====\nDRIVER    VOLUME NAME\nlocal     default_naming_ex_vol\n=====networks=====\nNETWORK ID     NAME                    DRIVER    SCOPE\n75034612bd8f   bridge                  bridge    local\n4599a4f61b14   default_naming_ex_net   bridge    local\nf8b2503d0640   host                    host      local\nb1d7e6bda275   none                    null      local\n\n\n\n\nvolume/сеть не создаются?\nУбедитесь, что они указаны под ключами volumes/networks в каком-либо из контейнеров. В противном случае они не создаются.\nВ следующем примере из папки vol_net_missed разворачивается приложение. И хотя в соответсвующем docker-compose.yaml заданы volume ex_vol и сеть ex_net, при запуске приложения создается только безимянный volume (видимо создаваемый postgres по умолчанию) и сеть которая всегда создается docker-compose по умолчанию vol_net_missed_default.\nПримеры удачного создания сетей/volumes представлены выше.\n\n%%bash\ncd vol_net_missed\ndocker-compose up -d &&gt; /dev/null\necho '====volumes====='\ndocker volume ls\necho '=====networks====='\ndocker network ls\ndocker-compose down -v &&gt; /dev/null\n\n====volumes=====\nDRIVER    VOLUME NAME\nlocal     48e548f659d0e05ba4a981e4b17e9b2b1835ca62aeb0cbfa08974d23d7772469\n=====networks=====\nNETWORK ID     NAME                     DRIVER    SCOPE\n75034612bd8f   bridge                   bridge    local\nf8b2503d0640   host                     host      local\nb1d7e6bda275   none                     null      local\nbcef30a7a541   vol_net_missed_default   bridge    local\n\n\n\n\nМонтирование директорий из директирии запуска\nИногда требуется сослатся на папку из которой производится запуск docker-compose. Для того можно использовать ${PWD}/&lt;дирректория&gt; или ./&lt;дирректория&gt;\nКогда мы, например, в docker монтировали файл/папку, через bind mount мы писали что-то вроде:\ndocker run --rm\\\n    -v $(pwd)/&lt;название файла/папки&gt;:&lt;путь в контейнере&gt;\nАналогичная запись в docker-compose.yaml:\nservices:\n  &lt;название сервиса&gt;:\n    volumes:\n      - ${PWD}/&lt;название файла/папки&gt;:&lt;путь в контейнере&gt;\nили\nservices:\n  &lt;название сервиса&gt;:\n    volumes:\n      - ./&lt;название файла/папки&gt;:&lt;путь в контейнере&gt;\nПример далее, заодно покажем как пользоваться bind mount в docker-compose.\ndocker-compose файл лежит в папке pwd_example. В этой же папке создается файл test_file, который потом указано монтировать в файле docker-compose.yaml, так - ${PWD}/test_file:/test_file1 и как - ./test_file:test_file2.\nЗатем в файлы в файловой системе контейнера вносятся изменения.\nПосле закрытия контейнера, все изменения в файле остаются в исходном файле на хосте.\n\n%%bash\n\ncd pwd_example\necho \"Сообщение с хоста\" &gt; test_file\necho \"=====Исходный файл=====\"\ncat test_file\n\ndocker-compose up -d &&gt; /dev/null\ndocker exec test_cont bash -c \"echo 'Сообщение из контейнера 1' &gt;&gt; test_file1\"\ndocker exec test_cont bash -c \"echo 'Сообщение из контейнера 2' &gt;&gt; test_file2\"\ndocker-compose down &&gt; /dev/null\n\necho \"=====Измененный файл=====\"\ncat test_file\n\n=====Исходный файл=====\nСообщение с хоста\n=====Измененный файл=====\nСообщение с хоста\nСообщение из контейнера 1\nСообщение из контейнера 2\n\n\n\n\nОбновление docker-compose приложения\nЕсли вдруг, в docker-compose.yaml были внесены некоторые изменения, то не обязательно опускать приложение - можно его просто запустить наново, docker-compose подхватит, что это то же самое приложение (видимо по папке запуска) и обновит его.\nВ следующем примере, я сначала запускаю docker-compose с приложением в котором единтсвенный контейнер имеет имя first_name, затем подменяю docker-compose.yaml на тот, который содержит контейнер названный second_name.\nЯ хочу показать то, что несмотря на то что запуска два и немного разных контейнер то остается один и меняет имя в соовтесвии с актуальным docker-compose.yaml\n\n%%bash\ncd refresh\ncat first.yaml &gt; docker-compose.yaml\ndocker-compose up -d &&gt; /dev/null\necho \"=====Первый запуск=====\"\ndocker ps -a --format '{{.Names}}'\ncat second.yaml &gt; docker-compose.yaml\ndocker-compose up -d &&gt; /dev/null\necho \"=====Второй запуск=====\"\ndocker ps -a --format '{{.Names}}'\ndocker-compose down &&gt; /dev/null\n\n=====Первый запуск=====\nfirst_name\n=====Второй запуск=====\nsecond_name"
  },
  {
    "objectID": "Docker/logs.html",
    "href": "Docker/logs.html",
    "title": "Logging in docker",
    "section": "",
    "text": "Logs can:\n\nWrite them using the standard output stream;\nWrite to a predefined file (there is nothing special here, you can just use volume or bind mount);\nA combination of the two previously mentioned methods.\n\n\nlogs command\nAllows to read, what happened in container terminal.\nIn following example I: - I create conteiner which generates either a message in stdout or an error in stderr every second; - Wait 10 seconds; - Use docker logs to see what events happen in container.\n\n%%bash\ncd logs_examples\ndocker build -t log_example . &&gt; /dev/null\n\ndocker run -d --rm --name log_example log_example stream.py &&gt; /dev/null\nsleep 10\n\necho \"=====generated log=====\"\ndocker logs log_example\n\ndocker stop log_example &&gt; /dev/null\ndocker rmi log_example &&gt; /dev/null\n\n=====generated log=====\nJust message\nJust message\nJust message\nJust message\nJust message\n\n\nSome error\nSome error\nSome error\nSome error\n\n\nNote for some reason Jupyter puts errors in the first place in its output - so you may always get errors first when trying to reproduce the results of the following example.\nActually, you can avoid this by redirecting the error stream to the normal stream, by using 2&gt;&1 bash construction:\n\n%%bash\ncd logs_examples\ndocker build -t log_example . &&gt; /dev/null\n\ndocker run -d --rm --name log_example log_example stream.py &&gt; /dev/null\nsleep 10\n\necho \"=====generated log=====\"\ndocker logs log_example 2&gt;&1\n\ndocker stop log_example &&gt; /dev/null\ndocker rmi log_example &&gt; /dev/null\n\n=====generated log=====\nJust message\nJust message\nJust message\nJust message\nSome error\nSome error\nSome error\nJust message\nJust message\n\n\n\n-f - read container output in runtime\n\n\n\n-t - get time of the message\n\n%%bash\ncd logs_examples\ndocker build -t log_example . &&gt; /dev/null\n\ndocker run -d --rm --name log_example log_example stream.py &&gt; /dev/null\nsleep 10\n\necho \"=====generated log=====\"\ndocker logs -t log_example\n\ndocker stop log_example &&gt; /dev/null\ndocker rmi log_example &&gt; /dev/null\n\n=====generated log=====\n2023-06-12T08:42:12.063382853Z Just message\n2023-06-12T08:42:13.064541066Z Just message\n2023-06-12T08:42:14.065711848Z Just message\n2023-06-12T08:42:17.069086047Z Just message\n2023-06-12T08:42:19.071241128Z Just message\n2023-06-12T08:42:20.072419403Z Just message\n\n\n2023-06-12T08:42:15.066916732Z Some error\n2023-06-12T08:42:16.068155506Z Some error\n2023-06-12T08:42:18.070257834Z Some error\n\n\n\n\n\nGet log path on the computer\nIf you need to get the path to the Docker container log on your computer, you should use the construction shown below.\n\n%%bash\ncd logs_examples\ndocker build -t log_example . &&gt; /dev/null\n\ndocker run -d --rm --name log_example log_example stream.py &&gt; /dev/null\necho \"=====path to the log=====\"\ndocker inspect --format \"{{.LogPath}}\" log_example\n\ndocker stop log_example &&gt; /dev/null\ndocker rmi log_example &&gt; /dev/null\n\n=====path to the log=====\n/var/lib/docker/containers/cb2fb4f9b5d8829c1afb6b78293cf9487c0266d4152bd2f9a3f9d9d766740216/cb2fb4f9b5d8829c1afb6b78293cf9487c0266d4152bd2f9a3f9d9d766740216-json.log\n\n\nI can’t get superuser access from jupyter, but the log file should look like this:\n    {\"log\":\"Just message\\n\",\"stream\":\"stdout\",\"time\":\"2023-03-10T15:25:01.788547631Z\"}\n    {\"log\":\"Just message\\n\",\"stream\":\"stdout\",\"time\":\"2023-03-10T15:25:02.788842446Z\"}\n    {\"log\":\"Just message\\n\",\"stream\":\"stdout\",\"time\":\"2023-03-10T15:25:03.788818941Z\"}\n    {\"log\":\"Just message\\n\",\"stream\":\"stdout\",\"time\":\"2023-03-10T15:25:04.789226578Z\"}\n    {\"log\":\"Some error\\n\",\"stream\":\"stderr\",\"time\":\"2023-03-10T15:25:05.789279007Z\"}\n    {\"log\":\"Some error\\n\",\"stream\":\"stderr\",\"time\":\"2023-03-10T15:25:06.789563215Z\"}\n    {\"log\":\"Just message\\n\",\"stream\":\"stdout\",\"time\":\"2023-03-10T15:25:07.7895666Z\"}\n    {\"log\":\"Just message\\n\",\"stream\":\"stdout\",\"time\":\"2023-03-10T15:25:08.789954572Z\"}\n    {\"log\":\"Some error\\n\",\"stream\":\"stderr\",\"time\":\"2023-03-10T15:25:09.790014965Z\"}\n    {\"log\":\"Just message\\n\",\"stream\":\"stdout\",\"time\":\"2023-03-10T15:25:10.790300833Z\"}\n    {\"log\":\"Some error\\n\",\"stream\":\"stderr\",\"time\":\"2023-03-10T15:25:11.790511897Z\"}\n    {\"log\":\"Just message\\n\",\"stream\":\"stdout\",\"time\":\"2023-03-10T15:25:12.790821869Z\"}\n    {\"log\":\"Some error\\n\",\"stream\":\"stderr\",\"time\":\"2023-03-10T15:25:13.790939824Z\"}\n    {\"log\":\"Just message\\n\",\"stream\":\"stdout\",\"time\":\"2023-03-10T15:25:14.791204789Z\"}\n    {\"log\":\"Some error\\n\",\"stream\":\"stderr\",\"time\":\"2023-03-10T15:25:15.791283359Z\"}\n    {\"log\":\"Just message\\n\",\"stream\":\"stdout\",\"time\":\"2023-03-10T15:25:16.791536539Z\"}\n    {\"log\":\"Some error\\n\",\"stream\":\"stderr\",\"time\":\"2023-03-10T15:25:17.791786679Z\"}\n    {\"log\":\"Just message\\n\",\"stream\":\"stdout\",\"time\":\"2023-03-10T15:25:18.791807047Z\"}\n    {\"log\":\"Just message\\n\",\"stream\":\"stdout\",\"time\":\"2023-03-10T15:25:19.792075268Z\"}\n    {\"log\":\"Just message\\n\",\"stream\":\"stdout\",\"time\":\"2023-03-10T15:25:20.792325859Z\"}\n    {\"log\":\"Just message\\n\",\"stream\":\"stdout\",\"time\":\"2023-03-10T15:25:21.792410762Z\"}\n    {\"log\":\"Some error\\n\",\"stream\":\"stderr\",\"time\":\"2023-03-10T15:25:22.792794096Z\"}\n    {\"log\":\"Just message\\n\",\"stream\":\"stdout\",\"time\":\"2023-03-10T15:25:23.792780477Z\"}\n    {\"log\":\"Some error\\n\",\"stream\":\"stderr\",\"time\":\"2023-03-10T15:25:24.793194852Z\"}\n    {\"log\":\"Just message\\n\",\"stream\":\"stdout\",\"time\":\"2023-03-10T15:25:25.793462485Z\"}\n    {\"log\":\"Some error\\n\",\"stream\":\"stderr\",\"time\":\"2023-03-10T15:25:26.793712972Z\"}\n    {\"log\":\"Just message\\n\",\"stream\":\"stdout\",\"time\":\"2023-03-10T15:25:27.793920753Z\"}\n    {\"log\":\"Just message\\n\",\"stream\":\"stdout\",\"time\":\"2023-03-10T15:25:28.794071626Z\"}\n    {\"log\":\"Some error\\n\",\"stream\":\"stderr\",\"time\":\"2023-03-10T15:25:29.794379101Z\"}\n    {\"log\":\"Just message\\n\",\"stream\":\"stdout\",\"time\":\"2023-03-10T15:25:30.794698985Z\"}\n    {\"log\":\"Some error\\n\",\"stream\":\"stderr\",\"time\":\"2023-03-10T15:25:31.794924583Z\"}\n    {\"log\":\"Some error\\n\",\"stream\":\"stderr\",\"time\":\"2023-03-10T15:25:32.795122566Z\"}\n    {\"log\":\"Just message\\n\",\"stream\":\"stdout\",\"time\":\"2023-03-10T15:25:33.795242226Z\"}\n    {\"log\":\"Some error\\n\",\"stream\":\"stderr\",\"time\":\"2023-03-10T15:25:34.795374507Z\"}\n    {\"log\":\"Some error\\n\",\"stream\":\"stderr\",\"time\":\"2023-03-10T15:25:35.795542017Z\"}\n    {\"log\":\"Just message\\n\",\"stream\":\"stdout\",\"time\":\"2023-03-10T15:25:36.795638844Z\"}\n    {\"log\":\"Just message\\n\",\"stream\":\"stdout\",\"time\":\"2023-03-10T15:25:37.795888676Z\"}\n    {\"log\":\"Just message\\n\",\"stream\":\"stdout\",\"time\":\"2023-03-10T15:25:38.796024537Z\"}\nThe fields in this example should have the following meaning:\n\nlog - message that was logged;\nstream - stream that was used;\ntime - time when it was made.\n\n\n\nStandart/error streams\n\nFrom terminal\nBasic Linux utilities for processing terminal output that are specified for the standard stream and don’t care about the error stream.\nFor example, if you use the head utility to handle program output:\n\n%%bash\n\ncd logs_examples\necho \"=====head result=====\"\npython3 stream.py -n 10| head -n 5\necho \"=====grep result=====\"\npython3 stream.py -n 10| grep message\n\n=====head result=====\nJust message\nJust message\nJust message\nJust message\n=====grep result=====\nJust message\nJust message\nJust message\nJust message\nJust message\nJust message\n\n\nSome error\nSome error\nSome error\nSome error\nSome error\nSome error\nSome error\nSome error\nSome error\nSome error\n\n\nDespite the fact that I asked for head -n 5, which means that 5 lines should be printed in the command output, I got many more lines. This happens because utilities such as head, tail, grep and so on only work with the standard stream and ignore the error. But the error stream has priority over the standard stream - so it will be printed anyway.\nAs a solution you can redirect error stream info to standart stream by using 2&gt;&1 construction after the command displaying result:\n\n%%bash\n\ncd logs_examples\necho \"=====head result=====\"\npython3 stream.py -n 10 2&gt;&1| head -n 5\necho \"=====grep result=====\"\npython3 stream.py -n 10 2&gt;&1|grep message\n\n=====head result=====\nSome error\nSome error\nSome error\nSome error\nSome error\n=====grep result=====\nJust message\nJust message\nJust message\n\n\nThis way the Linux commands will see no difference between error and standard streams, but the error stream will still be displayed first - which you can see in the head command result.\nBut in the context of Docker container logging, if you use 2&gt;&1 with docker logs, it will be sufficient to print standard and error streams in apparent order. This is probably because you are not extracting messages from the runtime, but from a file somewhere.\n\n%%bash\ncd logs_examples\ndocker build -t log_example . &&gt; /dev/null\n\ndocker run -d --rm --name log_example log_example stream.py &&gt; /dev/null\nsleep 10\n\necho \"=====generated log=====\"\ndocker logs log_example 2&gt;&1| head -n 5\n\ndocker stop log_example &&gt; /dev/null\ndocker rmi log_example &&gt; /dev/null\n\n=====generated log=====\nJust message\nSome error\nJust message\nSome error\nSome error\n\n\n\n\nTo file\nYou can save streams to files by using the following constructions:\n\n&gt; - rewrite file with the results of the standart stream;\n&gt;&gt; - complete file with the result of the standart stream;\n2&gt; - rewrite file with the result of the error stream;\n2&gt;&gt; - complete file with the result of the error stream.\n\nThe following example shows how divide standart and error streams into two files.\n\n%%bash\ncd logs_examples\npython3 stream.py -n 20 &gt; standart_stream 2&gt; error_stream\n\necho \"=====standart stream=====\"\ncat standart_stream\necho \"=====error stream=====\"\ncat error_stream\n\nrm standart_stream error_stream\n\n=====standart stream=====\nJust message\nJust message\nJust message\nJust message\nJust message\nJust message\nJust message\nJust message\nJust message\nJust message\nJust message\nJust message\n=====error stream=====\nSome error\nSome error\nSome error\nSome error\nSome error\nSome error\nSome error\nSome error"
  },
  {
    "objectID": "layout/sources.html",
    "href": "layout/sources.html",
    "title": "Sources",
    "section": "",
    "text": "Stepic html css course (rus);\nMDN Documenting web technologies, including CSS, HTML, and JavaScript;\n\nCSS float property."
  },
  {
    "objectID": "layout/css/margin.html",
    "href": "layout/css/margin.html",
    "title": "Margin (out space)",
    "section": "",
    "text": "This option allows you to add some extra space between an object and the objects surrounding it."
  },
  {
    "objectID": "layout/css/margin.html#basic",
    "href": "layout/css/margin.html#basic",
    "title": "Margin (out space)",
    "section": "Basic",
    "text": "Basic\nBy using this property you can set distance from outside elemnts. Use following syntax:\n\nmargin-&lt;top/left/right/bottom&gt;: &lt;value&gt;;\nmargin: &lt;value for all sides&gt;;\nmargin: &lt;top and bottom&gt; &lt;left and right&gt;;\nmargin: &lt;top&gt; &lt;left and right&gt; &lt;bottom&gt;;\nmargin: &lt;top&gt; &lt;right&gt; &lt;bottom&gt; &lt;left&gt;.\n\n\n%%HTML\n&lt;div style=\"border:solid green\"&gt;\n    &lt;div style=\"border:solid 10px; margin-top: 10px\"&gt;Upper element&lt;/div&gt;\n    &lt;div style=\"border:solid 10px; margin: 50px 0px 100px 50px\"&gt;Middle element&lt;/div&gt;\n    &lt;div style=\"border:solid 10px\"&gt;Lower element&lt;/div&gt;\n&lt;/div&gt;\n\n\n    Upper element\n    Middle element\n    Lower element"
  },
  {
    "objectID": "layout/css/margin.html#available-values",
    "href": "layout/css/margin.html#available-values",
    "title": "Margin (out space)",
    "section": "Available values",
    "text": "Available values\nValue can be setted as:\n\npx, cm, pt etc.\ninherit - use option like parent;\nauto.\n\n\nInherit\n\n%%HTML\n&lt;div style=\"border:solid green\"&gt;\n    &lt;div style=\"border:solid 10px red; margin: 0px 30px 100px 50px\"&gt;\n        &lt;div style=\"border:solid 10px; margin: inherit\"&gt;margin: inherit&lt;/div&gt;\n    &lt;/div&gt;\n&lt;/div&gt;\n\n\n    \n        margin: inherit\n    \n\n\n\n\n\nAuto\nWith this option, the element will take up all available vertical space and be centred horizontally.\n\n%%HTML\n&lt;div style=\"border:solid green\"&gt;\n    &lt;div style=\"border:solid 10px red; margin: auto;width: 500px; height: 200px\"&gt;\n        border:solid 10px red; margin: auto;width: 500px; height: 200px\n    &lt;/div&gt;\n&lt;/div&gt;\n\n\n    \n        border:solid 10px red; margin: auto;width: 500px; height: 200px\n    \n\n\n\n\n%%HTML\n&lt;div style=\"border:solid green\"&gt;\n    &lt;div&gt;Some other div&lt;/div&gt;\n    &lt;div style=\"border:solid 10px red; margin: auto;width: 500px; height: 200px\"&gt;\n        border:solid 10px red; margin: auto;width: 500px; height: 200px\n    &lt;/div&gt;\n&lt;/div&gt;\n\n\n    Some other div\n    \n        border:solid 10px red; margin: auto;width: 500px; height: 200px"
  },
  {
    "objectID": "layout/css/margin.html#slamming-the-indentation",
    "href": "layout/css/margin.html#slamming-the-indentation",
    "title": "Margin (out space)",
    "section": "Slamming the indentation",
    "text": "Slamming the indentation\nIf you are unig two elememtns close and:\n\nTop element has bottom margin;\nBottom element has top margin;\n\nThe browser interprets this as an offset equal to the maximum of the original. In the following example, you can compare two blocks with two 200px margins and two blocks with only one 400px margin.\n\n%%HTML\n&lt;div style=\"border:solid green\"&gt;\n    &lt;div style=\"margin-bottom: 200px;border:solid red\"&gt;Top div&lt;/div&gt;\n    &lt;div style=\"margin-top: 200px; border:solid red\"&gt; Bottom div&lt;/div&gt;\n&lt;/div&gt;\n&lt;div style=\"border:solid green\"&gt;\n    &lt;div style=\"margin-bottom: 400px;border:solid red\"&gt;Top div&lt;/div&gt;\n    &lt;div style=\"border:solid red\"&gt; Bottom div&lt;/div&gt;\n&lt;/div&gt;\n\n\n    Top div\n     Bottom div\n\n\n    Top div\n     Bottom div\n\n\n\n\nPercentage\nYou can set the margin relative to the parent size.\nSo in the following example, a parent div is created with a width of 500px. And two child elements, one using margin-left: 50%, the other margin-left: 250px - both elements have a similar margin from the left edge of the parent.\n\n%%HTML\n&lt;div style=\"border:solid green; width: 500px; height: 100px\"&gt;\n    &lt;div style=\"margin-left: 50%;border:solid red\"&gt;margin-left: 50%&lt;/div&gt;\n    &lt;div style=\"margin-left: 250px;border:solid red\"&gt;margin-left: 250px&lt;/div&gt;\n&lt;/div&gt;\n\n\n    margin-left: 50%\n    margin-left: 250px"
  },
  {
    "objectID": "layout/css/style_application.html",
    "href": "layout/css/style_application.html",
    "title": "Style application",
    "section": "",
    "text": "This page is about various ways to align css to a specific group of elements."
  },
  {
    "objectID": "layout/css/style_application.html#integrated",
    "href": "layout/css/style_application.html#integrated",
    "title": "Style application",
    "section": "Integrated",
    "text": "Integrated\nBy using style attribute inside definition of element:\n\n%%HTML\n&lt;p style=\"color:red\"&gt;Red paragraph&lt;/p&gt;\n\nRed paragraph"
  },
  {
    "objectID": "layout/css/style_application.html#insideoutsize",
    "href": "layout/css/style_application.html#insideoutsize",
    "title": "Style application",
    "section": "Inside/Outsize",
    "text": "Inside/Outsize\n\nInside\nYou can use &lt;style&gt; tag inside &lt;head&gt; of html page.\n\n\nOutsize\nWith &lt;link rel=\"stylesheet\"&gt; tag for giving reference for special css file."
  },
  {
    "objectID": "layout/css/style_application.html#basic-selectors",
    "href": "layout/css/style_application.html#basic-selectors",
    "title": "Style application",
    "section": "Basic selectors",
    "text": "Basic selectors\nYou can apply some style:\n\nto all definitions of specific tag;\nto elements with specific class .class_name;\nto element with specific id #id_name;\nto each element of the page with universal selector *.\n\nSo the basic css syntax looks like:\n    * {property_1:value_1, property_1:value_1, ...}\n    &lt;tag name 1&gt; {property_1_1:value_1_1, property_1_2:value_1_2, ...}\n    &lt;tag name 2&gt; {property_2_1:value_2_1, property_2_2:value_2_2, ...}\n    ...\n    &lt;tag name n&gt; {property_n_1:value_n_1, property_n_2:value_n_2, ...}\n    .&lt;class name 1&gt; {property_n+1_1:value_n+1_1, property_n+1_2:value_n+1_2, ...}\n    .&lt;class name 2&gt; {property_n+2_1:value_n+2_1, property_n+2_2:value_n+2_2, ...}\n    ...\n    .&lt;class name k&gt; {property_n+k_1:value_n+k_1, property_n+k_2:value_n+k_2, ...}\n    #&lt;id name 1&gt; property_n+k+1_1:value_n+k+1_1, property_n+k+1_2:value_n+k+1_2, ...}\n    #&lt;id name 2&gt; property_n+k+2_1:value_n+k+2_1, property_n+k+2_2:value_n+k+2_2, ...}\n    ...\n    #&lt;id name m&gt; property_n+k+m_1:value_n+k+m_1, property_n+k+m_2:value_n+k+m_2, ...}\nIn the following example, I use all of these options.\n\n%%writefile style_application_files/selectors.html\n\n&lt;head&gt;\n&lt;style&gt;\n  * {background-color: purple;}\n  h1 {background-color: red;}\n  #spec_id {background-color: green;} \n  .spec_class {background-color: yellow;}\n&lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n\n&lt;p&gt;not header at all&lt;/p&gt;\n&lt;h1&gt;just header&lt;/h1&gt;\n&lt;h1 class=\"spec_class\"&gt;header with class&lt;/h1&gt;\n&lt;h1  id=\"spec_id\" class=\"spec_class\"&gt;header with id&lt;/h1&gt;\n&lt;h1  id=\"spec_id\" class=\"spec_class\" style=\"background-color: blue;\"&gt;header with style&lt;/h1&gt;\n\n&lt;/body&gt;\n\nOverwriting style_application_files/selectors.html\n\n\nSee the results here.\nNote the last example shows that properties mentioned for tag are less important than properties mentioned for class. Id properties are more important than class properties. And properties mentioned in the style attribute have the higher priority."
  },
  {
    "objectID": "layout/css/style_application.html#combination-selectors",
    "href": "layout/css/style_application.html#combination-selectors",
    "title": "Style application",
    "section": "Combination selectors",
    "text": "Combination selectors\nYou can define selector relatevety other selector.\n\nDescendant selector A B\nIf you are using the following syntax.\nA B : {property_1:value_1, property_1:value_1, ...}\nWhere A and B are some other selectors. You will apply properties to items belonging to selector B, but only to A’s descendant at the same time.\nFor example I setted blue font and red background for each element with class my_class. But by using div .my_class {...} selector I set a different background color for each .my_class instance inside div.\n\n%%writefile style_application_files/descendant_selector.html\n&lt;header&gt;&lt;style&gt;\n    .my_class {\n        background: red;\n        color: blue\n    }\n    div .my_class {\n        background: green\n    }\n&lt;/style&gt;&lt;/header&gt;\n&lt;p class=\"my_class\"&gt;Just paragraph&lt;/p&gt;\n&lt;div&gt;\n    &lt;p class=\"my_class\"&gt;Paragraph in div&lt;/p&gt;\n    &lt;table&gt;\n        &lt;tr&gt;\n            &lt;td&gt;&lt;text class=\"my_class\"&gt;my_class&lt;/text&gt;&lt;/td&gt;\n            &lt;td&gt;&lt;text&gt;some other cell&lt;/text&gt;&lt;/text&gt;&lt;/td&gt;&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n            &lt;td&gt;&lt;text&gt;some other cell&lt;/text&gt;&lt;/td&gt;\n            &lt;td&gt;&lt;text class=\"my_class\"&gt;my_class&lt;/text&gt;&lt;/td&gt;\n        &lt;/tr&gt;\n    &lt;/table&gt;\n&lt;/div&gt;\n&lt;p class=\"my_class\"&gt;Just paragraph&lt;/p&gt;\n\nOverwriting style_application_files/descendant_selector.html\n\n\nSee the result here.\n\n\nDaughter selector A &gt; B\nIf you are using the following syntax:\nA &gt; B : {property_1:value_1, property_1:value_1, ...}\nWhere A and B are some other selectors. You will apply properties to items belonging to selector B, but only to A’s daughter element at the same time (an element B is a child of element A if element B is a direct descendant of element A, i.e. there are no other levels of inheritance between them).\nSo in the following example I define red background for colour for each p tag. But by using the syntax .some_class &gt; p {background: yellow} I set yellow back ground to each daughter of .some_class instances.\nThe main feature of the example is that I have used pargraphs:\n\nOnly paragraph - obvious red colour;\nDaughter paragraph of &lt;div class=\"some_class\"&gt; - obvious yellow color;\nDescendant of &lt;div class=\"some_class\"&gt; but not daughter - red color.\n\n\n%%writefile style_application_files/daughter_selector.html\n&lt;header&gt;\n    &lt;style&gt;\n        .some_class {\n            background: red\n        }\n        .some_class &gt; p {\n            background: yellow\n        }\n    &lt;/style&gt;\n&lt;/header&gt;\n\n&lt;p&gt;some random paragraph&lt;/p&gt;\n&lt;div class=\"some_class\"&gt;\n    &lt;p&gt;Test paragraph daughter of some_class&lt;/p&gt;\n    &lt;div&gt;\n        &lt;p&gt;Test paragraph descendant but not daughter of some_class&lt;/p&gt;\n    &lt;/div&gt;\n&lt;/div&gt;\n\nOverwriting style_application_files/daughter_selector.html\n\n\nSee the result here.\n\n\nNext element selector A+B\nIf you are using the following syntax:\nA+B : {property_1:value_1, property_1:value_1, ...}\nWhere A and B are some other selectors. Will apply properties to all elements belonging to the B selector, but only next to elements belonging to A within a parent.\nSo in following example I using syntax p+p {background:purple}. As a result every paragraph after another paragraph will have a purple background:\n\nFirst paragraph - obviously nothing before that =&gt; no colour;\nSecond paragraph - next to the first paragraph =&gt; purple colour;\nThird paragraph - next to the second paragraph =&gt; purple colour;\nForth paragraph - the first inside this div =&gt; no colour;\nFifth paragraph - next to the forth in the same div =&gt; purple color;\nSixth paragraph - next to div, not paragraph =&gt; no color.\n\n\n%%writefile style_application_files/next_element_selector.html\n\n&lt;head&gt;\n    &lt;style&gt;\n        p~p {background:purple}\n    &lt;/style&gt;\n&lt;/head&gt;\n\n&lt;p&gt;First paragraph&lt;/p&gt;\n&lt;p&gt;Second paragraph&lt;/p&gt;\n&lt;p&gt;Third paragraph&lt;/p&gt;\n\n&lt;div&gt;\n    &lt;p&gt;Fourth paragraph (in div)&lt;/p&gt;\n    &lt;p&gt;Fifth paragraph (in div)&lt;/p&gt;\n&lt;/div&gt;\n\n&lt;p&gt;Sixth paragraph&lt;/p&gt;\n\nOverwriting style_application_files/next_element_selector.html\n\n\nSee results here.\n\n\nAfter element selector A~B\nIf you are using the following syntax:\nA~B : {property_1:value_1, property_1:value_1, ...}\nWhere A and B are some other selectors. Will apply properties to all elements belonging to the B selector, but only after elements belonging to A within a parent.\nIn following exmaple I use syntax #my_id~p {background: yellow}. So every paragraph after an element with id=my_id with same parent should have yellow background. See result here.\nA detailed breakdown of the result:\n\nFirst paragraph - obviously nothing before that =&gt; no colour;\nSecond paragraph - have only first paragraph before wich don’t have any id =&gt; no colour;\nThird paragraph - have second paragraph before, which have an id=my_id =&gt; yellow colour;\nForth paragraph - first paragraph in the div =&gt; no colour;\nFifth paragraph - have second paragraph before, which have an id=my_id =&gt; yellow colour.\n\n\n%%writefile style_application_files/after_element_selector.html\n&lt;head&gt;\n    &lt;style&gt;\n        #my_id~p {background: yellow}\n    &lt;/style&gt;\n&lt;/head&gt;\n\n&lt;p&gt;First paragraph&lt;/p&gt;\n&lt;p id=\"my_id\"&gt;Second paragraph with id&lt;/p&gt;\n&lt;p&gt;Third paragraph&lt;/p&gt;\n&lt;div&gt;\n    &lt;p&gt;Fourth paragraph&lt;/p&gt;\n&lt;/div&gt;\n&lt;p&gt;Fiftht paragraph&lt;/p&gt;\n\nOverwriting style_application_files/after_element_selector.html\n\n\nSee result here.\nNote аlways prioritise the style that is listed later. In following example I have two divs:\n\nFirst div:\n\nFirst paragraph make second paragraph to have yellow background;\nThird paragraph make fourth, fifth, sixth to have purple colour;\nSixth paragraph doesn’t return yellow colour;\n\nSecond div:\n\nFist paragraph make all paragraphs after to have a purple color;\nThird paragraph doesn’t make the folowing paragraphs yellow.\n\n\n\n%%writefile style_application_files/after_element_selector_hierarchy.html\n&lt;head&gt;\n    &lt;style&gt;\n        #id_one~p {background: yellow}\n        #id_two~p {background: purple}\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;h1&gt;div1&lt;/h1&gt;\n&lt;div&gt;\n    &lt;p id=\"id_one\"&gt;first paragraph div1&lt;/p&gt;\n    &lt;p&gt;second paragraph div1&lt;/p&gt;\n    &lt;p id=\"id_two\"&gt;third paragraph div1&lt;/p&gt;\n    &lt;p&gt;forth paragraph div1&lt;/p&gt;\n    &lt;p&gt;fifth paragraph div1&lt;/p&gt;\n    &lt;p id=\"id_one\"&gt;sixth paragraph div1&lt;/p&gt;\n    &lt;p id=\"id_one\"&gt;seventh paragraph div1&lt;/p&gt;\n&lt;/div&gt;\n\n&lt;h1&gt;div2&lt;/h1&gt;\n&lt;div&gt;\n    &lt;p id=\"id_two\"&gt;first paragraph div2&lt;/p&gt;\n    &lt;p&gt;second paragraph div2&lt;/p&gt;\n    &lt;p id=\"id_one\"&gt;third paragraph div2&lt;/p&gt;\n    &lt;p&gt;forth paragraph div2&lt;/p&gt;\n    &lt;p&gt;fifth paragraph div2&lt;/p&gt;\n&lt;/div&gt;\n\nOverwriting style_application_files/after_element_selector_hierarchy.html\n\n\nSee result here.\nLooks like that style for id_two was defined later so this style is preferable than style defined for id_one."
  },
  {
    "objectID": "layout/css/padding.html",
    "href": "layout/css/padding.html",
    "title": "Padding (in space)",
    "section": "",
    "text": "By using this property you can set distance between element borders and it’s content. Use following syntax:\n\npadding-&lt;top/left/right/bottom&gt;: &lt;value&gt;;\npadding: &lt;value for all sides&gt;;\npadding: &lt;top and bottom&gt; &lt;left and right&gt;;\npadding: &lt;top&gt; &lt;left and right&gt; &lt;bottom&gt;;\npadding: &lt;top&gt; &lt;right&gt; &lt;bottom&gt; &lt;left&gt;.\n\nAvailable values:\n\npx, cm, em, % etc;\ninherit - take the same value as the parent;\n\n\n%%HTML\n&lt;div style=\"margin: 20px 40%; padding: 15px; border:solid\"&gt;\n    &lt;div style=\"padding: 50px;border:solid\"&gt; I have 50px padding&lt;/div&gt;\n    &lt;div style=\"padding: inherit;border:solid\"&gt; I have inherited padding&lt;/div&gt;\n    &lt;div style=\"padding: 30px 40%;border:solid\"&gt; I have 30px 40% padding&lt;/div&gt;\n&lt;/div&gt;\n\n\n     I have 50px padding\n     I have inherited padding\n     I have 30px 40% padding"
  },
  {
    "objectID": "layout/html/tables.html",
    "href": "layout/html/tables.html",
    "title": "Tables",
    "section": "",
    "text": "This page is about creatinga tables in html pages"
  },
  {
    "objectID": "layout/html/tables.html#basic-table-tr-td",
    "href": "layout/html/tables.html#basic-table-tr-td",
    "title": "Tables",
    "section": "Basic (<table>, <tr>, <td>)",
    "text": "Basic (&lt;table&gt;, &lt;tr&gt;, &lt;td&gt;)\n\n&lt;table&gt; - tag defines new table;\n&lt;tr&gt; - (table row) define the new row for the table;\n&lt;td&gt; - (table data) define the new cell.\n\n\n%%HTML\n&lt;table&gt;\n    &lt;tr&gt;\n        &lt;td&gt;row1 cell1&lt;/td&gt;\n        &lt;td&gt;row1 cell2&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n        &lt;td&gt;row2 cell1&lt;/td&gt;\n        &lt;td&gt;row2 cell2&lt;/td&gt;\n    &lt;/tr&gt;\n&lt;/table&gt;\n\n\n\n\nrow1 cell1\nrow1 cell2\n\n\nrow2 cell1\nrow2 cell2"
  },
  {
    "objectID": "layout/html/tables.html#table-header-th",
    "href": "layout/html/tables.html#table-header-th",
    "title": "Tables",
    "section": "Table header (<th>)",
    "text": "Table header (&lt;th&gt;)\nInstead of using &lt;td&gt; tag you can use &lt;th&gt; which will be interpreted as the table header, and printed as bold text.\n\n%%HTML\n&lt;table&gt;\n    &lt;tr&gt;\n        &lt;td&gt;row1 cell1&lt;/td&gt;\n        &lt;td&gt;row1 cell2&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n        &lt;th&gt;row2 cell1&lt;/th&gt;\n        &lt;th&gt;row2 cell2&lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n        &lt;td&gt;row2 cell1&lt;/td&gt;\n        &lt;td&gt;row2 cell2&lt;/td&gt;\n    &lt;/tr&gt;\n&lt;/table&gt;\n\n\n\n\nrow1 cell1\nrow1 cell2\n\n\nrow2 cell1\nrow2 cell2\n\n\nrow2 cell1\nrow2 cell2"
  },
  {
    "objectID": "layout/html/tables.html#caption",
    "href": "layout/html/tables.html#caption",
    "title": "Tables",
    "section": "Caption",
    "text": "Caption\nTag &lt;caption&gt; inside &lt;table&gt; will allow to show a captions of the tables. Interesting that you can put it anywhere - it prints at the top of the page anyway.\n\n%%HTML\n&lt;table&gt;\n    &lt;tr&gt;\n        &lt;td&gt;row1 cell1&lt;/td&gt;\n        &lt;td&gt;row1 cell2&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n        &lt;td&gt;row2 cell1&lt;/td&gt;\n        &lt;td&gt;row2 cell2&lt;/td&gt;\n    &lt;/tr&gt;\n    &lt;caption&gt; Test caption&lt;/caption&gt;\n    &lt;tr&gt;\n        &lt;td&gt;row2 cell1&lt;/td&gt;\n        &lt;td&gt;row2 cell2&lt;/td&gt;\n    &lt;/tr&gt;\n&lt;/table&gt;\n\n\n\n    \n        row1 cell1\n        row1 cell2\n    \n    \n        row2 cell1\n        row2 cell2\n    \n     Test caption\n    \n        row2 cell1\n        row2 cell2"
  },
  {
    "objectID": "layout/html/tables.html#fixed-table-head",
    "href": "layout/html/tables.html#fixed-table-head",
    "title": "Tables",
    "section": "Fixed table head",
    "text": "Fixed table head\nIt’s really convenient in case of long tables to be able to always see the header of the table. So in this subsection I want to discuss details about building tables with fidex headers and scrollable bodies.\n\nMinimal setup\nThe minimum you need to create a fixed header is to use position: sticky; top: 0; The css properties for the thead tag and the element it contains must be scrollable and limited in size. As in the example below.\n\n%%HTML\n&lt;div style=\"overflow-y: auto; height: 250px;\"&gt; \n    &lt;table&gt; \n      &lt;thead style=\"position: sticky; top: 0;\"&gt; \n        &lt;tr&gt; \n          &lt;th&gt;Col 1&lt;/th&gt; \n          &lt;th&gt;Col 2&lt;/th&gt; \n        &lt;/tr&gt; \n      &lt;/thead&gt; \n  \n      &lt;tbody&gt; \n        &lt;tr&gt;&lt;td&gt;1.1&lt;/td&gt;&lt;td&gt;1.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;2.1&lt;/td&gt;&lt;td&gt;2.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.1&lt;/td&gt;&lt;td&gt;3.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;4.1&lt;/td&gt;&lt;td&gt;4.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;5.1&lt;/td&gt;&lt;td&gt;5.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;6.1&lt;/td&gt;&lt;td&gt;6.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;7.1&lt;/td&gt;&lt;td&gt;7.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;8.1&lt;/td&gt;&lt;td&gt;8.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;9.1&lt;/td&gt;&lt;td&gt;9.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;10.1&lt;/td&gt;&lt;td&gt;10.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;11.1&lt;/td&gt;&lt;td&gt;11.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;12.1&lt;/td&gt;&lt;td&gt;12.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;13.1&lt;/td&gt;&lt;td&gt;13.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;14.1&lt;/td&gt;&lt;td&gt;14.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;15.1&lt;/td&gt;&lt;td&gt;15.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;16.1&lt;/td&gt;&lt;td&gt;16.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;17.1&lt;/td&gt;&lt;td&gt;17.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;18.1&lt;/td&gt;&lt;td&gt;18.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;19.1&lt;/td&gt;&lt;td&gt;19.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;20.1&lt;/td&gt;&lt;td&gt;20.2&lt;/td&gt;&lt;/tr&gt;\n      &lt;/tbody&gt; \n        \n    &lt;/table&gt; \n&lt;/div&gt;\n\n \n    \n\n\n\nCol 1\nCol 2\n\n\n\n\n1.1\n1.2\n\n\n2.1\n2.2\n\n\n3.1\n3.2\n\n\n4.1\n4.2\n\n\n5.1\n5.2\n\n\n6.1\n6.2\n\n\n7.1\n7.2\n\n\n8.1\n8.2\n\n\n9.1\n9.2\n\n\n10.1\n10.2\n\n\n11.1\n11.2\n\n\n12.1\n12.2\n\n\n13.1\n13.2\n\n\n14.1\n14.2\n\n\n15.1\n15.2\n\n\n16.1\n16.2\n\n\n17.1\n17.2\n\n\n18.1\n18.2\n\n\n19.1\n19.2\n\n\n20.1\n20.2\n\n\n\n \n\n\n\n\n\nBasic setup\nThere are some features that allow to make a scrollable table with fixed header nicer than minimal setup:\n\nbackground: for head tag - allow to hide table values under header.\nwidth: 100%; for the table tag, make the table fit the whole parent object, so the scrollbar will fit just as well as the side of the object.\n\n\n%%HTML\n&lt;div style=\"overflow-y: auto; height: 250px;\"&gt; \n    &lt;table style=\"width: 100%;\"&gt; \n      &lt;thead style=\"position: sticky; top: 0; background: #ABDD93;\"&gt; \n        &lt;tr&gt; \n          &lt;th&gt;Col 1&lt;/th&gt; \n          &lt;th&gt;Col 2&lt;/th&gt; \n        &lt;/tr&gt; \n      &lt;/thead&gt; \n  \n      &lt;tbody&gt; \n        &lt;tr&gt;&lt;td&gt;1.1&lt;/td&gt;&lt;td&gt;1.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;2.1&lt;/td&gt;&lt;td&gt;2.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;3.1&lt;/td&gt;&lt;td&gt;3.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;4.1&lt;/td&gt;&lt;td&gt;4.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;5.1&lt;/td&gt;&lt;td&gt;5.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;6.1&lt;/td&gt;&lt;td&gt;6.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;7.1&lt;/td&gt;&lt;td&gt;7.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;8.1&lt;/td&gt;&lt;td&gt;8.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;9.1&lt;/td&gt;&lt;td&gt;9.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;10.1&lt;/td&gt;&lt;td&gt;10.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;11.1&lt;/td&gt;&lt;td&gt;11.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;12.1&lt;/td&gt;&lt;td&gt;12.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;13.1&lt;/td&gt;&lt;td&gt;13.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;14.1&lt;/td&gt;&lt;td&gt;14.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;15.1&lt;/td&gt;&lt;td&gt;15.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;16.1&lt;/td&gt;&lt;td&gt;16.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;17.1&lt;/td&gt;&lt;td&gt;17.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;18.1&lt;/td&gt;&lt;td&gt;18.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;19.1&lt;/td&gt;&lt;td&gt;19.2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;20.1&lt;/td&gt;&lt;td&gt;20.2&lt;/td&gt;&lt;/tr&gt;\n      &lt;/tbody&gt; \n        \n    &lt;/table&gt; \n&lt;/div&gt;\n\n \n    \n\n\n\nCol 1\nCol 2\n\n\n\n\n1.1\n1.2\n\n\n2.1\n2.2\n\n\n3.1\n3.2\n\n\n4.1\n4.2\n\n\n5.1\n5.2\n\n\n6.1\n6.2\n\n\n7.1\n7.2\n\n\n8.1\n8.2\n\n\n9.1\n9.2\n\n\n10.1\n10.2\n\n\n11.1\n11.2\n\n\n12.1\n12.2\n\n\n13.1\n13.2\n\n\n14.1\n14.2\n\n\n15.1\n15.2\n\n\n16.1\n16.2\n\n\n17.1\n17.2\n\n\n18.1\n18.2\n\n\n19.1\n19.2\n\n\n20.1\n20.2"
  },
  {
    "objectID": "layout/html/other.html",
    "href": "layout/html/other.html",
    "title": "Other",
    "section": "",
    "text": "Here are features that I haven’t had time to put into some more specific pages yet.\nfrom IPython.display import HTML"
  },
  {
    "objectID": "layout/html/other.html#id---object-identificator",
    "href": "layout/html/other.html#id---object-identificator",
    "title": "Other",
    "section": "id - object identificator",
    "text": "id - object identificator"
  },
  {
    "objectID": "layout/html/other.html#style-set-a-style-of-element",
    "href": "layout/html/other.html#style-set-a-style-of-element",
    "title": "Other",
    "section": "style set a style of element",
    "text": "style set a style of element\nAllows to use integrated CSS for html elements.\n&lt;property1 name&gt;:&lt;setted value&gt;;&lt;property2 name&gt;:&lt;setted value&gt;;...;&lt;propertyn name&gt;:&lt;setted value&gt;\n\n%%HTML\n&lt;p style=\"color:blue\"&gt;Blue text color&lt;/p&gt;\n&lt;p style=\"background:red\"&gt;Red background color&lt;/p&gt;\n&lt;p style=\"background:red;color:blue\"&gt;Red background color&lt;/p&gt;\n\nBlue text color\nRed background color\nRed background color"
  },
  {
    "objectID": "layout/html/other.html#title---hint-text",
    "href": "layout/html/other.html#title---hint-text",
    "title": "Other",
    "section": "title - hint text",
    "text": "title - hint text\nIf you hold the mouse for some time on the element created by the tag with the title attribute - close to the mouse, a small hint appears. So try it wit text which is the result for the next cell.\n\n%%HTML\n&lt;p title=\"hello im title\"&gt;text with title&lt;/p&gt;\n\ntext with title"
  },
  {
    "objectID": "layout/html/other.html#p---paragraph",
    "href": "layout/html/other.html#p---paragraph",
    "title": "Other",
    "section": "<p> - paragraph",
    "text": "&lt;p&gt; - paragraph\nCreates a new paragraph.\n\n%%HTML\n&lt;p&gt;Paragraph 1&lt;/p&gt;&lt;p&gt;Paragraph 2&lt;/p&gt;&lt;p&gt;...&lt;/p&gt;&lt;p&gt;Paragraph n&lt;/p&gt;\n\nParagraph 1Paragraph 2...Paragraph n"
  },
  {
    "objectID": "layout/html/other.html#hr---thematic-break",
    "href": "layout/html/other.html#hr---thematic-break",
    "title": "Other",
    "section": "<hr> - thematic break",
    "text": "&lt;hr&gt; - thematic break\nIt will be displayed as horizontal line by page width.\n\n%%HTML\nText with sence 1\n&lt;hr&gt;\nText with sence 2\n\nText with sence 1\n\nText with sence 2"
  },
  {
    "objectID": "layout/html/other.html#span",
    "href": "layout/html/other.html#span",
    "title": "Other",
    "section": "<span>",
    "text": "&lt;span&gt;\nThis tag exists to assign different styles to the text, without getting any unintended changes set by the style.\nIn following example i use style with &lt;div&gt; tag and &lt;span&gt; tag:\n\n&lt;div&gt; tag sets the text in a separate paragraph;\n&lt;span&gt; tag just apply font style to text.\n\n\n%%HTML\n&lt;span style=\"color:red\"&gt;Text&lt;/span&gt; surrounded by snap.\n&lt;div style=\"color:red\"&gt;Text&lt;/div&gt; surrounded by div.    \n\nText surrounded by snap.\nText surrounded by div."
  },
  {
    "objectID": "layout/html/other.html#pre---preformatted-text",
    "href": "layout/html/other.html#pre---preformatted-text",
    "title": "Other",
    "section": "<pre> - preformatted text",
    "text": "&lt;pre&gt; - preformatted text\nThe &lt;pre&gt; tag in HTML is used to define preformatted text, which preserves both spaces and line breaks. It is commonly used to display code snippets or other types of text that require a fixed-width font and exact formatting.\nSo in the following example, аor the first two stanzas I do not use the &lt;pre&gt; tag for the remaining ones I use:\n\n%%HTML\nIn the realm of dreams we wander,\nWhere reality is torn asunder.\nWhispers of love and tales untold,\nA symphony of emotions, bold.\n\nStars dance upon a velvet sky,\nAs moonbeams paint a lullaby.\nNature's canvas, a masterpiece,\nWhere hearts find solace and peace.\n&lt;pre&gt;\n\nThrough winding paths we tread,\nSeeking answers that lie ahead.\nLife's tapestry, a vibrant hue,\nEach moment, a story to pursue.\n\nSo let us cherish this fleeting rhyme,\nEmbrace the beauty in every chime.\nFor in these words, a glimpse we find,\nOf the magic that dwells in mankind.\n&lt;/pre&gt;\n\nIn the realm of dreams we wander,\nWhere reality is torn asunder.\nWhispers of love and tales untold,\nA symphony of emotions, bold.\n\nStars dance upon a velvet sky,\nAs moonbeams paint a lullaby.\nNature's canvas, a masterpiece,\nWhere hearts find solace and peace.\nThrough winding paths we tread,\nSeeking answers that lie ahead.\nLife's tapestry, a vibrant hue,\nEach moment, a story to pursue.\n\nSo let us cherish this fleeting rhyme,\nEmbrace the beauty in every chime.\nFor in these words, a glimpse we find,\nOf the magic that dwells in mankind."
  },
  {
    "objectID": "layout/html/other.html#i---italic",
    "href": "layout/html/other.html#i---italic",
    "title": "Other",
    "section": "<i> - italic",
    "text": "&lt;i&gt; - italic\n\n%%HTML\n&lt;i&gt;Some emphasized text&lt;/i&gt;\n\nSome emphasized text"
  },
  {
    "objectID": "layout/html/other.html#em---important-italic",
    "href": "layout/html/other.html#em---important-italic",
    "title": "Other",
    "section": "<em> - important italic",
    "text": "&lt;em&gt; - important italic\nIt looks just like bold but It has special importance for search engines.\n\n%%HTML\n&lt;em&gt;Some emphasized text&lt;/em&gt;'\n\nSome emphasized text'"
  },
  {
    "objectID": "layout/html/other.html#dfn---tag-for-definitions",
    "href": "layout/html/other.html#dfn---tag-for-definitions",
    "title": "Other",
    "section": "<dfn> - tag for definitions",
    "text": "&lt;dfn&gt; - tag for definitions\nIf you want to define smt. you should use this tag. By default it is displayed in italics.\n\n%%HTML\n&lt;dfn&gt;Definition&lt;/dfn&gt;\n\nDefinition"
  },
  {
    "objectID": "layout/html/other.html#b---bold",
    "href": "layout/html/other.html#b---bold",
    "title": "Other",
    "section": "<b> - bold",
    "text": "&lt;b&gt; - bold\n\n%%HTML\n&lt;b&gt;Some emphasized text&lt;/b&gt;\n\nSome emphasized text"
  },
  {
    "objectID": "layout/html/other.html#strong---inportant-bold",
    "href": "layout/html/other.html#strong---inportant-bold",
    "title": "Other",
    "section": "<strong> - inportant bold",
    "text": "&lt;strong&gt; - inportant bold\nIt looks just like bold but It has special importance for search engines.\n\n%%HTML\n&lt;strong&gt;Some emphasized text&lt;/strong&gt;\n\nSome emphasized text"
  },
  {
    "objectID": "layout/html/other.html#small---smaller-text",
    "href": "layout/html/other.html#small---smaller-text",
    "title": "Other",
    "section": "<small> - smaller text",
    "text": "&lt;small&gt; - smaller text\nText surrounded by this tag will be one size smaller than the parent font.\n\n%%HTML\n&lt;p&gt;Parent font1 &lt;small&gt;small font&lt;/small&gt; parent font2&lt;/p&gt;\n\nParent font1 small font parent font2"
  },
  {
    "objectID": "layout/html/other.html#del---deleted-info",
    "href": "layout/html/other.html#del---deleted-info",
    "title": "Other",
    "section": "<del> - deleted info",
    "text": "&lt;del&gt; - deleted info\nThis tag is used to mark outdated information that should be deleted from the document. This will look like crossed out text.\n\n%%HTML\n&lt;del&gt; outdated information&lt;/del&gt;\n\n outdated information"
  },
  {
    "objectID": "layout/html/other.html#u---underlined-text",
    "href": "layout/html/other.html#u---underlined-text",
    "title": "Other",
    "section": "<u> - underlined text",
    "text": "&lt;u&gt; - underlined text\nText which just will be showen as underlined.\n\n%%HTML\n&lt;u&gt;Just underlined text&lt;/u&gt;\n\nJust underlined text"
  },
  {
    "objectID": "layout/html/other.html#ins---inserted-info",
    "href": "layout/html/other.html#ins---inserted-info",
    "title": "Other",
    "section": "<ins> - inserted info",
    "text": "&lt;ins&gt; - inserted info\nThis tag usually surrounds text that was inserted later in the page. It looks like underlined text.\n\n%%HTML\n&lt;ins&gt; new information&lt;/ins&gt;\n\n new information"
  },
  {
    "objectID": "layout/html/other.html#q---quote",
    "href": "layout/html/other.html#q---quote",
    "title": "Other",
    "section": "<q> - quote",
    "text": "&lt;q&gt; - quote\nYou should surround quotes with this tag. Browsers usually display them surrounded by \".\n\n%%HTML\n&lt;q&gt;Fedor Kobak truly ingenious&lt;/q&gt;\n\nFedor Kobak truly ingenious"
  },
  {
    "objectID": "layout/html/other.html#blockquote---multiline-quote",
    "href": "layout/html/other.html#blockquote---multiline-quote",
    "title": "Other",
    "section": "<blockquote> - multiline quote",
    "text": "&lt;blockquote&gt; - multiline quote\nYou should surround long quotations with this tag.\n\n%%HTML\n&lt;blockquote&gt;\nPeople think focus means saying yes to the thing you've got to focus on.&lt;br&gt; It means saying no to the hundred other good ideas that there are. You have to pick carefully.\n&lt;/blockquote&gt;\n\n\nPeople think focus means saying yes to the thing you've got to focus on. It means saying no to the hundred other good ideas that there are. You have to pick carefully."
  },
  {
    "objectID": "layout/html/other.html#img-add-image-to-html",
    "href": "layout/html/other.html#img-add-image-to-html",
    "title": "Other",
    "section": "<img> add image to html",
    "text": "&lt;img&gt; add image to html\n\nsrc - set picture\nYou can add url of the image or filepath on computer. In this argument you can use:\n\nurl;\nfilename on computer;\nbase64 - use the following syntax data:image/&lt;img type&gt;;base64;&lt;base64 code&gt;:\n\nget base64 from bash: base64 &lt;filename&gt;;\nget base64 from python: import base64; base64.b64encode(&lt;binary picture data&gt;).\n\n\n\n%%HTML\n&lt;img src=\"https://content.codecademy.com/courses/web-101/web101-image_brownbear.jpg\"/&gt;\n&lt;img src=\"html_files/display_picture.jpg\"&gt;\n&lt;img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAwAAAAMCAYAAABWdVznAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAA\nSklEQVQokc2QSQ4AIAgDW+L/v1wvmhDFhXiRG800kKEEITHmF3IGxsxmJHHhjwKfLF0VIpXLdwiY\nFPuPYKm9dCp1GABKD3clOS0V5DQSGUBEBc8AAAAASUVORK5CYII=\" width=200 height=200&gt;\n\n\n\n\n\n\n\n\nalt - text hint\nDescribes the text that will be used if the picture cannot be uploaded. There no option to show how it works in jupyter notebook, but the html page with the following code:\n\n%%bash\ncat html_files/img_alt_ex.html\n\n&lt;img src=\"unreal image\" alt=\"description of some image\"/&gt;\n\n\nYou can check here."
  },
  {
    "objectID": "layout/html/other.html#video---insert-video",
    "href": "layout/html/other.html#video---insert-video",
    "title": "Other",
    "section": "<video> - insert video",
    "text": "&lt;video&gt; - insert video\n\ncontrols handle video\nAttributes allow you to set controls for handling the video playback process. In the following example I have inserted the first video with the control attribute and the second without.\n\n%%HTML\n&lt;video src=\"html_files/some_video.webm\" widht=200 height=200 controls&gt;\n&lt;/video&gt;\n&lt;video src=\"html_files/some_video.webm\" widht=200 height=200&gt;\n&lt;/video&gt;"
  },
  {
    "objectID": "layout/html/other.html#div---division",
    "href": "layout/html/other.html#div---division",
    "title": "Other",
    "section": "<div> - division",
    "text": "&lt;div&gt; - division\nHelps to group html elements."
  },
  {
    "objectID": "layout/html/other.html#sec-style_tag",
    "href": "layout/html/other.html#sec-style_tag",
    "title": "Other",
    "section": "<style> - style for element group",
    "text": "&lt;style&gt; - style for element group\nBy useing this tag inside &lt;head&gt; tag you can set some css properties.\nIn the following example I set styles for all &lt;p&gt; and &lt;img&gt; tags in html page. You can check the results here. I can’t show in Jupyter notebook because the results are applied to the whole notebook.\n\nwith open(\"html_files/style_tag.html\") as file:\n    print(file.read())\n\n&lt;head&gt;\n    &lt;style&gt;\n        p {text-align:center; color: red}\n        img {border: 5px solid blue}\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;p&gt; Test paragraph1&lt;/p&gt;\n    &lt;p&gt; Test paragraph2&lt;/p&gt;\n    &lt;img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAwAAAAMCAYAAABWdVznAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAA\nSklEQVQokc2QSQ4AIAgDW+L/v1wvmhDFhXiRG800kKEEITHmF3IGxsxmJHHhjwKfLF0VIpXLdwiY\nFPuPYKm9dCp1GABKD3clOS0V5DQSGUBEBc8AAAAASUVORK5CYII=\"&gt;\n&lt;/body&gt;"
  },
  {
    "objectID": "layout/html/other.html#link",
    "href": "layout/html/other.html#link",
    "title": "Other",
    "section": "<link>",
    "text": "&lt;link&gt;\nUsed inside the &lt;head&gt; tag to load an external resource.\n\nhref - url\n\n\nrel - relationship\nDefine the relationship between html document and external resource.\n\nstylesheet - used for loading css file\nHere you can describe file that should be used as style table. In the following example I show html file and associated css file.\n\ndisplay(HTML(\n    '''\n    &lt;hr&gt;&lt;text style=\\\"font-size:16px\\\"&gt;HTML&lt;/text&gt;\n    '''\n))\n\nwith open(\"html_files/css_example.html\") as file:\n    print(file.read())\n\ndisplay(HTML(\n    '''\n    &lt;hr&gt;&lt;text style=\\\"font-size:16px\\\"&gt;CSS&lt;/text&gt;\n    '''\n))\n\nwith open(\"html_files/css_example.css\") as file:\n    print(file.read())\n\n\n    HTML\n    \n\n\n&lt;head&gt;\n    &lt;link href=\"css_example.css\" rel=\"stylesheet\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;p&gt;Paragraph 1&lt;/p&gt;\n    &lt;p&gt;Paragraph 2&lt;/p&gt;\n&lt;/body&gt;\np {\n  background-color: powderblue;\n  color: red\n}\n\n\n\n    CSS\n    \n\n\nYou can find the results here."
  },
  {
    "objectID": "layout/html/other.html#a---link",
    "href": "layout/html/other.html#a---link",
    "title": "Other",
    "section": "<a> - link",
    "text": "&lt;a&gt; - link\n\nhref - url\nurl to which reference is made.\n\n\ntarget - widow\nDefine how exactly html page should be opened:\n\n_self - use the same tab where the link was displayed;\n_blank - use new tab;\n_parent- ???;\n_top - ???.\n\nYou can try different options here.\n\n\ndownload\nTells the browser that the document must be downloaded and not opened in the browser. So in the following example, I have created a link that allows you to download the page from the previous example.\n\n%%HTML\n&lt;a href=\"html_files/a_target.html\" download&gt;Download a_target.html&lt;/a&gt;\n\nDownload a_target.html"
  },
  {
    "objectID": "layout/html/other.html#details---hiden-info",
    "href": "layout/html/other.html#details---hiden-info",
    "title": "Other",
    "section": "<details> - hiden info",
    "text": "&lt;details&gt; - hiden info\n\nBasics\nYou can describe some information that is hidden by default, but appears when you press the “arrow” button.\n\n%%HTML\n&lt;details&gt;\nSome hidden information.\n&lt;/details&gt;\n\n\nSome hidden information.\n\n\n\nBy using &lt;summary&gt; tag you can add extra information.\n\n%%HTML\n&lt;details&gt;\n    &lt;summary&gt;Details title&lt;button&gt;Test button&lt;/button&gt;&lt;/summary&gt;\n    Some hidden information\n&lt;/details&gt;\n\n\n    Details titleTest button\n    Some hidden information\n\n\n\n\n\nOverlapping\nBy default, expanded &lt;details&gt; will move the following content. So the following example shows how expanded details moves div.\n\n\n%%HTML\n&lt;details&gt;\n    line 0 &lt;br&gt;line 1 &lt;br&gt;line 2 &lt;br&gt;line 3 &lt;br&gt;line 4 &lt;br&gt;line 5 &lt;br&gt;line 6 &lt;br&gt;line 7 &lt;br&gt;line 8 &lt;br&gt;line 9 &lt;br&gt;\n&lt;/details&gt;\n&lt;details&gt;\n    line 0 &lt;br&gt;line 1 &lt;br&gt;line 2 &lt;br&gt;line 3 &lt;br&gt;line 4 &lt;br&gt;line 5 &lt;br&gt;line 6 &lt;br&gt;line 7 &lt;br&gt;line 8 &lt;br&gt;line 9 &lt;br&gt;\n&lt;/details&gt;\n&lt;div&gt;\n    Some other content\n&lt;/div&gt;\n\n\n    line 0 line 1 line 2 line 3 line 4 line 5 line 6 line 7 line 8 line 9 \n\n\n    line 0 line 1 line 2 line 3 line 4 line 5 line 6 line 7 line 8 line 9 \n\n\n    Some other content"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Knowledge",
    "section": "",
    "text": "Website provided by Fedor Kobak, with various studies in data science, computer science and related fields."
  },
  {
    "objectID": "jupyter/nbconvert/nbconvert_examples.html",
    "href": "jupyter/nbconvert/nbconvert_examples.html",
    "title": "Источники",
    "section": "",
    "text": "Как пользоваться nbconvert\nnbconvernt моежт быть использован как утилита командной строки или же python библиотека. Тут я буду рассматривать nbconvert как python библиотеку.\nimport io\n\nimport nbformat\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom traitlets.config import Config\n\nfrom nbconvert import HTMLExporter,\\\n                        RSTExporter"
  },
  {
    "objectID": "jupyter/nbconvert/nbconvert_examples.html#htmlexporter",
    "href": "jupyter/nbconvert/nbconvert_examples.html#htmlexporter",
    "title": "Источники",
    "section": "HTMLExporter",
    "text": "HTMLExporter\nПозволяет экспортировать файл в html.\n\nhtml_exporter = HTMLExporter(template_name='classic')\n(body, resources) = html_exporter.from_notebook_node(notebook)\n\nИ так возвращаяется два объекта\n\nbody\nПервый это тело html файла, его можно сразу сохранить и получается вполне себе html.\n\nfile = open(\"basic_html_exporter.html\", \"w+\")\nfile.write(body)\nfile.close()\n\nНо при спользовании такого подхода есть проблема - картинки, формируемые как часть markdown разметки не будут встроены в .html, а будут лишь ссылками на файлы.\n\n\nresources\nСписок ресурсов используемых для формирования jupyter.\n\nresources\n\nResourcesDict(None,\n              {'metadata': ResourcesDict(None, {'name': 'Notebook'}),\n               'output_extension': '.html',\n               'deprecated': &lt;function nbconvert.exporters.templateexporter.deprecated(msg)&gt;,\n               'theme': 'light',\n               'include_css': &lt;function nbconvert.exporters.html.HTMLExporter._init_resources.&lt;locals&gt;.resources_include_css(name)&gt;,\n               'include_lab_theme': &lt;function nbconvert.exporters.html.HTMLExporter._init_resources.&lt;locals&gt;.resources_include_lab_theme(name)&gt;,\n               'include_js': &lt;function nbconvert.exporters.html.HTMLExporter._init_resources.&lt;locals&gt;.resources_include_js(name)&gt;,\n               'include_url': &lt;function nbconvert.exporters.html.HTMLExporter._init_resources.&lt;locals&gt;.resources_include_url(name)&gt;,\n               'require_js_url': 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js',\n               'mathjax_url': 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe',\n               'jquery_url': 'https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js',\n               'jupyter_widgets_base_url': 'https://unpkg.com/',\n               'widget_renderer_url': '',\n               'html_manager_semver_range': '*',\n               'inlining': {'css': ['pre { line-height: 125%; }\\ntd.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\\nspan.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\\ntd.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\\nspan.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\\n.highlight .hll { background-color: #ffffcc }\\n.highlight { background: #f8f8f8; }\\n.highlight .c { color: #3D7B7B; font-style: italic } /* Comment */\\n.highlight .err { border: 1px solid #FF0000 } /* Error */\\n.highlight .k { color: #008000; font-weight: bold } /* Keyword */\\n.highlight .o { color: #666666 } /* Operator */\\n.highlight .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\\n.highlight .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\\n.highlight .cp { color: #9C6500 } /* Comment.Preproc */\\n.highlight .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\\n.highlight .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\\n.highlight .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\\n.highlight .gd { color: #A00000 } /* Generic.Deleted */\\n.highlight .ge { font-style: italic } /* Generic.Emph */\\n.highlight .gr { color: #E40000 } /* Generic.Error */\\n.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */\\n.highlight .gi { color: #008400 } /* Generic.Inserted */\\n.highlight .go { color: #717171 } /* Generic.Output */\\n.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\\n.highlight .gs { font-weight: bold } /* Generic.Strong */\\n.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\\n.highlight .gt { color: #0044DD } /* Generic.Traceback */\\n.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\\n.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\\n.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\\n.highlight .kp { color: #008000 } /* Keyword.Pseudo */\\n.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\\n.highlight .kt { color: #B00040 } /* Keyword.Type */\\n.highlight .m { color: #666666 } /* Literal.Number */\\n.highlight .s { color: #BA2121 } /* Literal.String */\\n.highlight .na { color: #687822 } /* Name.Attribute */\\n.highlight .nb { color: #008000 } /* Name.Builtin */\\n.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */\\n.highlight .no { color: #880000 } /* Name.Constant */\\n.highlight .nd { color: #AA22FF } /* Name.Decorator */\\n.highlight .ni { color: #717171; font-weight: bold } /* Name.Entity */\\n.highlight .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\\n.highlight .nf { color: #0000FF } /* Name.Function */\\n.highlight .nl { color: #767600 } /* Name.Label */\\n.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\\n.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */\\n.highlight .nv { color: #19177C } /* Name.Variable */\\n.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\\n.highlight .w { color: #bbbbbb } /* Text.Whitespace */\\n.highlight .mb { color: #666666 } /* Literal.Number.Bin */\\n.highlight .mf { color: #666666 } /* Literal.Number.Float */\\n.highlight .mh { color: #666666 } /* Literal.Number.Hex */\\n.highlight .mi { color: #666666 } /* Literal.Number.Integer */\\n.highlight .mo { color: #666666 } /* Literal.Number.Oct */\\n.highlight .sa { color: #BA2121 } /* Literal.String.Affix */\\n.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */\\n.highlight .sc { color: #BA2121 } /* Literal.String.Char */\\n.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */\\n.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\\n.highlight .s2 { color: #BA2121 } /* Literal.String.Double */\\n.highlight .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\\n.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */\\n.highlight .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\\n.highlight .sx { color: #008000 } /* Literal.String.Other */\\n.highlight .sr { color: #A45A77 } /* Literal.String.Regex */\\n.highlight .s1 { color: #BA2121 } /* Literal.String.Single */\\n.highlight .ss { color: #19177C } /* Literal.String.Symbol */\\n.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */\\n.highlight .fm { color: #0000FF } /* Name.Function.Magic */\\n.highlight .vc { color: #19177C } /* Name.Variable.Class */\\n.highlight .vg { color: #19177C } /* Name.Variable.Global */\\n.highlight .vi { color: #19177C } /* Name.Variable.Instance */\\n.highlight .vm { color: #19177C } /* Name.Variable.Magic */\\n.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */']},\n               'raw_mimetypes': ['text/html', ''],\n               'global_content_filter': {'include_code': True,\n                'include_markdown': True,\n                'include_raw': True,\n                'include_unknown': True,\n                'include_input': True,\n                'include_output': True,\n                'include_output_stdin': False,\n                'include_input_prompt': True,\n                'include_output_prompt': True,\n                'no_prompt': False}})"
  },
  {
    "objectID": "jupyter/nbconvert/nbconvert_examples.html#rstexporter",
    "href": "jupyter/nbconvert/nbconvert_examples.html#rstexporter",
    "title": "Источники",
    "section": "RSTExporter",
    "text": "RSTExporter\nПозволяет диаграммы на графике экспортировать не в .html файл а в ресурсы.\nСпособ использования ничем не отличатеся от рассмотренного выше HTMLExporter.\n\nrst_exporter = RSTExporter()\n(body, resources) = rst_exporter.from_notebook_node(notebook)\n\nНо полученный body содержит только текст.\n\nfile = open(\"rst_exporter.html\", \"w+\")\nfile.write(body)\nfile.close()\n\nНекоторые дополнительные элементы сохранены в resources. Так, например, картинки-результаты выполнения ячеек будут лежать resources[\"outputs\"]. Далее пример их извлечения:\n\noutputs = resources[\"outputs\"]\noutputs_len = len(outputs)\n\nplt.figure(figsize = [5,20])\nfor i, pic_name in enumerate(outputs.keys()):\n    plt.subplot(outputs_len, 1, i+1)\n    plt.xticks([]); plt.yticks([])\n    plt.imshow(\n        plt.imread(\n            io.BytesIO(outputs[pic_name]), format='jpeg'\n        )\n    )\n\n\n\n\nГде были сохранены стили, и почему слетают способы формирования ячеек не понятно."
  },
  {
    "objectID": "sql/load_tables.html",
    "href": "sql/load_tables.html",
    "title": "Load tables",
    "section": "",
    "text": "This page is about how to read information in postgres database.\n\nCSV\nIn this section I will show you how to load information from a csv file. Main source of information was this page.\nLong story short:\n\nCreate table;\nRead “*.csv” file using COPY sql command.\n\nThe following example just does all of that stuff.\n\n%%bash\n# creating a container and comming to psql command line\ndocker run -d --rm\\\n    --name csv_loading_postgres\\\n    -e POSTGRES_PASSWORD=docker_app\\\n    -v ./load_tables/CSV/iris_csv.csv:/iris_csv.csv\\\n    postgres:15.4 &&gt; /dev/null\nsleep 5\ndocker exec -i csv_loading_postgres psql -U postgres -d postgres\n\n-- creating table\nCREATE TABLE main_table(\n    sepallength REAL,\n    sepalwidth REAL,\n    petallength REAL,\n    petalwidth REAL,\n    class VARCHAR(20)\n);\n-- loading csv table to created table\nCOPY main_table(sepallength, sepalwidth, petallength, petalwidth,class)\nFROM '/iris_csv.csv'\nDELIMITER ','\nCSV HEADER;\n\n-- showing results\nSELECT * FROM main_table LIMIT 10;\n\nCREATE TABLE\nCOPY 150\n sepallength | sepalwidth | petallength | petalwidth |    class    \n-------------+------------+-------------+------------+-------------\n         5.1 |        3.5 |         1.4 |        0.2 | Iris-setosa\n         4.9 |          3 |         1.4 |        0.2 | Iris-setosa\n         4.7 |        3.2 |         1.3 |        0.2 | Iris-setosa\n         4.6 |        3.1 |         1.5 |        0.2 | Iris-setosa\n           5 |        3.6 |         1.4 |        0.2 | Iris-setosa\n         5.4 |        3.9 |         1.7 |        0.4 | Iris-setosa\n         4.6 |        3.4 |         1.4 |        0.3 | Iris-setosa\n           5 |        3.4 |         1.5 |        0.2 | Iris-setosa\n         4.4 |        2.9 |         1.4 |        0.2 | Iris-setosa\n         4.9 |        3.1 |         1.5 |        0.1 | Iris-setosa\n(10 rows)\n\n\n\n\n!docker stop csv_loading_postgres &&gt; /dev/null"
  },
  {
    "objectID": "sql/python_interaction/python_from_container.html",
    "href": "sql/python_interaction/python_from_container.html",
    "title": "Python from container",
    "section": "",
    "text": "For me it’s common task to build interaption with container which handle python program.\nSo in this section I want to show you how to build such an interaction."
  },
  {
    "objectID": "sql/python_interaction/python_from_container.html#python-image",
    "href": "sql/python_interaction/python_from_container.html#python-image",
    "title": "Python from container",
    "section": "Python image",
    "text": "Python image\nYou need to create a Python image. The following cell describes the docker file that’s used to create the image.\n\n%%writefile python_container/dockerfile\nFROM python:3.11.4\nWORKDIR program\nCOPY script.py script.py\nCOPY requirements.txt requirements.txt\nRUN pip3 install -r requirements.txt\n\nOverwriting python_container/dockerfile\n\n\nThe next cell describes the requirements.txt we’ll need for this container.\n\n%%writefile python_container/requirements.txt\nnumpy==1.24.2\npsycopg2-binary==2.9.6\n\nOverwriting python_container/requirements.txt\n\n\nFinally, we build the image.\n\n%%bash\ndocker build -t pg_example_python python_container &&gt; /dev/null"
  },
  {
    "objectID": "sql/python_interaction/python_from_container.html#start-containers",
    "href": "sql/python_interaction/python_from_container.html#start-containers",
    "title": "Python from container",
    "section": "Start containers",
    "text": "Start containers\nWe need to create at least one table in our postgres database. So in the following cell we store the script that will be used to initialise the database.\n\n%%writefile python_container/create_table.sql\nCREATE TABLE main_table(\n    id TEXT NOT NULL,\n    text TEXT NOT NULL\n);\n\nWriting python_container/create_table.sql\n\n\nNext, start the python and postgres containers.\nYou should share the same net between containers, in the following example which is completed by test_project_net.\n\n%%bash\n# network\ndocker network create test_project_net &&gt; /dev/null\n# postgres\ndocker run --rm -d\\\n    --name pg_example_posgres_cont\\\n    -e POSTGRES_USER=docker_app\\\n    -e POSTGRES_PASSWORD=docker_app\\\n    -e POSTGRES_DB=docker_app_db\\\n    --net=test_project_net\\\n    -v ./python_container/create_table.sql:/docker-entrypoint-initdb.d/create_table.sql\\\n    postgres:15.4 &&gt; /dev/null\n# python\ndocker run --rm -itd\\\n    --name pg_example_python_cont\\\n    --net=test_project_net\\\n    pg_example_python &&gt; /dev/null"
  },
  {
    "objectID": "sql/python_interaction/python_from_container.html#executing-python",
    "href": "sql/python_interaction/python_from_container.html#executing-python",
    "title": "Python from container",
    "section": "Executing python",
    "text": "Executing python\nIn the next cell, we run the Python script in the Python container. The script simply adds some random values to the main_table from the ohter container. The message “Adding records is done!” indicates that the Python program was executed normally.\n\n%%bash\ndocker exec pg_example_python_cont bash -c \"python3 script.py\"\n\nAdding records is done!"
  },
  {
    "objectID": "sql/python_interaction/python_from_container.html#check-the-table",
    "href": "sql/python_interaction/python_from_container.html#check-the-table",
    "title": "Python from container",
    "section": "Check the table",
    "text": "Check the table\nSo now, to make sure we have done everything right, let us select values from the created table.\n\n%%bash\ndocker exec pg_example_posgres_cont bash -c \\\n\"psql -U docker_app -d docker_app_db -c \\\"SELECT * FROM main_table;\\\"\"\n\n id |         text         \n----+----------------------\n 0  | rslmljobfthxylvqmxjx\n 1  | bfrtjjwbrmvmezusqqbt\n 2  | opujcieiufdicgkkerup\n 3  | lnuczeqsgxuqxeqfklva\n 4  | sgmbhqliwlidpgpuguyh\n 5  | cmotqncbzehqgovnmain\n 6  | xnrkwzjcnwsfqzmgnnol\n 7  | hjiihyoenlmxeydsdcnc\n 8  | bdrsfcxsbwzxieiityah\n 9  | dozhyfgltysgubaekvca\n 10 | yyjtebgjeiryoozwnliq\n 11 | httaabixdruuncanpkdx\n 12 | fvilvakiasqmkpmoxxze\n 13 | sitdcxqjzdmcrefldcjc\n 14 | jlqyxrhsgxenzmmihtsc\n 15 | rumlutiupquvfucxfmgn\n 16 | arsdyzmlxfruodumuupk\n 17 | jzjoyadntdjbcfucgsku\n 18 | xiqpdchofpxsksjlmvwv\n 19 | wuqfxhniyrusexnjrcin\n(20 rows)"
  },
  {
    "objectID": "sql/python_interaction/python_from_container.html#stop-containers",
    "href": "sql/python_interaction/python_from_container.html#stop-containers",
    "title": "Python from container",
    "section": "Stop containers",
    "text": "Stop containers\n\n%%bash\ndocker stop pg_example_posgres_cont pg_example_python_cont &&gt; /dev/null\ndocker network rm test_project_net &&gt; /dev/null"
  },
  {
    "objectID": "sql/python_interaction/python_from_container.html#clear-python-image",
    "href": "sql/python_interaction/python_from_container.html#clear-python-image",
    "title": "Python from container",
    "section": "Clear python image",
    "text": "Clear python image\nTo avoid creating a lot of rubbish in docker images, you should delete the container created for this example.\n\n%%bash\ndocker rmi pg_example_python &&gt; /dev/null"
  },
  {
    "objectID": "sql/python_interaction/python_on_host.html",
    "href": "sql/python_interaction/python_on_host.html",
    "title": "Python on host",
    "section": "",
    "text": "Sometimes it’s good to run database-related code from your local machine. So I show a way to build interaption between the database and the host program."
  },
  {
    "objectID": "sql/python_interaction/python_on_host.html#database-container",
    "href": "sql/python_interaction/python_on_host.html#database-container",
    "title": "Python on host",
    "section": "Database container",
    "text": "Database container\nScript that is used ot create table in database described in the following cell:\n\n%%writefile python_on_host/create_table.sql\nCREATE TABLE main_table(\n    id TEXT NOT NULL,\n    text TEXT NOT NULL\n);\n\nWriting python_on_host/create_table.sql\n\n\nNote you need to specify the port to be referred to later in the Python program.\n\n%%bash\ndocker run --rm -d\\\n    --name pg_example_posgres_cont\\\n    -e POSTGRES_USER=docker_app\\\n    -e POSTGRES_PASSWORD=docker_app\\\n    -e POSTGRES_DB=docker_app_db\\\n    -p 5431:5432\\\n    -v ./python_on_host/create_table.sql:/docker-entrypoint-initdb.d/create_table.sql\\\n    postgres:15.4 &&gt; /dev/null"
  },
  {
    "objectID": "sql/python_interaction/python_on_host.html#python-program",
    "href": "sql/python_interaction/python_on_host.html#python-program",
    "title": "Python on host",
    "section": "Python program",
    "text": "Python program\nI’m just going to connect to the database from this notebook.\n\nEstablish conneciton\nIn the psycopg2.connect function, mention port used in postgres container creation and localhost argument for host parameter.\n\nimport psycopg2\n\nconn = psycopg2.connect(\n    port = \"5431\", # same as when creating a postgres container\n    dbname = \"docker_app_db\",\n    user = \"docker_app\",\n    password = \"docker_app\",\n    host= \"localhost\"\n)"
  },
  {
    "objectID": "sql/python_interaction/python_on_host.html#insert-information",
    "href": "sql/python_interaction/python_on_host.html#insert-information",
    "title": "Python on host",
    "section": "Insert information",
    "text": "Insert information\nTo understand that everything works insert a few lines into database.\n\nimport random\nimport string\n\ncur = conn.cursor()\nfor i in range(20):\n    text = ''.join(random.choices(string.ascii_lowercase, k=20))\n    query = f\"INSERT INTO main_table (id, text) VALUES ('{i}', '{text}');\"\n    cur.execute(query)\ncur.close()"
  },
  {
    "objectID": "sql/python_interaction/python_on_host.html#check-the-result",
    "href": "sql/python_interaction/python_on_host.html#check-the-result",
    "title": "Python on host",
    "section": "Check the result",
    "text": "Check the result\nQuery from python.\n\ncur = conn.cursor()\ncur.execute(\"SELECT * FROM main_table;\")\nfor i in cur:\n    print(i)\ncur.close()\n\n('0', 'oylvndzcdwjbvaqhcipe')\n('1', 'uhgzsoakjwsaekpjvqqm')\n('2', 'ynelbigrcuhgipfwdwsi')\n('3', 'jokojjrucprmxopvturd')\n('4', 'rkdlrxbhhlwymqfkxpft')\n('5', 'sukhsdqtfthqedbjyztn')\n('6', 'taihpxcxtwjmupdjxidl')\n('7', 'uvuyltiriwusqnzsbema')\n('8', 'tdxphsmxkjmhhxfjgbcb')\n('9', 'tecdszaaicciqlppjckh')\n('10', 'tfqssrggcfqkaebjaitx')\n('11', 'ljrreafprdqxrlmrcqaz')\n('12', 'mfpaqefutrthoimtcxwu')\n('13', 'ewynczxdhguwahyrcrjn')\n('14', 'pzhymsodljpaoykckyqe')\n('15', 'odewjbtonkbzqmvyzauu')\n('16', 'zrkmmbidnlzppyrhtqjq')\n('17', 'cnczkqtpznuagodepkwi')\n('18', 'waspsclkkazunxvecfyf')\n('19', 'zsxzovelsmduduneufkz')\n\n\nQuery from container.\nNote Before quering from the container, you need to commit the changes from the connection.\n\nconn.commit()\n\n\n%%bash\ndocker exec pg_example_posgres_cont \\\n    psql --username docker_app --dbname docker_app_db -c 'SELECT * FROM main_table;'\n\n id |         text         \n----+----------------------\n 0  | oylvndzcdwjbvaqhcipe\n 1  | uhgzsoakjwsaekpjvqqm\n 2  | ynelbigrcuhgipfwdwsi\n 3  | jokojjrucprmxopvturd\n 4  | rkdlrxbhhlwymqfkxpft\n 5  | sukhsdqtfthqedbjyztn\n 6  | taihpxcxtwjmupdjxidl\n 7  | uvuyltiriwusqnzsbema\n 8  | tdxphsmxkjmhhxfjgbcb\n 9  | tecdszaaicciqlppjckh\n 10 | tfqssrggcfqkaebjaitx\n 11 | ljrreafprdqxrlmrcqaz\n 12 | mfpaqefutrthoimtcxwu\n 13 | ewynczxdhguwahyrcrjn\n 14 | pzhymsodljpaoykckyqe\n 15 | odewjbtonkbzqmvyzauu\n 16 | zrkmmbidnlzppyrhtqjq\n 17 | cnczkqtpznuagodepkwi\n 18 | waspsclkkazunxvecfyf\n 19 | zsxzovelsmduduneufkz\n(20 rows)\n\n\n\nClose connection.\n\nconn.close()"
  },
  {
    "objectID": "sql/python_interaction/python_on_host.html#stop-container",
    "href": "sql/python_interaction/python_on_host.html#stop-container",
    "title": "Python on host",
    "section": "Stop container",
    "text": "Stop container\n\n%%bash\ndocker stop pg_example_posgres_cont\n\npg_example_posgres_cont"
  },
  {
    "objectID": "sql/datetime/add_period.html",
    "href": "sql/datetime/add_period.html",
    "title": "Add period",
    "section": "",
    "text": "If you have columns of type datetime. Sometimes you need to add multiple elements of typical period types such as days, months, years, etc.\nIt is interesting to note that different DBMS have different schemes for this. I will describe some of them here."
  },
  {
    "objectID": "sql/datetime/add_period.html#postgres-interval",
    "href": "sql/datetime/add_period.html#postgres-interval",
    "title": "Add period",
    "section": "Postgres (INTERVAL)",
    "text": "Postgres (INTERVAL)\nTo perfrom such operation in postgres use syntax &lt;date&gt; INTERVAL '&lt;number&gt;' &lt;date unit&gt;.\nThe following cell creates a container with a database and a table with a DATE column in it.\n\n%%bash\ndocker run --rm -d\\\n    -e POSTGRES_PASSWORD=postgres \\\n    --name interval_example \\\n    postgres:15.4 &&gt; /dev/null\nsleep 5\ndocker exec -i interval_example psql -U postgres -d postgres\n\nCREATE TABLE tab(\n    var1 DATE\n);\nINSERT INTO tab (var1) VALUES\n('2022-12-20'),\n('2021-10-15'),\n('2018-06-15');\n\nSELECT * FROM tab;\n\nCREATE TABLE\nINSERT 0 3\n    var1    \n------------\n 2022-12-20\n 2021-10-15\n 2018-06-15\n(3 rows)\n\n\n\nHere is a query that shows the original column and the column with the added interval.\n\n%%bash\ndocker exec -i interval_example psql -U postgres -d postgres\nSELECT var1, (var1  + INTERVAL '10' MONTH) add_monthes FROM tab;\n\n    var1    |     add_monthes     \n------------+---------------------\n 2022-12-20 | 2023-10-20 00:00:00\n 2021-10-15 | 2022-08-15 00:00:00\n 2018-06-15 | 2019-04-15 00:00:00\n(3 rows)\n\n\n\nDon’t forget to stop the container when you have finished playing with the examples.\n\n%%bash\ndocker stop interval_example"
  },
  {
    "objectID": "sql/datetime/add_period.html#sqlite-date",
    "href": "sql/datetime/add_period.html#sqlite-date",
    "title": "Add period",
    "section": "SQLite (DATE)",
    "text": "SQLite (DATE)\nTo perform such an operation in SQLite, use the syntax DATE(&lt;date&gt;, \"+N &lt;unit&gt;\").\nIn the following cell, a litesql database is created with a table containing datetime columns.\n\n%%bash\nsqlite3 add_period_files/dump.sql\n\nCREATE TABLE IF NOT EXISTS tab (\n    var1 DATE\n);\nDELETE FROM tab;\nINSERT INTO tab (var1) VALUES\n('2022-12-20'),\n('2021-10-15'),\n('2018-06-15');\n\nHere is an example of using the DATE function to add some units to the date column.\n\n%%bash\nsqlite3 add_period_files/dump.sql\n\nSELECT var1, DATE(var1, \"+10 month\")  FROM tab;\n\n2022-12-20|2023-10-20\n2021-10-15|2022-08-15\n2018-06-15|2019-04-15"
  },
  {
    "objectID": "sql/select/fill_empty.html",
    "href": "sql/select/fill_empty.html",
    "title": "Fill empty (COALESCE/IFNULL)",
    "section": "",
    "text": "Sometimes tables contain empty values. But in some cases it’s convenient to replace such values with a specific value. So in this section I’ll show you some features of databases that can help you to do this.\nThe next cell creates a docker container with postgres, which will be used for the experiments on this page.\n%%bash\ndocker run --rm -d\\\n    -e POSTGRES_PASSWORD=postgres \\\n    --name fill_empty \\\n    postgres:15.4 &&gt; /dev/null\nsleep 5\ndocker exec -i fill_empty psql -U postgres -d postgres\n\nCREATE TABLE tab(\n    var1 TEXT,\n    var2 TEXT\n);\nINSERT INTO tab (var1, var2) VALUES\n(NULL, 'world'),\n('hello', NULL),\n(NULL, 'test line');\n\nCREATE TABLE\nINSERT 0 3\nInitial state of the table used for the experiment.\n%%bash\ndocker exec -i fill_empty psql -U postgres -d postgres\nSELECT * FROM tab;\n\n var1  |   var2    \n-------+-----------\n       | world\n hello | \n       | test line\n(3 rows)\nNote Don’t forget to close the container when you have finished playing with the examples.\n!docker stop fill_empty\n\nfill_empty"
  },
  {
    "objectID": "sql/select/fill_empty.html#in-query",
    "href": "sql/select/fill_empty.html#in-query",
    "title": "Fill empty (COALESCE/IFNULL)",
    "section": "In query",
    "text": "In query\nIt’s a very common case when empty values have business sense, but in query results you need to replace them. So you can write SELECT to replace NULL in query results, this section is focused on it.\n\nPostgres (COALESCE)\nIn postgres you have to use the COALENCE function. Use the syntax COALESCE(&lt;column name&gt;, &lt;value to replace&gt;). So in the following cell such syntax is used to replace empty values.\n\n%%bash\ndocker exec -i fill_empty psql -U postgres -d postgres\n\nSELECT \n    COALESCE(var1, 'NAN text') var1,\n    COALESCE(var2, 'NAN text') var2\nFROM tab;\n\n   var1   |   var2    \n----------+-----------\n NAN text | world\n hello    | NAN text\n NAN text | test line\n(3 rows)\n\n\n\n\n\nAlternative (IFNULL)\nIFNULL is an alternative function that can be used to replace empty values in query statements.\nIt’s not supported by postgres. So to make sure, I’ll show you in the following cell that there will be an error if you try to use it in postgres.\n\n%%bash\ndocker exec -i fill_empty psql -U postgres -d postgres\nSELECT \n    IFNULL(var1, 'NAN text') var1,\n    IFNULL(var2, 'NAN text') var2\nFROM tab;\n\nERROR:  function ifnull(text, unknown) does not exist\nLINE 2:     IFNULL(var1, 'NAN text') var1,\n            ^\nHINT:  No function matches the given name and argument types. You might need to add explicit type casts.\n\n\nSo I will use sqlite base to demonstrate how IFNULL works. In the following example is the process of creating sample database.\n\n%%bash\nsqlite3 fill_empty_files/dump.sql\n\nCREATE TABLE IF NOT EXISTS tab (\n    var1 TEXT,\n    var2 TEXT\n);\nDELETE FROM tab;\nINSERT INTO tab (var1, var2) VALUES\n(NULL, 'world'),\n('hello', NULL),\n(NULL, 'test line');\n\nHere is what the query to the example database looks like, without any of the tricks involved in replacing values.\n\n%%bash\nsqlite3 fill_empty_files/dump.sql\n\nSELECT * FROM tab;\n\n|world\nhello|\n|test line\n\n\nThe syntax is exactly the same as for COALESCE: IFNULL(&lt;column name&gt;, &lt;value to replace&gt;).\nActually, sqlite supports both COALESCE and IFNULL, so in the following example both have been used to replace empty values in different columns.\n\n%%bash\nsqlite3 fill_empty_files/dump.sql\n\nSELECT \n    IFNULL(var1, 'replaced by IFNULL') var1,\n    COALESCE(var2, 'relaced by COALESCE') var2\nFROM tab;\n\nreplaced by IFNULL|world\nhello|relaced by COALESCE\nreplaced by IFNULL|test line"
  },
  {
    "objectID": "sql/select/fill_empty.html#change-base",
    "href": "sql/select/fill_empty.html#change-base",
    "title": "Fill empty (COALESCE/IFNULL)",
    "section": "Change base",
    "text": "Change base\nSometimes empty values in database appear as errors or business sense of data table can be selected. So you can face with case when you need to replace all values in the database, not only in the results of the specific query.\nFor such purpose you can use syntax:\nUPDATE &lt;table_name&gt;\nSET &lt;column name&gt; = &lt;replace value&gt;\nWHERE &lt;column name&gt; IS NULL;\nThe following cell uses such syntax to replace some empty values.\n\n%%bash\ndocker exec -i fill_empty psql -U postgres -d postgres\n\nUPDATE tab\nSET var1 = 'replaced'\nWHERE var1 IS NULL;\n\nUPDATE 2\n\n\nLet’s see the result of the transformation.\n\n%%bash\ndocker exec -i fill_empty psql -U postgres -d postgres\nSELECT * FROM tab;\n\n   var1   |   var2    \n----------+-----------\n hello    | \n replaced | world\n replaced | test line\n(3 rows)"
  },
  {
    "objectID": "sql/select/expand_array.html",
    "href": "sql/select/expand_array.html",
    "title": "Expand array (UNNEST)",
    "section": "",
    "text": "Sometimes you need to deal with arrays of arrays or, more scientifically, a column containing arrays in each record.\nSo you can easily perform this operation using the UNNEST(&lt;column&gt;) syntax after the SELECT keyword."
  },
  {
    "objectID": "sql/select/expand_array.html#using-example",
    "href": "sql/select/expand_array.html#using-example",
    "title": "Expand array (UNNEST)",
    "section": "Using example",
    "text": "Using example\nSo in the following cell I’ll create an example database that will be used in the cells below to show the properties of the keyword UNNEST.\n\n%%bash\ndocker run --rm -d\\\n    -e POSTGRES_PASSWORD=postgres \\\n    --name unnest_example_postgres \\\n    postgres:15.4 &&gt; /dev/null\nsleep 5\ndocker exec -i unnest_example_postgres psql -U postgres -d postgres\n\nCREATE TABLE example_table(\n    col1 TEXT,\n    col2 INT[],\n    col3 TEXT[]\n);\n\nINSERT INTO example_table(col1, col2, col3) VALUES\n('A', '{1,2,3}', '{\"hello\", \"fedor\"}'),\n('B', '{3,2}', '{\"test\"}'),\n('C', '{3}', '{\"value, with, commas\", \"value with spaces\"}'),\n('D', '{3,4,5}', '{\"final_element\"}');\n\nSELECT * FROM example_table;\n\nCREATE TABLE\nINSERT 0 4\n col1 |  col2   |                    col3                     \n------+---------+---------------------------------------------\n A    | {1,2,3} | {hello,fedor}\n B    | {3,2}   | {test}\n C    | {3}     | {\"value, with, commas\",\"value with spaces\"}\n D    | {3,4,5} | {final_element}\n(4 rows)\n\n\n\nNote don’t forget to stop the container when you’ve finished playing with it.\n\n!docker stop unnest_example_postgres\n\nunnest_example_postgres"
  },
  {
    "objectID": "sql/select/expand_array.html#basic-example",
    "href": "sql/select/expand_array.html#basic-example",
    "title": "Expand array (UNNEST)",
    "section": "Basic example",
    "text": "Basic example\nSo here I just using unnest for some for some arbitrary column. The values of other columns will be duplicated so that each element of the unnested array corresponds to an element that corresponded to the original record.\n\n%%bash\ndocker exec -i unnest_example_postgres psql -U postgres -d postgres\n\nSELECT col1, UNNEST(col2), col3 FROM example_table;\n\n col1 | unnest |                    col3                     \n------+--------+---------------------------------------------\n A    |      1 | {hello,fedor}\n A    |      2 | {hello,fedor}\n A    |      3 | {hello,fedor}\n B    |      3 | {test}\n B    |      2 | {test}\n C    |      3 | {\"value, with, commas\",\"value with spaces\"}\n D    |      3 | {final_element}\n D    |      4 | {final_element}\n D    |      5 | {final_element}\n(9 rows)"
  },
  {
    "objectID": "sql/select/expand_array.html#multicolumn-unnest",
    "href": "sql/select/expand_array.html#multicolumn-unnest",
    "title": "Expand array (UNNEST)",
    "section": "Multicolumn unnest",
    "text": "Multicolumn unnest\nUsing multiple UNNESTs in the same query leads to unexpected results for me. Not separate records for each combination of array elements, but records with pairwise matches, with skips in case arrays for the same record are of different lengths.\n\n%%bash\ndocker exec -i unnest_example_postgres psql -U postgres -d postgres\n\nSELECT col1, UNNEST(col2), UNNEST(col3) FROM example_table;\n\n col1 | unnest |       unnest        \n------+--------+---------------------\n A    |      1 | hello\n A    |      2 | fedor\n A    |      3 | \n B    |      3 | test\n B    |      2 | \n C    |      3 | value, with, commas\n C    |        | value with spaces\n D    |      3 | final_element\n D    |      4 | \n D    |      5 | \n(10 rows)\n\n\n\nIf you want to get a separate record for each combination of unnested values, just use subquery.\nSo in the following example, there is a separate record for each combination of values in the corresponding col2 and col3 records.\n\n%%bash\ndocker exec -i unnest_example_postgres psql -U postgres -d postgres\n\nSELECT col1, col2, UNNEST(col3)\nFROM (SELECT \n        col1, UNNEST(col2) AS col2, col3 \n    FROM example_table\n) AS t;\n\n col1 | col2 |       unnest        \n------+------+---------------------\n A    |    1 | hello\n A    |    1 | fedor\n A    |    2 | hello\n A    |    2 | fedor\n A    |    3 | hello\n A    |    3 | fedor\n B    |    3 | test\n B    |    2 | test\n C    |    3 | value, with, commas\n C    |    3 | value with spaces\n D    |    3 | final_element\n D    |    4 | final_element\n D    |    5 | final_element\n(13 rows)\n\n\n\nAlternative solution, using artificial cross-joining to deploy the array into a relational format.\nNote now I don’t have time, but in general it would be great to compare the performance of this and solution with subquieri.\n\n%%bash\ndocker exec -i unnest_example_postgres psql -U postgres -d postgres\n\nSELECT col1, unnested_col2, UNNEST(col3) AS col3\nFROM example_table, UNNEST(col2) AS unnested_col2;\n\n col1 | unnested_col2 |        col3         \n------+---------------+---------------------\n A    |             1 | hello\n A    |             1 | fedor\n A    |             2 | hello\n A    |             2 | fedor\n A    |             3 | hello\n A    |             3 | fedor\n B    |             3 | test\n B    |             2 | test\n C    |             3 | value, with, commas\n C    |             3 | value with spaces\n D    |             3 | final_element\n D    |             4 | final_element\n D    |             5 | final_element\n(13 rows)"
  },
  {
    "objectID": "sql/select/aggregation/conditions_in_aggregats.html",
    "href": "sql/select/aggregation/conditions_in_aggregats.html",
    "title": "Conditions in aggregats (FILTER)",
    "section": "",
    "text": "You may need to perform aggregation only on records that meet certain conditions.\nIn the next cell, I create everything I need for the examples in this page.\n\n%%bash\ndocker run --rm -d\\\n    -e POSTGRES_PASSWORD=postgres \\\n    --name cond_in_agregats_example \\\n    postgres:15.4 &&gt; /dev/null\nsleep 5\ndocker exec -i cond_in_agregats_example psql -U postgres -d postgres\n\nCREATE TABLE aggregation_table(\n    col1 TEXT,\n    col2 INT\n);\nINSERT INTO aggregation_table(col1, col2) VALUES\n('A', 1),\n('A', 2),\n('A', 1),\n('B', 2),\n('B', 1),\n('B', 2);\nSELECT * FROM aggregation_table;\n\nCREATE TABLE\nINSERT 0 6\n col1 | col2 \n------+------\n A    |    1\n A    |    2\n A    |    1\n B    |    2\n B    |    1\n B    |    2\n(6 rows)\n\n\n\nNote don’t forget to stop the container when you finish playing with examples.\n\n%%bash\ndocker stop cond_in_agregats_example\n\ncond_in_agregats_example\n\n\nLet’s say that for each unique value in column col2, we need to create a column that indicates how many times that value corresponds to groups in column col1.\nTo complete it, you need to group by col1 and apply COUNT to col2, but for COUNT you need to use only those records that correspond to specific value of col2. Such an operation can be performed using the syntax COUNT(col1) FILTER (WHERE col2=&lt;specified value&gt;).\nSo in the following cell, the task is completed.\n\n%%bash\ndocker exec -i cond_in_agregats_example psql -U postgres -d postgres\nSELECT\n    col1,\n    COUNT(col2) FILTER (WHERE col2=1) \"col2=1\",\n    COUNT(col2) FILTER (WHERE col2=2) \"col2=2\"\nFROM aggregation_table\nGROUP BY col1\n\n col1 | col2=1 | col2=2 \n------+--------+--------\n B    |      1 |      2\n A    |      2 |      1\n(2 rows)"
  },
  {
    "objectID": "sql/select/aggregation/conditions_on_aggregats.html",
    "href": "sql/select/aggregation/conditions_on_aggregats.html",
    "title": "Conditions on aggregats (HAVING)",
    "section": "",
    "text": "In the next cell, I create everything I need for the examples in this page.\n%%bash\ndocker run --rm -d\\\n    -e POSTGRES_PASSWORD=postgres \\\n    --name cond_on_agregats_example \\\n    postgres:15.4 &&gt; /dev/null\nsleep 5\ndocker exec -i cond_on_agregats_example psql -U postgres -d postgres\n\nCREATE TABLE aggregation_table(\n    col1 TEXT,\n    col2 INT\n);\nINSERT INTO aggregation_table(col1, col2) VALUES\n('A', 5),\n('A', 1),\n('B', 2),\n('B', 1),\n('C', 3),\n('C', 4);\n\nCREATE TABLE\nINSERT 0 6\nNote don’t forget to stop the container when you finish playing with examples.\n!docker stop cond_on_agregats_example &&gt; /dev/null\nLet’s begin with the view of the example table.\n%%bash \ndocker exec -i cond_on_agregats_example psql -U postgres -d postgres\n\nSELECT * FROM aggregation_table;\n\n col1 | col2 \n------+------\n A    |    5\n A    |    1\n B    |    2\n B    |    1\n C    |    3\n C    |    4\n(6 rows)\nNow a problem: I need to aggregate SUM(col2) by the values of col1 and I only get the results where the sums are greater than 5. So I need to set a condition on a result of the aggregation."
  },
  {
    "objectID": "sql/select/aggregation/conditions_on_aggregats.html#wont-work",
    "href": "sql/select/aggregation/conditions_on_aggregats.html#wont-work",
    "title": "Conditions on aggregats (HAVING)",
    "section": "Won’t work",
    "text": "Won’t work\nThe first thing that comes to mind is to use the arger functions inside the WHERE block. This will cause an error because the WHERE block in sql is executed before all aggregations.\n\n%%bash \ndocker exec -i cond_on_agregats_example psql -U postgres -d postgres\n\nSELECT col1, SUM(col2)\nFROM aggregation_table WHERE SUM(col2) &gt; 5\nGROUP BY col1;\n\nERROR:  aggregate functions are not allowed in WHERE\nLINE 2: FROM aggregation_table WHERE SUM(col2) &gt; 5\n                                     ^"
  },
  {
    "objectID": "sql/select/aggregation/conditions_on_aggregats.html#subquery",
    "href": "sql/select/aggregation/conditions_on_aggregats.html#subquery",
    "title": "Conditions on aggregats (HAVING)",
    "section": "Subquery",
    "text": "Subquery\nA possible but not optimal solution is to use aggregation in the subquery and then describe the condition on the aggregation in the external query.\nIn the following example, I just solve the problem mentioned at the beginning of the page using this path.\n\n%%bash \ndocker exec -i cond_on_agregats_example psql -U postgres -d postgres\n\nSELECT * FROM (\n    SELECT col1, SUM(col2)\n    FROM aggregation_table\n    GROUP BY col1\n) AS tab1\nWHERE sum &gt; 5;\n\n col1 | sum \n------+-----\n C    |   7\n A    |   6\n(2 rows)"
  },
  {
    "objectID": "sql/select/aggregation/conditions_on_aggregats.html#having",
    "href": "sql/select/aggregation/conditions_on_aggregats.html#having",
    "title": "Conditions on aggregats (HAVING)",
    "section": "HAVING",
    "text": "HAVING\nThere’s a special keyword for describing these cases: HAVING, which is the same as WHERE, but for aggregates. It’s the optimal way to solve such a task.\nYou have to use HAVING after GROUP BY satement.\nIn the following example, I just solve the problem mentioned at the beginning of the page using this path.\n\n%%bash \ndocker exec -i cond_on_agregats_example psql -U postgres -d postgres\n\nSELECT col1, SUM(col2)\nFROM aggregation_table\nGROUP BY col1\nHAVING SUM(col2) &gt; 5;\n\n col1 | sum \n------+-----\n C    |   7\n A    |   6\n(2 rows)\n\n\n\nNote You can also use conditions on aggregates not mentioned in the SELECT statement. So in the next cell I got sums of col2 by col1, but only for cases where the average of col2 by col1 is greater than 3.\n\n%%bash \ndocker exec -i cond_on_agregats_example psql -U postgres -d postgres\n\nSELECT col1, SUM(col2)\nFROM aggregation_table\nGROUP BY col1\nHAVING AVG(col2) &gt; 3;\n\n col1 | sum \n------+-----\n C    |   7\n(1 row)\n\n\n\nNote You can use aggregation variables in conditions. So in the following example I got sums of col2 only for certain values of col1.\n\n%%bash \ndocker exec -i cond_on_agregats_example psql -U postgres -d postgres\n\nSELECT col1, SUM(col2)\nFROM aggregation_table\nGROUP BY col1\nHAVING col1 IN ('A', 'B');\n\n col1 | sum \n------+-----\n B    |   3\n A    |   6\n(2 rows)"
  },
  {
    "objectID": "sql/select/conditional.html",
    "href": "sql/select/conditional.html",
    "title": "Conditional (CASE)",
    "section": "",
    "text": "Source - commandprompt.com.\nConditional SELECT allows you to define which values you get for certain cases in data values. In general, you can use something like this inside a SELECT block:\nNote ELSE block is optional.\n%%bash\ndocker run --rm -d\\\n    -e POSTGRES_PASSWORD=docker_app \\\n    --name conditional_postgres_exmaples \\\n    postgres:15.4 &&gt; /dev/null\nsleep 5\ndocker exec -i conditional_postgres_exmaples psql -U postgres -d postgres\n\nCREATE TABLE tab (\n    var1 INT,\n    var2 INT,\n    var3 TEXT\n);\n\nINSERT INTO tab (var1, var2, var3) VALUES\n(1, 2, 'foo'),\n(4, 5, 'bar'),\n(2, 3, 'bar'),\n(4, 3, 'foo');\n\nSELECT * FROM tab;\n\nCREATE TABLE\nINSERT 0 4\n var1 | var2 | var3 \n------+------+------\n    1 |    2 | foo\n    4 |    5 | bar\n    2 |    3 | bar\n    4 |    3 | foo\n(4 rows)\nNote Don’t forget to stop the container when you’ve finished playing with it.\n%%bash\ndocker stop conditional_postgres_exmaples;\n\nconditional_postgres_exmaples"
  },
  {
    "objectID": "sql/select/conditional.html#basic-example",
    "href": "sql/select/conditional.html#basic-example",
    "title": "Conditional (CASE)",
    "section": "Basic example",
    "text": "Basic example\nIn my opinion, the most popular cases for this operation are the categorisation of numerical variables and the replacement of categorical variables. So in the following cell such operations are performed.\n\n%%bash\ndocker exec -i conditional_postgres_exmaples psql -U postgres -d postgres\nSELECT\n    CASE\n        WHEN var1+var2 &gt; 5 THEN '&gt;5'\n        ELSE '&lt;=5'\n    END num_to_cat,\n    CASE\n        WHEN var3='foo' THEN 'new foo'\n        WHEN var3='bar' THEN 'new bar'\n    END replace\nFROM tab;\n\n num_to_cat | replace \n------------+---------\n &lt;=5        | new foo\n &gt;5         | new bar\n &lt;=5        | new bar\n &gt;5         | new foo\n(4 rows)"
  },
  {
    "objectID": "sql/select/conditional.html#nested",
    "href": "sql/select/conditional.html#nested",
    "title": "Conditional (CASE)",
    "section": "Nested",
    "text": "Nested\nYou can use the CASE value as the result of an external CASE statement. So here is an example of how it can be used:\n\n%%bash\ndocker exec -i conditional_postgres_exmaples psql -U postgres -d postgres\nSELECT\n    CASE\n        WHEN var1+var2 &gt; 5 THEN\n            CASE\n                WHEN var3='foo' THEN '&gt;5 and foo'\n                ELSE 'bar'\n            END\n        ELSE '&lt;=5'\n    END num_to_cat\nFROM tab;\n\n num_to_cat \n------------\n &lt;=5\n bar\n &lt;=5\n &gt;5 and foo\n(4 rows)"
  },
  {
    "objectID": "sql/select/join.html",
    "href": "sql/select/join.html",
    "title": "JOIN",
    "section": "",
    "text": "Join is a keyword that allows you to combine information from different tables. It’s really important, but sometimes it’s hard to understand, so this whole page is about features and usecases of joins in Postgres SQL.\nIn the following cell, I’m creating a database that will be used for all the examples on this page.\n%%bash\ndocker run -d --rm\\\n    --name join_example\\\n    -e POSTGRES_PASSWORD=postgres\\\n    postgres:15.4 &&gt; /dev/null\nsleep 5\ndocker exec -i join_example psql -U postgres -d postgres\n\nCREATE TABLE central_table (\n    id INT NOT NULL,\n    someinfo1_id INT NOT NULL,\n    someinfo2_id INT NOT NULL\n);\nINSERT INTO central_table (id, someinfo1_id, someinfo2_id) VALUES\n(1, 1, 1),\n(2, 2, 2),\n(3, 2, 1);\n\nCREATE TABLE some_info1 (\n    id INT NOT NULL,\n    some_info TEXT\n);\nINSERT INTO some_info1 (id, some_info) VALUES\n(1, 'some_info1.1'),\n(2, 'some_info1.2'),\n(3, 'some_info1.3');\n\nCREATE TABLE some_info2 (\n    id INT NOT NULL,\n    some_info TEXT\n);\nINSERT INTO some_info2 (id, some_info) VALUES\n(1, 'some_info2.1'),\n(2, 'some_info2.2'),\n(3, 'some_info2.3');\n\nCREATE TABLE\nINSERT 0 3\nCREATE TABLE\nINSERT 0 3\nCREATE TABLE\nINSERT 0 3\nIn basic case examples I’ll use tables central_table, some_info1, some_info2. So in the next cell I just show their content.\n%%bash\ndocker exec -i join_example psql -U postgres -d postgres\n\nSELECT * FROM central_table;\nSELECT * FROM some_info1;\nSELECT * FROM some_info2;\n\n id | someinfo1_id | someinfo2_id \n----+--------------+--------------\n  1 |            1 |            1\n  2 |            2 |            2\n  3 |            2 |            1\n(3 rows)\n\n id |  some_info   \n----+--------------\n  1 | some_info1.1\n  2 | some_info1.2\n  3 | some_info1.3\n(3 rows)\n\n id |  some_info   \n----+--------------\n  1 | some_info2.1\n  2 | some_info2.2\n  3 | some_info2.3\n(3 rows)\nNote Remember to close the container when you have finished playing with the examples.\n!docker stop join_example\n\njoin_example"
  },
  {
    "objectID": "sql/select/join.html#short-names",
    "href": "sql/select/join.html#short-names",
    "title": "JOIN",
    "section": "Short names",
    "text": "Short names\nYou can use short names for the tables involved in the join to make it easier to write code. Just give the new name of the table separated by a space from the original name of the table.\nIn the next cell I set the sent name for the table central_table and the si1 name for the table some_info1 and use it everywhere to refer to the tables used in the join.\n\n%%bash\ndocker exec -i join_example psql -U postgres -d postgres\n\nSELECT sent.id, si1.some_info FROM \n    central_table sent\n    JOIN\n    some_info1 si1\n    ON sent.someinfo1_id=si1.id\n;\n\n id |  some_info   \n----+--------------\n  1 | some_info1.1\n  2 | some_info1.2\n  3 | some_info1.2\n(3 rows)"
  },
  {
    "objectID": "sql/select/join.html#more-tables",
    "href": "sql/select/join.html#more-tables",
    "title": "JOIN",
    "section": "More tables",
    "text": "More tables\nYou can use more than two tables in a join. Just use multiple JOIN ... ON ... blocks to select more information.\nSo in the following example, I simply combine all the sample database information into one query!\n\n%%bash\ndocker exec -i join_example psql -U postgres -d postgres\n\nSELECT sent.id, si1.some_info, si2.some_info FROM \n    central_table sent\n    JOIN\n    some_info1 si1\n    ON sent.someinfo1_id=si1.id\n    JOIN\n    some_info2 si2\n    ON sent.someinfo2_id=si2.id\n;\n\n id |  some_info   |  some_info   \n----+--------------+--------------\n  1 | some_info1.1 | some_info2.1\n  3 | some_info1.2 | some_info2.1\n  2 | some_info1.2 | some_info2.2\n(3 rows)"
  },
  {
    "objectID": "sql/select/join.html#duplication",
    "href": "sql/select/join.html#duplication",
    "title": "JOIN",
    "section": "Duplication",
    "text": "Duplication\nIf in joined table will be duplicates in join key, you will find that some tables of original table have been duplicated.\nIn the example for this page, central_table.someinfo1_id takes the value 2 twice. So if you join it to some_info, the record of some_info that has id=2 will be duplicated, just like in the following cell.\n\n%%bash\ndocker exec -i join_example psql -U postgres -d postgres\n\nSELECT * \nFROM some_info1\nFULL OUTER JOIN central_table ON some_info1.id=central_table.someinfo1_id;\n\n id |  some_info   | id | someinfo1_id | someinfo2_id \n----+--------------+----+--------------+--------------\n  1 | some_info1.1 |  1 |            1 |            1\n  2 | some_info1.2 |  2 |            2 |            2\n  2 | some_info1.2 |  3 |            2 |            1\n  3 | some_info1.3 |    |              |             \n(4 rows)"
  },
  {
    "objectID": "sql/select/join.html#join-types",
    "href": "sql/select/join.html#join-types",
    "title": "JOIN",
    "section": "Join types",
    "text": "Join types\nThis diagram describes the different types of joins available in SQL."
  },
  {
    "objectID": "python/sqlalchemy/create_table.html",
    "href": "python/sqlalchemy/create_table.html",
    "title": "Create table",
    "section": "",
    "text": "On this page I will show how to move the data model declared in sqlalchemy to a database."
  },
  {
    "objectID": "python/sqlalchemy/create_table.html#create-container",
    "href": "python/sqlalchemy/create_table.html#create-container",
    "title": "Create table",
    "section": "Create container",
    "text": "Create container\nFor example, the postgres container is used. It also shows that in the initial list of tables is emtpy.\n\n%%bash\ndocker run -d --rm\\\n    --name create_table_example\\\n    -e POSTGRES_PASSWORD=postgres\\\n    -p 5000:5432\\\n    postgres:15.4 &&gt; /dev/null\nsleep 5\n\ndocker exec create_table_example \\\n    psql -U postgres -d postgres -c \"\\dt\"\n\nDid not find any relations."
  },
  {
    "objectID": "python/sqlalchemy/create_table.html#python-code",
    "href": "python/sqlalchemy/create_table.html#python-code",
    "title": "Create table",
    "section": "Python code",
    "text": "Python code\nIn the following cell, the data model is defined and moved to the database. Key code is Base.metadata.create_all(...) - duplicate data model in the database.\n\nfrom sqlalchemy import (\n    create_engine, \n    Column, \n    Integer,\n    String\n)\nfrom sqlalchemy.orm import sessionmaker, declarative_base\n\nURL = \"postgresql://postgres:postgres@localhost:5000/postgres\"\nengine = create_engine(URL)\n\nSessionLocal = sessionmaker(\n    autoflush=False,\n    autocommit=False,\n    bind=engine\n)\n\n# defining data model\nBase = declarative_base()\nclass TestTable(Base):\n    __tablename__=\"test_table\"\n    id = Column(Integer, primary_key=True)\n    numeric_var = Column(Integer)\n    text_var = Column(String)\n\n# duplicate datamodel in the database\nBase.metadata.create_all(engine)"
  },
  {
    "objectID": "python/sqlalchemy/create_table.html#check-result",
    "href": "python/sqlalchemy/create_table.html#check-result",
    "title": "Create table",
    "section": "Check result",
    "text": "Check result\nThe following cell from the container with database runs:\n\n\\dt to list created tables;\nSELECT * FROM test_table; - to get head of the created table.\n\nThe results in the database correspond to the declared data model.\n\n%%bash\ndocker exec -i create_table_example \\\n    psql -U postgres -d postgres\n\n\\dt\nSELECT * FROM test_table;\n\n           List of relations\n Schema |    Name    | Type  |  Owner   \n--------+------------+-------+----------\n public | test_table | table | postgres\n(1 row)\n\n id | numeric_var | text_var \n----+-------------+----------\n(0 rows)"
  },
  {
    "objectID": "python/sqlalchemy/create_table.html#stop-the-container",
    "href": "python/sqlalchemy/create_table.html#stop-the-container",
    "title": "Create table",
    "section": "Stop the container",
    "text": "Stop the container\n\n!docker stop create_table_example\n\ncreate_table_example"
  },
  {
    "objectID": "python/sqlalchemy/add_record.html",
    "href": "python/sqlalchemy/add_record.html",
    "title": "Add record",
    "section": "",
    "text": "In this page I’ll show you how to add a record to a database using sqlachemy."
  },
  {
    "objectID": "python/sqlalchemy/add_record.html#create-database",
    "href": "python/sqlalchemy/add_record.html#create-database",
    "title": "Add record",
    "section": "Create database",
    "text": "Create database\nFor this page postgres docker container is used. In the following cell I will create database schema just in sqlalchemy and push it into database.\n\n!docker run -d --rm\\\n    --name add_record_example\\\n    -e POSTGRES_PASSWORD=postgres\\\n    -p 5000:5432\\\n    postgres:15.4 &&gt; /dev/null\n!sleep 5\n\nfrom sqlalchemy import (\n    create_engine, \n    Column, \n    Integer,\n    String\n)\nfrom sqlalchemy.orm import sessionmaker, declarative_base\n\nURL = \"postgresql://postgres:postgres@localhost:5000/postgres\"\nengine = create_engine(URL)\n\nSessionLocal = sessionmaker(\n    autoflush=False,\n    autocommit=False,\n    bind=engine\n)\n\n# defining data model\nBase = declarative_base()\nclass TestTable(Base):\n    __tablename__=\"test_table\"\n    id = Column(Integer, primary_key=True)\n    numeric_var = Column(Integer)\n    text_var = Column(String)\n\n# duplicate datamodel in the database\nBase.metadata.create_all(engine)\n\nMake sure that the created table is initially empty.\n\n!docker exec add_record_example psql -U postgres -d postgres -c \"SELECT * FROM test_table;\"\n\n id | numeric_var | text_var \n----+-------------+----------\n(0 rows)"
  },
  {
    "objectID": "python/sqlalchemy/add_record.html#example",
    "href": "python/sqlalchemy/add_record.html#example",
    "title": "Add record",
    "section": "Example",
    "text": "Example\nTo add a record to the database, you must use the session’s add method. As an argument you must pass an instance of the &lt;table class&gt; that describes the record.\nIn the following example, a few records with random content have been added to the database.\nNote In this example, I don’t set a value for the id field of TestTable because it is the primary key and the table will set it itself.\nNote At the end, session.commit is called - it is needed because of features of the session declaration.\n\nfrom random import randint\nsession = SessionLocal()\nfor i in range(10):\n    session.add(\n        TestTable(\n            numeric_var=randint(0,100),\n            text_var=\"\".join([chr(randint(97, 107)) for i in range(10)])\n        )\n    )\nsession.commit()\n\nLet’s check the contents of the test_table.\n\n!docker exec add_record_example psql -U postgres -d postgres -c \"SELECT * FROM test_table;\"\n\n id | numeric_var |  text_var  \n----+-------------+------------\n  1 |           6 | ejkagfkdgi\n  2 |           3 | ddfaaedjfd\n  3 |           4 | gjchfeahhi\n  4 |          54 | kcfkbfjhjj\n  5 |          56 | gbfejdaecf\n  6 |          14 | cigagdihha\n  7 |          49 | kfcfcjgkej\n  8 |          65 | ajhahgcgff\n  9 |          20 | jgchadeedj\n 10 |          89 | jdkhfdefdc\n(10 rows)"
  },
  {
    "objectID": "python/sqlalchemy/add_record.html#stop-container",
    "href": "python/sqlalchemy/add_record.html#stop-container",
    "title": "Add record",
    "section": "Stop container",
    "text": "Stop container\n\n!docker stop add_record_example\n\nadd_record_example"
  },
  {
    "objectID": "python/basics/class_interface/python_class_interface.html",
    "href": "python/basics/class_interface/python_class_interface.html",
    "title": "Иточники",
    "section": "",
    "text": "Изучение того как в python работать с интерфесми (как концепцией)\n\nhttps://realpython.com/python-interface/\n\nИнтерфейс - некоторая абстакция, которая позволяет определить какими методы должны быть обязательно реализованы в наследниках.\n\nНе формальное определение интефейса\nРассмотрим пример - класс парсер. При том там надо реализовать парсеры для pdf документов и для документов из электронной почнты (eml)\n\nclass InformalParserInterface:\n    '''\n        Класс который определят все общее \n        принадлежащее парсерам\n    '''\n    def load_data_source(self, path: str, file_name : str) -&gt; str:\n        '''\n            Пусть пасерам надо доставать данные из различных\n            файлов по пути path\n        '''\n        pass\n    \n    def extrac_text(self, full_file_name: str):\n        '''\n            Пусть всем парсерам надо уметь доставать данные из\n            диррекстивно указанного пути\n        ''' \n        pass\n\nИтак, были описаны возможности которыми должен обладать любой парсер. Далее конктеретизация.\n\nclass PdfParser(InformalParserInterface):\n    '''\n        Конкредная реализация парсера для pdf документов\n    '''\n    def load_data_source(self, path: str, file_name:str) -&gt; str:\n        '''\n            Определяем как именно должен работать парсинг\n            для pdf документов\n        '''\n        pass\n    \n    def extract_text(self, full_file_path:str) -&gt; dict:\n        '''\n            Тут также определяем как именно должен работать \n            парсинг для pdf документов\n        '''\n        pass\n    \nclass EmlParser(InformalParserInterface):\n    '''\n        Конкретная реализация для документов электронной почты\n    '''\n    def load_data_source(self, path:str, file_name:str)-&gt;str:\n        '''\n            Определяем как именно должен работать парсинг для email документов\n        '''\n        pass\n    \n    def extract_text_from_email(self, full_file_path:str)-&gt;str:\n        '''\n            Метод определенный только для документов email,\n            но он по прежнему определяет как должен работать\n            парсинг для email документов\n        '''\n        pass\n\nУбедимся, что подклассы созданные реализации являются классами-наследниками для базового класса InformalParserInterface с использованием функции issubclass\n\nissubclass(PdfParser, InformalParserInterface)\n\nTrue\n\n\n\nissubclass(EmlParser, InformalParserInterface)\n\nTrue\n\n\nИдея, которую доносят в источнике 1, состоит в том, что хорошо чтобы issubclass(EmlParser, InformalParserInterface) возвращало False так как, мы не переопределили extract_text и EmlParser не может считаться полноценной реализацией интерфейса InformalParserInterface.\n\n\nИспользование Метаклассов\nИдея создания интерфейса через метакласс, сосотои в том, что интерфейс имеет метакласс, в котором переопределены. __instancecheck__ и __subclasscheck__ (подробнее об этих базовых методах можно узать тут https://github.com/Dranikf/knowledge_bank/blob/main/python_class_spesials/python_class_specials.ipynb в разделе “Методы-&gt;Методы метаклассов”). Приведенным ниже образом.\n\nclass ParserMeta(type):\n    '''\n        Мета-парсер который будет использоваться для\n        создания парсеров\n    '''\n    def __subclasscheck__(cls, subclass):\n        '''\n            Все классы наследующие этот класс \n            в качесве мета класса будут, будут своими \n            экземплярами (в смысле функции issubclass) \n            воспринимать лишь те классы, в которых объявлены \n            и определены методы load_data_source и extract_text.\n        '''\n        return (\n            hasattr(subclass, 'load_data_source') and\n            callable(subclass.load_data_source) and\n            hasattr(subclass, 'extract_text') and\n            callable(subclass.extract_text)\n        )\n    \n    \n    def __instancecheck__(cls, instance):\n        '''\n            Все классы наследующие это класс в качестве\n            мета класса, своими экземплярами будут воспринимпть\n            лишь те объекты, классы которых воспинимаются\n            наследниками\n        '''\n        return cls.__subclasscheck__(type(instance))\n    \n\n    \nclass UpdatedInformalParserInterface(metaclass = ParserMeta):\n    '''\n        Объявляем обновленный парсер-интерфейс\n    '''\n    pass\n\nРассмотрим, тот-же пример.\n\nclass PdfParserNew():\n    '''\n        Новая конкредная реализация парсера для pdf документов\n    '''\n    def load_data_source(self, path: str, file_name:str) -&gt; str:\n        '''\n            Определяем как именно должен работать парсинг\n            для pdf документов\n        '''\n        pass\n    \n    def extract_text(self, full_file_path:str) -&gt; dict:\n        '''\n            Тут также определяем как именно должен работать \n            парсинг для pdf документов\n        '''\n        pass\n    \nclass EmlParserNew:\n    '''\n        Конкретная реализация для документов электронной почты\n    '''\n    def load_data_source(self, path:str, file_name:str)-&gt;str:\n        '''\n            Определяем как именно должен работать парсинг для email документов\n        '''\n        pass\n    \n    def extract_text_from_email(self, full_file_path:str)-&gt;str:\n        '''\n            Метод определенный только для документов email,\n            но он по прежнему определяет как должен работать\n            парсинг для email документов\n        '''\n        pass\n\nПроверяем результат выполнения функции issubclass для новосозданных классов.\n\nissubclass(PdfParserNew, UpdatedInformalParserInterface)\n\nTrue\n\n\n\nissubclass(EmlParserNew, UpdatedInformalParserInterface)\n\nFalse\n\n\nТак формально UpdatedInformalParserInterface не является реализацией интерфейса EmlParserNew.\nНо такая реализация по прежнему не являтся правильной Рассмотрим результат метода __mro__ для PdfParserNew (__mro__ - одно из специальных полей).\n\nPdfParserNew.__mro__\n\n(__main__.PdfParserNew, __main__.UpdatedInformalParserInterface, object)\n\n\nТак в __mro__ для класса PdfParserNew, не видно, что он как-либо связан с UpdatedInformalParserInterface. Такую ситуалию еще описывают, что UpdatedInformalParserInterface является виртуальным базовым классом для класса PdfParserNew.\nВпрочем, новерное, это можно преодолеть следующей реализацией pdf парсера.\n\nclass PdfParserNew2(UpdatedInformalParserInterface):\n    '''\n        Новая конкредная реализация парсера для pdf документов\n    '''\n    def load_data_source(self, path: str, file_name:str) -&gt; str:\n        '''\n            Определяем как именно должен работать парсинг\n            для pdf документов\n        '''\n        pass\n    \n    def extract_text(self, full_file_path:str) -&gt; dict:\n        '''\n            Тут также определяем как именно должен работать \n            парсинг для pdf документов\n        '''\n        pass\n\n\nissubclass(PdfParserNew2, UpdatedInformalParserInterface)\n\nTrue\n\n\n\nPdfParserNew2.__mro__\n\n(__main__.PdfParserNew2, __main__.UpdatedInformalParserInterface, object)\n\n\n\n\nФормальная реализация интерфейса\nДля формальной реализации интерфейсов используется модуль abc\n\nimport abc"
  },
  {
    "objectID": "python/basics/operators/indexing.html",
    "href": "python/basics/operators/indexing.html",
    "title": "Indexing ([])",
    "section": "",
    "text": "In Python, all indices have an alternate index which is negative. Suppose we have a list of n elements, so elements will have indexes from 0 to n-1 and from -n to -1.\nThe following table puts positive and negative indexing in python into a visual correlation.\n\n\n\n\nPositive indexing\n\n\n0\n\n\n1\n\n\n…\n\n\nn-1\n\n\n\n\nNegative indexing\n\n\n-n\n\n\n-n+1\n\n\n…\n\n\n-1\n\n\n\n\nIt can be very useful to access items from the end of the collection. So in the last example, I easily accessed the penultimate item in the list.\n\nlist(range(50))[-2]\n\n48"
  },
  {
    "objectID": "python/basics/operators/indexing.html#negative-index",
    "href": "python/basics/operators/indexing.html#negative-index",
    "title": "Indexing ([])",
    "section": "",
    "text": "In Python, all indices have an alternate index which is negative. Suppose we have a list of n elements, so elements will have indexes from 0 to n-1 and from -n to -1.\nThe following table puts positive and negative indexing in python into a visual correlation.\n\n\n\n\nPositive indexing\n\n\n0\n\n\n1\n\n\n…\n\n\nn-1\n\n\n\n\nNegative indexing\n\n\n-n\n\n\n-n+1\n\n\n…\n\n\n-1\n\n\n\n\nIt can be very useful to access items from the end of the collection. So in the last example, I easily accessed the penultimate item in the list.\n\nlist(range(50))[-2]\n\n48"
  },
  {
    "objectID": "python/basics/operators/order.html",
    "href": "python/basics/operators/order.html",
    "title": "Order",
    "section": "",
    "text": "Here I’ll show features related to the order of execution of expressions with different combinations of operators.\nSomewhere on the internet I found such a picture describing the order of execution for Python operators.\nThe top operators are executed first, regardless of where they appear in the expression. Operators at the same level will be executed in the sequence as they are specified in the expression according to the rule specified in the “Associativity” column."
  },
  {
    "objectID": "python/basics/operators/order.html#easy-examples",
    "href": "python/basics/operators/order.html#easy-examples",
    "title": "Order",
    "section": "Easy examples",
    "text": "Easy examples\nNow some simple examples that demonstrate how to use the proposed table in practice.\n\nOperator sequence\nThe following example show that ** will be executed before * anyway. Calculation always follows the way \\(4^2=16 \\rightarrow 16*2 = 32\\) and never \\(2*4=8 \\rightarrow 8^2=64\\).\n\nprint(2*4**2)\nprint(4**2*2)\n\n32\n\n\n\n\nAssociativity\nAs mentioned for the operators / and *, they are executed from left to right. So the following example will use logic \\(3/3=1 \\rightarrow 1*2=2\\), not \\(3*2=6 \\rightarrow 3/6=0.5\\).\n\n3/3*2\n\n2.0\n\n\nOne more example shows how associativity rool changes the result with exactly same operands.\n\nIn first case it uses logic \\(3/3=1 \\rightarrow [1/3]=1\\);\nIn second case it uses logic \\([3/3]=0 \\rightarrow 0/3=0\\).\n\n\nprint(3/3%3)\nprint(3%3/3)\n\n1.0\n0.0"
  },
  {
    "objectID": "python/basics/operators/in.html",
    "href": "python/basics/operators/in.html",
    "title": "Is in array (in)",
    "section": "",
    "text": "This keyword allows you to check an element in an array."
  },
  {
    "objectID": "python/basics/operators/in.html#list",
    "href": "python/basics/operators/in.html#list",
    "title": "Is in array (in)",
    "section": "list",
    "text": "list\n\ntest_list = [1,2,3,4,5] \nprint(5 in test_list)\nprint(6 in test_list)\n\nTrue\nFalse"
  },
  {
    "objectID": "python/basics/operators/in.html#tuple",
    "href": "python/basics/operators/in.html#tuple",
    "title": "Is in array (in)",
    "section": "tuple",
    "text": "tuple\n\ntest_tuple = (1,2,3,4,5)\nprint(5 in test_tuple)\nprint(6 in test_tuple)\n\nTrue\nFalse"
  },
  {
    "objectID": "python/basics/operators/in.html#str",
    "href": "python/basics/operators/in.html#str",
    "title": "Is in array (in)",
    "section": "str",
    "text": "str\n\ntest_str = \"this is the test str contains subline to check it\"\n\nprint(\"subline\" in test_str)\nprint(\"subliner\" in test_str)\n\nTrue\nFalse"
  },
  {
    "objectID": "python/basics/operators/in.html#dict",
    "href": "python/basics/operators/in.html#dict",
    "title": "Is in array (in)",
    "section": "dict",
    "text": "dict\nChecks if the element on the left side of in is in the keys (not values) of the dictionary on the right side of in.\n\ntest_dict = {\"a\":1, \"b\":2}\nprint(\"a\" in test_dict)\nprint(1 in test_dict)\n\nTrue\nFalse"
  },
  {
    "objectID": "python/basics/operators/in.html#not-in",
    "href": "python/basics/operators/in.html#not-in",
    "title": "Is in array (in)",
    "section": "not in",
    "text": "not in\nThis operator returns True if the collection from the right side of the operator contains the value from the left side of the operator, otherwise it returns False.\nLooks like it’s syntax sugar, because now I don’t see any cases where it differs from the expression not (&lt;value&gt; in &lt;collection&gt;). The following example supports this idea.\n\nexample_collection = [\"a\", \"b\", \"c\"]\n\nprint(not (\"a\" in example_collection)) \nprint(\"a\" not in example_collection)\n\nFalse\nFalse"
  },
  {
    "objectID": "python/basics/exceptions.html",
    "href": "python/basics/exceptions.html",
    "title": "Exceptions",
    "section": "",
    "text": "If you want to handle multiple exceptions for a block, you can use one of the following options:\n\n\nTo use the same code to handle different types of exceptions, you can simply mention them as tuples in the condition for the except block.\nIn the following example, I have called a random exception type, but only three of four possible error types are mentioned in the except block. So I will only get a message from the except block until I get the unmentioned error type.\nI would like to emphasise once again that this is not done to handle all error types, there is a special design for this which is described here.\n\ntest_lst = [1,2]\ntry_functions = {\n    \"ZeroDivisionError\": lambda: 8/0, # \n    \"IndexError\" : lambda: test_lst[5],\n    \"TypeError\" : lambda: \"hello\" + 4,\n    \"NameError\" : lambda: unknown_name\n}\nerror_types = list(try_functions.keys())\n\nfor i in range(10):\n    try:\n        this_type = np.random.choice(error_types)\n        print(f\"====I got a type {this_type}====\")\n        try_functions[this_type]()\n    except (ZeroDivisionError, IndexError, TypeError):\n        print(\"I handle all mentioned exceptions\")\n\n====I got a type IndexError====\nI handle all mentioned exceptions\n====I got a type TypeError====\nI handle all mentioned exceptions\n====I got a type ZeroDivisionError====\nI handle all mentioned exceptions\n====I got a type TypeError====\nI handle all mentioned exceptions\n====I got a type NameError====\n\n\nNameError: name 'unknown_name' is not defined\n\n\n\n\n\nYou can set code to handle a particular type of error, and do it several times for a try block. All you have to do is mention several except blocks one after the other.\nSo in the following example, I call random error in a loop, and different errors have different handlers. You can see that there is a specific message for each iteration.\n\nimport numpy as np\n\ntest_lst = [1,2]\ntry_functions = {\n    \"ZeroDivisionError\": lambda: 8/0, # \n    \"IndexError\" : lambda: test_lst[5],\n    \"TypeError\" : lambda: \"hello\" + 4,\n}\nerror_types = list(try_functions.keys())\n\nfor i in range(10):\n    try:\n        this_type = np.random.choice(error_types)\n        print(f\"====I got a type {this_type}====\")\n        try_functions[this_type]()\n    except ZeroDivisionError:\n        print(\"This is divison by zero (first option)\")\n    except IndexError:\n        print(\"This is wrong index (second option)\")\n    except TypeError:\n        print(\"This is wrong operations with types (third option)\")\n\n====I got a type TypeError====\nThis is wrong operations with types (third option)\n====I got a type ZeroDivisionError====\nThis is divison by zero (first option)\n====I got a type TypeError====\nThis is wrong operations with types (third option)\n====I got a type IndexError====\nThis is wrong index (second option)\n====I got a type TypeError====\nThis is wrong operations with types (third option)\n====I got a type TypeError====\nThis is wrong operations with types (third option)\n====I got a type ZeroDivisionError====\nThis is divison by zero (first option)\n====I got a type TypeError====\nThis is wrong operations with types (third option)\n====I got a type ZeroDivisionError====\nThis is divison by zero (first option)\n====I got a type TypeError====\nThis is wrong operations with types (third option)\n\n\n\n\n\nYou can define any number of `except’ blocks for the same exception type, but only the first one will be called.\nIn the following example, even though I declared two codes for the ZeroDivisionError type exception, only the first one was executed.\n\ntry:\n    1/0\nexcept ZeroDivisionError:\n    print(\"First code to handle exception\")\nexcept ZeroDivisionError:\n    print(\"Second code to handle exception\")\n\nFirst code to handle exception"
  },
  {
    "objectID": "python/basics/exceptions.html#same-except-block",
    "href": "python/basics/exceptions.html#same-except-block",
    "title": "Exceptions",
    "section": "",
    "text": "To use the same code to handle different types of exceptions, you can simply mention them as tuples in the condition for the except block.\nIn the following example, I have called a random exception type, but only three of four possible error types are mentioned in the except block. So I will only get a message from the except block until I get the unmentioned error type.\nI would like to emphasise once again that this is not done to handle all error types, there is a special design for this which is described here.\n\ntest_lst = [1,2]\ntry_functions = {\n    \"ZeroDivisionError\": lambda: 8/0, # \n    \"IndexError\" : lambda: test_lst[5],\n    \"TypeError\" : lambda: \"hello\" + 4,\n    \"NameError\" : lambda: unknown_name\n}\nerror_types = list(try_functions.keys())\n\nfor i in range(10):\n    try:\n        this_type = np.random.choice(error_types)\n        print(f\"====I got a type {this_type}====\")\n        try_functions[this_type]()\n    except (ZeroDivisionError, IndexError, TypeError):\n        print(\"I handle all mentioned exceptions\")\n\n====I got a type IndexError====\nI handle all mentioned exceptions\n====I got a type TypeError====\nI handle all mentioned exceptions\n====I got a type ZeroDivisionError====\nI handle all mentioned exceptions\n====I got a type TypeError====\nI handle all mentioned exceptions\n====I got a type NameError====\n\n\nNameError: name 'unknown_name' is not defined"
  },
  {
    "objectID": "python/basics/exceptions.html#different-except-blocks",
    "href": "python/basics/exceptions.html#different-except-blocks",
    "title": "Exceptions",
    "section": "",
    "text": "You can set code to handle a particular type of error, and do it several times for a try block. All you have to do is mention several except blocks one after the other.\nSo in the following example, I call random error in a loop, and different errors have different handlers. You can see that there is a specific message for each iteration.\n\nimport numpy as np\n\ntest_lst = [1,2]\ntry_functions = {\n    \"ZeroDivisionError\": lambda: 8/0, # \n    \"IndexError\" : lambda: test_lst[5],\n    \"TypeError\" : lambda: \"hello\" + 4,\n}\nerror_types = list(try_functions.keys())\n\nfor i in range(10):\n    try:\n        this_type = np.random.choice(error_types)\n        print(f\"====I got a type {this_type}====\")\n        try_functions[this_type]()\n    except ZeroDivisionError:\n        print(\"This is divison by zero (first option)\")\n    except IndexError:\n        print(\"This is wrong index (second option)\")\n    except TypeError:\n        print(\"This is wrong operations with types (third option)\")\n\n====I got a type TypeError====\nThis is wrong operations with types (third option)\n====I got a type ZeroDivisionError====\nThis is divison by zero (first option)\n====I got a type TypeError====\nThis is wrong operations with types (third option)\n====I got a type IndexError====\nThis is wrong index (second option)\n====I got a type TypeError====\nThis is wrong operations with types (third option)\n====I got a type TypeError====\nThis is wrong operations with types (third option)\n====I got a type ZeroDivisionError====\nThis is divison by zero (first option)\n====I got a type TypeError====\nThis is wrong operations with types (third option)\n====I got a type ZeroDivisionError====\nThis is divison by zero (first option)\n====I got a type TypeError====\nThis is wrong operations with types (third option)"
  },
  {
    "objectID": "python/basics/exceptions.html#ony-one-exception-per-type",
    "href": "python/basics/exceptions.html#ony-one-exception-per-type",
    "title": "Exceptions",
    "section": "",
    "text": "You can define any number of `except’ blocks for the same exception type, but only the first one will be called.\nIn the following example, even though I declared two codes for the ZeroDivisionError type exception, only the first one was executed.\n\ntry:\n    1/0\nexcept ZeroDivisionError:\n    print(\"First code to handle exception\")\nexcept ZeroDivisionError:\n    print(\"Second code to handle exception\")\n\nFirst code to handle exception"
  },
  {
    "objectID": "python/basics/class_spesials/python_class_specials.html",
    "href": "python/basics/class_spesials/python_class_specials.html",
    "title": "Источники информации",
    "section": "",
    "text": "Разбор спецальных меодов/полей в языке программирования python3"
  },
  {
    "objectID": "python/basics/class_spesials/python_class_specials.html#методы-метаклассов",
    "href": "python/basics/class_spesials/python_class_specials.html#методы-метаклассов",
    "title": "Источники информации",
    "section": "Методы метаклассов",
    "text": "Методы метаклассов\n\n__instancecheck__\nДоступен пример использования https://github.com/Dranikf/knowledge_bank/blob/main/python_class_interface/python_class_interface.ipynb (см. раздел “Использование метаклассов”)\nБудет вызван для класса при использовании встроенной функции isinstace(instance, class) - функции которая (при __instancecheck__ оставленного по умолчанию) вернет True если первый аргумент экземпляр класса указанного вторым.\n\nisinstance(Auto(\"Gelentvagen\"), TechThing)\n\n__isinstacecheck__ called\nGelentvagen удачно был удален\n\n\nTrue\n\n\nПритом, ход приведенный в следующей ячейке, не сработает.\n\nisinstance(Auto(\"Toyota\"), Auto)\n\nToyota удачно был удален\n\n\nTrue\n\n\nВидимо потому, что в класс Auto не указан метакласс Thing как это сделано для класса TechThing. Весьма обывательское объяснение, но возможно в будующем я буду знать об этой концепции больше.\n\n\n__subclasscheck__\nДоступен пример использования https://github.com/Dranikf/knowledge_bank/blob/main/python_class_interface/python_class_interface.ipynb (см. раздел “Использование метаклассов”)\nБудет вызван для класса при использовании встроенной функции issubclass(class1, class2) которая (при __subclasscheck__ оставленной по умолчанию), вернет True если class1 наследник class2.\n\nissubclass(Auto, TechThing)\n\n__issubclasscheck__ called\n\n\nTrue\n\n\nПритом, ход приведенный в следующей ячейке, сработает, в отличии от аналогичной ситуации для другого специального метода метаклассов __instancecheck__.\n\nissubclass(Auto, Machine)\n\n__issubclasscheck__ called\n\n\nTrue\n\n\nПочему так, на данном этапе погружения в pyhton, остается загадкой."
  },
  {
    "objectID": "python/basics/class_spesials/python_class_specials.html#методы-приведения-к-типу",
    "href": "python/basics/class_spesials/python_class_specials.html#методы-приведения-к-типу",
    "title": "Источники информации",
    "section": "Методы приведения к типу",
    "text": "Методы приведения к типу\n\n__bool__\nЭтот метод будет вызван для любого экземпляра переданного в базовую функцию bool\n\nprint(\"Gili result\", bool(Auto(\"Gili\")))\nprint(\"None result\", bool(Auto(None)))\n\nGili удачно был удален\nGili result True\nNone удачно был удален\nNone result False\n\n\n\n\n__complex__\nЭтот метод будет вызваться при передече экземпляра класса в функцию complex. complex производит преведение переданного объекта к типу complex. В данном случае я в преобразование заложил чтобы действительная и мнимая части числа были равны числу букв в марке автомобиля.\n\ncomplex(Auto(\"Lexus\"))\n\nLexus удачно был удален\n\n\n(5+5j)\n\n\n\n\n__float__\nЭтот метод будет вызваться при предаче экземпляра класса в функцию float. float произодит приведение переданного объекта к типу float. В данном случае я в преобразование заложил, чтобы возвращалось число символов в поле marka преобарзованное к типу float\n\nfloat(Auto(\"Hyundai\"))\n\nHyundai удачно был удален\n\n\n7.0\n\n\n\n\n__int__\nЭтот метод будет вызываться при передаче экземпляра класса в функцию int. int производит приведенение переданного объекта к типу int. В данном случае я в преобразование заложил, чтобы приведение означало подсчет числа символов в марке автомобиля.\n\nint(Auto(\"Mitsubishi\"))\n\nMitsubishi удачно был удален\n\n\n10\n\n\n\n\n__str__\nЭтот метод будет вызываться при передаче экземпляра класса в функцию str. str производит приведение переданного объекта к строковому типу. В данном случае я в преобразование заложил, чтобы приведение просто возвращало марку автомобиля.\n\nstr(Auto(\"Honda\"))\n\nHonda удачно был удален\n\n\n'Honda'"
  },
  {
    "objectID": "python/basics/class_spesials/python_class_specials.html#методы-для-работы-с-индексами",
    "href": "python/basics/class_spesials/python_class_specials.html#методы-для-работы-с-индексами",
    "title": "Источники информации",
    "section": "Методы для работы с индексами",
    "text": "Методы для работы с индексами\n\n__setitem__\nМетод, который будет вызваться при использовании оператора [] с присвоением. В метод __setitem__ должен содерать два аргумента: 1. индекс - объект указонный в скобках; 2. значение - присваимое заначение (после оператора =).\nБыл заложен смысл извлечения замены символа в марке автомобиля.\n\nmaz_car = Auto(\"Maserati\")\nmaz_car[3] = \"t\"\nprint(maz_car)\n\nMastrati удачно был удален\nMastrati\n\n\n\n\n__getitem__\nМетод, который будет вызваться при использовании оператора [], для извелечения значения. В данном случае, индекс используется как индекс марки автомобиля.\n\nbug_car = Auto(\"Bugatti\")\nbug_car[:3]\n\n'Bug'\n\n\n\n\n__delitem__\nМетод, который будет вызываться при использовании оператора [] вместе с оператором del. В данном случае, из марки автомобиля\n\nbmw_car = Auto(\"BMW\")\ndel bmw_car[1]\nbmw_car.marka\n\nBW удачно был удален\n\n\n'BW'\n\n\n\n\nМножественный индекс\nПри передаче множественного индекса, функции __setitem__, __getitem__ и __delitem__ в аргуметы соответвующие индексу получают картеж.\n\nind_exmpl = indexer_example()\n\nind_exmpl[3,4,5,\"str index\"]\n\nпришёл инедкс (3, 4, 5, 'str index')\n\n\n\n\nОпретор : внутри индекса\nПри использовании : внутри индекса в методы отвечающие за управление поведением класса придет slice\n\nind_exmpl = indexer_example()\n\nind_exmpl[:3]\n\nпришёл инедкс slice(None, 3, None)"
  },
  {
    "objectID": "python/basics/class_spesials/python_class_specials.html#протокол-итерации",
    "href": "python/basics/class_spesials/python_class_specials.html#протокол-итерации",
    "title": "Источники информации",
    "section": "Протокол итерации",
    "text": "Протокол итерации\nИли методы которые делают экземпляры класса iterable.\n\n__iter__\nВызывается для того, что-бы “предупредить” объект, что по нему будут итерироваться - можно провести некоторый процессинг, который будет готовить этот класс к итерированию по нему. Будет вызван при: - Передаче экземпляра в функцию iter(); - При использовании после оператора in в цикле for.\nОжидается возврат любого объекта у которого переопределен метод __next__. Чаще всего возврящают self но не всегда.\n\n\n__next__\nОпределяет что класс будет возвращать при каждой следующей итерации по нему. Будет вызван при: - Передаче экземпляра в фнукцию next(); - При каждой идерации цикла for по объекту.\nВозвращать следует, то что должно попасть в теририрующую переменную на этой итерации. В момент, когда требуется прeкратить процесс итерирования следует использовать raise StopIteration.\n\n\nБазовый пример\nЭти операторы лучше рассматривать в комбинации, потому общий пример для них:\n\nclass ar_progression_shower:\n    '''\n        Класс имплементирует расчет элементов\n        арифметической прогрессии до определенного\n        наблюдения в прогрессии\n    '''\n    def __init__(self, a0 = 0, n = 5, d = 3):\n        self.n = n\n        self.a0 = a0\n        self.d = d\n        \n    def __iter__(self):\n        print(\"был вызван __iter__\")\n        self.i = 0\n        self.curr_a = self.a0\n        return self\n    \n    def __next__(self):\n        print(\"был вызван __next__\")\n        if self.i &lt; self.n:\n            self.curr_a = self.a0 + self.d*self.i\n            self.i += 1\n            return \"{} : {}\".format(self.i-1, self.curr_a)\n        else:\n            print(\"вызван StopIteration\")\n            raise StopIteration\n        \n        \n        \nexample_iter = ar_progression_shower()\n\nМожно использовать методы iter() и next().\n\niter(example_iter)\n\nprint(next(example_iter))\nprint(next(example_iter))\nprint(next(example_iter))\nprint(next(example_iter))\nprint(next(example_iter))\n\nбыл вызван __iter__\nбыл вызван __next__\n0 : 0\nбыл вызван __next__\n1 : 3\nбыл вызван __next__\n2 : 6\nбыл вызван __next__\n3 : 9\nбыл вызван __next__\n4 : 12\n\n\nНо в случае, если вывалиться за допустииое число итераций. Вывод будет следующий.\n\niter(example_iter)\n\nprint(next(example_iter))\nprint(next(example_iter))\nprint(next(example_iter))\nprint(next(example_iter))\nprint(next(example_iter))\nprint(next(example_iter))\n\nбыл вызван __iter__\nбыл вызван __next__\n0 : 0\nбыл вызван __next__\n1 : 3\nбыл вызван __next__\n2 : 6\nбыл вызван __next__\n3 : 9\nбыл вызван __next__\n4 : 12\nбыл вызван __next__\nвызван StopIteration\n\n\nStopIteration: \n\n\nВсе те-же самые результаты при использовании цикла.\n\nfor val in example_iter:\n    print(val)\n\nбыл вызван __iter__\nбыл вызван __next__\n0 : 0\nбыл вызван __next__\n1 : 3\nбыл вызван __next__\n2 : 6\nбыл вызван __next__\n3 : 9\nбыл вызван __next__\n4 : 12\nбыл вызван __next__\nвызван StopIteration\n\n\n\ntype(iter(example_iter))\n\nбыл вызван __iter__\n\n\n__main__.ar_progression_shower\n\n\n\n\nФишки\n\nБез __next__\n\nclass test:\n    def __iter__(self):\n        return self\n    \niter(test())\n\nTypeError: iter() returned non-iterator of type 'test'\n\n\n\nclass test:\n    def __next__(self):\n        return 0\n    def __iter__(self):\n        return self\n    \niter(test())\n\n&lt;__main__.test at 0x7f081022c220&gt;\n\n\n\n\nlist/tuple/dict как результат __iter__\nСами по себе эти пипы не переопределяют __next__, как следвие, вернуть их как из __iter__ не получится.\n\nclass test():\n    def __iter__(self):\n        return [10, 20, 30, 40]\n    \niter(test())\n\nTypeError: iter() returned non-iterator of type 'list'\n\n\n\nclass test():\n    def __iter__(self):\n        return (10, 20, 30, 40)\n    \niter(test())\n\nTypeError: iter() returned non-iterator of type 'tuple'\n\n\n\nclass test():\n    def __iter__(self):\n        return {\"a\":10, \"b\":20, \"c\":30, \"d\":40}\n    \niter(test())\n\nTypeError: iter() returned non-iterator of type 'dict'\n\n\nНо можно вернуть результат передачи их функции iter(). Обращею отдельное внимание на типы данных результатов.\n\nclass test():\n    def __iter__(self):\n        return iter([10, 20, 30, 40])\n    \ntype(iter(test()))\n\nlist_iterator\n\n\n\nclass test():\n    def __iter__(self):\n        return iter((10, 20, 30, 40))\n    \ntype(iter(test()))\n\ntuple_iterator\n\n\n\nclass test():\n    def __iter__(self):\n        return iter({\"a\":10, \"b\":20, \"c\":30, \"d\":40})\n    \ntype(iter(test()))\n\ndict_keyiterator"
  },
  {
    "objectID": "python/basics/class_spesials/python_class_specials.html#другие-методы",
    "href": "python/basics/class_spesials/python_class_specials.html#другие-методы",
    "title": "Источники информации",
    "section": "Другие методы",
    "text": "Другие методы\n\n__init__\nБудет вызван при создании экземпляра класса\n\nAuto(\"mersedes\").marka\n\nmersedes удачно был удален\n\n\n'mersedes'\n\n\n\n\n__del__\nБудет вызван при удалении экземпляра класса\n\na = Auto(\"москвич\")\ndel a\n\nмосквич удачно был удален\n\n\n\n\n__call__\nВызывается при попытке “вызвать” экземпляр класса, то есть ипользование оператора () для экземплара класса.\n\nAuto(\"tesla\")()\n\ntesla удачно был удален\n\n\n'вы вызвали tesla'\n\n\n\n\n__len__\nВызывается при передаче экземпляра в функцию базовую функцию len. В данном случае я заложил возврат числа символов в марке.\n\nlen(Auto(\"Mazda\"))\n\nMazda удачно был удален\n\n\n5\n\n\n\n\n__index__\nВызывается при передаче экземпляра в одну из базовых функций bin, oct и hex. Вернуть из __index__ следует число, которое в зависимости от вызванной функции будет преобразовано соответсвенно к соответсвующей системе исчисления. В данном случае __index__ всегда возвращает число 17.\n\nmy_auto = Auto(\"Lambargini\")\n\nprint(\"Приведение к бинарному виду \", bin(my_auto))\nprint(\"Приведение к восьмеричному виду \", oct(my_auto))\nprint(\"Приведение к шестнадцатиричному виду \", hex(my_auto))\n\nLambargini удачно был удален\nПриведение к бинарному виду  0b10001\nПриведение к восьмеричному виду  0o21\nПриведение к шестнадцатиричному виду  0x11\n\n\n\n\n__round__\nВызвается при передаче экземпляра базовой функции round.\n\njeep_auto = Auto(\"Jeep\")\nround(jeep_auto).marka\n\nJeep rounded удачно был удален\n\n\n'Jeep rounded'"
  },
  {
    "objectID": "python/basics/cli_arguments/argparse.html",
    "href": "python/basics/cli_arguments/argparse.html",
    "title": "Library argparse",
    "section": "",
    "text": "Argparse is a library that allows you to describe parameters that should be passed to the program when called from the command line. There are few central features of ArgumentParser.add_argument method:\n\nTo define positional parameters of the program, use only line as first positional argument of the add_argument;\nTo define an option, add the prefix -- to the first positional argument of the add_argument;\nYou can access to the values by using ArgumentParser.parse_args().&lt;option dest&gt;.\n\nSo these features are used in the following example, that takes some arguments and prints it if needed.\n\n%%writefile argparse_files/basic_argparse.py\nimport argparse\n\nmy_parser = argparse.ArgumentParser(\n    description='Process some integers.'\n)\nmy_parser.add_argument(\n    \"positional\", \n    help = \"Example of the positional argument.\",\n)\nmy_parser.add_argument(\n    \"--option\",\n    help = \"Option that takes value.\"\n)\n\nargs = my_parser.parse_args()\nprint(\n    \"positional : \", args.positional, \"\\n\",\n    \"option : \", args.option,\n    sep = \"\"\n)\n\nOverwriting argparse_files/basic_argparse.py\n\n\nHere is the --help output for the created programme.\n\n%%bash\npython3 argparse_files/basic_argparse.py --help\n\nusage: basic_argparse.py [-h] [--option OPTION] positional\n\nProcess some integers.\n\npositional arguments:\n  positional       Example of the positional argument.\n\noptions:\n  -h, --help       show this help message and exit\n  --option OPTION  Option that takes value.\n\n\nHere is example of calling programm.\n\n%%bash\npython3 argparse_files/basic_argparse.py\\\n    positional_value\\\n    --option option_value\n\npositional : positional_value\noption : option_value"
  },
  {
    "objectID": "python/basics/cli_arguments/argparse.html#basic-usage",
    "href": "python/basics/cli_arguments/argparse.html#basic-usage",
    "title": "Library argparse",
    "section": "",
    "text": "Argparse is a library that allows you to describe parameters that should be passed to the program when called from the command line. There are few central features of ArgumentParser.add_argument method:\n\nTo define positional parameters of the program, use only line as first positional argument of the add_argument;\nTo define an option, add the prefix -- to the first positional argument of the add_argument;\nYou can access to the values by using ArgumentParser.parse_args().&lt;option dest&gt;.\n\nSo these features are used in the following example, that takes some arguments and prints it if needed.\n\n%%writefile argparse_files/basic_argparse.py\nimport argparse\n\nmy_parser = argparse.ArgumentParser(\n    description='Process some integers.'\n)\nmy_parser.add_argument(\n    \"positional\", \n    help = \"Example of the positional argument.\",\n)\nmy_parser.add_argument(\n    \"--option\",\n    help = \"Option that takes value.\"\n)\n\nargs = my_parser.parse_args()\nprint(\n    \"positional : \", args.positional, \"\\n\",\n    \"option : \", args.option,\n    sep = \"\"\n)\n\nOverwriting argparse_files/basic_argparse.py\n\n\nHere is the --help output for the created programme.\n\n%%bash\npython3 argparse_files/basic_argparse.py --help\n\nusage: basic_argparse.py [-h] [--option OPTION] positional\n\nProcess some integers.\n\npositional arguments:\n  positional       Example of the positional argument.\n\noptions:\n  -h, --help       show this help message and exit\n  --option OPTION  Option that takes value.\n\n\nHere is example of calling programm.\n\n%%bash\npython3 argparse_files/basic_argparse.py\\\n    positional_value\\\n    --option option_value\n\npositional : positional_value\noption : option_value"
  },
  {
    "objectID": "python/basics/cli_arguments/argparse.html#metavar",
    "href": "python/basics/cli_arguments/argparse.html#metavar",
    "title": "Library argparse",
    "section": "metavar",
    "text": "metavar\nOfficial description, to my mind, isn’t really clear so here I present alternative description.\nIt’s typical for CLI utilities to show in the help how to pass values to them. For example, grep has a --label option that takes a value, and it’s help describes that you should use the syntax --label=&lt;passed value&gt; for it, which is just shown in the cell below. So here LABEL is a value that should be replaced with an argument during the call, and this type of constraint is called metavar.\n\n%%bash\ngrep --help | grep label\n\n      --label=LABEL         use LABEL as the standard input file name prefix\n\n\nIn argparse, every option that takes an argument has metavar as the upper case of dest, which is typical for cli units. But you can set your own using the metavar argument of the add_argument method. For arguments that take multiple values, you can set metavar as a tuple.\nThe following example shows the features mentioned:\n\n%%writefile argparse_files/metavar.py\nimport argparse\n\nmy_parser = argparse.ArgumentParser(\n    description='Process some integers.'\n)\nmy_parser.add_argument(\"--option\")\nmy_parser.add_argument(\"--set_metavar\", metavar = \"this_is_metavar\")\nmy_parser.add_argument(\n    \"--double_metavar\",\n    nargs=2,\n    metavar=('value1', 'value2')\n)\n\nargs = my_parser.parse_args()\nfor key, value in vars(args).items():\n    print(key, value)\n\nOverwriting argparse_files/metavar.py\n\n\nThis is what the --help of this programme looks like. Pay attention to the metavars of the options.\n\n%%bash\npython3 argparse_files/metavar.py --help\n\nusage: metavar.py [-h] [--option OPTION] [--set_metavar this_is_metavar]\n                  [--double_metavar value1 value2]\n\nProcess some integers.\n\noptions:\n  -h, --help            show this help message and exit\n  --option OPTION\n  --set_metavar this_is_metavar\n  --double_metavar value1 value2\n\n\nAnd the example of the execution of the described program:\n\n%%bash\npython3 argparse_files/metavar.py \\\n    --option option_value \\\n    --set_metavar metavar_value \\\n    --double_metavar value1 value2\n\noption option_value\nset_metavar metavar_value\ndouble_metavar ['value1', 'value2']"
  },
  {
    "objectID": "python/basics/basic_datatypes/list.html",
    "href": "python/basics/basic_datatypes/list.html",
    "title": "list",
    "section": "",
    "text": "Is an array that you can change at runtime."
  },
  {
    "objectID": "python/basics/basic_datatypes/list.html#pop---delete-by-index",
    "href": "python/basics/basic_datatypes/list.html#pop---delete-by-index",
    "title": "list",
    "section": "pop - delete by index",
    "text": "pop - delete by index\nYou can delete element by index and get it back.\n\ntest_list = [3,4,5,2]\nprint(\"Poped element\", test_list.pop(2))\nprint(\"Result list\", test_list)\n\nPoped element 5\nResult list [3, 4, 2]\n\n\nBy default it deltes the first element.\n\ntest_list = [5,6,4,3]\nprint(\"Poped elemnt\", test_list.pop())\nprint(\"Result list\", test_list)\n\nPoped elemnt 3\nResult list [5, 6, 4]"
  },
  {
    "objectID": "python/basics/basic_datatypes/list.html#remove---delete-by-value",
    "href": "python/basics/basic_datatypes/list.html#remove---delete-by-value",
    "title": "list",
    "section": "remove - delete by value",
    "text": "remove - delete by value\nYou can delete element by value.\n\ntest_list = [4,5,4,5]\nprint(\"initial list\", test_list)\ntest_list.remove(4)\nprint(\"resutl list\", test_list)\n\ninitial list [4, 5, 4, 5]\nresutl list [5, 4, 5]\n\n\nNote that if there’s no removing value in the source list, you’ll have an error.\n\ntest_list = [5,3,4,1]\ntest_list.remove(6)\n\nValueError: list.remove(x): x not in list"
  },
  {
    "objectID": "python/basics/basic_datatypes/list.html#index---index-of-value",
    "href": "python/basics/basic_datatypes/list.html#index---index-of-value",
    "title": "list",
    "section": "index - index of value",
    "text": "index - index of value\nUsing this method you can get the index by which you can get the first occurrence of this element in the list.\nIn the following example I ask .index(3) and got 2 as a result because the first occurrence of number 3 in the 2 index.\nNote any other entry of the element will be ignored, just like the second 3 value with 4 index in the example.\n\ntest_list = [1,2,3,4,3,5]\ntest_list.index(3)\n\n2"
  },
  {
    "objectID": "python/basics/basic_datatypes/list.html#appned-vs",
    "href": "python/basics/basic_datatypes/list.html#appned-vs",
    "title": "list",
    "section": "appned vs +",
    "text": "appned vs +\nThere are two common ways to extend a Python list: by using the append method and the + operator. There is a significant difference: + creates a new list and copies operands to it, but append changes the initial list.\nThe following example:\n\nPrints “Initial list: ”;\nPrints id of list modified with + operator - it has different id with initial list;\nPrints the id of the list modified with the append method - it has the same id as the initial list.\n\n\ninternal_list = [1,2,3,4,5]\nprint(\"Initial list:\", id(internal_list))\nprint(\"Plus list:\", id(internal_list + [3]))\ninternal_list.append(3)\nprint(\"Append list:\", id(internal_list))\n\nInitial list: 139771686692224\nPlus list: 139771686592256\nAppend list: 139771686692224"
  },
  {
    "objectID": "python/basics/basic_datatypes/list.html#drop-duplicates",
    "href": "python/basics/basic_datatypes/list.html#drop-duplicates",
    "title": "list",
    "section": "drop duplicates",
    "text": "drop duplicates\n\nlist(set(&lt;input&gt;))\nThe easiest way is to convert the input list to set and then back to list. set can’t have duplicates, so they’re dropped.\nNote this option does not save the original order of the items in the list, because the `set’ data type ignores the order of the items.\nThe following example allows you to obtain unique values, but despite the fact that the elements in the original list are not in ascending order, they are in the result of the transformation.\n\ntest_list = [1,4,4,2,2,3,3]\nlist(set(test_list))\n\n[1, 2, 3, 4]\n\n\n\n\nSaving the order\nIt is possible to loop through the elements of the transformed list and check if the result of the item method for the selected element corresponds to the iteration number:\n\nIf the iteration number and the result of the item method match, it means that this is the first occurrence of this element and it should be included in the result;\nOtherwise, it means that this item has already been included in the result earlier and it should not be added to the result.\n\nThe following cell shows a possible implementation of the described algorithm. As a result, 4 appears immediately after 1, as in the original list.\n\ntest_list = [1,4,4,2,2,3,3]\n[val for i, val in enumerate(test_list) if test_list.index(val) == i]\n\n[1, 4, 2, 3]"
  },
  {
    "objectID": "python/basics/basic_datatypes/str.html",
    "href": "python/basics/basic_datatypes/str.html",
    "title": "str",
    "section": "",
    "text": "This data type describes the variables in which you want to store the text information."
  },
  {
    "objectID": "python/basics/basic_datatypes/str.html#istitle---is-first-capital",
    "href": "python/basics/basic_datatypes/str.html#istitle---is-first-capital",
    "title": "str",
    "section": "istitle - is first capital",
    "text": "istitle - is first capital\nLets you check if all words in the calling string are capitalised.\nThe following example shows how it works for\n\nLower case only;\nOne capitalised word;\nLine with several capitalised words;\nLine with several words but only the first one is capitalised.\n\n\ntest_lines =[\n    \"test line\",\n    \"Test\",\n    \"Test Line\",\n    \"Test line\"\n]\n\nfor test_line in test_lines:\n    print(f\"\\\"{test_line}\\\".istitle() -&gt; {test_line.istitle()}\")\n\n\"test line\".istitle() -&gt; False\n\"Test\".istitle() -&gt; True\n\"Test Line\".istitle() -&gt; True\n\"Test line\".istitle() -&gt; False"
  },
  {
    "objectID": "python/basics/basic_datatypes/tuple.html",
    "href": "python/basics/basic_datatypes/tuple.html",
    "title": "tuple",
    "section": "",
    "text": "Is an array that can’t be changed at runtime."
  },
  {
    "objectID": "python/basics/basic_datatypes/tuple.html#one-element-tuple",
    "href": "python/basics/basic_datatypes/tuple.html#one-element-tuple",
    "title": "tuple",
    "section": "One element tuple",
    "text": "One element tuple\nTo create one element tuple you have to use the syntax (&lt;element&gt;,) not (&lt;element&gt;). By default, parentheses are not used to create a tuple, but to prioritise executions in the expression, so you have to use at least one comma inside the parentheses to tell the interpreter that it is the tuple.\n\nprint(\"Just a variable\", (4))\nprint(\"One element tuple\", (5,))\n\nJust a variable 4\nOne element tuple (5,)"
  },
  {
    "objectID": "python/basics/paths_settings/pythonpath.html",
    "href": "python/basics/paths_settings/pythonpath.html",
    "title": "Python path",
    "section": "",
    "text": "Sometimes it’s hard to export Python files from other Python files or Jupyter notebooks. Usually problem related to list of folders where Python interpreter tends to search for names."
  },
  {
    "objectID": "python/basics/paths_settings/pythonpath.html#sys.path",
    "href": "python/basics/paths_settings/pythonpath.html#sys.path",
    "title": "Python path",
    "section": "sys.path",
    "text": "sys.path\nIs a list that contains all folders where interpreter will find names you try to export.\n\nThis jupyter\nIn the following example I have listed mine. The first is the folder where this notebook was started.\n\nimport sys\nsys.path\n\n['/home/fedor/Documents/knowledge/python/basics/paths_settings',\n '/usr/lib/python310.zip',\n '/usr/lib/python3.10',\n '/usr/lib/python3.10/lib-dynload',\n '',\n '/home/fedor/.local/lib/python3.10/site-packages',\n '/usr/local/lib/python3.10/dist-packages',\n '/usr/lib/python3/dist-packages']\n\n\n\n\nCLI\nBut let’s try the same trick with a Python file run from the CLI.\nThe following cell shows the code of the file I’m going to run.\n\n%%writefile python_path_files/python_path_files.py\nimport sys\nfor p in sys.path:\n    print(p)\n\nOverwriting print_sys_folder/print_sys_folder.py\n\n\nRun the code from bash - as a result, the first item of the lsit contains the same folder as file in.\n\n!python3 python_path_files/python_path_files.py\n\n/home/fedor/Documents/knowledge/python/basics/paths_settings/print_sys_folder\n/usr/lib/python310.zip\n/usr/lib/python3.10\n/usr/lib/python3.10/lib-dynload\n/home/fedor/.local/lib/python3.10/site-packages\n/usr/local/lib/python3.10/dist-packages\n/usr/lib/python3/dist-packages\n\n\n\n\nAdd item\nThe simplest way to solve file access problems is to add the appropriate directory to the os.path list.\nIn the following cell I just create a python file that cantain only one print - printing the message marks that the module has been successfully loaded. It is created in the folder python_path_files/some_hidden_module so that files from python_path_files cannot see it.\n\n%%writefile python_path_files/some_hidden_module/test_module.py\nprint(\"Test module successfully loaded!\")\n\nOverwriting python_path_files/some_hidden_module/test_module.py\n\n\nNext cell defines file in python_path_files. It will try to import file from previous cell before and after some_hiddem_module added to sys.path, in fail case it will print a corresponding message.\n\n%%writefile python_path_files/append_to_path.py\nimport os\nimport sys\n\n\ndef try_import_module():\n    try:\n        import test_module\n    except ModuleNotFoundError:\n        print(\"Module import went wrong!\")\n\ntry_import_module()\nsys.path.append(\"some_hidden_module\")\ntry_import_module()\n\nOverwriting python_path_files/append_to_path.py\n\n\nThe run of the file show:\n\nIn the first case I got an error message;\nBut after appending some_hidden_module to sys.path everything works fine.\n\n\n%%bash\ncd python_path_files\npython3 append_to_path.py\n\nModule import went wrong!\nTest module successfully loaded!"
  },
  {
    "objectID": "python/basics/paths_settings/pythonpath.html#environment-variable",
    "href": "python/basics/paths_settings/pythonpath.html#environment-variable",
    "title": "Python path",
    "section": "Environment variable",
    "text": "Environment variable\nYou can define the environment variable PYTHONPATH and list the paths with “:” - they will automatically appear in the sys.path of python running in that environment.\nSo in the following cell we have run some code that defines PYTHONPATH as two paths separated by “:”, then run a script that prints out sys.path. As a result you can see that the mentioned paths have been added to the sys.path list.\n\n%%bash\nexport PYTHONPATH=\"example/path1:example/path2\"\necho $PYTHONPATH\n\necho\necho \"=====python output=====\"\npython3 python_path_files/print_sys_folder.py\n\nexample/path1:example/path2\n\n=====python output=====\n/home/fedor/Documents/knowledge/python/basics/paths_settings/python_path_files\n/home/fedor/Documents/knowledge/python/basics/paths_settings/example/path1\n/home/fedor/Documents/knowledge/python/basics/paths_settings/example/path2\n/usr/lib/python310.zip\n/usr/lib/python3.10\n/usr/lib/python3.10/lib-dynload\n/home/fedor/.local/lib/python3.10/site-packages\n/usr/local/lib/python3.10/dist-packages\n/usr/lib/python3/dist-packages"
  },
  {
    "objectID": "python/basics/tkinter/list_box.html",
    "href": "python/basics/tkinter/list_box.html",
    "title": "Источники",
    "section": "",
    "text": "Виджет Listbox из библиотеки tkinter\n\nфайл предоставленный на курсах от belhard по программированию на python (см. в тойже папке, в которой лежит этот .ipynb);\n\n\nСодержание\n\nМетоды;\n\ninsert вставка элементов в Listbox;\nget получение элементов из Listbox;\ndelete удаление элементо из Listbox;\ncurselection\n\nПоля.\n\n\n\nПодготовка\n\nfrom tkinter import *\n\ndef get_l_box():\n    '''\n        Для того, чтобы кратко создавать окно с `Listbox` внутри сделал функию.\n    '''\n    root = Tk()\n\n    lst_box = Listbox()\n    lst_box.pack()\n\n    return root, lst_box\n\ndef insert_123(lst_box):\n    '''\n        Для быстрой вставки\n        one\n        two \n        three\n        в переданный listbox\n    '''\n    for i in (\"one\", \"two\", \"three\"):\n        lst_box.insert(0, i)\n\n\n\nМетоды\n\ninsert\nПроизводит вставку элементов в Listbox\nАгрументы:\n\nИдекс вставки;\n\n\nroot, lst_box = get_l_box()\n\nfor i in (\"one\", \"two\", \"three\"):\n    lst_box.insert(0, i)\n    \nroot.mainloop()\n\n\n\n\nget\nПроизводит извлечение элементов из Listbox\nАргументы для извлечения одного элемента:\n\nиндекс или срез извлекаемых элементов;\n\n\nroot, lst_box = get_l_box()\ninsert_123(lst_box)\n    \nprint(\"Извлечение из индекса '1'\", lst_box.get(1))\n\nroot.mainloop()\n\nИзвлечение из индекса '1' two\n\n\nАргументы для извлечения слеза:\n\nпервый индекс среза;\nпоследний индекс среза.\n\n\nroot, lst_box = get_l_box()\ninsert_123(lst_box)\n    \nprint(\"Извлечение среза с от 0 до 1\", lst_box.get(0,1))\n\nroot.mainloop()\n\nИзвлечение среза с от 0 до 1 ('three', 'two')\n\n\n\n\ndelete\nПроизводит удаление по индексу или срезу.\nАргументы для удаления по индексу:\n\nИндекс удаляемого элемента.\n\n\nroot, lst_box = get_l_box()\ninsert_123(lst_box)\n    \nlst_box.delete(2)\n\nroot.mainloop()\n\n\nАргументы для удаления среза:\n\nпервый индекс среза;\nпоследний индекс среза.\n\n\nroot, lst_box = get_l_box()\ninsert_123(lst_box)\n    \nlst_box.delete(1,2)\n\nroot.mainloop()\n\n\n\n\ncurselection\nПолучение в виде картежа выделенных индексов.\n\nroot, lst_box = get_l_box()\ninsert_123(lst_box)\nlst_box[\"selectmode\"] = EXTENDED\n\ndef print_sel_ind():\n    sel_lab[\"text\"] = lst_box.curselection()\n\nButton(\n    text = \"Вывести индексы\",\n    command = print_sel_ind\n).pack()\n\nsel_lab = Label()\nsel_lab.pack()\n\nroot.mainloop()\n\n\n\n\n\nПоля\n\nselectmode\nОпределяет будет доступен ли множествнный выбор. В следующем примере создаются 2, ListBox в одном из которых доступен множественный выбор во втором нет.\n\nroot, lst_box1 = get_l_box()\nlst_box2 = Listbox(selectmode = EXTENDED)\nlst_box2.pack()\n\nfor i in (\"one\", \"two\", \"three\"):\n    lst_box1.insert(0, i)\n    lst_box2.insert(0,i)\n\nroot.mainloop()"
  },
  {
    "objectID": "python/basics/regex/regex.html",
    "href": "python/basics/regex/regex.html",
    "title": "Источники",
    "section": "",
    "text": "Регулярные выражения (regex)\nТут разберем реализацию регулярных выражений в python. Вообще, конечно, регулярные выражения используются во многих языках программирования и выходят за контекст python. Но никаким языком программирования я и близко так не владею, как владею python (на сегодняшний день). Потому тут будут разбораны и общие идеи лежащие в синтаксисе регулярных выражений. Разбор общих элементов регулярных выражений я надеюсь вынести на уровень выше.\n\nhttps://docs.python.org/3/howto/regex.html#regex-howto статья в официальной докумментации python в которой описаны принципы использования regex."
  },
  {
    "objectID": "python/basics/numpy/set_printoptions.html",
    "href": "python/basics/numpy/set_printoptions.html",
    "title": "Источники информации",
    "section": "",
    "text": "В numpy, как оказалось можно настроить вывод инофрмации на в консоль, делается это с помощью функции set_printoptions, которой и посвящена эта книжка, далее я постепенно разбираю аргументы функции, распространенные и интерестные случаи из моей практики связанные с этой функцией.\n\nimport numpy as np\n\nnp.random.seed(1)\n# эксперементальный набор данных, который будет исопльзоваться для описания большинства примеров\narr = np.random.uniform(-10, 10, 20)\n\n\nhttps://numpy.org/doc/stable/reference/generated/numpy.set_printoptions.html. \n\n\nДалее разоран каждый аргумент:\n\nprecision;\nthreshold;\nedgeitems;\nlinewidth;\nsuppress;\nnanstr;\ninfstr;\nsing;\nformatter;\nfloatmode;\nlegacy.\n\n\n\nprecision\nПозволяет настроить сколько знаков после запятой numpy будет отображать. По умолчанию 8.\n\nprint(\"precision = 3\")\nnp.set_printoptions(\n    precision = 3\n)\nprint(arr)\nprint(\"precision = 5\")\nnp.set_printoptions(\n    precision = 5\n)\nprint(arr)\n\nprecision = 3\n[-1.66   4.406 -9.998 -3.953 -7.065 -8.153 -6.275 -3.089 -2.065  0.776\n -1.616  3.704 -5.911  7.562 -9.452  3.409 -1.654  1.174 -7.192 -6.038]\nprecision = 5\n[-1.65956  4.40649 -9.99771 -3.95335 -7.06488 -8.15323 -6.2748  -3.08879\n -2.06465  0.77633 -1.61611  3.70439 -5.91096  7.56235 -9.45225  3.40935\n -1.6539   1.1738  -7.19226 -6.03797]\n\n\n\n\nthreshold\nПозволяет настроить насколько большие массивы будут выводиться без “сокращения”. (По умолчанию 1000)\n\nprint(\"threshold = 20; приведёт к полному отображению массива\")\nnp.set_printoptions(threshold = 20)\nprint(arr)\nprint(\"threshold = 19; приведёт к сокращенному отображению массива\")\nnp.set_printoptions(threshold = 19)\nprint(arr)\n\nthreshold = 20; приведёт к полному отображению массива\n[-1.65956  4.40649 -9.99771 -3.95335 -7.06488 -8.15323 -6.2748  -3.08879\n -2.06465  0.77633 -1.61611  3.70439 -5.91096  7.56235 -9.45225  3.40935\n -1.6539   1.1738  -7.19226 -6.03797]\nthreshold = 19; приведёт к сокращенному отображению массива\n[-1.65956  4.40649 -9.99771 ...  1.1738  -7.19226 -6.03797]\n\n\n\n\nedgeitems\nУкажет сколько в сокращенном отображении массива должно элементов отображаться с каждой стороны. По умолчанию 3.\n\nnp.set_printoptions(\n    edgeitems=3,\n    threshold=0 #для того, \n    # чтобы заставить любой массив\n    # отображаться в сокращенной форме\n)\nprint(\"edgeitems=3\")\nprint(arr)\nnp.set_printoptions(\n    edgeitems=5\n)\nprint(\"edgeitems=5\")\nprint(arr)\n\nedgeitems=3\n[-1.65956  4.40649 -9.99771 ...  1.1738  -7.19226 -6.03797]\nedgeitems=5\n[-1.65956  4.40649 -9.99771 -3.95335 -7.06488 ...  3.40935 -1.6539\n  1.1738  -7.19226 -6.03797]\n\n\n\n\nlinewidth\nСколько символов будет отображено в каждой строке, по умолчанию используется 75. [ и \\n тоже учитываются. Потому всегда получается на 3 символа из массива меньше. Похоже результат приблизительный, потому как указанное linewidth может быть не кратно числу символов матрицы. Более того, строки бывают разные - минусы, и значение аргумента precision будут влиять на длинну строк.\nВот пару примеров\nlinewidth = 40\n\nnp.set_printoptions(\n    linewidth = 40,\n    threshold=1000 # для того, чтобы вне зависимости\n    # от предыдущих дейсвий отображался полный массив\n)\nprint(arr)\n\nprint(\"Число символов по строкам (\\\\n был удален)\")\nmatr_strs = str(arr).split(\"\\n\")\n[len(s) for s in matr_strs]\n\n[-1.65956  4.40649 -9.99771 -3.95335\n -7.06488 -8.15323 -6.2748  -3.08879\n -2.06465  0.77633 -1.61611  3.70439\n -5.91096  7.56235 -9.45225  3.40935\n -1.6539   1.1738  -7.19226 -6.03797]\nЧисло символов по строкам (\\n был удален)\n\n\n[36, 36, 36, 36, 37]\n\n\nlinewidth = 20\n\nnp.set_printoptions(\n    linewidth = 20,\n    threshold=1000 # для того, чтобы вне зависимости\n    # от предыдущих дейсвий отображался полный массив\n)\nprint(arr)\n\nprint(\"Число символов по строкам (\\\\n был удален)\")\nmatr_strs = str(arr).split(\"\\n\")\n[len(s) for s in matr_strs]\n\n[-1.65956  4.40649\n -9.99771 -3.95335\n -7.06488 -8.15323\n -6.2748  -3.08879\n -2.06465  0.77633\n -1.61611  3.70439\n -5.91096  7.56235\n -9.45225  3.40935\n -1.6539   1.1738\n -7.19226 -6.03797]\nЧисло символов по строкам (\\n был удален)\n\n\n[18, 18, 18, 18, 18, 18, 18, 18, 17, 19]\n\n\n\n\nsuppress\nОпределяет, будет ли использована экспоненциальная форма записи числа. В случае True числа всегда будут печататьсяв привычном формате, будучи сокращенными в соответствии со значением аргумента precision. В случае False, если в массиве находится число менее 0.0001 или отношение максимального и модуля минимального элемента массива составляет более 1000, весь массив будет напечатан в экспоненциальной форме. По умолчанию всегда False.\n\nОдно маленькое число (&lt;0.0001)\n\nunavailible_number = np.array([0.000099])\navailible_number = np.array([0.0001])\n\nnp.set_printoptions(\n    suppress = True,\n    linewidth = 20, # это для того, чтобы числа\n    # помещались в вывод\n    precision = 8 # это чтобы числа, которые я указал\n    # не округлились автоматически\n)\n\nprint(\"=================suppress = True================\")\nprint(\"availible number\", availible_number)\nprint(\"unavailible number\", unavailible_number, end = \"\\n\\n\\n\")\n\nnp.set_printoptions(suppress = False)\n\nprint(\"=================suppress = False================\")\nprint(\"availible number\", availible_number)\nprint(\"unavailible number\", unavailible_number)\n\n=================suppress = True================\navailible number [0.0001]\nunavailible number [0.000099]\n\n\n=================suppress = False================\navailible number [0.0001]\nunavailible number [9.9e-05]\n\n\n\n\nОтношение максимального числа к минимальному составляет более 1000\n\navailible_arr = np.array([1.0, 1000])\nunavailible_arr = np.array([1.0, 1001])\n\nnp.set_printoptions(\n    suppress = True,\n    linewidth = 20, # это для того, чтобы числа\n    # помещались в вывод\n    precision = 8 # это чтобы числа, которые я указал\n    # не округлились автоматически\n)\n\nprint(\"=================suppress = True==================\")\nprint(\"unavailible arr\", unavailible_arr)\nprint(\"availible arr\", availible_arr, end=\"\\n\\n\\n\")\n\nnp.set_printoptions(suppress = False)\nprint(\"=================suppress = False==================\")\nprint(\"unavailible arr\", unavailible_arr)\nprint(\"availible arr\", availible_arr, end=\"\\n\\n\\n\")\n\n=================suppress = True==================\nunavailible arr [   1. 1001.]\navailible arr [   1. 1000.]\n\n\n=================suppress = False==================\nunavailible arr [1.000e+00\n 1.001e+03]\navailible arr [   1. 1000.]\n\n\n\n\n\n\nЗаметим, что это работет только для массивов типа float\n\nnp.set_printoptions(suppress = False)\n\nprint(\"int не меняются\")\nints_arr = np.array([1, 1001])\nprint(ints_arr, end = \"\\n\\n\\n\")\n\nfloats_arr = np.array([1.0, 1001])\nprint(\"float меняется\")\nprint(floats_arr)\n\nint не меняются\n[   1 1001]\n\n\nfloat меняется\n[1.000e+00\n 1.001e+03]\n\n\n\n\n\nnanstr\nПозволяет настроить как будет отображен np.NaN в коммандном окне. По умолчанию 'nan'.\n\nnp.set_printoptions(nanstr = \"Нет данных\")\n\nnan_arr = np.fromfunction(\n    lambda i, j: np.where(\n        (i+j)%2, 0, np.NaN\n    ),\n    (4,4)\n)\n\nnan_arr\n\narray([[Нет данных,\n                0.,\n        Нет данных,\n                0.],\n       [        0.,\n        Нет данных,\n                0.,\n        Нет данных],\n       [Нет данных,\n                0.,\n        Нет данных,\n                0.],\n       [        0.,\n        Нет данных,\n                0.,\n        Нет данных]])\n\n\n\n\ninfstr\nПозволяет настроить как будет отображент np.inf в коммандном окне. По умолчанию 'inf'\n\nnp.set_printoptions(infstr = \"Бесконечность\")\n\nnan_arr = np.fromfunction(\n    lambda i, j: np.where(\n        (i+j)%2, -np.inf, np.inf\n    ),\n    (4,4)\n)\n\nnan_arr\n\narray([[ Бесконечность,\n        -Бесконечность,\n         Бесконечность,\n        -Бесконечность],\n       [-Бесконечность,\n         Бесконечность,\n        -Бесконечность,\n         Бесконечность],\n       [ Бесконечность,\n        -Бесконечность,\n         Бесконечность,\n        -Бесконечность],\n       [-Бесконечность,\n         Бесконечность,\n        -Бесконечность,\n         Бесконечность]])\n\n\n\n\nsign\nПозволяет определить как будет при выводе записан символ + для положительных дробных чисел, может принимать значения:\n\n'-' будет обозначать, что знак + следует просто опускать;\n'+' будет обозначать, что знак + будет явно записан;\n' ' будет обозначать, что знак на месте каждого знака + будет помещен ' ' (пробел).\n\nПо умолчанию используется значение '-'.\n\nnp.set_printoptions(\n    sign = '-',\n    linewidth = 10\n)\nprint(np.array([1.0]))\n\nnp.set_printoptions(sign = '+')\nprint(np.array([1.0]))\n\nnp.set_printoptions(sign = ' ')\nprint(np.array([1.0]))\n\n[1.]\n[+1.]\n[ 1.]\n\n\n\nЗамети, что это работает только для типа данных float\n\nnp.set_printoptions(sign = ' ')\nnp.array([1])\n\narray([1])\n\n\n\n\nЭто не работает когда numpy подгоняет под отображение матрицей\nВ следующем примере, по идее, не отрицательные числа должны не иметь пробелов вместо минуса, но для того, чтобы матрица была “ровная”, подставляется побел.\n\nnp.set_printoptions(\n    sign = '-',\n    linewidth = 40\n)\narr\n\narray([-1.65955991,  4.40648987,\n       -9.9977125 , -3.95334855,\n       -7.06488218, -8.1532281 ,\n       -6.27479577, -3.08878546,\n       -2.06465052,  0.77633468,\n       -1.61610971,  3.70439001,\n       -5.91095501,  7.56234873,\n       -9.45224814,  3.4093502 ,\n       -1.65390395,  1.17379657,\n       -7.19226123, -6.03797022])\n\n\n\n\n\nformatter\nПозволяет для каждого типа определить функцию как элементы этого массива будут преобразованы в функции. Функция ожидает словарь, в котором для каждого типа массива numpy может быть указан способ превращения в строку. Более подробно о всех типах можно уздать из оффициальной документации. Мы же рассмотрим только случаи показавшиеся мне интерестными:\n\nБазовый случай рассмотрим с использованием типа данных bool\n\nbool_exprimental = np.array([True, False])\ndef bool_formatter(val):\n    return \"Истинна\" if val else \"Ложь\"\n\nnp.set_printoptions(\n    formatter={'bool':bool_formatter}\n)\nprint(\"bool dtype\")\nprint(bool_exprimental)\n\nprint(\"int dtype\")\nprint(arr)\n\nbool dtype\n[Истинна Ложь]\nint dtype\n[-1.65955991  4.40648987 -9.9977125\n -3.95334855 -7.06488218 -8.1532281\n -6.27479577 -3.08878546 -2.06465052\n  0.77633468 -1.61610971  3.70439001\n -5.91095501  7.56234873 -9.45224814\n  3.4093502  -1.65390395  1.17379657\n -7.19226123 -6.03797022]\n\n\nИспользование пустого словаря переданного formatter приведёт к отмене всех установлленых функций форматирования.\n\nnp.set_printoptions(formatter={})\nprint(bool_exprimental)\n\n[ True False]\n\n\n\n\nКлюч all позволит применить этот форматер ко всем типам данных\nВ следующем примере для ключа all применяется функция которая для чисел типа numpy.float64 вернет Дробное а для любого другого числа Что-то ещё.\n\nnp.set_printoptions(\n    formatter = {\n        'all' : lambda val: \n        \"Дробное\" if type(val) == np.float64 else \"Что-то ещё\"\n    }\n)\n\nprint(\"float64 примет вид\")\nprint(arr)\nprint(\"int64 примет вид\")\nprint(np.array([0, 1, 2, 3]))\n\nfloat64 примет вид\n[Дробное Дробное Дробное Дробное\n Дробное Дробное Дробное Дробное\n Дробное Дробное Дробное Дробное\n Дробное Дробное Дробное Дробное\n Дробное Дробное Дробное Дробное]\nint64 примет вид\n[Что-то ещё Что-то ещё Что-то ещё\n Что-то ещё]\n\n\nИнтерестно, какой из ключей будет приоритетнее all или для некоторого конкретного типа.\n\nnp.set_printoptions(\n    formatter = {\n        'bool' : lambda x: \"ключ для bool\",\n        'all' : lambda x: \"ключ для all\"\n    }\n)\n\nbool_array = np.random.choice([True, False], 20)\nbool_array\n\narray([ключ для bool, ключ для bool,\n       ключ для bool, ключ для bool,\n       ключ для bool, ключ для bool,\n       ключ для bool, ключ для bool,\n       ключ для bool, ключ для bool,\n       ключ для bool, ключ для bool,\n       ключ для bool, ключ для bool,\n       ключ для bool, ключ для bool,\n       ключ для bool, ключ для bool,\n       ключ для bool, ключ для bool])\n\n\nПохоже, ключ определнного типа имеет приоритет.\n\n\n\nfloatmode\nПозволяет уточникть повередение преобразования дробных чисел в случае, если precision имеет спорное знечение. Вообще все сводится к двум случаям: - Число не может быть однозначно определено используя столько знаков; - Для однозначного определения числа слишком много знаков.\nВ следующем массиве при precision = 3 представлены оба случая.\n\nnp.set_printoptions(precision = 3)\n\ntest_arr = np.array(\n    [0.4839, 0.7]\n)\n\nДалее рассмотрим значения, которые может принимать floatmode.\n\nfixed\nОзначат безприкословное исполнение числа символов указанное, в precision. То есть неоднозначно определяемые числа будут округлены а числа с излишним количесвом знаков для однозначного определния будут дополнены 0.\n\nnp.set_printoptions(\n    floatmode = 'fixed',\n    precision = 3\n)\n\ntest_arr\n\narray([0.484, 0.700])\n\n\n\n\nunique\nОзанчает, что будет использовано ровно столько знаков, сколько требуется для однозначного определения числа. По сути, это эквивалентно полному игнорированию аргумента precision.\n\nnp.set_printoptions(\n    floatmode = 'unique'\n)\n\ntest_arr\n\narray([0.4839, 0.7   ])\n\n\n\n\nmaxprec\nИспользуется не более чем precision цифр. То есть, несмотря на неоднозначно определяемые числа используется не более precision знаков, а для числа, содеражащего менее precision цифр не происходит автодополнения нулями.\n\nnp.set_printoptions(floatmode = \"maxprec\")\ntest_arr\n\narray([0.484, 0.7  ])\n\n\n\n\nmaxprec_equal\nБудет обозначать что используется всегда не более precision знаков. Но, в случае, если ни для однозначного определния любого из чисел требуется меньше знаков чем precision то будет использовано ровно столько занков сколько требуется. При этом все числа имеют одинаковое число выводимых знаков, т.е. вслучае если число (\\(a\\)) однозначно определяется меньшим числом знаков нежели число определившие число знаков, то число \\(a\\) при выводе будет дополнено нулями.\n\nnp.set_printoptions(floatmode = \"maxprec_equal\")\nprint('требуется 4 занка для определения первого числа')\nprint(test_arr)\nprint('требуется 2 знака для определения первого числа')\nprint(np.array([0.55, 0.5]))\n\nтребуется 4 занка для определения первого числа\n[0.484 0.700]\nтребуется 2 знака для определения первого числа\n[0.55 0.50]\n\n\n\n\n\nlegacy\nИспользуется для ограницазции поддержки старого когда. Может быть использован для возврата к версиям:\n\n1.13;\n1.21;\nFalse - гворит о том, что не надо использовать legacy.\n\n\n1.13\nИз документации удалось выяснить только что предыдущие версии отличаются заменой знака на пропуск для полижительного числа. В документации упоминаются и прочие особенности отображения для нульмерных массивов, которые до сих поа для меня остаются загадкой.\n\ntest_arr = np.array([1.123456789])\n\nnp.set_printoptions(legacy = False)\nprint(test_arr)\n\nnp.set_printoptions(legacy = '1.13')\nprint(test_arr)\n\n[1.123]\n[ 1.123]\n\n\n\n\n1.21\nУтвержается об сособенностях в отображении для сложных структурных типов. Но пока о об таких типах мне тоже не известно, потому, пока просто будем иметь эту возможность ввиду.\n\ntest_arr = np.array(\n    [('Rex', 9, 81.0), ('Fido', 3, 27.0)],\n    dtype=[('name', 'U10'), ('age', 'i4'), ('weight', 'f4')]\n)\n\nnp.set_printoptions(legacy = False)\nprint(test_arr)\n\nnp.set_printoptions(legacy = '1.21')\nprint(test_arr)\n\n[('Rex', 9, 81.) ('Fido', 3, 27.)]\n[('Rex', 9, 81.) ('Fido', 3, 27.)]"
  },
  {
    "objectID": "python/torch/vision/cnn_classification.html",
    "href": "python/torch/vision/cnn_classification.html",
    "title": "CNN classification",
    "section": "",
    "text": "The process of building a convolutional neural network for image classification is described here.\nimport pickle\n\nimport numpy as np\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\nimport torchvision.transforms as T\nfrom torchvision.datasets import CIFAR10\n\nimport matplotlib.pyplot as plt\n\nfrom IPython.display import clear_output\nfrom tqdm import tqdm\nimport pickle"
  },
  {
    "objectID": "python/torch/vision/cnn_classification.html#data",
    "href": "python/torch/vision/cnn_classification.html#data",
    "title": "CNN classification",
    "section": "Data",
    "text": "Data\nFor this notebook, CIFAR10 is used. So it’s loaded in the following cell.\n\ntrain_dataset = CIFAR10(\n    './cifar10', \n    train=True, \n    transform=T.ToTensor(),\n    download = True\n)\nvalid_dataset = CIFAR10(\n    './cifar10', \n    train=False,\n    transform=T.ToTensor(),\n    download=True\n)\n\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n\nSo here is some pictures each belongs to some class. Below is visualisation that contains random picutres from sample with it classes in title.\n\nnp.random.seed(15)\nindx = np.random.choice(np.arange(len(train_dataset)), 10)\nt = [train_dataset[i] for i in indx]\n\ncifar10_classes = {\n    0: 'airplane',\n    1: 'automobile',\n    2: 'bird',\n    3: 'cat',\n    4: 'deer',\n    5: 'dog',\n    6: 'frog',\n    7: 'horse',\n    8: 'ship',\n    9: 'truck'\n}\n\nplt.figure(figsize = (10, 5))\nfor i in range(len(indx)):\n\n    plt.subplot(1, len(indx), i + 1)\n    plt.title(cifar10_classes[t[i][1]])\n    plt.imshow(T.ToPILImage()(t[i][0]))\n    plt.xticks([]);plt.yticks([])\n\n\n\n\nThere are some tricks at the data level:\n\nBoth training and validation data need to be better normalised;\nFor the training data, we added some augmentations that usually make the model more robust.\n\n\nmeans = (train_dataset.data / 255).mean(axis=(0, 1, 2))\nstds = (train_dataset.data / 255).std(axis=(0, 1, 2))\n\ntrain_transforms = T.Compose(\n    [\n        T.AutoAugment(T.AutoAugmentPolicy.CIFAR10),\n        T.ToTensor(),\n        T.Normalize(mean=means, std=stds)\n    ]\n)\n\ntest_transforms = T.Compose(\n    [\n        T.ToTensor(),\n        T.Normalize(mean=means, std=stds)\n    ]\n)\n\ntrain_dataset = CIFAR10(\n    './cifar10', \n    train=True, \n    transform=train_transforms,\n    download = True\n)\nvalid_dataset = CIFAR10(\n    './cifar10', \n    train=False,\n    transform=test_transforms,\n    download=True\n)\n\nFiles already downloaded and verified\nFiles already downloaded and verified"
  },
  {
    "objectID": "python/torch/vision/cnn_classification.html#architecture",
    "href": "python/torch/vision/cnn_classification.html#architecture",
    "title": "CNN classification",
    "section": "Architecture",
    "text": "Architecture\nI borrowed the architecture from this notebook.\n\nclass SimpleResidualBlock(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(\n            in_channels=3, \n            out_channels=3, \n            kernel_size=3, \n            stride=1, \n            padding=1\n        )\n        self.relu1 = nn.ReLU()\n        self.conv2 = nn.Conv2d(\n            in_channels=3, \n            out_channels=3, \n            kernel_size=3, \n            stride=1, \n            padding=1\n        )\n        self.relu2 = nn.ReLU()\n        \n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.relu1(out)\n        out = self.conv2(out)\n        return self.relu2(out) + x\n\ndef conv_block(in_channels, out_channels, pool=False):\n    layers = [\n        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n        nn.BatchNorm2d(out_channels), \n        nn.ReLU(inplace=True)\n    ]\n    if pool: layers.append(nn.MaxPool2d(2))\n    return nn.Sequential(*layers)\n\nclass Model(nn.Module):\n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n        \n        self.conv1 = conv_block(in_channels, 64)\n        self.conv2 = conv_block(64, 128, pool=True)\n        self.res1 = nn.Sequential(\n            conv_block(128, 128), \n            conv_block(128, 128)\n        )\n        \n        self.conv3 = conv_block(128, 256, pool=True)\n        self.conv4 = conv_block(256, 512, pool=True)\n        self.res2 = nn.Sequential(\n            conv_block(512, 512), \n            conv_block(512, 512)\n        )\n        \n        self.classifier = nn.Sequential(\n            nn.MaxPool2d(4), \n            nn.Flatten(), \n            nn.Linear(512, num_classes)\n        )\n        \n    def forward(self, xb):\n        out = self.conv1(xb)\n        out = self.conv2(out)\n        out = self.res1(out) + out\n        out = self.conv3(out)\n        out = self.conv4(out)\n        out = self.res2(out) + out\n        out = self.classifier(out)\n        return out"
  },
  {
    "objectID": "python/torch/vision/cnn_classification.html#fitting",
    "href": "python/torch/vision/cnn_classification.html#fitting",
    "title": "CNN classification",
    "section": "Fitting",
    "text": "Fitting\nHere is everything we needed to make the model fit. Everything is quite simple except learning the scheduler. It’s a tool that allows to reduce the steps of the optimiser to get better optimisation results.\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n# model = Model().to(device)\nmodel = Model(3, 10).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma = 0.5)\nloss_fn = nn.CrossEntropyLoss()\n\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=256, \n    shuffle=True, \n    num_workers=4, \n    pin_memory=True\n)\nvalid_loader = DataLoader(\n    valid_dataset, \n    batch_size=256, \n    shuffle=False, \n    num_workers=4, \n    pin_memory=True\n)\n\ndef train(model) -&gt; tuple[float, float]:\n    model.train()\n\n    train_loss = 0\n    total = 0\n    correct = 0\n\n    for x, y in tqdm(train_loader, desc='Train'):\n        x, y = x.to(device), y.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(x)\n\n        loss = loss_fn(output, y)\n\n        train_loss += loss.item()\n\n        loss.backward()\n\n        optimizer.step()\n        \n        _, y_pred = torch.max(output, 1)\n        total += y.size(0)\n        correct += (y_pred == y).sum().item()\n\n    train_loss /= len(train_loader)\n    accuracy = correct / total\n\n    return train_loss, accuracy\n\n\n@torch.inference_mode()\ndef evaluate(model, loader) -&gt; tuple[float, float]:\n    model.eval()\n\n    total_loss = 0\n    total = 0\n    correct = 0\n\n    for x, y in tqdm(loader, desc='Evaluation'):\n        x, y = x.to(device), y.to(device)\n\n        output = model(x)\n\n        loss = loss_fn(output, y)\n\n        total_loss += loss.item()\n\n        _, y_pred = torch.max(output, 1)\n        total += y.size(0)\n        correct += (y_pred == y).sum().item()\n\n    total_loss /= len(loader)\n    accuracy = correct / total\n\n    return total_loss, accuracy\n\ndef plot_stats(\n    train_loss: list[float],\n    valid_loss: list[float],\n    train_accuracy: list[float],\n    valid_accuracy: list[float],\n    title: str\n):\n    plt.figure(figsize=(15, 18))\n\n    plt.subplot(211)\n    plt.title(title + ' loss')\n    plt.plot(train_loss, label='Train loss')\n    plt.plot(valid_loss, label='Valid loss')\n    plt.legend()\n    plt.grid()\n\n\n    plt.subplot(212)\n    plt.title(title + ' accuracy')\n    plt.plot(train_accuracy, label='Train accuracy')\n    plt.plot(valid_accuracy, label='Valid accuracy')\n    plt.legend()\n    plt.grid()\n\nThis model fitting requires a noticeable amount of computation, but we have calculated everything and saved the results, the next cell just shows the code used for training.\nhistory = {\n    \"train_loss\" : [],\n    \"valid_loss\" : [],\n    \"train_accuracy\" : [],\n    \"valid_accuracy\" : []\n}\nfor epoch in range(50):\n    train_loss, train_accuracy = train(model)\n    valid_loss, valid_accuracy = evaluate(model, valid_loader)\n\n    history[\"train_loss\"].append(train_loss)\n    history[\"valid_loss\"].append(valid_loss)\n\n    history[\"train_accuracy\"].append(train_accuracy)\n    history[\"valid_accuracy\"].append(valid_accuracy)\n\n    clear_output()\n\n    plot_stats(\n        history[\"train_loss\"], history[\"valid_loss\"],\n        history[\"train_accuracy\"], history[\"valid_accuracy\"],\n        \"Learning curves\"\n    )\n    plt.show()\n\n    scheduler.step()\ntorch.save(model.state_dict(), open(\"cnn_classification_files/mod.pck\", \"wb\"))\npickle.dump(history, open(\"cnn_classification_files/fit_history.pck\", \"wb\"))\nHere is plotted learning curves for model. As you can see the final accuracy is 90%.\n\nhistory = pickle.load(open(\"cnn_classification_files/fit_history.pck\", \"rb\"))\n\nplot_stats(\n    history[\"train_loss\"], history[\"valid_loss\"],\n    history[\"train_accuracy\"], history[\"valid_accuracy\"],\n    \"Learning curves\"\n)\n\nfinal_accuracy = history[\"valid_accuracy\"][-1]\nplt.axhline(final_accuracy, color=\"gray\", linestyle=\"--\")\n\nplt.yticks(list(plt.yticks()[0]) + [round(final_accuracy, 2)])\nplt.show()"
  },
  {
    "objectID": "python/torch/vision/cnn_classification.html#playing-with-model",
    "href": "python/torch/vision/cnn_classification.html#playing-with-model",
    "title": "CNN classification",
    "section": "Playing with model",
    "text": "Playing with model\nLet’s check models accruraby but not with strickt metric, but with our eyes. Sometimes it’s ever for numan hard to understand what exactly displayed on 32x32 piture. So let’s see what model thinks but not strict classes, but the ratio of model certainties in different classes.\nHere is fitted model so you can just download it and do what ever you want.\n\nmodel = Model(3, 10)\n\nwith open(\"cnn_classification_files/mod.pck\", \"rb\") as f:\n    model.load_state_dict(torch.load(f, map_location=torch.device('cpu')))\n\nmodel = model.eval()\n\nno_transform_valid = CIFAR10(\n    './cifar10', \n    train=False,\n    download=True\n)\n\nindx = np.random.choice(np.arange(len(no_transform_valid)), 10)\npredicts = model(\n    torch.stack(\n        [train_transforms(no_transform_valid[i][0]) for i in indx]\n    )\n)\n\npredicts\n\nfor n, i in enumerate(indx):\n    plt.figure(figsize = (10, 3))\n    \n    plt.subplot(121)\n    plt.title(\"Picutre\")\n    plt.imshow(no_transform_valid[i][0])\n    plt.xticks([]); plt.yticks([])\n    \n    plt.subplot(122)\n    plt.title(\"Model's opinion\")\n    plt.bar(\n        cifar10_classes.values(), \n        (predicts[n] + predicts[n].min().abs()).detach().numpy()\n    )\n    plt.xticks(rotation = 45);plt.yticks([])\n    plt.show()\n\nFiles already downloaded and verified"
  },
  {
    "objectID": "python/torch/vision/pretrains.html",
    "href": "python/torch/vision/pretrains.html",
    "title": "Pretrains",
    "section": "",
    "text": "Sometimes you don’t need to create a model from scratch, you can take an existing model and tune it for your task. Here are some ready-made models available in Pytorch.\nHere here you can find some models that available in the torch.\nimport torch\nfrom torch import nn"
  },
  {
    "objectID": "python/torch/vision/pretrains.html#importing-model",
    "href": "python/torch/vision/pretrains.html#importing-model",
    "title": "Pretrains",
    "section": "Importing model",
    "text": "Importing model\nLet’s take for example alexnet. In the following cell it imported and displayed.\n\nmodel = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\nmodel\n\nUsing cache found in /home/fedor/.cache/torch/hub/pytorch_vision_v0.10.0\n\n\nAlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=9216, out_features=4096, bias=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=4096, out_features=4096, bias=True)\n    (5): ReLU(inplace=True)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)"
  },
  {
    "objectID": "python/torch/vision/pretrains.html#replace-layers",
    "href": "python/torch/vision/pretrains.html#replace-layers",
    "title": "Pretrains",
    "section": "Replace layers",
    "text": "Replace layers\nEven if you’ve taken an off-the-shelf architecture, sometimes you need to adapt the model for your specific task. So you need to replace some layers with other layers. Suppose you have a task that requires you to classify into ten classes, but classical alexnet model adapted to ImageNet dataset has 1000 outputs. So you have to replace at least the last layer and rebuild the model.\nIn the following cell I’ll refer to the classifier block of Alexnet and replace its layer with nn.Linear with a chosen number of outputs. Note the last layer of the net - it has out_features specified in the cell:\n\nmodel.classifier[-1] = nn.Linear(in_features = 4096, out_features = 10)\nmodel\n\nAlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=9216, out_features=4096, bias=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=4096, out_features=4096, bias=True)\n    (5): ReLU(inplace=True)\n    (6): Linear(in_features=4096, out_features=10, bias=True)\n  )\n)"
  },
  {
    "objectID": "python/torch/vision/datasets.html",
    "href": "python/torch/vision/datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "torchvision is the development of a more basic torch. So it’s datasets have some features that will be described here.\nfrom IPython.display import HTML\n\nimport torch\nfrom torchvision.datasets import ImageFolder\n\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\nfrom torchvision.datasets import CIFAR10\n\nimport torchvision.transforms as T\n\ndef vis_dataset(dataset):\n    '''\n    Fucntion that visualises classified \n    picture dataset. For each class there is\n    a line that contains all pictures of that class.\n\n    Agruments\n    ------------\n    dataset (torchvision.datasets.vivion.VisisonDataset) :\n            dataset that needed to be visualised;\n    '''\n    separated = {}\n    for v in dataset:\n        if not(v[1] in separated):\n            separated[v[1]] = []\n        separated[v[1]].append(v[0])\n    \n    classes_count = len(separated)\n    plt.figure(figsize = (15, 5))\n    \n    for i, (class_name, class_picks) in enumerate(separated.items()):\n        plt.subplot(classes_count, 1, i + 1)\n    \n        plt.title(f\"class : {class_name}\")\n        plt.imshow(T.ToPILImage()(\n            make_grid(\n                [T.ToTensor()(pic) for pic in class_picks], \n                nrow=len(class_picks)\n            )\n        ))\n        plt.xticks([]);plt.yticks([])"
  },
  {
    "objectID": "python/torch/vision/datasets.html#image-folder",
    "href": "python/torch/vision/datasets.html#image-folder",
    "title": "Datasets",
    "section": "Image folder",
    "text": "Image folder\nIt’s really common to work with data structured like this: there’s folder_1, folder_2, …, folder_n, so there are specific folders for each class. And each folder contains images that belong to that class. There are prepared folders that show what it might look like. The following cell shows the tree of this folder.\n\n%%bash\ntree datasets_files/image_dataset\n\ndatasets_files/image_dataset\n├── class1\n│   ├── 1.png\n│   ├── 2.png\n│   ├── 3.png\n│   ├── 4.png\n│   ├── 5.png\n│   ├── 6.png\n│   ├── 7.png\n│   ├── 8.png\n│   └── 9.png\n└── class2\n    ├── 1.png\n    ├── 2.png\n    ├── 3.png\n    ├── 4.png\n    ├── 5.png\n    ├── 6.png\n    ├── 7.png\n    ├── 8.png\n    └── 9.png\n\n2 directories, 18 files\n\n\nSo you can use torchvision.datasets.ImageFolder to load this type of file. In the next cell, ImageFolder has been applied to the folder from the previous cell. The firls element of the retrieved dataset is also displayed:\n\nimg_dataset = ImageFolder(\"datasets_files/image_dataset\")\ndisplay(img_dataset[0])\n\n(&lt;PIL.Image.Image image mode=RGB size=1024x1024 at 0x7FA072398880&gt;, 0)\n\n\nIt should support most of the features of `torchvision.datasets.vision.VisionDataset’ because it’s an ancestor of that class. The next cell shows the list of ancestors:\n\ndisplay(ImageFolder.mro())\n\n[torchvision.datasets.folder.ImageFolder,\n torchvision.datasets.folder.DatasetFolder,\n torchvision.datasets.vision.VisionDataset,\n torch.utils.data.dataset.Dataset,\n typing.Generic,\n object]\n\n\nAnd finally, here are some visualisations of picuteres in the classroom, to make sure that there are images and perhaps show some features for manipulating with them.\n\nvis_dataset(img_dataset)"
  },
  {
    "objectID": "python/torch/vision/datasets.html#transformations",
    "href": "python/torch/vision/datasets.html#transformations",
    "title": "Datasets",
    "section": "Transformations",
    "text": "Transformations\nIn torchvision datasets there is an option that allows to store transformations that need to be applied to instances of the dataset. It’s called transfrom, target_transform and transforms.\n\nBasic transform\nIs a transformation to be applied to the input data (X).\nSo in the following cell, torch.transformation.RandomHorizontalFlip is used for the Image dataset. So now you don’t need to apply a transformation to each image, just access them using indexing operators - torch will do it all for you.\n\nwith_transforms = ImageFolder(\n    \"datasets_files/image_dataset\",\n    transform = T.RandomHorizontalFlip(p=1)\n)\nno_transforms = ImageFolder(\n    \"datasets_files/image_dataset\"\n)\nplt.subplot(121)\nplt.title(\"No transformations\")\nplt.imshow(no_transforms[0][0])\nplt.xticks([]); plt.yticks([])\n\nplt.subplot(122)\nplt.title(\"With transformations\")\nplt.imshow(with_transforms[0][0])\nplt.yticks([]); plt.xticks([])\n\nplt.show()\n\n\n\n\n\n\ntarget_transform\nYou can apply some tranformations to the target. For example in the following cell is applied transformation thar replace 0 with A, 1 with B. You can see the raw output of the dataset and visualisation - both have “A” and “B” instead of 1 and 2.\n\ntarget_transformed = ImageFolder(\n    \"datasets_files/image_dataset\",\n    target_transform=T.Lambda(lambda x: \"A\" if x==0 else \"B\")\n)\n\n\ndisplay(HTML(\"&lt;h4&gt;Raw display&lt;/h4&gt;\"))\nfor v in target_transformed:\n    display(v)\ndisplay(HTML(\"&lt;h4&gt;Visualisation&lt;/h4&gt;\"))\nvis_dataset(target_transformed)\n\nRaw display\n\n\n(&lt;PIL.Image.Image image mode=RGB size=1024x1024 at 0x7FA0781E82E0&gt;, 'A')\n\n\n(&lt;PIL.Image.Image image mode=RGB size=1024x1024 at 0x7FA0722944C0&gt;, 'A')\n\n\n(&lt;PIL.Image.Image image mode=RGB size=1024x1024 at 0x7FA072294F70&gt;, 'A')\n\n\n(&lt;PIL.Image.Image image mode=RGB size=1024x1024 at 0x7FA0781E82E0&gt;, 'A')\n\n\n(&lt;PIL.Image.Image image mode=RGB size=1024x1024 at 0x7FA078241B70&gt;, 'A')\n\n\n(&lt;PIL.Image.Image image mode=RGB size=1024x1024 at 0x7FA072294F70&gt;, 'A')\n\n\n(&lt;PIL.Image.Image image mode=RGB size=1024x1024 at 0x7FA0722944C0&gt;, 'A')\n\n\n(&lt;PIL.Image.Image image mode=RGB size=1024x1024 at 0x7FA0781E82E0&gt;, 'A')\n\n\n(&lt;PIL.Image.Image image mode=RGB size=1024x1024 at 0x7FA078241720&gt;, 'A')\n\n\n(&lt;PIL.Image.Image image mode=RGB size=1024x1024 at 0x7FA0722944C0&gt;, 'B')\n\n\n(&lt;PIL.Image.Image image mode=RGB size=1024x1024 at 0x7FA072297A60&gt;, 'B')\n\n\n(&lt;PIL.Image.Image image mode=RGB size=1024x1024 at 0x7FA072294F70&gt;, 'B')\n\n\n(&lt;PIL.Image.Image image mode=RGB size=1024x1024 at 0x7FA078242470&gt;, 'B')\n\n\n(&lt;PIL.Image.Image image mode=RGB size=1024x1024 at 0x7FA072294F70&gt;, 'B')\n\n\n(&lt;PIL.Image.Image image mode=RGB size=1024x1024 at 0x7FA078240EE0&gt;, 'B')\n\n\n(&lt;PIL.Image.Image image mode=RGB size=1024x1024 at 0x7FA072294F70&gt;, 'B')\n\n\n(&lt;PIL.Image.Image image mode=RGB size=1024x1024 at 0x7FA078242CB0&gt;, 'B')\n\n\n(&lt;PIL.Image.Image image mode=RGB size=1024x1024 at 0x7FA072294F70&gt;, 'B')\n\n\nVisualisation\n\n\n\n\n\n\n\nChange transformations\nThere is a slightly confusing situation with changing transformations in Torch. The best universal way is to recreate the dataset.\nA very common example is when you have a data set and you need to normalise it. You need to load it to calculate the mean and the standard deviation. But then you re-create it using the mean and the standard deviation that you have previously calculated for the normalisation transformation.\nThe following example shows how normalisation can be performed on a sample dataset. As you can see, the normalisation worked well - we ended up with a data set with zero bias and one as the standard deviation:\n\ndataset = ImageFolder(\"datasets_files/image_dataset\", transform=T.ToTensor())\nX_as_tensor = torch.stack([x for x, y in dataset])\nmean = X_as_tensor.mean(axis = (0, 2, 3))\nstd = X_as_tensor.std(axis = (0, 2, 3))\n\nprint(\"Initial mean:\", mean)\nprint(\"Initial std:\", std, end = \"\\n\\n\")\n\ndataset = ImageFolder(\n    \"datasets_files/image_dataset\",\n    transform = T.Compose([\n        T.ToTensor(),\n        T.Normalize(mean, std)\n    ])\n)\nX_as_tensor = torch.stack([x for x, y in dataset])\nmean = X_as_tensor.mean(axis = (0, 2, 3))\nstd = X_as_tensor.std(axis = (0, 2, 3))\n\nprint(\"Normilised mean:\", mean)\nprint(\"Normilised std:\", std)\n\nInitial mean: tensor([0.4904, 0.4030, 0.3632])\nInitial std: tensor([0.2590, 0.2315, 0.2351])\n\nNormilised mean: tensor([ 1.5563e-07, -6.8711e-08, -1.5274e-07])\nNormilised std: tensor([1., 1., 1.])"
  },
  {
    "objectID": "python/torch/vision/basic.html",
    "href": "python/torch/vision/basic.html",
    "title": "Basic",
    "section": "",
    "text": "Here are the basics of working with images in Torch.\nimport numpy as np\n\nfrom PIL import Image\n\nimport torchvision\nfrom torchvision import transforms\n\nfrom IPython import display\n\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "python/torch/vision/basic.html#load-image",
    "href": "python/torch/vision/basic.html#load-image",
    "title": "Basic",
    "section": "Load image",
    "text": "Load image\nThe PIL library can be used to load images. For example, we will use a really simple small picture to see patterns in the matrix representation of the picture. So in the following cell I load and display the picture that will be used:\n\nsimple_image_path = \"basic_files/test_picture.png\"\nimage = Image.open(simple_image_path)\nimage\n\n\n\n\nThe picture is very small, so it can be difficult to understand what exactly is on the picture. In the following cell I use matplotlib to dispalay interpolation of the picture:\n\nplt.imshow(\n    plt.imread(simple_image_path, format = \"jpg\"), \n    interpolation='nearest'\n)\nplt.xticks([])\nplt.yticks([])\nplt.show()\n\n\n\n\nAnd now let’s tranfrom picture to the torch tensor. I deliberately kept the picture using only 3 colour channels. So now we can see the tensor of dimension (3, &lt;picture width&gt;, &lt;picture height&gt;), that literaly follows the pattern of the picture.\n\ntransforms.ToTensor()(image)\n\ntensor([[[1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980]],\n\n        [[0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980]],\n\n        [[0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980],\n         [0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0980, 0.2000, 0.2980]]])"
  },
  {
    "objectID": "python/torch/vision/basic.html#image-sample-normalisation",
    "href": "python/torch/vision/basic.html#image-sample-normalisation",
    "title": "Basic",
    "section": "Image sample normalisation",
    "text": "Image sample normalisation\n\ncifar10 = torchvision.datasets.CIFAR10(\n    \"cifar10\",\n    download = True,\n    transform = transforms.ToTensor()\n)\n\nFiles already downloaded and verified\n\n\n\nmeans = (cifar10.data / 255).mean(axis = (0,1,2))\nprint(\"means\", means)\nstds = (cifar10.data / 255).std(axis = (0,1,2))\nprint(\"stds\", stds)\n\nmeans [0.49139968 0.48215841 0.44653091]\nstds [0.24703223 0.24348513 0.26158784]\n\n\ntransforms.Normalize(mean = means, std = stds)(torch.tensor(cifar10.data / 255))"
  },
  {
    "objectID": "python/torch/tensor/basic.html",
    "href": "python/torch/tensor/basic.html",
    "title": "Tensor",
    "section": "",
    "text": "Here I want to describe the basic data type for pytorch - toorch.tensor.\nimport torch\nimport numpy as np"
  },
  {
    "objectID": "python/torch/tensor/basic.html#create-tensor",
    "href": "python/torch/tensor/basic.html#create-tensor",
    "title": "Tensor",
    "section": "Create tensor",
    "text": "Create tensor\nUse class torch.tensor which can accept a random list as a parameter.\n\ntorch.tensor([[1,2,3], [3,4,5]], dtype = torch.float32)\n\ntensor([[1., 2., 3.],\n        [3., 4., 5.]])"
  },
  {
    "objectID": "python/torch/tensor/basic.html#shape",
    "href": "python/torch/tensor/basic.html#shape",
    "title": "Tensor",
    "section": "Shape",
    "text": "Shape\nSimply refer to the shape attribute to get the dimensionality of the tensor. You will got torch.Size as the result, you can use [] to access values of this object.\n\nshape_return = torch.tensor([[1,2,3], [1,2,3]]).shape\ndisplay(shape_return)\nprint(f\"rows : {shape_return[0]}, columns: {shape_return[1]}\")\n\ntorch.Size([2, 3])\n\n\nrows : 2, columns: 3"
  },
  {
    "objectID": "python/torch/tensor/basic.html#random-values",
    "href": "python/torch/tensor/basic.html#random-values",
    "title": "Tensor",
    "section": "Random values",
    "text": "Random values\nUse torch.rand to get random tensor where values will follow uniform distribution from 0 to 1.\n\ntorch.rand(10, 3)\n\ntensor([[0.3386, 0.0154, 0.7047],\n        [0.7331, 0.9721, 0.5746],\n        [0.7657, 0.4934, 0.5701],\n        [0.6945, 0.3883, 0.5244],\n        [0.7672, 0.1538, 0.1033],\n        [0.7886, 0.6982, 0.2064],\n        [0.2538, 0.6580, 0.8651],\n        [0.6573, 0.4551, 0.3203],\n        [0.1855, 0.3056, 0.8196],\n        [0.0160, 0.9934, 0.5827]])"
  },
  {
    "objectID": "python/torch/texts_classification.html",
    "href": "python/torch/texts_classification.html",
    "title": "Text classification",
    "section": "",
    "text": "It’s common task for machine learning and in this notebook is showen how it can be solved with deep learning, in particular pytorch.\nimport numpy as np\nimport pandas as pd\n\nimport re\nimport string\nfrom collections import Counter\n\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.utils.data import (\n    DataLoader, random_split, Dataset\n)\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch.nn.functional as F\n\nimport gensim.downloader as api\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output"
  },
  {
    "objectID": "python/torch/texts_classification.html#data",
    "href": "python/torch/texts_classification.html#data",
    "title": "Text classification",
    "section": "Data",
    "text": "Data\nHere we deal with pretty simple dataset. That just contains review on movies and markers that show weather this comment is positive or negative. We will build a model that can “read” the content and evaluate whether a given comment is positive or negative.\n\ndata = pd.read_csv(\"texts_classification_files/IMDB Dataset.csv\")\n#data = pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\ndata.head(5)\n\n\n\n\n\n\n\n\nreview\nsentiment\n\n\n\n\n0\nOne of the other reviewers has mentioned that ...\npositive\n\n\n1\nA wonderful little production. &lt;br /&gt;&lt;br /&gt;The...\npositive\n\n\n2\nI thought this was a wonderful way to spend ti...\npositive\n\n\n3\nBasically there's a family where a little boy ...\nnegative\n\n\n4\nPetter Mattei's \"Love in the Time of Money\" is...\npositive"
  },
  {
    "objectID": "python/torch/texts_classification.html#text-vectorisation",
    "href": "python/torch/texts_classification.html#text-vectorisation",
    "title": "Text classification",
    "section": "Text vectorisation",
    "text": "Text vectorisation\nWe need to vectorise our text. There is method that allows to each word match some vector. Here we are using ready tool for this purpose.\n\nfasttext = api.load('glove-twitter-25')\n\nHere is example how this tool can be used for some words.\n\nprint(\"man -&gt;\", fasttext[\"man\"])\nprint(\"king -&gt;\", fasttext[\"king\"])\n\nman -&gt; [ 0.37013  -0.39648  -0.021712 -0.6301   -0.3189    0.34329   0.10968\n  0.4879   -0.48663   0.36837  -0.39179   0.25414  -4.9282    0.067597\n  0.37147   0.36817   1.1655    0.092116 -0.87735  -0.74562   0.40903\n  1.5672   -0.23879   0.24755   0.76386 ]\nking -&gt; [-0.74501  -0.11992   0.37329   0.36847  -0.4472   -0.2288    0.70118\n  0.82872   0.39486  -0.58347   0.41488   0.37074  -3.6906   -0.20101\n  0.11472  -0.34661   0.36208   0.095679 -0.01765   0.68498  -0.049013\n  0.54049  -0.21005  -0.65397   0.64556 ]"
  },
  {
    "objectID": "python/torch/texts_classification.html#vocabulary",
    "href": "python/torch/texts_classification.html#vocabulary",
    "title": "Text classification",
    "section": "Vocabulary",
    "text": "Vocabulary\nVocabulary is object that describes all worlds that we allowed to use in our model and basic operation under them.\nThe following cell represents possible realisation of the vocabulary in python:\n\nclass Vocabulary:\n    def __init__(self, texts: list[str], min_freq: int = 10):\n        text = ' '.join(texts)\n\n        text = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \" \", text)\n        text = re.sub(\"[^a-zA-Z]\", \" \", text)\n\n        while '  ' in text:\n            text = text.replace('  ', ' ')\n\n        words = text.strip().lower().split()\n\n        c = Counter(words)\n\n        self.vocabulary = list(set([word for word in words if c[word] &gt;= min_freq]))\n        self.vocabulary.append('&lt;unk&gt;')\n        self._idx2word = {i: word for i, word in enumerate(self.vocabulary)}\n        self._word2idx = {word: i for i, word in enumerate(self.vocabulary)}\n\n    def get_vocabulary(self):\n        return self.vocabulary\n\n    def idx2word(self, idx: int):\n        if idx not in self._idx2word:\n            return '&lt;unk&gt;'\n\n        return self._idx2word[idx]\n\n    def word2idx(self, word: str):\n        word = word.lower()\n        if word not in self._word2idx:\n            return self._word2idx['&lt;unk&gt;']\n\n        return self._word2idx[word]\n    \n    def encode(self, text):\n        result = []\n\n        for word in text.split():\n            result.append(self.word2idx(word))\n\n        return result\n\n    def build_vectors(self, fasttext):\n        vectors = []\n        \n        for word in self.vocabulary:\n            if fasttext.has_index_for(word):\n                vectors.append(fasttext[word])\n            else:\n                vectors.append(np.zeros(25))\n\n        return np.stack(vectors)\n\nThe vocabulary must be common to both the training and test datasets, so in the following cell the vocabulary has been created under words that can be found in the given sample.\n\ndata[\"sentiment\"] = data[\"sentiment\"].replace({\"positive\" : 1, \"negative\" : 0})\nvocab = Vocabulary(data[\"review\"].values)"
  },
  {
    "objectID": "python/torch/texts_classification.html#datasetsdataloaders",
    "href": "python/torch/texts_classification.html#datasetsdataloaders",
    "title": "Text classification",
    "section": "Datasets/dataloaders",
    "text": "Datasets/dataloaders\nThe following cell extends torch.dataset to work with data from the example under consideration and creates ti’s instances for training and test samples.\n\nclass IMDB(Dataset):\n    def __init__(self, texts, labels, vocab):\n        \n        self.texts = texts\n        self.labels = labels\n        self.vocab = vocab\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n\n        text = torch.LongTensor(self.vocab.encode(text))\n        label = torch.FloatTensor([label])\n\n        return text, label\n\n    def __len__(self):\n        return len(self.texts)\n\n\ntrain_dataset, test_dataset = np.split(data, [45000], axis = 0)\ntrain_dataset = IMDB(\n    train_dataset[\"review\"].values, \n    train_dataset[\"sentiment\"].values, \n    vocab\n)\ntest_dataset = IMDB(\n    test_dataset[\"review\"].values, \n    test_dataset[\"sentiment\"].values, \n    vocab\n)\n\nSame for dataloaders.\n\npad_idx = len(vocab.vocabulary)\ndef collate_fn(batch):\n    texts = pad_sequence(\n        [b[0] for b in batch], \n        padding_value=pad_idx, \n        batch_first=True\n    )\n    labels = torch.stack([b[1] for b in batch])\n    \n    return texts, labels\n\n\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=64, \n    collate_fn=collate_fn, \n    pin_memory=True\n)\ntest_loader = DataLoader(\n    test_dataset, \n    batch_size=64, \n    collate_fn=collate_fn, \n    pin_memory=True\n)"
  },
  {
    "objectID": "python/torch/texts_classification.html#model-architecture",
    "href": "python/torch/texts_classification.html#model-architecture",
    "title": "Text classification",
    "section": "Model architecture",
    "text": "Model architecture\n\nclass TextCNN(nn.Module):\n    def __init__(self, vocab_size: int, embedding_dim: int, pad_idx):\n        super().__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n\n        self.convs = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.Conv2d(\n                        in_channels = 1, \n                        out_channels = 16, \n                        kernel_size = (fs, embedding_dim)\n                    ),\n                    nn.Dropout(0.5)\n                )\n                for fs in [2, 3, 4, 5]\n            ]\n        )\n        \n        self.fc = nn.Linear(4 * 16, 1)\n    \n    def forward(self, x):\n        x = self.embedding(x)\n        x = x.unsqueeze(1)\n\n        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n        x = [F.max_pool1d(_, _.shape[2]).squeeze(2) for _ in x]\n\n        return self.fc(torch.cat(x, dim = 1))"
  },
  {
    "objectID": "python/torch/texts_classification.html#model-fitting-functions",
    "href": "python/torch/texts_classification.html#model-fitting-functions",
    "title": "Text classification",
    "section": "Model fitting functions",
    "text": "Model fitting functions\nThe following cells define a function that implements model optimisation, evaluation and visualisation procedures.\n\ndef binary_accuracy(preds, y):\n    rounded_preds = torch.round(torch.sigmoid(preds))\n    correct = (rounded_preds == y).float()\n    acc = correct.sum() / len(correct)\n    return acc.item()\n\n\ndef train(model) -&gt; tuple[float, float]:\n    model.train()\n\n    train_loss = 0\n    train_accuracy = 0\n\n    for x, y in tqdm(train_loader, desc='Train'):\n        x, y = x.to(device), y.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(x)\n\n        loss = loss_fn(output, y)\n\n        train_loss += loss.item()\n        train_accuracy += binary_accuracy(output, y)\n\n        loss.backward()\n\n        optimizer.step()\n\n    train_loss /= len(train_loader)\n    train_accuracy /= len(train_loader)\n\n    return train_loss, train_accuracy\n\n@torch.inference_mode()\ndef evaluate(model, loader) -&gt; tuple[float, float]:\n    model.eval()\n\n    total_loss = 0\n    total_accuracy = 0\n\n    for x, y in tqdm(loader, desc='Evaluation'):\n        x, y = x.to(device), y.to(device)\n\n        output = model(x)\n\n        loss = loss_fn(output, y)\n\n        total_loss += loss.item()\n        total_accuracy += binary_accuracy(output, y)\n\n    total_loss /= len(loader)\n    total_accuracy /= len(loader)\n\n    return total_loss, total_accuracy\n\ndef plot_stats(\n    train_loss: list[float],\n    valid_loss: list[float],\n    train_accuracy: list[float],\n    valid_accuracy: list[float],\n    title: str\n):\n    plt.figure(figsize=(16, 8))\n\n    plt.title(title + ' loss')\n\n    plt.plot(train_loss, label='Train loss')\n    plt.plot(valid_loss, label='Valid loss')\n    plt.legend()\n    plt.grid()\n\n    plt.show()\n\n    plt.figure(figsize=(16, 8))\n\n    plt.title(title + ' accuracy')\n    \n    plt.plot(train_accuracy, label='Train accuracy')\n    plt.plot(valid_accuracy, label='Valid accuracy')\n    plt.legend()\n    plt.grid()\n\n    plt.show()"
  },
  {
    "objectID": "python/torch/texts_classification.html#fitting",
    "href": "python/torch/texts_classification.html#fitting",
    "title": "Text classification",
    "section": "Fitting",
    "text": "Fitting\nThis is where everything necessary for running the optimisation algorithm is defined.\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\nmodel = TextCNN(\n    vocab_size=len(vocab.vocabulary) + 1, \n    embedding_dim=25,\n    pad_idx=pad_idx\n).to(device)\nvectors = vocab.build_vectors(fasttext)\nmodel.embedding.weight.data[:len(vectors)] = torch.from_numpy(vectors)\n\nloss_fn = nn.BCEWithLogitsLoss()\noptimizer = Adam(model.parameters(), lr=1e-3)\n\ncpu\n\n\nThe fitting process can be started with the next cell. It can take a considerable amount of time, so the results are saved in temporary files.\nhistory = {\n    \"train_loss\" : [],\n    \"valid_loss\" : [],\n    \"train_accuracy\" : [],\n    \"valid_accuracy\" : []\n}\nfor epoch in range(5):\n    train_loss, train_accuracy = train(model)\n    valid_loss, valid_accuracy = evaluate(model, test_loader)\n\n    history[\"train_loss\"].append(train_loss)\n    history[\"valid_loss\"].append(valid_loss)\n    history[\"train_accuracy\"].append(train_accuracy)\n    history[\"valid_accuracy\"].append(valid_accuracy)\n\n    clear_output()\n\n    plot_stats(\n        history[\"train_loss\"], history[\"valid_loss\"],\n        history[\"train_accuracy\"], history[\"valid_accuracy\"],\n        \"Learning curves\"\n    )\nwith open(\"texts_classification_files/model.pck\", \"wb\") as f:\n    torch.save(model.state_dict(), f)\nwith open(\"texts_classification_files/history.pck\", \"wb\") as f:\n    pickle.dump(history, f)\nThe following cell shows the learning curves of the model fitting process.\n\nwith open(\"texts_classification_files/history.pck\", \"rb\") as f:\n    history = pickle.load(f)\n\nplot_stats(\n    history[\"train_loss\"], history[\"valid_loss\"],\n    history[\"train_accuracy\"], history[\"valid_accuracy\"],\n    \"Learning curves\"\n)"
  },
  {
    "objectID": "python/torch/layers.html",
    "href": "python/torch/layers.html",
    "title": "Layers",
    "section": "",
    "text": "Layer in torch is some transformation with some inputs and some outputs.\nimport torch\nfrom torch import nn\nTo perform a layer transformation on the tensor X, simply use the syntax layer(X)."
  },
  {
    "objectID": "python/torch/layers.html#linear",
    "href": "python/torch/layers.html#linear",
    "title": "Layers",
    "section": "Linear",
    "text": "Linear\n\nDescription\ntorch.nn.linear simply performs operation:\n\\[X_{n\\times l}\\omega_{l\\times k} + b_k\\]\nWhere:\n\n\\(l\\) - number of inputs;\n\\(k\\) - number of outputs;\n\\(n\\) - number of input objects to transform;\n\\(X_{n\\times l}\\) - tensor of data for transformation;\n\\(\\omega_{l\\times k}\\) - weights of this layer;\n\\(b_k\\) - bias of this layer.\n\nThe following cell applies the tensor to some data and performs the same transformation by hand - the results are the same.\n\nin_features = 5\nout_features = 3\n\nlinear = nn.Linear(\n    in_features = in_features, \n    out_features = out_features\n)\n\nX = torch.rand(in_features)\n\nprint(\"Layer transformation\")\nprint(linear(X).tolist())\nprint(\"X@w+b\")\nprint((linear.weight@X + linear.bias).tolist())\n\nLayer transformation\n[0.21258635818958282, -0.4820197522640228, -0.04028485715389252]\nX@w+b\n[0.21258635818958282, -0.4820197522640228, -0.04028485715389252]"
  },
  {
    "objectID": "python/torch/multiclass_task.html",
    "href": "python/torch/multiclass_task.html",
    "title": "Multiclass task",
    "section": "",
    "text": "This page describes how to build a multiclass classification network using pytorch.\nimport pandas as pd\n\nfrom sklearn import datasets\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import TensorDataset, DataLoader"
  },
  {
    "objectID": "python/torch/multiclass_task.html#create-sample",
    "href": "python/torch/multiclass_task.html#create-sample",
    "title": "Multiclass task",
    "section": "Create sample",
    "text": "Create sample\nGenerate some data with a certain number of features and classes.\n\nn_features = 5\nn_classes = 3\n\nX, Y = datasets.make_classification(\n    n_features = 5, \n    n_classes = n_classes,\n    n_clusters_per_class = 1,\n    n_informative = 2,\n    n_redundant = 0,\n    n_samples = 10000,\n    random_state = 10\n)\n\nX, Y = torch.tensor(X, dtype = torch.float), torch.tensor(Y)"
  },
  {
    "objectID": "python/torch/multiclass_task.html#model",
    "href": "python/torch/multiclass_task.html#model",
    "title": "Multiclass task",
    "section": "Model",
    "text": "Model\nIn the next cell, we need to define a model where the number of inputs corresponds to the number of features of the input data and the number of outputs corresponds to the number of classes for the sample generated below.\n\nnetwork = nn.Sequential(\n    nn.Linear(in_features = n_features, out_features = 30),\n    nn.ReLU(),\n    nn.Linear(in_features = 30, out_features = n_classes)\n)\n\nThe output of the model for each object will be the number of floats corresponding to the number of classes in the input data. The index of the maximum value we will understand as the predicted class.\nThe following cell show how a whole complex of transformations can be performed in code:\n\noutput = network(X)\n\nshow_df = pd.DataFrame(\n    output.detach().numpy(),\n    columns = [f\"out {i}\" for i in range(n_classes)]\n)\nshow_df[\"predicted class\"] = output.max(axis = 1).indices\nshow_df[\"real_class\"] = Y\n\nshow_df.head(5)\n\n\n\n\n\n\n\n\nout 0\nout 1\nout 2\npredicted class\nreal_class\n\n\n\n\n0\n-0.131736\n0.335459\n-0.036521\n1\n1\n\n\n1\n0.123889\n-0.031864\n-0.228509\n0\n2\n\n\n2\n-0.137551\n0.272115\n-0.049151\n1\n1\n\n\n3\n-0.376087\n0.127549\n-0.022875\n1\n2\n\n\n4\n-0.129013\n0.456219\n-0.003062\n1\n1\n\n\n\n\n\n\n\nAnd, of course, we can use operations from the previous cell to calculate a fraction of correct predictions - accuracy:\n\naccyracy = (output.max(axis = 1).indices == Y).numpy().mean()\nprint(f\"accuracy - {accyracy}\")\n\naccuracy - 0.3848"
  },
  {
    "objectID": "python/torch/multiclass_task.html#optimization",
    "href": "python/torch/multiclass_task.html#optimization",
    "title": "Multiclass task",
    "section": "Optimization",
    "text": "Optimization\nNow we can do classic burner model optimisation pipline and print accuracy after each epoch - we get better and better results.\n\ndata_loder = DataLoader(\n    TensorDataset(X, Y), batch_size = 100\n)\n\noptimizer = optim.Adam(network.parameters(), lr = 1e-2)\nloss_fn = nn.CrossEntropyLoss()\n\nfor epoch in range(10):\n    for x, y in data_loder:\n        optimizer.zero_grad()\n        output = network(x)\n        loss_val = loss_fn(output, y)\n        loss_val.backward()\n        optimizer.step()\n    predict = network(X).max(axis = 1).indices\n    print(f\"epoch - {epoch}, accuracy - {((predict == Y)).numpy().mean()}\")\n\nepoch - 0, accuracy - 0.9231\nepoch - 1, accuracy - 0.924\nepoch - 2, accuracy - 0.9243\nepoch - 3, accuracy - 0.9242\nepoch - 4, accuracy - 0.9243\nepoch - 5, accuracy - 0.9245\nepoch - 6, accuracy - 0.9239\nepoch - 7, accuracy - 0.9244\nepoch - 8, accuracy - 0.9252\nepoch - 9, accuracy - 0.9254"
  },
  {
    "objectID": "python/plotly/hovers.html",
    "href": "python/plotly/hovers.html",
    "title": "Hovers",
    "section": "",
    "text": "Some items when you hover your mouse over them show a hint. To control the properties of this tooltip, plotly offers a number of properties that usually start with hover.\nPlotly user guide to work with it."
  },
  {
    "objectID": "python/plotly/hovers.html#basic",
    "href": "python/plotly/hovers.html#basic",
    "title": "Hovers",
    "section": "Basic",
    "text": "Basic\nSo in the following example there is a scatter with blue dots that have hovertemplate specified and a scatter with red dots that don’t have hovertemplate specified.\n\nimport numpy as np\nimport plotly.graph_objects as go\n\nsample_size = 100\n\nfig = go.Figure()\nfig.add_trace(\n    go.Scatter(\n        x = np.random.rand(sample_size),\n        y = np.random.rand(sample_size),\n        mode = \"markers\",\n        hovertemplate = \"x=%{x}&lt;br&gt;y=%{y}\",\n        name = \"I have hovertemplate\",\n        marker_size = 10\n    )\n)\nfig.add_trace(\n    go.Scatter(\n        x = np.random.rand(sample_size),\n        y = np.random.rand(sample_size),\n        mode = \"markers\",\n        marker={\"color\" : \"red\"},\n        name = \"No hovertemplate\",\n        marker_size = 10\n    )\n)\n\nfig.update_layout(height = 700)"
  },
  {
    "objectID": "python/plotly/hovers.html#variable",
    "href": "python/plotly/hovers.html#variable",
    "title": "Hovers",
    "section": "%{variable}",
    "text": "%{variable}\nYou can display different parameters of elements on the plots, just using syntax {%variable}. And you can add some formatting, see here and here for dates.\n\nimport numpy as np\nimport plotly.graph_objects as go\nfrom IPython.display import HTML\n\nsample_size = 100\n\nlanguages = [\"R\", \"Python\", \"Java Script\", \"Matlab\"]\nusers = [2, 5, 3, 2.5]\ntext = [\"textA\", \"TextB\", \"TextC\", \"TextD\"]\n\nfig = go.Figure()\nfig.add_trace(\n    go.Pie(\n        name = \"\",\n        values = users,\n        labels = languages,\n        text = text,\n        hovertemplate = \"%{label}: &lt;br&gt;Popularity: %{percent} &lt;br&gt;%{text}&lt;extra&gt;&lt;/extra&gt;\"\n    )\n)\nfig.show()\n\n\nfig = go.Figure()\nfig.add_trace(\n    go.Bar(\n        x = languages,\n        y = users,\n        text = text,\n        hovertemplate = \"%{x} has %{y} users &lt;extra&gt;&lt;/extra&gt;\"\n    )\n)\nfig.show()"
  },
  {
    "objectID": "python/plotly/hovers.html#extra",
    "href": "python/plotly/hovers.html#extra",
    "title": "Hovers",
    "section": "<extra>",
    "text": "&lt;extra&gt;\nAn additional block is added to the tooltip - by default, it is a semi-prominent part to the left of the main tooltip. Its content is resolved by the &lt;extra&gt; tag in hovertemplate. To get rid of it, the &lt;extra&gt; tag should be left empty.\nSo in the following example, I build the scatter with different cases with &lt;extra&gt;:\n\nsample_size = 50\n\nplot_params = [\n    dict(name = \"No hovertemplate\"),\n    dict(\n        hovertemplate = \"%{x} - %{y}\",\n        name = \"No extra\",\n    ),\n    dict(\n        hovertemplate = \"%{x} - %{y}&lt;extra&gt;&lt;/extra&gt;\",\n        name = \"Empty extra\",\n    ),\n    dict(\n        hovertemplate = \"%{x} - %{y}&lt;extra&gt;random hovertext %{hovertext}&lt;/extra&gt;\",\n        name = \"Content in extra\",\n        hovertext = [\n            \"\".join(\n                np.random.choice([i for i in \"abdcefghijkilm\"], 10)\n            )\n            for i in range(sample_size)\n        ]\n    )\n]\n\nfig = go.Figure()\nfor pp in plot_params:\n    fig.add_trace(\n        go.Scatter(\n            x = np.random.rand(sample_size),\n            y = np.random.rand(sample_size),\n            mode = \"markers\",\n            marker_size = 10,\n            **pp\n        )\n    )\n\nfig.update_layout(height = 700)\n\nfig.show()"
  },
  {
    "objectID": "python/plotly/color_from_scheme.html",
    "href": "python/plotly/color_from_scheme.html",
    "title": "Color from scheme",
    "section": "",
    "text": "There is a description of plotly colors here. But I haven’t found any information about extracting colours from a colour scheme in hex or rgb form. However, there are some features that provide such an option.. There is official documentation for them.\nThe following example shows this operation for the RdYlGn_r colour scale:\n\nIn the first step we got a colour scale by name from the function plotly.colors.get_colorscale, it will be a list of tuples in the format (number, colour) - looks like it’s a way to describe colour schemes in plotly;\nBy using sample_colorscale we can get more colors including intermediate states between colours specified in the colour scheme;\nIn the end, a set of colours was visualised.\n\n\nimport numpy as np\nimport pandas as pd\nimport plotly as pltly\n\nimport plotly.graph_objects as go\n\nfrom IPython.display import HTML\n\n# extracting colorscale\ncolorscale = pltly.colors.get_colorscale('RdYlGn_r')\ndisplay(HTML(\"&lt;text style='font-size:20px'&gt;Color scale&lt;/text&gt;\"))\ndisplay(pd.DataFrame(colorscale, columns = [\"\", \"\"]))\n\n# creating sample basend on colorscale\nsample_colorscale = pltly.colors.sample_colorscale(\n    colorscale,\n    np.linspace(0, 1, 20)\n)\ndisplay(HTML(\"&lt;text style='font-size:20px'&gt;Sample color scale&lt;/text&gt;\"))\nfor col in sample_colorscale:\n    print(col)\n\n\n# sample visualisation\ndisplay(HTML(\"&lt;text style='font-size:20px'&gt;&lt;/text&gt;\"))\ngo.Figure(\n    [\n        go.Bar(\n            orientation = \"h\",\n            y = [\"test color\"],\n            x = [1],\n            marker = dict(color = col),\n            hovertemplate=\"%{hovertext}&lt;extra&gt;&lt;/extra&gt;\",\n            hovertext=[col]\n        )\n        for col in sample_colorscale\n    ],\n    layout=dict(\n        barmode = \"stack\",\n        xaxis = dict(\n            range=[0,len(sample_colorscale)],\n            showticklabels=False, \n            showgrid=False\n        ),\n        showlegend=False,\n        title = \"Visualisation\",\n        dragmode = False\n    )\n).show()\n\nColor scale\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n0.0\nrgb(0,104,55)\n\n\n1\n0.1\nrgb(26,152,80)\n\n\n2\n0.2\nrgb(102,189,99)\n\n\n3\n0.3\nrgb(166,217,106)\n\n\n4\n0.4\nrgb(217,239,139)\n\n\n5\n0.5\nrgb(255,255,191)\n\n\n6\n0.6\nrgb(254,224,139)\n\n\n7\n0.7\nrgb(253,174,97)\n\n\n8\n0.8\nrgb(244,109,67)\n\n\n9\n0.9\nrgb(215,48,39)\n\n\n10\n1.0\nrgb(165,0,38)\n\n\n\n\n\n\n\nSample color scale\n\n\nrgb(0, 104, 55)\nrgb(14, 129, 68)\nrgb(30, 154, 81)\nrgb(70, 173, 91)\nrgb(109, 192, 100)\nrgb(142, 207, 103)\nrgb(174, 220, 111)\nrgb(201, 232, 129)\nrgb(225, 242, 150)\nrgb(245, 251, 177)\nrgb(255, 247, 177)\nrgb(254, 231, 150)\nrgb(254, 208, 126)\nrgb(253, 182, 104)\nrgb(250, 150, 86)\nrgb(245, 116, 70)\nrgb(232, 83, 55)\nrgb(217, 51, 40)\nrgb(191, 25, 39)\nrgb(165, 0, 38)"
  },
  {
    "objectID": "python/dash/components.html",
    "href": "python/dash/components.html",
    "title": "Components",
    "section": "",
    "text": "On this page I’ll be exploring the possibilities of using different Plotly components."
  },
  {
    "objectID": "python/dash/components.html#basic-example",
    "href": "python/dash/components.html#basic-example",
    "title": "Components",
    "section": "Basic example",
    "text": "Basic example\ndash.html.Details allows you to define a group of elements that hide some other components and show them on click.\n\nfrom jupyter_dash import JupyterDash\nfrom dash import dcc, html\nfrom IPython.display import clear_output\n\nmy_list = [\n    \"BelarusSales\",\n    \"SalesNetworkBelarus\",\n    \"BelSalesNetwork\",\n    \"SalesNetworkBel\",\n    \"SalesOfBelarus\",\n    \"BelSalesNetworks\",\n    \"BelarusSalesNetwork\",\n    \"SalesInBelarus\",\n]\n\napp = JupyterDash(__name__)\n\napp.layout = html.Details(\n    [html.H3(i) for i in my_list],\n)\n\napp.run_server(debug=False, port = 8053)\nclear_output()"
  },
  {
    "objectID": "python/dash/components.html#summary-caption",
    "href": "python/dash/components.html#summary-caption",
    "title": "Components",
    "section": "Summary (caption)",
    "text": "Summary (caption)\nIf you want to change the caption of the details (text near the arrow), you should create a Summary object as a child of the Details tag. The important feature is that you can put other objects (like buttons and so on) in the Summary.\nRead more:\n\nMDN web docs about Details tag;\nDash documentation about dash.html.Details;\nDash documentation about dash.html.Summary.\n\nSo in the following example there is a Details with the caption “My details” and an additional button in the header.\n\nfrom jupyter_dash import JupyterDash\nfrom dash import dcc, html\nfrom IPython.display import clear_output\n\nmy_list = [\n    \"BelarusSales\",\n    \"SalesNetworkBelarus\",\n    \"BelSalesNetwork\",\n    \"SalesNetworkBel\",\n    \"SalesOfBelarus\",\n    \"BelSalesNetworks\",\n    \"BelarusSalesNetwork\",\n    \"SalesInBelarus\",\n]\n\napp = JupyterDash(__name__)\n\ncontents = [html.H3(i) for i in my_list]\nsummary = [\n    html.Summary([\n        \"My details\", \n        html.Button(\"Summary button\")\n    ])\n]\n\napp.layout = html.Details(contents + summary)\n\napp.run_server(debug=False, port = 8054)\nclear_output()"
  },
  {
    "objectID": "python/dash/components.html#overlapping",
    "href": "python/dash/components.html#overlapping",
    "title": "Components",
    "section": "Overlapping",
    "text": "Overlapping\nYou can put any content in the Details section, and when the element is scrolled it should move any content that follows it. So in the example below I use both simple strings and other dash components.\n\nfrom jupyter_dash import JupyterDash\nfrom dash import dcc, html\nfrom IPython.display import clear_output\n\napp = JupyterDash(__name__)\n\ncontents1 = [elem for i in range(10) for elem in [f\"line{i}\", html.Br()]]\n\ncontents2 = [dcc.Checklist(\n    options = [f\"Checkbox{i}\" for i in range(10)]\n)]\n\napp.layout = html.Div([\n        html.Details(contents1 + contents2),\n    \"Some other contents\"\n])\n\napp.run_server(debug=False, port = 8055)\nclear_output()"
  },
  {
    "objectID": "python/dash/empty_figure.html",
    "href": "python/dash/empty_figure.html",
    "title": "Empty figure",
    "section": "",
    "text": "Sometimes you need to have an empty dashboard, in case there is no data or when you start the dashboard. By default, Plotly displays only axes with no data and a grid. To get rid of this ugly element, you can simply set the element to be completely transparent.\nSo in the following example you can compare the default and fully transparent initial graph for dash.\n\nfrom dash import dcc, html\nfrom jupyter_dash import JupyterDash\nfrom plotly import graph_objects as go\nfrom IPython.display import clear_output\n\napp = JupyterDash(__name__)\noptions = list(range(0,20))\nvalue = [1,5]\n\nlst_val_to_slider_marks = lambda value: {val:str(val) for val in value}\n\napp.layout = html.Div(\n    [\n        dcc.Graph(),\n        dcc.Graph(\n            figure = go.Figure(\n                layout = dict(\n                    xaxis = {\"visible\" : False},\n                    yaxis = {\"visible\" : False},\n                    paper_bgcolor = 'rgba(0, 0, 0, 0)',\n                    plot_bgcolor = 'rgba(0, 0, 0, 0)',\n                    dragmode = False\n                )\n            )\n        )\n    ]\n)\n\napp.run_server(debug=True)\nclear_output()"
  },
  {
    "objectID": "python/advanced/collections.html",
    "href": "python/advanced/collections.html",
    "title": "collections library",
    "section": "",
    "text": "Allows date types that enhance standard Python collections."
  },
  {
    "objectID": "python/advanced/collections.html#defaultdict",
    "href": "python/advanced/collections.html#defaultdict",
    "title": "collections library",
    "section": "defaultdict",
    "text": "defaultdict\nA type that represents a dictionary to which a key is automatically added when an attempt is made to access that key.\n\nBasics\nYou need to pass a function that will return the items that will be used as default values.\nIn the following example, I have created a dictionary where when I try to access an unknown key in the dictionary, an int is created, one more than the last time.\n\nfrom collections import defaultdict\n\ncounter = 0\n\ndef dict_filler():\n    global counter\n    counter += 1\n    return counter\n\nmy_defaultdict = defaultdict(dict_filler)\n\nprint(\"before cycle \", dict(my_defaultdict))\n\nfor i in range(5):\n    print(f\"my_defaultdict[\\\"element{i}\\\"] == \", my_defaultdict[f\"element{i}\"])\n\nprint(\"after cycle \", dict(my_defaultdict))\n\nbefore cycle  {}\nmy_defaultdict[\"element0\"] ==  1\nmy_defaultdict[\"element1\"] ==  2\nmy_defaultdict[\"element2\"] ==  3\nmy_defaultdict[\"element3\"] ==  4\nmy_defaultdict[\"element4\"] ==  5\nafter cycle  {'element0': 1, 'element1': 2, 'element2': 3, 'element3': 4, 'element4': 5}\n\n\nA more applied example is calculating the frequencies of random integers. It is enough to create a defaultdict which by default uses 0 as a value for an unknown key. Without any initialisation, you can just use the += operator for the dictionary, where each key corresponds to the number of times a given number is used.\n\nimport random\nfrom collections import defaultdict\nmy_defaultdict = defaultdict(int)\n\nfor i in range(500):\n    my_defaultdict[random.randint(1, 10)] += 1\n\nsorted_dict = dict(sorted(my_defaultdict.items(), key=lambda x: x[0]))\nprint(sorted_dict)\n\n{1: 47, 2: 54, 3: 43, 4: 52, 5: 46, 6: 57, 7: 47, 8: 57, 9: 52, 10: 45}\n\n\n\n\nTo/from dict\n\n\nFrom\nYou can convert a regular dict to the defaultdict simply by passing it as the second argument to the defalutdict function.\nSo in the following example I will convert initial_dict to default_dict and then show that it has got properties of defaultdcit.\n\nfrom collections import defaultdict\ninitial_dict = {\"key1\": 1, \"key2\":3}\ndefault_dict = defaultdict(int, initial_dict)\nmy_defalut_value = default_dict[\"key3\"]\nprint(default_dict)\n\ndefaultdict(&lt;class 'int'&gt;, {'key1': 1, 'key2': 3, 'key3': 0})\n\n\n\n\nTo\nJust pass defaultdict to the dict function to get it as a regular Python dict.\n\nfrom collections import defaultdict\ndefault_dict = defaultdict(int)\n# create dict using defaultdict mechanisms\nfor i in range(10): default_dict[i]\n# but for clearer printing, let's convert it \n# into a regular dictionary\ndict(default_dict)\n\n{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}"
  },
  {
    "objectID": "python/advanced/collections.html#counter",
    "href": "python/advanced/collections.html#counter",
    "title": "collections library",
    "section": "Counter",
    "text": "Counter\nThis is a datatype that allows you to count items in the collection.\n\nBasic\nYou will have the result just as dictionary with items &lt;value in input collection&gt; : &lt;number of occurrences of an element in the collection&gt;.\nThe following example shows how Counter is applied to a list of frequently recurring items.\n\nfrom random import randint\nfrom collections import Counter\n\ninput_lst = [randint(1,4) for i in range(10)]\nprint(\"Initial list:\", input_lst)\nCounter(input_lst)\n\nInitial list: [2, 2, 4, 1, 1, 2, 4, 2, 2, 2]\n\n\nCounter({2: 6, 4: 2, 1: 2})\n\n\n\n\ndict properties\nCounter objects have all the properties of a normal dictionary.\nSo in the following example I show that Counter can be easily accessed by inedex.\n\nfrom random import randint\nfrom collections import Counter\n\ninput_lst = [randint(1,4) for i in range(10)]\nprint(\"Initial list:\", input_lst)\nmy_counter = Counter(input_lst)\nprint(f\"You can find '{2}' element {my_counter[2]} times in the input array\")\n\nInitial list: [2, 2, 2, 1, 3, 4, 4, 4, 2, 4]\nYou can find '2' element 4 times in the input array\n\n\n\n\nUnhashable types\nAs we know, unhashable types cannot be used as keys in a dictionary. Counter is just a modified dictionary, so you may get trouble trying to apply the Counter to the collection with unhashable items in it:\n\nCounter([[1,2,3], \"test\"])\n\nTypeError: unhashable type: 'list'"
  },
  {
    "objectID": "python/advanced/ipython_magics.html",
    "href": "python/advanced/ipython_magics.html",
    "title": "IPython magics",
    "section": "",
    "text": "A special expression in IPython that allows you to use some of the special features provided by IPython. There are two types of magics:"
  },
  {
    "objectID": "python/advanced/ipython_magics.html#lsmagic",
    "href": "python/advanced/ipython_magics.html#lsmagic",
    "title": "IPython magics",
    "section": "%lsmagic",
    "text": "%lsmagic\nThis command allows you to list available IPython magics\n\n%lsmagic\n\nAvailable line magics:\n%alias  %alias_magic  %autoawait  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %colors  %conda  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %man  %matplotlib  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %pip  %popd  %pprint  %precision  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n\nAvailable cell magics:\n%%!  %%HTML  %%SVG  %%bash  %%capture  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n\nAutomagic is ON, % prefix IS NOT needed for line magics."
  },
  {
    "objectID": "python/advanced/ipython_magics.html#time",
    "href": "python/advanced/ipython_magics.html#time",
    "title": "IPython magics",
    "section": "%time",
    "text": "%time\nMeasures the time of execution of some expression.\n\nBasic example\nHere I just have a function that runs a cycle with a given number of iterations, and show results for 10 and 500 iterations.\n\ndef some_computing(i_count):\n    for i in range(i_count): 5+5\n\nprint(\"=====10 iterations=====\")\n%time some_computing(10)\nprint(\"=====500 iterations=====\")\n%time some_computing(500)\n\n=====10 iterations=====\nCPU times: user 2 µs, sys: 0 ns, total: 2 µs\nWall time: 3.58 µs\n=====500 iterations=====\nCPU times: user 14 µs, sys: 2 µs, total: 16 µs\nWall time: 16.7 µs\n\n\n\n\nMetrics meaning\n\nCPU times gives description of the duration by stage of execution:\n\nuser user-side duration;\nsys system-side duration;\ntotal total duration of the calculation.\n\nWall time duration of the calculation with overhead resources (whatever they may be).\n\nNote Nowadays, I don’t know exactly what the difference is between the user side and the system side - but you should learn Linux to find out."
  },
  {
    "objectID": "python/advanced/ipython_magics.html#timeit",
    "href": "python/advanced/ipython_magics.html#timeit",
    "title": "IPython magics",
    "section": "%timeit",
    "text": "%timeit\nEstimates the execution time of a certain command over a series of runs.\nIn the following example I have a function that runs a cycle with a given number of iterations, and show results for 10 and 500 iterations.\n\ndef some_computing(i_count):\n    for i in range(i_count): 5+5\n\nprint(\"=====10 iterations=====\")\n%timeit some_computing(10)\nprint(\"=====500 iterations=====\")\n%timeit some_computing(500)\n\n=====10 iterations=====\n203 ns ± 6.87 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n=====500 iterations=====\n5.88 µs ± 115 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)"
  },
  {
    "objectID": "python/advanced/ipython_magics.html#system-command",
    "href": "python/advanced/ipython_magics.html#system-command",
    "title": "IPython magics",
    "section": "! - system command",
    "text": "! - system command\nYou can access the environment you ran IPython from by simply adding ! before the line. For linux/macOS this is usually terminal line for windows it is usually powerShall.\nIn the following example, I ran the Linux ls command from Jupyter Cell.\n\n!ls -l\n\ntotal 196\n-rwxrwxr-x 1 fedor fedor 34744 жні 26 20:36 basic_datatypes.ipynb\ndrwxrwxr-x 2 fedor fedor  4096 жні 26 20:39 class_interface\ndrwxrwxr-x 2 fedor fedor  4096 жні 26 20:39 class_spesials\n-rw-rw-r-- 1 fedor fedor 11496 жні 26 20:36 collections.ipynb\ndrwxrwxr-x 2 fedor fedor  4096 жні 26 20:39 excel_export_files\n-rwxrwxr-x 1 fedor fedor  2109 жні 26 20:36 excel_export.ipynb\n-rwxrwxr-x 1 fedor fedor 14411 жні 26 20:36 exceptions.ipynb\n-rw-rw-r-- 1 fedor fedor  1696 жні 26 20:36 functions.ipynb\ndrwxrwxr-x 4 fedor fedor  4096 жні 26 20:39 ipython_magics_files\n-rw-rw-r-- 1 fedor fedor 18796 жні 27 12:34 ipython_magics.ipynb\ndrwxrwxr-x 2 fedor fedor  4096 жні 26 20:39 logging_files\n-rwxrwxr-x 1 fedor fedor 17750 жні 26 20:36 logging.ipynb\n-rwxrwxr-x 1 fedor fedor  5037 жні 26 20:36 multi_threads.ipynb\ndrwxrwxr-x 2 fedor fedor  4096 жні 26 20:39 numpy\ndrwxrwxr-x 2 fedor fedor  4096 жні 26 20:39 operators_files\n-rwxrwxr-x 1 fedor fedor 14722 жні 26 20:36 operators.ipynb\n-rwxrwxr-x 1 fedor fedor     0 жні 26 20:36 output.xlsx\n-rw-rw-r-- 1 fedor fedor  1448 жні 26 20:36 packages.ipynb\ndrwxrwxr-x 2 fedor fedor  4096 жні 26 20:39 regex\ndrwxrwxr-x 2 fedor fedor  4096 жні 26 20:39 tkinter\ndrwxrwxr-x 2 fedor fedor  4096 жні 26 20:36 venv_files\n-rwxrwxr-x 1 fedor fedor 13168 жні 26 20:36 virtual_environment.ipynb"
  },
  {
    "objectID": "python/advanced/ipython_magics.html#specific-code",
    "href": "python/advanced/ipython_magics.html#specific-code",
    "title": "IPython magics",
    "section": "Specific code",
    "text": "Specific code\nYou can use some other codes in IPython just specifying them with magic &lt;code type&gt;. Next, I list some of the options available.\nNote looks like this magic is only available in whole cell mode.\n\nbash\n\n%%bash\nls -l\n\ntotal 16\n-rw-rw-r-- 1 fedor fedor 4127 жні 26 18:40 magic_commands.ipynb\ndrwxrwxr-x 3 fedor fedor 4096 жні 25 15:10 nbconvert\ndrwxrwxr-x 4 fedor fedor 4096 жні 25 15:10 voila_vs_nbconvert_saving_html\n\n\n\n\nHTML\n\n%%HTML\n&lt;button&gt;Button as HTML element&lt;/button&gt;\n\nButton as HTML element"
  },
  {
    "objectID": "python/advanced/ipython_magics.html#load_ext",
    "href": "python/advanced/ipython_magics.html#load_ext",
    "title": "IPython magics",
    "section": "%load_ext",
    "text": "%load_ext\nAllows you to load extentions of IPython.  Learn more. Looks like that’s a topic for another page, but for now I’ll just mention a few extensions that I use.\n\nautoreload\nBy default, IPython will only read imported modules when their import is mentioned for the first time. But this option allows you to set some specific rules for importing modules. Official documentation.\nIn essence this extension doles out two magic %autoreload and %aimport:\n\n%autoreload - specifies the import mode;\n%aimport - sets exceptions for %autoreload if they are provided by the current mode.\n\nNote look like you should use the full path to the files if you want to mention them for %aimport.\n\n%autoreload 1/explicit\nBefore executing a cell, only modules mentioned with %aimport will be reloaded.\nIn the following example I:\n\nCreate two files fun&lt;i&gt; \\(i \\in {1,2}\\), each containing only one function that returns a line like \"Original line from fun&lt;i&gt;.py\";\nImport them using the %autoreload explicit magic and mentioning fun1 in %aimport.\nCall them, and got obvious result - Original line from fun&lt;i&gt;.py;\nThen change both functions to return New line from fun&lt;i&gt;.py;\nCall the functions again:\n\nFunction from fun1 has changed the line;\nFunction from fun2 still has the word Original at the beginning (because it was mentioned in %aimport as module to me reloaded).\n\n\n\n%%bash\ncat &gt; ipython_magics_files/fun1.py &lt;&lt; EOF\ndef return_line():\n    return \"Original line from fun1.py\"\nEOF\ncat &gt; ipython_magics_files/fun2.py &lt;&lt; EOF\ndef return_line():\n    return \"Original line from fun2.py\"\nEOF\n\n\n%load_ext autoreload\n%autoreload explicit\nfrom ipython_magics_files import fun1, fun2\n%aimport ipython_magics_files.fun1\n\nprint(fun1.return_line())\nprint(fun2.return_line())\n\nOriginal line from fun1.py\nOriginal line from fun2.py\n\n\n\n%%bash\ncat &gt; ipython_magics_files/fun1.py &lt;&lt; EOF\ndef return_line():\n    return \"New line from fun1.py\"\nEOF\ncat &gt; ipython_magics_files/fun2.py &lt;&lt; EOF\ndef return_line():\n    return \"New line from fun2.py\"\nEOF\n\n\nprint(fun1.return_line())\nprint(fun2.return_line())\n\nNew line from fun1.py\nOriginal line from fun2.py\n\n\n\n\n%autoreload 2/all\nIt’s the most common mode for me - reload all modules before running any cell except those mentioned in %aimport.\nIf you want to exclude some modules from reimport, you should use %aimport -&lt;module name&gt; (the “-” symbol is crucial here).\nIn the following example I:\n\nCreate two files fun&lt;i&gt; \\(i \\in {1,2}\\), each containing only one function that returns a line like \"Original line from fun&lt;i&gt;.py\";\nImport them using the %autoreload all magic and mentioning fun2 in %aimport.\nCall them, and got obvious result - Original line from fun&lt;i&gt;.py;\nThen change both functions to return New line from fun&lt;i&gt;.py;\nCall the functions again:\n\nFunction from fun1 has changed the line;\nFunction from fun2 still has the word Original at the beginning (because it was mentioned in %aimport as an exclusion).\n\n\n\n%%bash\ncat &gt; ipython_magics_files/fun1.py &lt;&lt; EOF\ndef return_line():\n    return \"Original line from fun1.py\"\nEOF\ncat &gt; ipython_magics_files/fun2.py &lt;&lt; EOF\ndef return_line():\n    return \"Original line from fun2.py\"\nEOF\n\n\n%load_ext autoreload\n%autoreload all\n\nfrom ipython_magics_files import fun1, fun2\n%aimport -ipython_magics_files.fun2\nprint(fun1.return_line())\nprint(fun2.return_line())\n\nOriginal line from fun1.py\nOriginal line from fun2.py\n\n\n\n%%bash\ncat &gt; ipython_magics_files/fun1.py &lt;&lt; EOF\ndef return_line():\n    return \"New line from fun1.py\"\nEOF\ncat &gt; ipython_magics_files/fun2.py &lt;&lt; EOF\ndef return_line():\n    return \"New line from fun2.py\"\nEOF\n\n\nprint(fun1.return_line())\nprint(fun2.return_line())\n\nNew line from fun1.py\nOriginal line from fun2.py"
  },
  {
    "objectID": "python/advanced/ipython_magics.html#writefile",
    "href": "python/advanced/ipython_magics.html#writefile",
    "title": "IPython magics",
    "section": "%%writefile",
    "text": "%%writefile\nThis is the magic that allows you to save the contents of the cell to a file. You need to use the syntax %%writefile &lt;directory&gt; in the first line of the cell to save it in the &lt;directory&gt;.\nAll code cells in this section are the same example and should be run one by one.\nA new folder has been created here. The ls command confirms that it’s empty.\n\n%%bash\nmkdir ipython_magics_files/writefile_files\nls ipython_magics_files/writefile_files\n\nUsing %%writefile to a non-existent file.\n\n%%writefile ipython_magics_files/writefile_files/writefile\nthis is the file from the cell\n\nWriting ipython_magics_files/writefile_files/writefile\n\n\nVerify that the file has been created and contains the content specified in the previous cell.\n\n%%bash \necho \"=====files=====\"\nls ipython_magics_files/writefile_files\necho \"=====content=====\"\ncat ipython_magics_files/writefile_files/writefile\n\n=====files=====\nwritefile\n=====content=====\nthis is the file from the cell\n\n\nNow let’s try using the same file, but for a different cell. The message Overwriting ... indicates that the content will be completely replaced.\n\n%%writefile ipython_magics_files/writefile_files/writefile\nsome other message for another cell\n\nOverwriting ipython_magics_files/writefile_files/writefile\n\n\nMake sure that the contents of the file are from the last cell.\n\n%%bash\ncat ipython_magics_files/writefile_files/writefile\n\nsome other message for another cell\n\n\nFinally delete the folder so that the content remains fully playable.\n\n%%bash\nrm -r ipython_magics_files/writefile_files/"
  },
  {
    "objectID": "python/advanced/geopy/distances.html",
    "href": "python/advanced/geopy/distances.html",
    "title": "Distances",
    "section": "",
    "text": "Geopy calculate distances.\nGeopy provides a tool for calculating distances on maps.\nfrom geopy import distance\nfrom geopy.geocoders import Nominatim"
  },
  {
    "objectID": "python/advanced/geopy/distances.html#compution",
    "href": "python/advanced/geopy/distances.html#compution",
    "title": "Distances",
    "section": "Compution",
    "text": "Compution\nYou need to use the geopy.distance.distance function to calculate the distance between two points. In the following example the distance between Minsk and Molodechno has been calculated - it looks quite close.\n\ngeolocator = Nominatim(user_agent = \"knowledge\")\n\nlocation1 = geolocator.geocode(\"Minsk\")\nlocation2 = geolocator.geocode(\"Molodechno\")\n\nmy_distance = distance.distance(\n    (location1.latitude, location1.longitude),\n    (location2.latitude, location2.longitude)\n)\nprint(f\"Minsk&lt;-&gt;Molodechno distance is {my_distance.kilometers} km\")\n\nMinsk&lt;-&gt;Molodechno distance is 64.89474666491046 km"
  },
  {
    "objectID": "python/advanced/geopy/distances.html#distance-object",
    "href": "python/advanced/geopy/distances.html#distance-object",
    "title": "Distances",
    "section": "Distance object",
    "text": "Distance object\ngeopy.distance.distance returns an object of type geopy.distance.geodesic. The main properties of the object are displayed in the following cell.\n\nfor field in dir(my_distance):\n    if field[0] != \"_\":\n        print(field)\n\nELLIPSOID\ndestination\nellipsoid_key\nfeet\nft\ngeod\nkilometers\nkm\nm\nmeasure\nmeters\nmi\nmiles\nnautical\nnm\nset_ellipsoid"
  },
  {
    "objectID": "python/advanced/airflow/tutorial_dag.html",
    "href": "python/advanced/airflow/tutorial_dag.html",
    "title": "Tutorial DAG",
    "section": "",
    "text": "Using this jupyter notebook you can run DAG from this example."
  },
  {
    "objectID": "python/advanced/airflow/tutorial_dag.html#dag-code",
    "href": "python/advanced/airflow/tutorial_dag.html#dag-code",
    "title": "Tutorial DAG",
    "section": "DAG code",
    "text": "DAG code\nThe following is the DAG code that we just copied from the example into the prepared folder.\n\n%%writefile tutorial_dag/tutorial_dug.py\n\nfrom datetime import datetime, timedelta\nfrom textwrap import dedent\n\n# The DAG object; we'll need this to instantiate a DAG\nfrom airflow import DAG\n\n# Operators; we need this to operate!\nfrom airflow.operators.bash import BashOperator\nwith DAG(\n    \"tutorial\",\n    # These args will get passed on to each operator\n    # You can override them on a per-task basis during operator initialization\n    default_args={\n        \"depends_on_past\": False,\n        \"email\": [\"airflow@example.com\"],\n        \"email_on_failure\": False,\n        \"email_on_retry\": False,\n        \"retries\": 1,\n        \"retry_delay\": timedelta(minutes=5),\n        # 'queue': 'bash_queue',\n        # 'pool': 'backfill',\n        # 'priority_weight': 10,\n        # 'end_date': datetime(2016, 1, 1),\n        # 'wait_for_downstream': False,\n        # 'sla': timedelta(hours=2),\n        # 'execution_timeout': timedelta(seconds=300),\n        # 'on_failure_callback': some_function, # or list of functions\n        # 'on_success_callback': some_other_function, # or list of functions\n        # 'on_retry_callback': another_function, # or list of functions\n        # 'sla_miss_callback': yet_another_function, # or list of functions\n        # 'trigger_rule': 'all_success'\n    },\n    description=\"A simple tutorial DAG\",\n    schedule=timedelta(days=1),\n    start_date=datetime(2021, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n\n    # t1, t2 and t3 are examples of tasks created by instantiating operators\n    t1 = BashOperator(\n        task_id=\"print_date\",\n        bash_command=\"date\",\n    )\n\n    t2 = BashOperator(\n        task_id=\"sleep\",\n        depends_on_past=False,\n        bash_command=\"sleep 5\",\n        retries=3,\n    )\n    t1.doc_md = dedent(\n        \"\"\"\\\n    #### Task Documentation\n    You can document your task using the attributes `doc_md` (markdown),\n    `doc` (plain text), `doc_rst`, `doc_json`, `doc_yaml` which gets\n    rendered in the UI's Task Instance Details page.\n    ![img](http://montcs.bloomu.edu/~bobmon/Semesters/2012-01/491/import%20soul.png)\n    **Image Credit:** Randall Munroe, [XKCD](https://xkcd.com/license.html)\n    \"\"\"\n    )\n\n    dag.doc_md = __doc__  # providing that you have a docstring at the beginning of the DAG; OR\n    dag.doc_md = \"\"\"\n    This is a documentation placed anywhere\n    \"\"\"  # otherwise, type it like this\n    templated_command = dedent(\n        \"\"\"\n    {% for i in range(5) %}\n        echo \"{{ ds }}\"\n        echo \"{{ macros.ds_add(ds, 7)}}\"\n    {% endfor %}\n    \"\"\"\n    )\n\n    t3 = BashOperator(\n        task_id=\"templated\",\n        depends_on_past=False,\n        bash_command=templated_command,\n    )\n\n    t1 &gt;&gt; [t2, t3]\n\nOverwriting tutorial_dag/tutorial_dug.py"
  },
  {
    "objectID": "python/advanced/airflow/tutorial_dag.html#container",
    "href": "python/advanced/airflow/tutorial_dag.html#container",
    "title": "Tutorial DAG",
    "section": "Container",
    "text": "Container\nBy running the following cell you start an instance of ariflow with the prepared example dug in.\n\n!docker run -itd --rm\\\n    --name tutorial_dag\\\n    -p 8080:8080\\\n    -v ./tutorial_dag:/root/airflow/dags\\\n    airflow_tests &&gt; /dev/null\n\nLet’s check that the tutorial dag has been added to the airflow.\n\n!docker exec tutorial_dag airflow dags list\n\ndag_id   | filepath        | owner   | paused\n=========+=================+=========+=======\ntutorial | tutorial_dug.py | airflow | True  \n                                             \n\n\nDon’t forget to stop the container when you have finished playing with the example.\n\n!docker stop tutorial_dag &&gt; /dev/null"
  },
  {
    "objectID": "python/advanced/airflow/tutorial_dag.html#test-task",
    "href": "python/advanced/airflow/tutorial_dag.html#test-task",
    "title": "Tutorial DAG",
    "section": "Test task",
    "text": "Test task\nBy using the command airflow tasks test you can execute the task and get it’s output just in terminal. Like in the example below.\nNone For some unknown reason, the first execution of this command causes a Python error. This shouldn’t affect us for now - the logs are fine, but I need to look into it more.\n\n%%bash\necho \"=====test=====\"\ndocker exec tutorial_dag airflow tasks test tutorial print_date 2015-06-01\n\n=====test=====\n[2023-09-24T13:00:55.874+0000] {dagbag.py:539} INFO - Filling up the DagBag from /root/airflow/dags\n[2023-09-24T13:00:55.986+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=&lt;TaskInstance: tutorial.print_date __airflow_temporary_run_2023-09-24T13:00:44.709050+00:00__ [None]&gt;\n[2023-09-24T13:00:55.990+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=&lt;TaskInstance: tutorial.print_date __airflow_temporary_run_2023-09-24T13:00:44.709050+00:00__ [None]&gt;\n[2023-09-24T13:00:55.990+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 2\n[2023-09-24T13:00:55.990+0000] {taskinstance.py:1428} WARNING - cannot record queued_duration for task print_date because previous state change time has not been saved\n[2023-09-24T13:00:55.991+0000] {taskinstance.py:1380} INFO - Executing &lt;Task(BashOperator): print_date&gt; on 2015-06-01 00:00:00+00:00\n[2023-09-24T13:00:56.017+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='airflow@example.com' AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='tutorial' AIRFLOW_CTX_TASK_ID='print_date' AIRFLOW_CTX_EXECUTION_DATE='2015-06-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='__airflow_temporary_run_2023-09-24T13:00:44.709050+00:00__'\n[2023-09-24T13:00:56.019+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp\n[2023-09-24T13:00:56.020+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'date']\n[2023-09-24T13:00:56.027+0000] {subprocess.py:86} INFO - Output:\n[2023-09-24T13:00:56.028+0000] {subprocess.py:93} INFO - Sun Sep 24 13:00:56 UTC 2023\n[2023-09-24T13:00:56.028+0000] {subprocess.py:97} INFO - Command exited with return code 0\n[2023-09-24T13:00:56.043+0000] {taskinstance.py:1398} INFO - Marking task as SUCCESS. dag_id=tutorial, task_id=print_date, execution_date=20150601T000000, start_date=, end_date=20230924T130056"
  },
  {
    "objectID": "python/advanced/build_package.html",
    "href": "python/advanced/build_package.html",
    "title": "Build package",
    "section": "",
    "text": "This page is focused on ways to upload python project to PyPI. Check Packaging Python Projects on python.org."
  },
  {
    "objectID": "python/advanced/build_package.html#files",
    "href": "python/advanced/build_package.html#files",
    "title": "Build package",
    "section": "Files",
    "text": "Files\nThis section describes what files you need to create in the project folder to crate the package.\n\nSoucre files\nIn the folder src/exmaple_package I’ll just store files with sorce code of the project.\nNote The example_package folder is crucial here! It’s the literal name that will be used to import this module.\ninit.py\nIt’s common for folders containing Python modules to have __init__.py.\n\n%%writefile build_package_files/src/example_package/__init__.py\nprint(\"Init me!\")\n\nOverwriting build_package_files/src/example_package/__init__.py\n\n\nexample.py\nIs a toy module that can be imported after installing this library.\n\n%%writefile build_package_files/src/example_package/example.py\ndef do_someting():\n    print(\"doing something\")\n\nOverwriting build_package_files/src/example_package/example.py\n\n\n\n\nREADME\nActually you can skip this step, but without it you may get a warning when building the package.\n\n%%writefile build_package_files/README.md\n# Example\nThis is a toy package I'm using to get used to deploying Python packages on PyPI.\n\nOverwriting build_package_files/README.md\n\n\n\n\nToml\nFile describing the project.\n\n%%writefile build_package_files/pyproject.toml\n[project]\nname = \"example_package_FEDOR_KOBAK\"\nversion = \"0.0.3\"\nauthors = [\n  { name=\"Fedor Kobak\", email=\"kobfedsur@gmail.com\" },\n]\ndescription = \"A small example package\"\n\nOverwriting build_package_files/pyproject.toml\n\n\n\n\nModule building\nThere is a special module in Python build that can be used to build Python packages. Just use python3 -m build.\nNote Don’t forget to install this module, it is not out of the box.\n\n%%bash\ncd build_package_files\npython3 -m build\n\n* Creating venv isolated environment...\n* Installing packages in isolated environment... (setuptools &gt;= 40.8.0, wheel)\n* Getting build dependencies for sdist...\nrunning egg_info\nwriting src/example_package_FEDOR_KOBAK.egg-info/PKG-INFO\nwriting dependency_links to src/example_package_FEDOR_KOBAK.egg-info/dependency_links.txt\nwriting top-level names to src/example_package_FEDOR_KOBAK.egg-info/top_level.txt\nreading manifest file 'src/example_package_FEDOR_KOBAK.egg-info/SOURCES.txt'\nwriting manifest file 'src/example_package_FEDOR_KOBAK.egg-info/SOURCES.txt'\n* Building sdist...\nrunning sdist\nrunning egg_info\nwriting src/example_package_FEDOR_KOBAK.egg-info/PKG-INFO\nwriting dependency_links to src/example_package_FEDOR_KOBAK.egg-info/dependency_links.txt\nwriting top-level names to src/example_package_FEDOR_KOBAK.egg-info/top_level.txt\nreading manifest file 'src/example_package_FEDOR_KOBAK.egg-info/SOURCES.txt'\nwriting manifest file 'src/example_package_FEDOR_KOBAK.egg-info/SOURCES.txt'\nrunning check\ncreating example_package_FEDOR_KOBAK-0.0.3\ncreating example_package_FEDOR_KOBAK-0.0.3/src\ncreating example_package_FEDOR_KOBAK-0.0.3/src/example_package\ncreating example_package_FEDOR_KOBAK-0.0.3/src/example_package_FEDOR_KOBAK.egg-info\ncopying files to example_package_FEDOR_KOBAK-0.0.3...\ncopying README.md -&gt; example_package_FEDOR_KOBAK-0.0.3\ncopying pyproject.toml -&gt; example_package_FEDOR_KOBAK-0.0.3\ncopying src/example_package/__init__.py -&gt; example_package_FEDOR_KOBAK-0.0.3/src/example_package\ncopying src/example_package/example.py -&gt; example_package_FEDOR_KOBAK-0.0.3/src/example_package\ncopying src/example_package_FEDOR_KOBAK.egg-info/PKG-INFO -&gt; example_package_FEDOR_KOBAK-0.0.3/src/example_package_FEDOR_KOBAK.egg-info\ncopying src/example_package_FEDOR_KOBAK.egg-info/SOURCES.txt -&gt; example_package_FEDOR_KOBAK-0.0.3/src/example_package_FEDOR_KOBAK.egg-info\ncopying src/example_package_FEDOR_KOBAK.egg-info/dependency_links.txt -&gt; example_package_FEDOR_KOBAK-0.0.3/src/example_package_FEDOR_KOBAK.egg-info\ncopying src/example_package_FEDOR_KOBAK.egg-info/top_level.txt -&gt; example_package_FEDOR_KOBAK-0.0.3/src/example_package_FEDOR_KOBAK.egg-info\ncopying src/example_package_FEDOR_KOBAK.egg-info/SOURCES.txt -&gt; example_package_FEDOR_KOBAK-0.0.3/src/example_package_FEDOR_KOBAK.egg-info\nWriting example_package_FEDOR_KOBAK-0.0.3/setup.cfg\nCreating tar archive\nremoving 'example_package_FEDOR_KOBAK-0.0.3' (and everything under it)\n* Building wheel from sdist\n* Creating venv isolated environment...\n* Installing packages in isolated environment... (setuptools &gt;= 40.8.0, wheel)\n* Getting build dependencies for wheel...\nrunning egg_info\nwriting src/example_package_FEDOR_KOBAK.egg-info/PKG-INFO\nwriting dependency_links to src/example_package_FEDOR_KOBAK.egg-info/dependency_links.txt\nwriting top-level names to src/example_package_FEDOR_KOBAK.egg-info/top_level.txt\nreading manifest file 'src/example_package_FEDOR_KOBAK.egg-info/SOURCES.txt'\nwriting manifest file 'src/example_package_FEDOR_KOBAK.egg-info/SOURCES.txt'\n* Installing packages in isolated environment... (wheel)\n* Building wheel...\nrunning bdist_wheel\nrunning build\nrunning build_py\ncreating build\ncreating build/lib\ncreating build/lib/example_package\ncopying src/example_package/__init__.py -&gt; build/lib/example_package\ncopying src/example_package/example.py -&gt; build/lib/example_package\nrunning egg_info\nwriting src/example_package_FEDOR_KOBAK.egg-info/PKG-INFO\nwriting dependency_links to src/example_package_FEDOR_KOBAK.egg-info/dependency_links.txt\nwriting top-level names to src/example_package_FEDOR_KOBAK.egg-info/top_level.txt\nreading manifest file 'src/example_package_FEDOR_KOBAK.egg-info/SOURCES.txt'\nwriting manifest file 'src/example_package_FEDOR_KOBAK.egg-info/SOURCES.txt'\ninstalling to build/bdist.linux-x86_64/wheel\nrunning install\nrunning install_lib\ncreating build/bdist.linux-x86_64\ncreating build/bdist.linux-x86_64/wheel\ncreating build/bdist.linux-x86_64/wheel/example_package\ncopying build/lib/example_package/__init__.py -&gt; build/bdist.linux-x86_64/wheel/example_package\ncopying build/lib/example_package/example.py -&gt; build/bdist.linux-x86_64/wheel/example_package\nrunning install_egg_info\nCopying src/example_package_FEDOR_KOBAK.egg-info to build/bdist.linux-x86_64/wheel/example_package_FEDOR_KOBAK-0.0.3-py3.10.egg-info\nrunning install_scripts\ncreating build/bdist.linux-x86_64/wheel/example_package_FEDOR_KOBAK-0.0.3.dist-info/WHEEL\ncreating '/home/fedor/Documents/knowledge/python/basics/build_package_files/dist/.tmp-rnzqnn5v/example_package_FEDOR_KOBAK-0.0.3-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\nadding 'example_package/__init__.py'\nadding 'example_package/example.py'\nadding 'example_package_FEDOR_KOBAK-0.0.3.dist-info/METADATA'\nadding 'example_package_FEDOR_KOBAK-0.0.3.dist-info/WHEEL'\nadding 'example_package_FEDOR_KOBAK-0.0.3.dist-info/top_level.txt'\nadding 'example_package_FEDOR_KOBAK-0.0.3.dist-info/RECORD'\nremoving build/bdist.linux-x86_64/wheel\nSuccessfully built example_package_FEDOR_KOBAK-0.0.3.tar.gz and example_package_FEDOR_KOBAK-0.0.3-py3-none-any.whl"
  },
  {
    "objectID": "python/advanced/build_package.html#final-structure",
    "href": "python/advanced/build_package.html#final-structure",
    "title": "Build package",
    "section": "Final structure",
    "text": "Final structure\nAfter completing the previous steps, you should have the project structure described below. In the dist folder you will have archives that that make up the assembled module.\n\n%%bash\ntree build_package_files\n\nbuild_package_files\n├── dist\n│   ├── example_package_FEDOR_KOBAK-0.0.3-py3-none-any.whl\n│   └── example_package_FEDOR_KOBAK-0.0.3.tar.gz\n├── pyproject.toml\n├── README.md\n└── src\n    ├── example_package\n    │   ├── example.py\n    │   └── __init__.py\n    └── example_package_FEDOR_KOBAK.egg-info\n        ├── dependency_links.txt\n        ├── PKG-INFO\n        ├── SOURCES.txt\n        └── top_level.txt\n\n4 directories, 10 files"
  },
  {
    "objectID": "python/advanced/build_package.html#uploading",
    "href": "python/advanced/build_package.html#uploading",
    "title": "Build package",
    "section": "Uploading",
    "text": "Uploading\nI’ll use PyPI test - which is created to try uploading modules.\nRun the following command. I can’t just run it on the page because I need to complete authentication.\npython3 -m twine upload --repository testpypi build_package_files/dist/*\nNote Remove the --repository testpypi argument to use the original PyPI by default.\nSo after you complete all previous steps you have to have page for this library."
  },
  {
    "objectID": "python/advanced/build_package.html#using-new-package",
    "href": "python/advanced/build_package.html#using-new-package",
    "title": "Build package",
    "section": "Using new package",
    "text": "Using new package\nNow let’s download and use the package we just created. We install it using -i &lt;url&gt; because it is not stored in PyPI but in test PyPI. For the import we use the name example because all the code was stored in src/example but during the build we set src as the source folder.\n\n%%bash\npython3 -m venv build_package_files/venv\nsource build_package_files/venv/bin/activate\npip3 install -i https://test.pypi.org/simple/ example-package-FEDOR-KOBAK\n\necho; echo\n\npython3\nfrom example_package.example import do_someting\ndo_someting()\n\nLooking in indexes: https://test.pypi.org/simple/\nCollecting example-package-FEDOR-KOBAK\n  Using cached https://test-files.pythonhosted.org/packages/f8/af/c52553f3add7723736de1470fc56b01d9be04960cd5cf025586d8bf5cbac/example_package_FEDOR_KOBAK-0.0.3-py3-none-any.whl (1.6 kB)\nInstalling collected packages: example-package-FEDOR-KOBAK\nSuccessfully installed example-package-FEDOR-KOBAK-0.0.3\n\n\nInit me!\ndoing something\n\n\nNote for experiment was launched special virtual environment, but after playing with this notebook we don’t need it, so we can just delete it.\n\n%%bash\nrm -r build_package_files/venv"
  },
  {
    "objectID": "python/advanced/jinja.html",
    "href": "python/advanced/jinja.html",
    "title": "Jinja",
    "section": "",
    "text": "You can easily generate some html using jinja templates. For example, in the following cell I have displayed python dict as html using jinja templates.\n\nfrom jinja2 import Template\nfrom IPython.display import HTML\n\ndata = {'apple': 3, 'banana': 5, 'orange': 2}\n\ntemplate = Template('''\n    &lt;table&gt;\n        &lt;thead&gt;\n            &lt;tr&gt;\n                &lt;th&gt;Fruit&lt;/th&gt;\n                &lt;th&gt;Quantity&lt;/th&gt;\n            &lt;/tr&gt;\n        &lt;/thead&gt;\n        &lt;tbody&gt;\n            {% for key, value in data.items() %}\n                &lt;tr&gt;\n                    &lt;td&gt;{{ key }}&lt;/td&gt;\n                    &lt;td&gt;{{ value }}&lt;/td&gt;\n                &lt;/tr&gt;\n            {% endfor %}\n        &lt;/tbody&gt;\n    &lt;/table&gt;\n''')\n\nhtml = template.render(data=data)\nHTML(html)\n\n\n    \n\n\n\nFruit\nQuantity\n\n\n\n\napple\n3\n\n\nbanana\n5\n\n\norange\n2"
  },
  {
    "objectID": "python/advanced/jinja.html#generate-html",
    "href": "python/advanced/jinja.html#generate-html",
    "title": "Jinja",
    "section": "",
    "text": "You can easily generate some html using jinja templates. For example, in the following cell I have displayed python dict as html using jinja templates.\n\nfrom jinja2 import Template\nfrom IPython.display import HTML\n\ndata = {'apple': 3, 'banana': 5, 'orange': 2}\n\ntemplate = Template('''\n    &lt;table&gt;\n        &lt;thead&gt;\n            &lt;tr&gt;\n                &lt;th&gt;Fruit&lt;/th&gt;\n                &lt;th&gt;Quantity&lt;/th&gt;\n            &lt;/tr&gt;\n        &lt;/thead&gt;\n        &lt;tbody&gt;\n            {% for key, value in data.items() %}\n                &lt;tr&gt;\n                    &lt;td&gt;{{ key }}&lt;/td&gt;\n                    &lt;td&gt;{{ value }}&lt;/td&gt;\n                &lt;/tr&gt;\n            {% endfor %}\n        &lt;/tbody&gt;\n    &lt;/table&gt;\n''')\n\nhtml = template.render(data=data)\nHTML(html)\n\n\n    \n\n\n\nFruit\nQuantity\n\n\n\n\napple\n3\n\n\nbanana\n5\n\n\norange\n2"
  },
  {
    "objectID": "python/pandas/iloc.html",
    "href": "python/pandas/iloc.html",
    "title": "<DataFrame/Series>.iloc",
    "section": "",
    "text": "iloc is a way of selecting elements from DataFrames/Series using it index position like this [&lt;row number&gt;,&lt;column number&gt;].\nIn the next cell I create the data frame for the experiments.\n\nimport pandas as pd\nimport numpy as np\nfrom IPython.display import HTML\n\nsample_size = 10\n\ntest_df = pd.DataFrame(\n    {\n        \"a\":np.random.choice(range(10), sample_size),\n        \"b\":np.random.choice(range(10), sample_size)\n    },\n    index = [chr(i) for i in range(ord(\"a\"), ord(\"a\")+sample_size)]\n)\ntest_df\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\na\n5\n8\n\n\nb\n1\n4\n\n\nc\n7\n3\n\n\nd\n1\n3\n\n\ne\n4\n1\n\n\nf\n1\n5\n\n\ng\n9\n1\n\n\nh\n8\n0\n\n\ni\n9\n4\n\n\nj\n2\n3\n\n\n\n\n\n\n\n\nEditable\nYou can edit element that you access throw .iloc.\nSo in following example I edited one element by accessing it through iloc.\n\nedit_df_test = test_df.copy()\ndisplay(HTML(\"&lt;b&gt;Initial df&lt;/b&gt;\"))\ndisplay(edit_df_test)\n\nedit_df_test.iloc[1,1] = \"hello\"\ndisplay(HTML(\"&lt;b&gt;Edited df&lt;/b&gt;\"))\ndisplay(edit_df_test)\n\nInitial df\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\na\n5\n8\n\n\nb\n1\n4\n\n\nc\n7\n3\n\n\nd\n1\n3\n\n\ne\n4\n1\n\n\nf\n1\n5\n\n\ng\n9\n1\n\n\nh\n8\n0\n\n\ni\n9\n4\n\n\nj\n2\n3\n\n\n\n\n\n\n\nEdited df\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\na\n5\n8\n\n\nb\n1\nhello\n\n\nc\n7\n3\n\n\nd\n1\n3\n\n\ne\n4\n1\n\n\nf\n1\n5\n\n\ng\n9\n1\n\n\nh\n8\n0\n\n\ni\n9\n4\n\n\nj\n2\n3\n\n\n\n\n\n\n\nNote You can only edit an item via iloc if you access it directly. If you get a table column via df.iloc[&lt;row&gt;] and then apply the [&lt;column&gt;] operator to it, it won’t work.\nSo in the following example I try to edit pandas.DataFrame using df.iloc[&lt;row&gt;][&lt;column&gt;] to edit the dataframe, but find out that the dataframe has not been edited.\n\nedit_df_test = test_df.copy()\ndisplay(HTML(\"&lt;b&gt;Initial df&lt;/b&gt;\"))\ndisplay(edit_df_test)\n\nedit_df_test.iloc[1][1] = \"hello\"\ndisplay(HTML(\"&lt;b&gt;Edited df&lt;/b&gt;\"))\ndisplay(edit_df_test)\n\nInitial df\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\na\n5\n8\n\n\nb\n1\n4\n\n\nc\n7\n3\n\n\nd\n1\n3\n\n\ne\n4\n1\n\n\nf\n1\n5\n\n\ng\n9\n1\n\n\nh\n8\n0\n\n\ni\n9\n4\n\n\nj\n2\n3\n\n\n\n\n\n\n\nEdited df\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\na\n5\n8\n\n\nb\n1\n4\n\n\nc\n7\n3\n\n\nd\n1\n3\n\n\ne\n4\n1\n\n\nf\n1\n5\n\n\ng\n9\n1\n\n\nh\n8\n0\n\n\ni\n9\n4\n\n\nj\n2\n3"
  },
  {
    "objectID": "python/pandas/read_sql.html",
    "href": "python/pandas/read_sql.html",
    "title": "pandas.read_sql",
    "section": "",
    "text": "In this section I will describe the pandas.read_sql function as a basic way to load data from databases into pandas.\nThe next cell starts a database in a docker container, which will be used as an example.\nSQL script for initialisation postgres datase used in this example.\nimport pandas as pd\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.engine import URL \n\n!docker run --rm -d\\\n    -v ./postgres_inter_files/simple_example_db:/docker-entrypoint-initdb.d\\\n    -e POSTGRES_PASSWORD=docker_app\\\n    --name read_postgres_to_pandas_to_pandas\\\n    -p 5431:5432\\\n    postgres:15.4 &&gt; /dev/null\nNote Don’t forget to stop the cantainer when you have finished playing with the examples.\n!docker stop read_postgres_to_pandas &&gt; /dev/null"
  },
  {
    "objectID": "python/pandas/read_sql.html#sqlalchemy",
    "href": "python/pandas/read_sql.html#sqlalchemy",
    "title": "pandas.read_sql",
    "section": "SQLAlchemy",
    "text": "SQLAlchemy\nThere is a very important library for organising the interaction between the database and the pandas - SQLAlchemy.\n\nCreate engine in SQlAlchemy."
  },
  {
    "objectID": "python/pandas/read_sql.html#basic",
    "href": "python/pandas/read_sql.html#basic",
    "title": "pandas.read_sql",
    "section": "Basic",
    "text": "Basic\nYou can use pandas.read_sql and pass the created SQLAlchemy engine as con argument.\nSo in the following example, I start the postgres database in the docker container and then read information from it directly to pandas.\n\nurl_object = URL.create(\n    \"postgresql+psycopg2\",\n    username=\"postgres\",\n    password=\"docker_app\",\n    host=\"localhost\",\n    port=5431,\n    database=\"postgres\",\n)\nengine = create_engine(url_object)\n\ndf = pd.read_sql('SELECT * FROM main_table', con=engine, index_col = \"id\")\ndisplay(df)\n\n\n\n\n\n\n\n\ntext\n\n\nid\n\n\n\n\n\n0\nText1\n\n\n1\ntExT2\n\n\n3\nTEXT3"
  },
  {
    "objectID": "python/pandas/read_sql.html#connection-as-line",
    "href": "python/pandas/read_sql.html#connection-as-line",
    "title": "pandas.read_sql",
    "section": "Connection as line",
    "text": "Connection as line\nIt is possible not to create connection as strictly as shown in the previous example - you can use a string that contains all the necessary information in itself. And pass it as the connection argument. The pattern for this line will looks like dialect+driver://username:password@host:port/database.\nSo in the following example I just use this feature:\n\ndf = pd.read_sql(\n    'SELECT * FROM main_table', \n    con=\"postgresql+psycopg2://postgres:docker_app@localhost:5431/postgres\", \n    index_col = \"id\"\n)\ndisplay(df)\n\n\n\n\n\n\n\n\ntext\n\n\nid\n\n\n\n\n\n0\nText1\n\n\n1\ntExT2\n\n\n3\nTEXT3\n\n\n\n\n\n\n\nIn the case of postgres sql, you don’t even need to mention psycopg2 - it will be used by default. So the next cell is identical to the previous one, except that `driver’ isn’t mentioned.\n\ndf = pd.read_sql(\n    'SELECT * FROM main_table', \n    con=\"postgresql://postgres:docker_app@localhost:5431/postgres\", \n    index_col = \"id\"\n)\ndisplay(df)\n\n\n\n\n\n\n\n\ntext\n\n\nid\n\n\n\n\n\n0\nText1\n\n\n1\ntExT2\n\n\n3\nTEXT3\n\n\n\n\n\n\n\n\nNo SQLAlchemy\nActually, it’s possible to use the Postgres database without using SQLAlchemy - just pass the connection object from psycopg2 to the con parameter of the read_sql function.\nIn the following example, that’s exactly what I did - but I got a warning that the pandas connection was only tested with sqlAlchemy, so it’s better to use it.\n\nimport psycopg2\n\nconn = psycopg2.connect(\n    port = \"5431\", # same as when creating a postgres container\n    dbname = \"postgres\",\n    user = \"postgres\",\n    password = \"docker_app\",\n    host= \"localhost\"\n)\n\ndf = pd.read_sql('SELECT * FROM main_table', con=conn)\ndisplay(df)\nconn.close()\n\n/tmp/ipykernel_16808/3075988519.py:11: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n  df = pd.read_sql('SELECT * FROM main_table', con=conn)\n\n\n\n\n\n\n\n\n\nid\ntext\n\n\n\n\n0\n0\nText1\n\n\n1\n1\ntExT2\n\n\n2\n3\nTEXT3"
  },
  {
    "objectID": "python/pandas/read_sql.html#list-tables",
    "href": "python/pandas/read_sql.html#list-tables",
    "title": "pandas.read_sql",
    "section": "List tables",
    "text": "List tables\nYou cannot use \\dt to list available tables for the current database. Looks like it’s a peculiarity of sqlachemy - so you have to use pg_catalog.pg_tables.\nSo in the following cells I try both options, as you can see dt causes errors related to syntax.\n\ntry:\n    pd.read_sql(\"\\dt;\", con=engine)\nexcept Exception as e:\n    print(e)\n\n(psycopg2.errors.SyntaxError) syntax error at or near \"\\\"\nLINE 1: \\dt;\n        ^\n\n[SQL: \\dt;]\n(Background on this error at: https://sqlalche.me/e/14/f405)\n\n\n\npd.read_sql(\n    \"SELECT * FROM pg_catalog.pg_tables WHERE schemaname=\\'public\\'\",\n    con = engine\n)\n\n\n\n\n\n\n\n\nschemaname\ntablename\ntableowner\ntablespace\nhasindexes\nhasrules\nhastriggers\nrowsecurity\n\n\n\n\n0\npublic\nmain_table\npostgres\nNone\nFalse\nFalse\nFalse\nFalse"
  },
  {
    "objectID": "python/pandas/data_types/datetime.html",
    "href": "python/pandas/data_types/datetime.html",
    "title": "datetime",
    "section": "",
    "text": "In this section I will pay special attention to working with dates and times in Pandas.\nimport numpy as np\nimport pandas as pd\n\ntest_series = pd.Series(\n    np.random.choice(\n        pd.date_range(\"2021-01-01\", \"2022-12-31\"), \n        size=40\n))"
  },
  {
    "objectID": "python/pandas/data_types/datetime.html#freq",
    "href": "python/pandas/data_types/datetime.html#freq",
    "title": "datetime",
    "section": "freq",
    "text": "freq\nThis parameter allows you to set the step at which observations are added to the array. So in the following dataframe I have shown some options with arguments that should be passed as values of this parameter.\n\npd.DataFrame({\n    \"Days 'd'\" : pd.date_range(\"2021-01-01\", \"2021-01-10\", freq=\"d\")\n    \"Weeks 'W'\" : pd.date_range(\"2021-01-01\", \"2021-3-7\", freq=\"W\"),\n})\n\n\n\n\n\n\n\n\nWeeks 'W'\nDays 'd'\n\n\n\n\n0\n2021-01-03\n2021-01-01\n\n\n1\n2021-01-10\n2021-01-02\n\n\n2\n2021-01-17\n2021-01-03\n\n\n3\n2021-01-24\n2021-01-04\n\n\n4\n2021-01-31\n2021-01-05\n\n\n5\n2021-02-07\n2021-01-06\n\n\n6\n2021-02-14\n2021-01-07\n\n\n7\n2021-02-21\n2021-01-08\n\n\n8\n2021-02-28\n2021-01-09\n\n\n9\n2021-03-07\n2021-01-10"
  },
  {
    "objectID": "python/pandas/data_types/datetime.html#dt.day_of_week",
    "href": "python/pandas/data_types/datetime.html#dt.day_of_week",
    "title": "datetime",
    "section": "dt.day_of_week",
    "text": "dt.day_of_week\nYou can get days of the week.\nBy default it returns numbers representing the days of the week: 0-Monday,…,6-Sunday.\nSo in the following example I show the case for the week this page was created.\n\nweek_range = pd.date_range(\"2023-08-28\", \"2023-09-03\").to_series()\n\npd.DataFrame({\n    \"Original date\" : week_range,\n    \"Day of the week\" : week_range.dt.day_of_week\n})\n\n\n\n\n\n\n\n\nOriginal date\nDay of the week\n\n\n\n\n2023-08-28\n2023-08-28\n0\n\n\n2023-08-29\n2023-08-29\n1\n\n\n2023-08-30\n2023-08-30\n2\n\n\n2023-08-31\n2023-08-31\n3\n\n\n2023-09-01\n2023-09-01\n4\n\n\n2023-09-02\n2023-09-02\n5\n\n\n2023-09-03\n2023-09-03\n6"
  },
  {
    "objectID": "python/pandas/data_types/datetime.html#week-of-year",
    "href": "python/pandas/data_types/datetime.html#week-of-year",
    "title": "datetime",
    "section": "week of year",
    "text": "week of year\n\ndt.isocalendar().week\nYou can use the above function to find the week number for any date.\n\ntest_weeks = pd.date_range(\"2021-01-01\", \"2021-04-1\", freq=\"W\").to_series()\ntest_weeks.dt.isocalendar().week.rename(\"week number\").to_frame()\n\n\n\n\n\n\n\n\nweek number\n\n\n\n\n2021-01-03\n53\n\n\n2021-01-10\n1\n\n\n2021-01-17\n2\n\n\n2021-01-24\n3\n\n\n2021-01-31\n4\n\n\n2021-02-07\n5\n\n\n2021-02-14\n6\n\n\n2021-02-21\n7\n\n\n2021-02-28\n8\n\n\n2021-03-07\n9\n\n\n2021-03-14\n10\n\n\n2021-03-21\n11\n\n\n2021-03-28\n12\n\n\n\n\n\n\n\nNote The first days of a certain year may refer to the 54th week of the previous year. Documentation about this feature not really reach. The documentation about this function is not very extensive and does not mention in detail the exact algorithm for calculating the value in question. But in the next cell, I went through the dates of the border months of different summers. It turns out that the week refers to the year in which lies more number of its days and is numbered accordingly.\n\nfrom IPython.display import HTML\nfor y in range(2012, 2017):\n\n    next_y = y+1\n    \n    days = pd.date_range(\n        datetime(y, 12, 28), \n        datetime(next_y, 1, 3), freq=\"d\"\n    ).to_series()\n\n    display(HTML(f\"&lt;p style='font-size:150%'&gt;======{y}-{next_y}======&lt;/p&gt;\"))\n    display(pd.DataFrame({\n        \"Day\":days,\n        \"Day of week\":days.dt.day_of_week,\n        \"Week of year\":days.dt.isocalendar().week\n    }))\n\n======2012-2013======\n\n\n\n\n\n\n\n\n\nDay\nDay of week\nWeek of year\n\n\n\n\n2012-12-28\n2012-12-28\n4\n52\n\n\n2012-12-29\n2012-12-29\n5\n52\n\n\n2012-12-30\n2012-12-30\n6\n52\n\n\n2012-12-31\n2012-12-31\n0\n1\n\n\n2013-01-01\n2013-01-01\n1\n1\n\n\n2013-01-02\n2013-01-02\n2\n1\n\n\n2013-01-03\n2013-01-03\n3\n1\n\n\n\n\n\n\n\n======2013-2014======\n\n\n\n\n\n\n\n\n\nDay\nDay of week\nWeek of year\n\n\n\n\n2013-12-28\n2013-12-28\n5\n52\n\n\n2013-12-29\n2013-12-29\n6\n52\n\n\n2013-12-30\n2013-12-30\n0\n1\n\n\n2013-12-31\n2013-12-31\n1\n1\n\n\n2014-01-01\n2014-01-01\n2\n1\n\n\n2014-01-02\n2014-01-02\n3\n1\n\n\n2014-01-03\n2014-01-03\n4\n1\n\n\n\n\n\n\n\n======2014-2015======\n\n\n\n\n\n\n\n\n\nDay\nDay of week\nWeek of year\n\n\n\n\n2014-12-28\n2014-12-28\n6\n52\n\n\n2014-12-29\n2014-12-29\n0\n1\n\n\n2014-12-30\n2014-12-30\n1\n1\n\n\n2014-12-31\n2014-12-31\n2\n1\n\n\n2015-01-01\n2015-01-01\n3\n1\n\n\n2015-01-02\n2015-01-02\n4\n1\n\n\n2015-01-03\n2015-01-03\n5\n1\n\n\n\n\n\n\n\n======2015-2016======\n\n\n\n\n\n\n\n\n\nDay\nDay of week\nWeek of year\n\n\n\n\n2015-12-28\n2015-12-28\n0\n53\n\n\n2015-12-29\n2015-12-29\n1\n53\n\n\n2015-12-30\n2015-12-30\n2\n53\n\n\n2015-12-31\n2015-12-31\n3\n53\n\n\n2016-01-01\n2016-01-01\n4\n53\n\n\n2016-01-02\n2016-01-02\n5\n53\n\n\n2016-01-03\n2016-01-03\n6\n53\n\n\n\n\n\n\n\n======2016-2017======\n\n\n\n\n\n\n\n\n\nDay\nDay of week\nWeek of year\n\n\n\n\n2016-12-28\n2016-12-28\n2\n52\n\n\n2016-12-29\n2016-12-29\n3\n52\n\n\n2016-12-30\n2016-12-30\n4\n52\n\n\n2016-12-31\n2016-12-31\n5\n52\n\n\n2017-01-01\n2017-01-01\n6\n52\n\n\n2017-01-02\n2017-01-02\n0\n1\n\n\n2017-01-03\n2017-01-03\n1\n1\n\n\n\n\n\n\n\n\n\nweekofyear\nPandas datetime unit timestamp has a weekofyear parameter that you can combine with the apply method as in the next example.\n\ntest_weeks = pd.date_range(\"2021-01-01\", \"2021-04-1\", freq=\"W\").to_series()\ntest_weeks.apply(lambda val: val.weekofyear).rename(\"week number\").to_frame()\n\n\n\n\n\n\n\n\nweek number\n\n\n\n\n2021-01-03\n53\n\n\n2021-01-10\n1\n\n\n2021-01-17\n2\n\n\n2021-01-24\n3\n\n\n2021-01-31\n4\n\n\n2021-02-07\n5\n\n\n2021-02-14\n6\n\n\n2021-02-21\n7\n\n\n2021-02-28\n8\n\n\n2021-03-07\n9\n\n\n2021-03-14\n10\n\n\n2021-03-21\n11\n\n\n2021-03-28\n12"
  },
  {
    "objectID": "python/pandas/inline_columns_add.html",
    "href": "python/pandas/inline_columns_add.html",
    "title": "Inline column add (assign)",
    "section": "",
    "text": "Sometimes you need to create temporary columns for certain calculations and don’t need a new column in other calculations. For this purpose, it is very convenient to have a method that allows you to return the same database but with new columns. For such purposes you can use pandas.DataFrame.assign. See basic examples in pandas documentation.\nimport numpy as np\nimport pandas as pd\n\nfrom IPython.display import HTML"
  },
  {
    "objectID": "python/pandas/inline_columns_add.html#use-case",
    "href": "python/pandas/inline_columns_add.html#use-case",
    "title": "Inline column add (assign)",
    "section": "Use case",
    "text": "Use case\nLet’s say you need to group your data frame by the sum of the columns. Example data frame generated in the following cell.\n\nsample_size = 20\nnp.random.seed(10)\n\nshow_df = pd.DataFrame({\n    \"group col1\" : np.random.randint(10, 14, sample_size),\n    \"group col2\" : np.random.randint(10, 14, sample_size)\n})\nshow_df\n\n\n\n\n\n\n\n\ngroup col1\ngroup col2\n\n\n\n\n0\n11\n10\n\n\n1\n11\n13\n\n\n2\n10\n10\n\n\n3\n13\n10\n\n\n4\n10\n13\n\n\n5\n11\n13\n\n\n6\n13\n12\n\n\n7\n10\n10\n\n\n8\n11\n13\n\n\n9\n11\n12\n\n\n10\n10\n12\n\n\n11\n11\n11\n\n\n12\n11\n10\n\n\n13\n12\n10\n\n\n14\n10\n12\n\n\n15\n11\n11\n\n\n16\n10\n13\n\n\n17\n12\n12\n\n\n18\n10\n11\n\n\n19\n12\n11\n\n\n\n\n\n\n\nSome combinations of numbers can give the same sum. For example, 10 + 12 = 22 and 11 + 11 = 22. If you just groupby both columns it will think they are different combinations, but we need to aggregate by the sum of the columns.\nThe following cell shows that groupby both columns leads to wrong results.\n\nshow_df.groupby([\"group col1\", \"group col2\"])[\"group col1\"].count().to_dict()\n\n{(10, 10): 2,\n (10, 11): 1,\n (10, 12): 2,\n (10, 13): 2,\n (11, 10): 2,\n (11, 11): 2,\n (11, 12): 1,\n (11, 13): 3,\n (12, 10): 1,\n (12, 11): 1,\n (12, 12): 1,\n (13, 10): 1,\n (13, 12): 1}\n\n\nYou can perform it using temporary dataframe. But it cause some unnecessary code and variables. Like in cell below.\n\ntemp_df = show_df.copy()\ntemp_df[\"sum\"] = temp_df[\"group col1\"] + temp_df[\"group col2\"]\ndisplay(HTML('&lt;p style=\"font-size:15px\"&gt;Temp frame&lt;/p&gt;'))\ndisplay(temp_df)\ndisplay(HTML('&lt;p style=\"font-size:15px\"&gt;Result&lt;/p&gt;'))\ndisplay(temp_df.groupby(\"sum\")[\"group col1\"].count().to_dict())\n\nTemp frame\n\n\n\n\n\n\n\n\n\ngroup col1\ngroup col2\nsum\n\n\n\n\n0\n11\n10\n21\n\n\n1\n11\n13\n24\n\n\n2\n10\n10\n20\n\n\n3\n13\n10\n23\n\n\n4\n10\n13\n23\n\n\n5\n11\n13\n24\n\n\n6\n13\n12\n25\n\n\n7\n10\n10\n20\n\n\n8\n11\n13\n24\n\n\n9\n11\n12\n23\n\n\n10\n10\n12\n22\n\n\n11\n11\n11\n22\n\n\n12\n11\n10\n21\n\n\n13\n12\n10\n22\n\n\n14\n10\n12\n22\n\n\n15\n11\n11\n22\n\n\n16\n10\n13\n23\n\n\n17\n12\n12\n24\n\n\n18\n10\n11\n21\n\n\n19\n12\n11\n23\n\n\n\n\n\n\n\nResult\n\n\n{20: 2, 21: 3, 22: 5, 23: 5, 24: 4, 25: 1}\n\n\nBut using assign method you can perform the same operation just using one line of the code. The exampe is in the cell below.\n\nshow_df.assign(\n    sum = lambda row: row[\"group col1\"] + row[\"group col2\"]\n).groupby(\"sum\")[\"sum\"].count().to_dict()\n\n{20: 2, 21: 3, 22: 5, 23: 5, 24: 4, 25: 1}"
  },
  {
    "objectID": "python/sklearn/pipeline.html",
    "href": "python/sklearn/pipeline.html",
    "title": "Pipeline",
    "section": "",
    "text": "The sklearn.pipeline.Pipeline is a tool that allows you to create objects that contain all the necessary stages of model fitting. In other words, you can create your own estimator as a combination of different objects that perform data processing or model fitting.\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold, cross_validate\nfrom sklearn.metrics import mean_squared_error"
  },
  {
    "objectID": "python/sklearn/pipeline.html#demonstration-of-benefits",
    "href": "python/sklearn/pipeline.html#demonstration-of-benefits",
    "title": "Pipeline",
    "section": "Demonstration of benefits",
    "text": "Demonstration of benefits\nImagine that you need to create a model building pipeline that includes data standardisation and then model fitting. In this section I want to show the difference in code length and convenience of coding all by yourself and using the sklean.pipeline.Pipeline class.\n\nData generation\nIn the following cell I just generate a random regression task for use in the example.\n\nsample_size = 1000\nfeatures_count = 20\nnp.random.seed(50)\n\nX = []\n\nfor i in range(features_count):\n\n    mean = np.random.uniform(0,100)\n    std = np.abs(np.random.normal(0, 50))\n    \n    X.append(np.random.normal(mean, std, [sample_size, 1]))\n\nX = np.concatenate(X,axis=1)\ntheoretical_coefs = np.random.normal(0, 20, [features_count, 1])\ny = np.dot(X, theoretical_coefs) + np.random.normal(0, 500, sample_size)\n\n\n\nSelf coding\nSo here is code that does:\n\n10-fold split cross-validation for the named pipeline;\nDisplay cross-validation results;\nFit model to full data sample;\nCompute the mean prediction over the entire data sample.\n\nYou need to create a cycle that fits StandardScaler for current split and fit model to standardised data. After the cycle at the step of fitting the model to the whole data, you need to describe the whole pipeline again!\n\nmy_split = KFold(n_splits = 10)\ntrain_errors = []\ntest_errors = []\n\n\nfor train_ind, test_ind in my_split.split(X):\n    \n    this_scaler = StandardScaler().fit(X[train_ind, :])\n    \n    train_X = this_scaler.transform(X[train_ind, :])\n    train_y = y[train_ind]\n    \n    test_X = this_scaler.transform(X[test_ind,:])\n    test_y = y[test_ind]\n\n    model = LinearRegression().fit(train_X, train_y)\n    train_errors.append(mean_squared_error(train_y, model.predict(train_X)))\n    test_errors.append(mean_squared_error(test_y, model.predict(test_X)))\n\nprint(\"Train error:\", np.mean(np.array(train_errors)))\nprint(\"Test error:\", np.mean(np.array(test_errors)))\n\nstandart_X = StandardScaler().fit_transform(X)\nfinal_model = LinearRegression().fit(standart_X, y)\nprint(\"Mean predict\", np.mean(final_model.predict(standart_X)))\n\nTrain error: 3.5097084970944476e-22\nTest error: 3.4625913023012295e-22\nMean predict 19010.954067406543\n\n\n\n\nUsing sklearn.pipeline\nIn the following cell, I perform exactly the same calculations using only sklearn.pipeline.Pipeline.\nYou just need to define a my_pipline object where I describe the steps of the pipeline in the format [(&lt;name of step 1&gt;,&lt;object performing step 1&gt;), (&lt;name of step 2&gt;,&lt;object performing step 2&gt;), ...] and then just use it as a normal estimator - it will perform all the steps automatically.\nSo in the following cell it used in combination with cross_validate function to perform cross-validation, and after that just called fit(...).predict(...) to run the entire sample through the pipeline.\nThe results are exactly the same.\nLess code! Easier to manage!\n\nmy_split = KFold(n_splits = 10)\n\nmy_pipe = Pipeline([\n    (\"test_scaler\", StandardScaler()),\n    (\"my_model\", LinearRegression())\n])\n\ncv_results = cross_validate(\n    estimator = my_pipe,\n    X = X, y = y,\n    scoring=\"neg_mean_squared_error\",\n    cv = my_split,\n    return_train_score=True\n)\n\nprint(\"Train error:\", np.mean(cv_results[\"train_score\"]))\nprint(\"Test error:\", np.mean(cv_results[\"test_score\"]))\nprint(\"Mean predict\", np.mean(my_pipe.fit(X,y).predict(X)))\n\nTrain error: -3.5097084970944476e-22\nTest error: -3.4625913023012295e-22\nMean predict 19010.954067406543"
  },
  {
    "objectID": "python/sklearn/grid_search_cv.html",
    "href": "python/sklearn/grid_search_cv.html",
    "title": "Grid search CV",
    "section": "",
    "text": "sklearn.model_selection.GridSearchCV is an extremely useful tool that allows you to try out your model with different combinations of hyper-parameters.\nLearn more:"
  },
  {
    "objectID": "python/sklearn/grid_search_cv.html#use-one-traintest-split",
    "href": "python/sklearn/grid_search_cv.html#use-one-traintest-split",
    "title": "Grid search CV",
    "section": "Use one train/test split",
    "text": "Use one train/test split\nSometimes it’s not necessary to use cross-validation grid search, so here’s how to do a fit on train data and a validation on test once per parameter combination using sklearn.model_selection.GridSearchCV.\nThere is a cv argument in the GridSearchCV constructor. One of the options of arguments that can be passed is an iteramble object, where each element contains a tuple like object that contains train and test subsample indexes. So to achieve our goal we can pass an element list that contains a specific train/test split.\nSo in the following example is showen that self coded solution and using GridSearchCV object in described way will lead to same results. But the GridSearchCV option requires much less code and getting all features of GridSearchCV out of the box.\nThis cell:\n\nGerates sample;\nPerforms train/test split. Note that train_test_split is passed an array that matches the indices of the observations in the original sample, so that it also returns train/test split for sample indices;\nDefines the hyperparameter values that will be tried.\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(\n    n_samples = 500, \n    n_features = 2, \n    n_redundant = 0, \n    n_classes = 2,\n    random_state = 1\n)\n\nX_train, X_test, y_train, y_test, train_inds, test_inds =\\\n    train_test_split(X, y, np.arange(len(X)))\n\nparam_grid = {\n    \"max_leaf_nodes\" : [5, 10, 20, 50, 100],\n    \"max_depth\" : [3, 7, 10]\n}\n\nHere is a selfmade enumeration of all possible combinations of hyperparameters. The result will be an array with \\(AUC_{roc}\\) estimates on the test sample.\n\nfrom sklearn.metrics import roc_auc_score\n\nmy_tree = DecisionTreeClassifier(random_state = 1)\nroc_aucs = []\n\n\nfor max_leaf_nodes in param_grid[\"max_leaf_nodes\"]:\n    for max_depth in param_grid[\"max_depth\"]:\n        my_tree.set_params(\n            max_leaf_nodes = max_leaf_nodes,\n            max_depth = max_depth\n        )\n        my_tree.fit(X_train, y_train)\n        roc_aucs.append(roc_auc_score(\n            y_test,\n            my_tree.predict_proba(X_test)[:,1]\n        ))\n\nThe trick described above is used here. It leteraly does the same as the previous cell, but in much less code.\n\nfrom sklearn.model_selection import GridSearchCV\ngrid_search_cv = GridSearchCV(\n    estimator = my_tree,\n    scoring = \"roc_auc\",\n    param_grid = param_grid,\n    cv = [(train_inds, test_inds)]\n).fit(X, y)\n\nFinally let’s compare results of selfmade solution and the one performed by GridSearchCV - they are the same.\n\nimport pandas as pd\npd.DataFrame({\n    \"Self code\" : roc_aucs,\n    \"GridSearchCV\" : grid_search_cv.cv_results_[\"mean_test_score\"]\n})\n\n\n\n\n\n\n\n\nSelf code\nGridSearchCV\n\n\n\n\n0\n0.966872\n0.966872\n\n\n1\n0.966872\n0.963919\n\n\n2\n0.966872\n0.963919\n\n\n3\n0.963919\n0.963919\n\n\n4\n0.929122\n0.963919\n\n\n5\n0.929122\n0.966872\n\n\n6\n0.963919\n0.929122\n\n\n7\n0.932717\n0.932717\n\n\n8\n0.921161\n0.932717\n\n\n9\n0.963919\n0.932717\n\n\n10\n0.932717\n0.966872\n\n\n11\n0.911531\n0.929122\n\n\n12\n0.963919\n0.921161\n\n\n13\n0.932717\n0.911531\n\n\n14\n0.911531\n0.911531"
  },
  {
    "objectID": "python/fastapi/return_json.html",
    "href": "python/fastapi/return_json.html",
    "title": "Return JSON",
    "section": "",
    "text": "Container for the examples in this page.\n!docker run --rm -itd\\\n    --name test_container\\\n    -v ./return_json_files/app.py:/app.py\\\n    -p 8000:8000 \\\n    fastapi_experiment \\\n    uvicorn --host 0.0.0.0 --reload app:app &&gt;/dev/null\nNote Don’t forget to stop the container.\n!docker stop test_container &&gt;/dev/null"
  },
  {
    "objectID": "python/fastapi/return_json.html#example",
    "href": "python/fastapi/return_json.html#example",
    "title": "Return JSON",
    "section": "Example",
    "text": "Example\nTo return json from fastapi endpoin you should just return dictionary. It will be converted to the appropriate json and passed as a response.\nIf you send a request to the corresponding URL, you will get exactly the same as dict, except that all keys will be converted to strings. In the example, I deliberately create one of the keys of a numeric data type to demonstrate that it will be converted to a string when queried.\n\n%%writefile return_json_files/app.py\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/\")\ndef json_as_answer():\n    return {\n        \"key1\" : \"value1\",\n        \"key2\" : \"value2\",\n        6 : 34\n    }\n\nOverwriting return_json_files/app.py\n\n\n\n!curl localhost:8000\n\n{\"key1\":\"value1\",\"key2\":\"value2\",\"6\":34}"
  },
  {
    "objectID": "math/latex.html",
    "href": "math/latex.html",
    "title": "Latex",
    "section": "",
    "text": "You can find some additional information here:\n\n\nhttp://tex.imm.uran.ru/tex/2e/lshort2e/node52.html;\nhttps://ido.tsuab.ru/mod/book/view.php?id=62&chapterid=23;\nhttps://ru.overleaf.com/learn/latex/List_of_Greek_letters_and_math_symbols."
  },
  {
    "objectID": "math/latex.html#sources",
    "href": "math/latex.html#sources",
    "title": "Latex",
    "section": "",
    "text": "You can find some additional information here:\n\n\nhttp://tex.imm.uran.ru/tex/2e/lshort2e/node52.html;\nhttps://ido.tsuab.ru/mod/book/view.php?id=62&chapterid=23;\nhttps://ru.overleaf.com/learn/latex/List_of_Greek_letters_and_math_symbols."
  },
  {
    "objectID": "math/latex.html#comparison",
    "href": "math/latex.html#comparison",
    "title": "Latex",
    "section": "Comparison",
    "text": "Comparison\n\\leq - \\(\\leq\\), \\geq - \\(\\geq\\), \\approx - \\(\\approx\\), \\neq - \\(\\neq\\), \\sim - \\(\\sim\\)."
  },
  {
    "objectID": "math/latex.html#proofs-symbols",
    "href": "math/latex.html#proofs-symbols",
    "title": "Latex",
    "section": "Proofs symbols",
    "text": "Proofs symbols\n\\forall - \\(\\forall\\), \\Leftarrow - \\(\\Leftarrow\\), \\Rightarrow - \\(\\Rightarrow\\), \\Leftrightarrow - \\(\\Leftrightarrow\\), \\boxtimes - \\(\\boxtimes\\)."
  },
  {
    "objectID": "math/latex.html#greek-symbols",
    "href": "math/latex.html#greek-symbols",
    "title": "Latex",
    "section": "Greek symbols",
    "text": "Greek symbols\n\n\\alpha - \\(\\alpha\\);\n\\beta - \\(\\beta\\);\n\\gamma - \\(\\gamma\\);\n\\pi - \\(\\pi\\);\n\\phi, \\varphi, \\Phi - \\(\\phi\\), \\(\\varphi\\), \\(\\Phi\\);\n\\epsilon, \\varepsilon - \\(\\epsilon\\), \\(\\varepsilon\\);"
  },
  {
    "objectID": "math/latex.html#upper-excreta",
    "href": "math/latex.html#upper-excreta",
    "title": "Latex",
    "section": "Upper excreta",
    "text": "Upper excreta\n\n\\tilde{ffff} - \\(\\tilde{ffff}\\), \\widetilde{ffff} - \\(\\widetilde{ffff}\\);\n\\hat{ffff} - \\(\\hat{ffff}\\), \\widehat{ffff} - \\(\\widehat{ffff}\\);\n\\bar{ffff} - \\(\\bar{ffff}\\), \\overline{ffff} - \\(\\overline{ffff}\\)."
  },
  {
    "objectID": "math/latex.html#operations-with-sets",
    "href": "math/latex.html#operations-with-sets",
    "title": "Latex",
    "section": "Operations with sets",
    "text": "Operations with sets\n\nA \\in B - \\(A \\in B\\);\nA \\subset B - \\(A \\subset B\\);\nA \\supset B - \\(A \\supset B\\);\nA \\subseteq B - \\(A \\subseteq B\\);\nA \\supseteq B - \\(A \\supseteq B\\);\nA \\cup B - \\(A \\cup B\\);\nA \\cap B - \\(A \\cap B\\)."
  },
  {
    "objectID": "math/latex.html#joining-case",
    "href": "math/latex.html#joining-case",
    "title": "Latex",
    "section": "Joining case",
    "text": "Joining case\nThe following instructions are used to create a parenthesis in latex:\n\n\\begin{cases} &lt;expression&gt;  \\\\end{cases} - will put expression under the bracket;\n\\\\ - to jump to a new line for an expression under a bracket.\n\nFor example expression:\n$$\\begin{cases}\n      line1; \\\\\n      line2.\n\\end{cases}$$\nWill show markdown:\n\\[\\begin{cases}\n      line1; \\\\\n      line2.\n\\end{cases}\\]"
  },
  {
    "objectID": "math/latex.html#expression-numbers",
    "href": "math/latex.html#expression-numbers",
    "title": "Latex",
    "section": "Expression numbers",
    "text": "Expression numbers\nUsing command \\tag\nFor example:\n$$\\frac{\\delta}{\\gamma} \\tag{hello}$$\n\\[\\frac{\\delta}{\\gamma} \\tag{hello}\\]"
  },
  {
    "objectID": "math/latex.html#brakets",
    "href": "math/latex.html#brakets",
    "title": "Latex",
    "section": "Brakets",
    "text": "Brakets\n\nRounding\n\nFloor \\lfloor a \\rfloor - \\(\\lfloor a \\rfloor\\);\nCeil \\lceil a \\rceil - \\(\\lceil a \\rceil\\);\nNote \\(\\lfloor 5.31 \\rfloor = 5, \\lfloor -5.31 \\rfloor = -6, \\lceil 5.31 \\rceil = 6, \\lceil -5.31 \\rceil=-5\\).\n\n\n\nTo wrap in brackets\nExpression like:\n$$[\\frac{\\sum_i^n}{\\prod_i^n}]$$\nWill be interpreted like:\n\\[[\\frac{\\sum_i^n}{\\prod_i^n}]\\]\nThe problem is that a square bracket does not completely close the expression it surrounds. To fix this, you need to put the tag $\\left$ before the opening bracket and $\\right$ before the closing bracket. That is, the expression:\n$$\\left[\\frac{\\sum_i^n}{\\prod_i^n}\\right]$$\nWhich will be interpreted like this:\n\\[\\left[\\frac{\\sum_i^n}{\\prod_i^n}\\right]\\]\nYou can even use it with types of parentheses defined by other keywords. For example expression:\n\\left\\lceil \\frac{a}{b} \\right\\rceil\nwill look like:\n\\[\\left\\lceil \\frac{a}{b} \\right\\rceil\\]"
  },
  {
    "objectID": "math/latex.html#matrices",
    "href": "math/latex.html#matrices",
    "title": "Latex",
    "section": "Matrices",
    "text": "Matrices\nTo create the matrix, you will need: - Opening and closing brackets \\left(, \\right); - The \\betting{array} \\end{array} instruction will allow you to create table elements inside the bracket. (in order to start the wod after opening \\begin{array}, you will have to put \\\\); - The & symbol is used to move to the next element of the string; - To move to the next line element, the \\\\ is used; - To fill in the intermediate places between matrix elements, you may need to use multipo dots: - Horizontal dots \\cdots - \\(\\cdots\\); - Vertical dots \\vdots - \\(\\vdots\\); - Dianal polynomial dots - \\(\\ddots\\):\nThus an entry of the form:\n$$\\left(\\begin{array} \\\\\n    a_{11}&a_{12}&\\cdots&a_{1n} \\\\ \n    a_{21}&a_{22}&\\cdots&a_{2n} \\\\ \n    \\vdots&\\vdots&\\ddots&\\vdots \\\\\n    a_{n1}&a_{n2}&\\cdots&a_{nn} \n\\end{array}\\right)$$\nWill allow you to form an expression of the form:\n\\[\\left(\\begin{array} \\\\\n    a_{11}&a_{12}&\\cdots&a_{1n} \\\\\n    a_{21}&a_{22}&\\cdots&a_{2n} \\\\\n    \\vdots&\\vdots&\\ddots&\\vdots \\\\\n    a_{n1}&a_{n2}&\\cdots&a_{nn}\n\\end{array}\\right)\\]"
  },
  {
    "objectID": "math/latex.html#letters-with-empty-space",
    "href": "math/latex.html#letters-with-empty-space",
    "title": "Latex",
    "section": "Letters with empty space",
    "text": "Letters with empty space\nUsually used to denote moieties. To write a letter in this way, use the command \\mathbb{...}.\n$$\\mathbb{ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz}$$- \\(\\mathbb{ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz}\\)"
  },
  {
    "objectID": "math/cent_limit_theorem.html",
    "href": "math/cent_limit_theorem.html",
    "title": "Central limit theorem",
    "section": "",
    "text": "Let’s take \\(N\\) values \\(x_i\\) belonging to the same distribution with mathematical expectation \\(\\mu\\) and standard deviation \\(\\sigma\\).\nLet’s denote their sum:\n\\[S_N = \\sum_{i=1}^n x_i\\]\nCentral limit theorem says that \\(S_N \\sim N(\\mu N; \\sigma^2 N)\\) (\\(S_N\\) belongs to normal distibution with parameters \\(\\mu N, \\sigma^2 N\\))."
  },
  {
    "objectID": "math/cent_limit_theorem.html#formulation",
    "href": "math/cent_limit_theorem.html#formulation",
    "title": "Central limit theorem",
    "section": "",
    "text": "Let’s take \\(N\\) values \\(x_i\\) belonging to the same distribution with mathematical expectation \\(\\mu\\) and standard deviation \\(\\sigma\\).\nLet’s denote their sum:\n\\[S_N = \\sum_{i=1}^n x_i\\]\nCentral limit theorem says that \\(S_N \\sim N(\\mu N; \\sigma^2 N)\\) (\\(S_N\\) belongs to normal distibution with parameters \\(\\mu N, \\sigma^2 N\\))."
  },
  {
    "objectID": "math/cent_limit_theorem.html#numerical-proof",
    "href": "math/cent_limit_theorem.html#numerical-proof",
    "title": "Central limit theorem",
    "section": "Numerical proof",
    "text": "Numerical proof\nLet’s create samples of means of samples from different distributions and explore their characteristics.\nWe will plot the histogram of the sample of sums and perform the Cramér-von Mises test to understand if \\(S_N\\) belongs to the normal distribution.\nThe \\(H_0\\) of the Cramér-von Mises test in our case is that \\(S_N\\) is normally distributed, so if \\(p&gt;0.05\\) we cannot reject this hypothesis.\n\nimport numpy as np\n\nimport scipy\nfrom scipy.stats import shapiro, anderson\nfrom scipy.stats import cramervonmises\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef explore_is_normal(\n    mu : float, sigma : float, \n    sample : np.array, orig_distr_name: str\n    ):\n    \"\"\"\n    Function that returns whether the given sample\n    belongs to the normal distribution. It plots the\n    histogram of the distribution and prints the result of the\n    of the Cramér-von Mises test in the title.\n    Parameters\n    ----------\n    mu (float) : mathematical expectation of original random value;\n    sigma (float) : standart deviation of original random value;\n    sample (numpy.array) : sample of sums of original samples;\n    orig_distr_name (str) : name of original distribution\n        will be pritned on the title of the figure.\n\n    Returns\n    ---------\n    Plotted histogram of exploring sample.\n    \"\"\"\n    s_len = len(sample)\n    test_p = cramervonmises(\n        sample,\n        cdf=scipy.stats.norm(\n            mu*s_len,sigma*(s_len**(1/2))\n        ).cdf\n    ).pvalue\n    \n    sns.histplot(sample, kde=True)\n    plt.title(\n        f\"Original distr. - {orig_distr_name};\" \n        f\" Test result $p={str(round(test_p,2))}$\"\n    )\n\n\nNormal distribution\nThis shows that the sum of the random values sampled from the normal distribution belongs to the normal distribution.\n\nsample = np.random.normal(\n    2, 3, (10000, 10000)\n).sum(axis = 1)\nexplore(2, 3, sample, \"normal\")\n\n\n\n\n\n\nPoisson distribution\nFor this research, it is important that this distribution parameters are \\(\\mu=\\lambda\\) and \\(\\sigma=\\sqrt{\\lambda}\\).\n\n_lambda = 10\nsample = np.random.poisson(\n    _lambda, (10000, 10000)\n).sum(axis = 1)\nexplore(_lambda, _lambda**(1/2), sample, \"poison\")"
  }
]