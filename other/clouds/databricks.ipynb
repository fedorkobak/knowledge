{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a585528",
   "metadata": {},
   "source": [
    "# Databricks\n",
    "\n",
    "Databricks is a platform for manipulating data and data related processes: analitics and ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413304ad",
   "metadata": {},
   "source": [
    "## Feature store\n",
    "\n",
    "You can manipulate the feature store using the databricks Python SDK, module: `databricks.feature_engineering`. This is not provided with the Databricks Python SDK out of the box - install the separatre [PyPI published package](https://pypi.org/project/databricks-feature-engineering/).\n",
    "\n",
    "Create the feature store with code:\n",
    "\n",
    "```python\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "fe.create_table(\n",
    "    name=\"<catalog>.<schema>.<table name>\",\n",
    "    primary_keys=[\"<primary key 1>\", \"<primary key2>\"],\n",
    "    df=data,\n",
    "    description=\"This is some sort of description\",\n",
    "    tags={\"source\": \"bronze\", \"format\": \"delta\"}\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b637096b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Jobs&Workflows\n",
    "\n",
    "Jobs and workflows allows to orchestrate tasks, wich are pieces of code that perform actions on the platform, and build relationships between them.\n",
    "\n",
    "The following table lists teh ways you can define the databricks tasks.\n",
    "\n",
    "| Task Type | Description | Primary Use Case |\n",
    "| :--- | :--- | :--- |\n",
    "| **Notebook Task** | Runs a Databricks notebook written in Python, Scala, SQL, or R. | Executing interactive code, ETL logic, or ML training pipelines. |\n",
    "| **Pipeline Task** | Runs a specified Delta Live Tables (DLT) pipeline. | Orchestrating end-to-end declarative ETL/streaming data pipelines. |\n",
    "| **SQL File Task** | Executes a SQL script file stored in the workspace or a Git repository. | Running complex SQL transformations, DDL, or DML statements. |\n",
    "| **Python Script Task** | Executes a Python file on the cluster using `spark-submit`. | Running standard Python code, often with Spark (PySpark) libraries. |\n",
    "| **Python Wheel Task** | Runs a Python function packaged within a Python Wheel (`.whl`) file. | Running production-grade, modular, and version-controlled Python code. |\n",
    "| **JAR Task** | Executes a compiled Java or Scala application packaged as a JAR file. | Running compiled, production-ready code, typically for complex logic. |\n",
    "| **Spark-Submit Task** | Allows submission of a generic Spark application via the `spark-submit` command. | Running custom or highly specialized Spark applications. |\n",
    "| **dbt Task** | Runs one or more `dbt` (data build tool) commands. | Orchestrating and running dbt projects for data transformation. |\n",
    "| **Run Job Task** | Executes another Databricks Job as a task. | Creating nested, modular, or reusable workflows (Parent-Child jobs). |\n",
    "| **If/Else Condition Task**| Evaluates a condition and controls the execution flow of subsequent tasks. | Adding conditional logic (branching) to a workflow. |\n",
    "| **For Each Task** | Iterates over a collection of input values and runs a nested task for each value. | Parallel processing or batch operations over a list of items. |\n",
    "| **Dashboard Task** | Updates a Databricks SQL Dashboard. | Automating the refresh of business intelligence dashboards. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd71220",
   "metadata": {},
   "source": [
    "### Tasks communication\n",
    "\n",
    "To communicate between tasks you can set and read \"tasks values\".\n",
    "\n",
    "In python code use for that [dbutils.jobs.tasksValue](https://docs.databricks.com/aws/en/dev-tools/databricks-utils#taskvalues-subutility-dbutilsjobstaskvalues):\n",
    "\n",
    "- `dbutils.jobs.taskValues.set(key=\"<key>\", value=\"<value>\")` for setting a value.\n",
    "- `dbutils.jobs.taskValues.get(taskKey=\"<name of the previous task>\", key='key_from_script')` for reading the value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6744f361",
   "metadata": {},
   "source": [
    "## CLI\n",
    "\n",
    "The Databricks CLI allows you to manipulate your Databricks worksspace/account your machine command line. The following table shows corresponding subcommands:\n",
    "\n",
    "| Command group        | Description / purpose                                                                               |\n",
    "| -------------------- | --------------------------------------------------------------------------------------------------- |\n",
    "| **fs**               | Manage files in DBFS / file system (list, copy, delete, read)                                       |\n",
    "| **git-credentials**  | Manage personal access tokens for Databricks to perform operations on behalf of user                |\n",
    "| **repos**            | Manage Git repos within Databricks (import, sync, permissions)                                      |\n",
    "| **secrets**          | Manage secrets, scopes, and access control for secrets                                              |\n",
    "| **workspace**        | Handle workspace contents (notebooks, folders) and permissions                                      |\n",
    "| **cluster-policies** | Control rules and policies for cluster configurations                                               |\n",
    "| **clusters**         | Manage cluster lifecycle and settings                                                               |\n",
    "| **api**              | Call any Databricks REST API directly (for advanced or unsupported endpoints)                       |\n",
    "| **completion**       | Generate shell autocompletion scripts                                                               |\n",
    "| **configure**        | Set up and configure the Databricks CLI (e.g. host, profile)                                        |\n",
    "| **help**             | Display summary and help information for commands                                                   |\n",
    "| **bundle**           | Manage Databricks Asset Bundles (CI/CD-style deployments)                                           |\n",
    "| **labs**             | Work with experimental Labs applications and features in Databricks                                 |\n",
    "| **auth**             | Manage authentication, login, profiles, and tokens                                                  |\n",
    "| **current-user**     | Show information about the currently authenticated user or service principal                        |\n",
    "| **model-registry**   | Manage the workspaceâ€™s MLflow Model Registry: models, versions, transitions, metadata, and webhooks |\n",
    "\n",
    "Check more in:\n",
    "- [What is the Databricks CLI](https://docs.databricks.com/aws/en/dev-tools/cli/).\n",
    "- [Installation guide](https://docs.databricks.com/aws/en/dev-tools/cli/install)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a71514",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "If you have the Databricks CLI installed on your system, you should be able to run following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6337533",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databricks CLI\n",
      "\n",
      "Usage:\n",
      "  databricks [command]\n",
      "\n",
      "Databricks Workspace\n",
      "  fs                                     Filesystem related commands\n",
      "  git-credentials                        Registers personal access token for Databricks to do operations on behalf of the user.\n",
      "  repos                                  The Repos API allows users to manage their git repos.\n",
      "  secrets                                The Secrets API allows you to manage secrets, secret scopes, and access permissions.\n",
      "  workspace                              The Workspace API allows you to list, import, export, and delete notebooks and folders.\n",
      "\n",
      "Compute\n",
      "  cluster-policies                       You can use cluster policies to control users' ability to configure clusters based on a set of rules.\n",
      "  clusters                               The Clusters API allows you to create, start, edit, list, terminate, and delete clusters.\n",
      "  global-init-scripts                    The Global Init Scripts API enables Workspace administrators to configure global initialization scripts for their workspace.\n",
      "  instance-pools                         Instance Pools API are used to create, edit, delete and list instance pools by using ready-to-use cloud instances which reduces a cluster start and auto-scaling times.\n",
      "  instance-profiles                      The Instance Profiles API allows admins to add, list, and remove instance profiles that users can launch clusters with.\n",
      "  libraries                              The Libraries API allows you to install and uninstall libraries and get the status of libraries on a cluster.\n",
      "  policy-compliance-for-clusters         The policy compliance APIs allow you to view and manage the policy compliance status of clusters in your workspace.\n",
      "  policy-families                        View available policy families.\n",
      "\n",
      "Workflows\n",
      "  jobs                                   The Jobs API allows you to create, edit, and delete jobs.\n",
      "  policy-compliance-for-jobs             The compliance APIs allow you to view and manage the policy compliance status of jobs in your workspace.\n",
      "\n",
      "Delta Live Tables\n",
      "  pipelines                              The Delta Live Tables API allows you to create, edit, delete, start, and view details about pipelines.\n",
      "\n",
      "Machine Learning\n",
      "  experiments                            Experiments are the primary unit of organization in MLflow; all MLflow runs belong to an experiment.\n",
      "  model-registry                         Note: This API reference documents APIs for the Workspace Model Registry.\n",
      "\n",
      "Real-time Serving\n",
      "  serving-endpoints                      The Serving Endpoints API allows you to create, update, and delete model serving endpoints.\n",
      "\n",
      "Identity and Access Management\n",
      "  current-user                           This API allows retrieving information about currently authenticated user or service principal.\n",
      "  groups                                 Groups simplify identity management, making it easier to assign access to Databricks workspace, data, and other securable objects.\n",
      "  groups-v2                              Groups simplify identity management, making it easier to assign access to Databricks workspace, data, and other securable objects.\n",
      "  permissions                            Permissions API are used to create read, write, edit, update and manage access for various users on different objects and endpoints.\n",
      "  service-principals                     Identities for use with jobs, automated tools, and systems such as scripts, apps, and CI/CD platforms.\n",
      "  service-principals-v2                  Identities for use with jobs, automated tools, and systems such as scripts, apps, and CI/CD platforms.\n",
      "  users                                  User identities recognized by Databricks and represented by email addresses.\n",
      "  users-v2                               User identities recognized by Databricks and represented by email addresses.\n",
      "\n",
      "Databricks SQL\n",
      "  alerts                                 The alerts API can be used to perform CRUD operations on alerts.\n",
      "  alerts-legacy                          The alerts API can be used to perform CRUD operations on alerts.\n",
      "  alerts-v2                              New version of SQL Alerts.\n",
      "  dashboards                             In general, there is little need to modify dashboards using the API.\n",
      "  data-sources                           This API is provided to assist you in making new query objects.\n",
      "  queries                                The queries API can be used to perform CRUD operations on queries.\n",
      "  queries-legacy                         These endpoints are used for CRUD operations on query definitions.\n",
      "  query-history                          A service responsible for storing and retrieving the list of queries run against SQL endpoints and serverless compute.\n",
      "  warehouses                             A SQL warehouse is a compute resource that lets you run SQL commands on data objects within Databricks SQL.\n",
      "\n",
      "Unity Catalog\n",
      "  artifact-allowlists                    In Databricks Runtime 13.3 and above, you can add libraries and init scripts to the allowlist in UC so that users can leverage these artifacts on compute configured with shared access mode.\n",
      "  catalogs                               A catalog is the first layer of Unity Catalogâ€™s three-level namespace.\n",
      "  connections                            Connections allow for creating a connection to an external data source.\n",
      "  credentials                            A credential represents an authentication and authorization mechanism for accessing services on your cloud tenant.\n",
      "  entity-tag-assignments                 Tags are attributes that include keys and optional values that you can use to organize and categorize entities in Unity Catalog.\n",
      "  external-lineage                       External Lineage APIs enable defining and managing lineage relationships between Databricks objects and external systems.\n",
      "  external-locations                     An external location is an object that combines a cloud storage path with a storage credential that authorizes access to the cloud storage path.\n",
      "  external-metadata                      External Metadata objects enable customers to register and manage metadata about external systems within Unity Catalog.\n",
      "  functions                              Functions implement User-Defined Functions (UDFs) in Unity Catalog.\n",
      "  grants                                 In Unity Catalog, data is secure by default.\n",
      "  metastores                             A metastore is the top-level container of objects in Unity Catalog.\n",
      "  model-versions                         Databricks provides a hosted version of MLflow Model Registry in Unity Catalog.\n",
      "  online-tables                          Online tables provide lower latency and higher QPS access to data from Delta tables.\n",
      "  policies                               Attribute-Based Access Control (ABAC) provides high leverage governance for enforcing compliance policies in Unity Catalog.\n",
      "  quality-monitors                       A monitor computes and monitors data or model quality metrics for a table over time.\n",
      "  registered-models                      Databricks provides a hosted version of MLflow Model Registry in Unity Catalog.\n",
      "  resource-quotas                        Unity Catalog enforces resource quotas on all securable objects, which limits the number of resources that can be created.\n",
      "  rfa                                    Request for Access enables customers to request access to and manage access request destinations for Unity Catalog securables.\n",
      "  schemas                                A schema (also called a database) is the second layer of Unity Catalogâ€™s three-level namespace.\n",
      "  storage-credentials                    A storage credential represents an authentication and authorization mechanism for accessing data stored on your cloud tenant.\n",
      "  system-schemas                         A system schema is a schema that lives within the system catalog.\n",
      "  table-constraints                      Primary key and foreign key constraints encode relationships between fields in tables.\n",
      "  tables                                 A table resides in the third layer of Unity Catalogâ€™s three-level namespace.\n",
      "  temporary-path-credentials             Temporary Path Credentials refer to short-lived, downscoped credentials used to access external cloud storage locations registered in Databricks.\n",
      "  temporary-table-credentials            Temporary Table Credentials refer to short-lived, downscoped credentials used to access cloud storage locations where table data is stored in Databricks.\n",
      "  volumes                                Volumes are a Unity Catalog (UC) capability for accessing, storing, governing, organizing and processing files.\n",
      "  workspace-bindings                     A securable in Databricks can be configured as __OPEN__ or __ISOLATED__.\n",
      "\n",
      "Delta Sharing\n",
      "  providers                              A data provider is an object representing the organization in the real world who shares the data.\n",
      "  recipient-activation                   The Recipient Activation API is only applicable in the open sharing model where the recipient object has the authentication type of TOKEN.\n",
      "  recipients                             A recipient is an object you create using :method:recipients/create to represent an organization which you want to allow access shares.\n",
      "  shares                                 A share is a container instantiated with :method:shares/create.\n",
      "\n",
      "Settings\n",
      "  ip-access-lists                        IP Access List enables admins to configure IP access lists.\n",
      "  notification-destinations              The notification destinations API lets you programmatically manage a workspace's notification destinations.\n",
      "  settings                               Workspace Settings API allows users to manage settings at the workspace level.\n",
      "  token-management                       Enables administrators to get all tokens and delete tokens for other users.\n",
      "  tokens                                 The Token API allows you to create, list, and revoke tokens that can be used to authenticate and access Databricks REST APIs.\n",
      "  workspace-conf                         This API allows updating known workspace settings for advanced users.\n",
      "\n",
      "Developer Tools\n",
      "  bundle                                 Databricks Asset Bundles let you express data/AI/analytics projects as code.\n",
      "  sync                                   Synchronize a local directory to a workspace directory\n",
      "\n",
      "Vector Search\n",
      "  vector-search-endpoints                **Endpoint**: Represents the compute resources to host vector search indexes.\n",
      "  vector-search-indexes                  **Index**: An efficient representation of your embedding vectors that supports real-time and efficient approximate nearest neighbor (ANN) search queries.\n",
      "\n",
      "Dashboards\n",
      "  genie                                  Genie provides a no-code experience for business users, powered by AI/BI.\n",
      "  lakeview                               These APIs provide specific management operations for Lakeview dashboards.\n",
      "  lakeview-embedded                      Token-based Lakeview APIs for embedding dashboards in external applications.\n",
      "\n",
      "Marketplace\n",
      "  consumer-fulfillments                  Fulfillments are entities that allow consumers to preview installations.\n",
      "  consumer-installations                 Installations are entities that allow consumers to interact with Databricks Marketplace listings.\n",
      "  consumer-listings                      Listings are the core entities in the Marketplace.\n",
      "  consumer-personalization-requests      Personalization Requests allow customers to interact with the individualized Marketplace listing flow.\n",
      "  consumer-providers                     Providers are the entities that publish listings to the Marketplace.\n",
      "  provider-exchange-filters              Marketplace exchanges filters curate which groups can access an exchange.\n",
      "  provider-exchanges                     Marketplace exchanges allow providers to share their listings with a curated set of customers.\n",
      "  provider-files                         Marketplace offers a set of file APIs for various purposes such as preview notebooks and provider icons.\n",
      "  provider-listings                      Listings are the core entities in the Marketplace.\n",
      "  provider-personalization-requests      Personalization requests are an alternate to instantly available listings.\n",
      "  provider-provider-analytics-dashboards Manage templated analytics solution for providers.\n",
      "  provider-providers                     Providers are entities that manage assets in Marketplace.\n",
      "\n",
      "Apps\n",
      "  apps                                   Apps run directly on a customerâ€™s Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on.\n",
      "\n",
      "Clean Rooms\n",
      "  clean-room-asset-revisions             Clean Room Asset Revisions denote new versions of uploaded assets (e.g.\n",
      "  clean-room-assets                      Clean room assets are data and code objects â€” Tables, volumes, and notebooks that are shared with the clean room.\n",
      "  clean-room-auto-approval-rules         Clean room auto-approval rules automatically create an approval on your behalf when an asset (e.g.\n",
      "  clean-room-task-runs                   Clean room task runs are the executions of notebooks in a clean room.\n",
      "  clean-rooms                            A clean room uses Delta Sharing and serverless compute to provide a secure and privacy-protecting environment where multiple parties can work together on sensitive enterprise data without direct access to each other's data.\n",
      "\n",
      "Database\n",
      "  database                               Database Instances provide access to a database via REST API or direct SQL.\n",
      "  psql                                   Connect to the specified Database Instance\n",
      "\n",
      "Quality Monitor v2\n",
      "  quality-monitor-v2                     Manage data quality of UC objects (currently support schema).\n",
      "\n",
      "OAuth\n",
      "  service-principal-secrets-proxy        These APIs enable administrators to manage service principal secrets at the workspace level.\n",
      "\n",
      "Identity and Access Management (v2)\n",
      "  workspace-iam-v2                       These APIs are used to manage identities and the workspace access of these identities in <Databricks>.\n",
      "\n",
      "Additional Commands:\n",
      "  account                                Databricks Account Commands\n",
      "  api                                    Perform Databricks API call\n",
      "  auth                                   Authentication related commands\n",
      "  completion                             Generate the autocompletion script for the specified shell\n",
      "  configure                              Configure authentication\n",
      "  help                                   Help about any command\n",
      "  labs                                   Manage Databricks Labs installations\n",
      "  tag-policies                           The Tag Policy API allows you to manage policies for governed tags in Databricks.\n",
      "  version                                Retrieve information about the current version of this CLI\n",
      "  workspace-settings-v2                  APIs to manage workspace level settings.\n",
      "\n",
      "Flags:\n",
      "      --debug            enable debug logging\n",
      "  -h, --help             help for databricks\n",
      "  -o, --output type      output type: text or json (default text)\n",
      "  -p, --profile string   ~/.databrickscfg profile\n",
      "  -t, --target string    bundle target to use (if applicable)\n",
      "  -v, --version          version for databricks\n",
      "\n",
      "Use \"databricks [command] --help\" for more information about a command.\n"
     ]
    }
   ],
   "source": [
    "databricks --help"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
