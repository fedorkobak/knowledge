{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "This section focuses on ways to measure something numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy\n",
    "\n",
    "The popularity of this metric stems from the fact that it is differentiable, making it suitable to be used as a loss function when fitting the parameters of machine learning models.\n",
    "\n",
    "Check [Cross-Entropy section at MLGlossary](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy for $o$-th observation can be written using following formula:\n",
    "\n",
    "$$-\\sum_{c=1}^M y_{o, c} log(p_{o,c})\n",
    "\\\\\n",
    "y_{o,c} = \n",
    "\\begin{cases}\n",
    "1, & \\text{if the } o\\text{-th observation belongs to class } c, \\\\\n",
    "0, & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "- $M$: number of possible classes.\n",
    "- $o$: index of the observation.\n",
    "- $c$: index of the class.\n",
    "- $p_{o,c}$: predicted probability that object $o$ belongs to class $c$; it must satisfy all probability properties, specifically $\\sum_{c=1}^M p_{o,c} = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Popular particular case is cross entropy for binary classification:\n",
    "\n",
    "$$-(y_o log[p_o] + [1-y_o] log[1-p_o])$$\n",
    "$$\n",
    "y_o=\\begin{cases}\n",
    "1, & \\text{if the } o\\text{-th observation has a learnt trait}, \\\\\n",
    "0, & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- $p_o$: probability that trait under consideration manifests itself in an $o$-th object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider example estimating performance of the predicted probabilites to the array where each object belongs to one of three classes - $\\{1, 2, 2, 3, 3, 1, 3\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has to be transformed into an array like $y_{o, c}$. The following cell shows the corresponding Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.array([1, 2, 2, 3, 3, 1, 3])\n",
    "y = np.concatenate(\n",
    "    [\n",
    "        (array==i).astype(int)[None, :].T\n",
    "        for i in np.sort(np.unique(array))\n",
    "    ], \n",
    "    axis=1\n",
    ")\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we have three algorithms that return probabilities $p_{o,c}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.67695441, 0.01024423, 0.31280136],\n",
       "       [0.36137308, 0.53013996, 0.10848696],\n",
       "       [0.11463932, 0.78747887, 0.09788181],\n",
       "       [0.03796145, 0.29451329, 0.66752526],\n",
       "       [0.00204705, 0.265555  , 0.73239795],\n",
       "       [0.5446722 , 0.32421575, 0.13111205],\n",
       "       [0.33074201, 0.2575146 , 0.41174339]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.41470252, 0.2086117 , 0.37668579],\n",
       "       [0.21103849, 0.49388948, 0.29507203],\n",
       "       [0.21701083, 0.52880434, 0.25418484],\n",
       "       [0.28397734, 0.18396883, 0.53205384],\n",
       "       [0.24361812, 0.0690321 , 0.68734978],\n",
       "       [0.4491705 , 0.52132683, 0.02950267],\n",
       "       [0.24151501, 0.21116556, 0.54731942]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.39799116, 0.42683509, 0.17517376],\n",
       "       [0.29774474, 0.3534999 , 0.34875537],\n",
       "       [0.21948697, 0.51583723, 0.2646758 ],\n",
       "       [0.05610816, 0.49294396, 0.45094788],\n",
       "       [0.13172646, 0.32382048, 0.54445306],\n",
       "       [0.49503411, 0.38716011, 0.11780577],\n",
       "       [0.22672704, 0.34266168, 0.43061128]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "def generare_preds(y: np.ndarray, factor: float):\n",
    "    ans = np.random.rand(*y.shape) + y*factor\n",
    "    return ans/ans.sum(axis=1, keepdims=True)\n",
    "\n",
    "best_probs, good_probs, bad_probs = [\n",
    "    generare_preds(y=y, factor=0.6)\n",
    "    for factor in [0.6, 0.4, 0.1]\n",
    "]\n",
    "\n",
    "for v in [best_probs, good_probs, bad_probs]:\n",
    "    display(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the \"algorithms\" are generated in such a way that there is a decrease in quality. The first and second \"algorithms\" have almost perfect accuracy, but the predictions of the first \"algorithm\" are more confident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the components of $y_{o, c} \\log(p_{o,c})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39015135, 0.        , 0.        ],\n",
       "       [0.        , 0.63461423, 0.        ],\n",
       "       [0.        , 0.23891875, 0.        ],\n",
       "       [0.        , 0.        , 0.40417805],\n",
       "       [0.        , 0.        , 0.31143127],\n",
       "       [0.60757114, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.88735497]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.88019384, 0.        , 0.        ],\n",
       "       [0.        , 0.70544352, 0.        ],\n",
       "       [0.        , 0.63713679, 0.        ],\n",
       "       [0.        , 0.        , 0.6310106 ],\n",
       "       [0.        , 0.        , 0.37491197],\n",
       "       [0.80035273, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.6027227 ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.92132549, 0.        , 0.        ],\n",
       "       [0.        , 1.03987208, 0.        ],\n",
       "       [0.        , 0.66196401, 0.        ],\n",
       "       [0.        , 0.        , 0.79640352],\n",
       "       [0.        , 0.        , 0.60797354],\n",
       "       [0.7031286 , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.84254951]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for p in [best_probs, good_probs, bad_probs]:\n",
    "    display(-np.log(p)*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting that only the predictions for $y_{o,c}$ play a role; more confident predictions generate less cross-entropy for that observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just to be sure, let's compute the average cross-entropy for the entire sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49631710718508326"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6616817361606729"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7961738233757771"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for p in [best_probs, good_probs, bad_probs]:\n",
    "    display(np.sum(-np.log(p)*y, axis=1, keepdims=True).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, higher-quality algorithms received a lower score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
