{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00ba84a0",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "Embedding is the representation of something as a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b7a0b",
   "metadata": {},
   "source": [
    "## Dence vs Sparce\n",
    "\n",
    "There are two types of embeddings: **dence** and **sparce**.\n",
    "\n",
    "**Sparce** embeddings usually result in a high-dimensional vector close to the vocabulary size (30 000 is a typical size). Each position of in the encoding vector corresponds to a specific token in the vocabulary, which makes the interpreting the results easier.\n",
    "\n",
    "**Dence** embeddings typically have fewer dimentions (384, 768, or 1024 elements), and the position of an element is not directly related to a specific token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0306d887",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Word2Vec (W2V) is an approach to building word embeddings based on the context. Words with similar contexts will have similar embeddings.\n",
    "\n",
    "To each word corresponds two vectors:\n",
    "\n",
    "- $u_i \\in \\mathbb{R}^n$: center vector.\n",
    "- $\\nu_i \\in \\mathbb{R}^n$: context vector.\n",
    "\n",
    "Now, let's consider words, $i$ and $j$. The probability of encounting word $i$ in the context of word $j$ we'll define as following:\n",
    "\n",
    "$$p_{ij} = \\sigma(u_i^T \\nu_j)$$\n",
    "\n",
    "- $\\sigma$: sigmoid function.\n",
    "\n",
    "The optimization algorithm looks for $u_i$ and $\\nu_i$ that maximize $p_{ij}$ when word $i$ contains word $j$ in its context and minimize it when it does not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb97ef6",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "BERT is a popular model for building embeddings. Developed by Google, it has a many modicications for specific tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3d869d",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "BERT training icluded solving two tasks: Masked token prediction (MTP) and Next Sentence Prediction (NSP)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1925bdad",
   "metadata": {},
   "source": [
    "**Masked Token Prediction (MTP)**: some tokens of the sentence were masked or replaced and the goal of the model was to predict the correct token in that places.\n",
    "\n",
    "Replacements usually follow empirical rules:\n",
    "\n",
    "- 15% of the tokens form the original data are selected to participate in loss function calculation.\n",
    "- 80% of the selected tokens must be replaced with a mask (the original BERT typically uses the special `[MASKED]` token).\n",
    "- 10% of the selected tokens must be replaced with random ones.\n",
    "- 10% of the selected tokens must be left unchanged.\n",
    "\n",
    "This rule is important for avoiding model overfitting. The specific values were discovered through experementation.\n",
    "\n",
    "As example consider the following sentence:\n",
    "\n",
    "- *Finetuning sparse embedding models involves several components: the model, datasets, loss functions, training arguments, evaluators, and the trainer class.*\n",
    "\n",
    "For simplicity, we will consider the case of white space tokenization.\n",
    "\n",
    "- *Finetuning sparse embedding <span style=\"color:#A0A35B\">[MASKED]</span> involves several <span style=\"color:green\">components:</span> the model, datasets, loss <span style=\"color:red\">dogs</span>, training arguments, <span style=\"color:#A0A35B\">[MASKED]</span>, and the trainer class.*\n",
    "\n",
    "We expect the model to predict something like this:\n",
    "\n",
    "- `[-][-][-][models][-][-][components:][-][-][-][-][functions][-][-][evaluators][-][-][-]`\n",
    "\n",
    "Here `[-]` marks tokens that are unimportant to us - they do not appear in 15% of the \"interesting\" tokens and are not used in the loss function calculation. However, the words that appear in 15% of the \"interesting\" tokens must be determined directly to minimize the loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8f229f",
   "metadata": {},
   "source": [
    "**Next Sentence Prediction (NSP)**: The model is given a sentence that is separated by a special token. It must to classify whether the second part matches the first.\n",
    "\n",
    "The original approach by google sentence uses:\n",
    "\n",
    "- `[CLS]`:  token that idicates where the outcome should come from.\n",
    "- `[SEP]`: token separates the the part of the sentence that is guaranteed to be correct and, possibly, uncorrect.\n",
    "\n",
    "Suppose the training dataset contains the following sentence:\n",
    "\n",
    "- *Born in Nine Mile, Jamaica, Marley began his career in 1963, after forming the group Teenagers with Peter Tosh and Bunny Wailer, which became the Wailers*.\n",
    "\n",
    "For NSP it can be transformed as:\n",
    "\n",
    "- ***[CLS]** Born in Nine Mile, Jamaica, Marley began his career in 1963, after forming the **[SEP]** group Teenagers with Peter Tosh and Bunny Wailer, which became the Wailers*.\n",
    "\n",
    "In this case, the output sequence at the positions corresponding to the [CLS] must contain a signal that can be interpreted as \"true\".\n",
    "\n",
    "Or sentence can be transformed as:\n",
    "\n",
    "- ***[CLS]** Born in Nine Mile, Jamaica, Marley began his career in 1963, after forming the **[SEP]** mama of hand typing fast*.\n",
    "\n",
    "This makes no sense, so the model trains to predict a \"false\" signal in hte possition corresponding to [CLS] token."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
