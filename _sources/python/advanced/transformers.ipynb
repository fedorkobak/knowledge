{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18428b2c-1235-48c9-8f91-450e5e7c3144",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "`transformers` is a library in python that allows you to use pre-trained machine learning models that belong to the transformers architecture. So in the following example, the pre-trained Bert model has been loaded and deiaplayed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9389c4f5-0976-475c-913c-8db542879212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8803b34-a5cb-4182-ae2e-d114f5d7d971",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4d2120f-e773-4bc3-8cee-7042c2a83187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "model = BertModel.from_pretrained('bert-base-cased')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87a2ef8-5703-4895-8a22-2254789de471",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "To use the model correctly, you may need a tokiniser - a program that trains text to tokens. Trainformers also have special tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70504625-388f-47ac-ad23-6a8491350c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a49544-7d5d-4119-acc3-9cd470a048fe",
   "metadata": {},
   "source": [
    "So here is example how \"Hello!\" phrase can be tokinized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93d99edc-79fb-4dd3-acb9-f603a0be0ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 8667,  106,  102]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(\n",
    "    'Hello!', \n",
    "    add_special_tokens=True, \n",
    "    return_token_type_ids=False, \n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7af3ce-1efe-42ae-ba21-65a667a1a315",
   "metadata": {},
   "source": [
    "## Model usage\n",
    "\n",
    "Here we combine the downloaded tokiniser and model and use it to run some phrase through the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddee02cb-473e-492e-b598-b18d0558e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode_plus(\n",
    "    'Hello!', \n",
    "    add_special_tokens=True, \n",
    "    return_token_type_ids=False, \n",
    "    return_tensors='pt'\n",
    ")\n",
    "res = model(**encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672437e2-4086-408e-bf7d-90183171d72d",
   "metadata": {},
   "source": [
    "We've got back an instance of a specific class that implements the result of the BERT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01a0bc82-a2cc-4758-a334-bd21925945e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a60659-1265-474e-91f8-b6b7d98e0611",
   "metadata": {},
   "source": [
    "It contains two objects:\n",
    "\n",
    "- `last_hidden_state` - state of last hidden layer;\n",
    "- `pooler_output` - output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6834cba7-c8be-406c-8ebf-f135d6e2b5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Last hidden state:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.6283,  0.2166,  0.5605,  ...,  0.0136,  0.6158, -0.1712],\n",
      "         [ 0.6108, -0.2253,  0.9263,  ..., -0.3028,  0.4500, -0.0714],\n",
      "         [ 0.8040,  0.1809,  0.7076,  ..., -0.0685,  0.4837, -0.0774],\n",
      "         [ 1.3290,  0.2360,  0.4567,  ...,  0.1509,  0.9621, -0.4841]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br><b>Pooler output:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7105,  0.4876,  0.9999, -0.9947,  0.9599,  0.9521,  0.9767, -0.9946,\n",
      "         -0.9815, -0.6238,  0.9776,  0.9984, -0.9989, -0.9998,  0.8559, -0.9755,\n",
      "          0.9895, -0.5281, -1.0000, -0.7414, -0.7056, -0.9999,  0.2901,  0.9786,\n",
      "          0.9729,  0.0734,  0.9828,  1.0000,  0.8981, -0.1109,  0.2780, -0.9920,\n",
      "          0.8693, -0.9985,  0.1461,  0.2067,  0.8092, -0.2430,  0.8580, -0.9585,\n",
      "         -0.8130, -0.6138,  0.7961, -0.5727,  0.9737,  0.2362, -0.1194, -0.0789,\n",
      "          0.0031,  0.9997, -0.9519,  0.9899, -0.9962,  0.9931,  0.9950,  0.5050,\n",
      "          0.9952,  0.1090, -0.9994,  0.3416,  0.9792,  0.2506,  0.8923, -0.2238,\n",
      "          0.3518, -0.5293, -0.9570,  0.1357, -0.3313,  0.1627, -0.0078,  0.3608,\n",
      "          0.9833, -0.9160,  0.0196, -0.9141,  0.2075, -0.9999,  0.9449,  1.0000,\n",
      "          0.7796, -0.9997,  0.9935, -0.2309, -0.7830,  0.8880, -0.9994, -0.9994,\n",
      "          0.0160, -0.6875,  0.9554, -0.9846,  0.7813, -0.9322,  1.0000, -0.9511,\n",
      "         -0.1583,  0.3866,  0.9699, -0.8283, -0.7689,  0.9346,  0.9994, -0.9965,\n",
      "          0.9992,  0.8548, -0.9459, -0.9451,  0.8292,  0.0406,  0.9891, -0.9857,\n",
      "         -0.9555,  0.0784,  0.9803, -0.9302,  0.9901,  0.8020, -0.2408,  1.0000,\n",
      "         -0.2367,  0.9689,  0.9982,  0.8761, -0.8891, -0.2167, -0.7227,  0.9385,\n",
      "         -0.7771, -0.5691,  0.8276, -0.9881, -0.9987,  0.9994, -0.2452,  1.0000,\n",
      "         -0.9992,  0.9935, -0.9999, -0.8808, -0.7632, -0.1224, -0.9886,  0.0386,\n",
      "          0.9893,  0.0950, -0.9691, -0.8293,  0.7363, -0.9115,  0.4936,  0.6483,\n",
      "         -0.9581,  0.9715,  0.9982,  0.9653,  0.9890,  0.1850, -0.9722,  0.8544,\n",
      "          0.9774, -0.9995,  0.8701, -0.9960,  0.9993,  0.9746,  0.8500, -0.9971,\n",
      "          0.9999, -0.8101,  0.0206,  0.0232,  0.0982, -0.9993,  0.4584,  0.4759,\n",
      "          0.8759,  0.9993, -0.9945,  0.9995,  0.8257, -0.0958,  0.8097,  0.9992,\n",
      "         -0.9966, -0.9701, -0.9847,  0.2290,  0.8039,  0.8126,  0.4859,  0.9628,\n",
      "          0.9992,  0.7803, -0.9974, -0.3289,  0.9692, -0.1398,  1.0000, -0.4062,\n",
      "         -0.9998, -0.7585,  0.9499,  0.9883, -0.2351,  0.9828, -0.7602, -0.1856,\n",
      "          0.9882, -0.9727,  0.9989,  0.4863,  0.9102,  0.8847,  0.9915, -0.9258,\n",
      "         -0.0788,  0.1543, -0.7543,  0.9999, -0.9996, -0.2332,  0.5874, -0.9942,\n",
      "         -0.9979,  0.9794, -0.0194, -0.8693, -0.2168,  0.7979,  0.2004,  0.9521,\n",
      "          0.9900, -0.6276, -0.7824, -0.9998, -0.9979, -0.8998, -0.9672,  0.0430,\n",
      "          0.6920, -0.3973, -0.9400, -0.9990,  0.9653,  0.4508, -0.8941,  0.1173,\n",
      "         -0.7708, -0.9993,  0.6723, -0.9301, -0.9982,  0.9996, -0.7676,  0.9977,\n",
      "          0.9586, -0.9940,  0.8469, -0.9993, -0.1126, -0.9871,  0.6339,  0.6951,\n",
      "         -0.6402, -0.0494,  0.9932, -0.9665, -0.8052,  0.8924, -0.9999,  0.9337,\n",
      "         -0.2121,  0.9991,  0.8133,  0.2323,  0.9850,  0.9504, -0.9842, -0.9998,\n",
      "          0.9709,  0.8598, -0.9926, -0.1464,  0.9999, -0.9989, -0.8643, -0.9607,\n",
      "         -0.9940, -0.9996,  0.2709, -0.8859,  0.2709,  0.9882,  0.6940,  0.1279,\n",
      "          0.9929,  0.9910,  0.2286, -0.3951,  0.1043, -0.9768, -0.9314,  0.9270,\n",
      "          0.1190, -1.0000,  0.9999, -0.9937,  0.9782,  0.9644, -0.9963,  0.8284,\n",
      "          0.1232, -0.9757,  0.0153,  0.9999,  0.9855, -0.1870,  0.2675,  0.9221,\n",
      "         -0.3209,  0.6954, -0.8953, -0.7505,  0.2031, -0.9330,  0.9959,  0.7163,\n",
      "         -0.9901,  0.9979,  0.0020,  0.8427, -0.8604,  0.9138,  0.9909, -0.1418,\n",
      "         -0.6551,  0.0506, -0.1576, -0.9825,  0.2171, -0.9974, -0.5776,  0.9791,\n",
      "          0.9842, -0.9881,  0.9842, -0.0714,  0.9470, -0.9988,  1.0000, -0.9961,\n",
      "          0.0919,  0.8178, -0.8827, -0.6587,  0.9920,  0.9926,  0.9770, -0.9782,\n",
      "         -0.8553,  0.8854,  0.9670, -0.9807, -0.0805, -0.9996, -0.8613,  0.9954,\n",
      "          0.9971,  0.0548, -0.1328, -0.9985,  0.9674, -0.8499, -0.9574, -0.0873,\n",
      "         -0.8494,  0.8687,  0.9986, -0.7471,  0.7234,  0.1623, -0.9843,  0.9408,\n",
      "          0.9139,  0.9998, -0.9575,  0.6504,  0.9878, -0.2068, -0.8754,  0.6124,\n",
      "          0.9995, -0.9634, -0.2490, -0.9995, -0.0428, -0.6623, -0.3127, -0.6662,\n",
      "          0.0496, -0.8922,  0.9753,  0.0668,  0.8677, -0.4532,  0.9877, -0.1256,\n",
      "         -0.0084, -0.3633, -0.4310,  0.5263,  0.2795,  0.9855, -0.9623,  0.9997,\n",
      "         -0.2454, -1.0000, -0.9985, -0.8149, -0.9996,  0.8664, -0.9936,  0.9833,\n",
      "          0.9645, -0.9990, -0.9995, -0.9971, -0.9827,  0.8944,  0.7312, -0.0281,\n",
      "          0.3282, -0.0801, -0.0505, -0.5034,  0.0436, -0.9379, -0.6098, -0.9991,\n",
      "          0.9075, -1.0000, -0.8943,  0.9983, -0.9974, -0.9702, -0.9073, -0.5143,\n",
      "         -0.8728,  0.5948,  0.9871, -0.4346, -0.8071, -0.9995,  0.9870, -0.8455,\n",
      "          0.0911, -0.9254, -0.9747,  0.9997,  0.8546, -0.1713, -0.0070, -0.9989,\n",
      "          0.9919, -0.9512, -0.9604, -0.9775,  0.2517, -0.9669, -0.9998,  0.0274,\n",
      "          0.9981,  0.9918,  0.9872,  0.3586, -0.4554, -0.9641,  0.1662, -0.9999,\n",
      "          0.8734,  0.9199, -0.9861, -0.7962,  0.9937,  0.9731, -0.9745, -0.9908,\n",
      "          0.9605,  0.3886,  0.9752, -0.6621, -0.5456,  0.3875,  0.0179, -0.9900,\n",
      "         -0.9324,  0.9966, -0.9996,  0.9857,  0.9975,  0.9992, -0.2936,  0.1672,\n",
      "         -0.9914, -0.9807, -0.5163,  0.3407, -0.9999,  0.9999, -1.0000,  0.4725,\n",
      "         -0.8529,  0.9192,  0.9880, -0.4080, -0.9999, -0.9998,  0.3443,  0.1171,\n",
      "          0.9887,  0.4144,  0.1948, -0.6747, -0.3843,  0.9973, -0.8765, -0.8126,\n",
      "         -0.9988,  0.9997,  0.4865, -0.9982,  0.9962, -0.9995,  0.8606,  0.9813,\n",
      "          0.8977,  0.9750, -0.9995,  1.0000, -0.9998,  0.9968, -1.0000, -0.9994,\n",
      "          0.9998, -0.9914, -0.8091, -0.9997, -0.9992,  0.7226,  0.1572, -0.5659,\n",
      "          0.9883, -0.9998, -0.9987, -0.4541, -0.9309, -0.8762,  0.9972, -0.8189,\n",
      "          0.9894, -0.0908,  0.9623,  0.3485,  0.9983,  0.9907, -0.7728, -0.8770,\n",
      "         -0.9934,  0.9906, -0.6931,  0.4029,  0.9709, -0.0175, -0.8430,  0.3529,\n",
      "         -0.9967,  0.5958,  0.1322,  0.9368,  0.9241,  0.8697, -0.1084, -0.5757,\n",
      "         -0.3093, -0.9923,  0.5895, -0.9995,  0.9794, -0.9611,  0.0368, -0.4532,\n",
      "          0.2907, -0.9616,  0.9996,  0.9988, -0.9890,  0.0842,  0.9875, -0.8991,\n",
      "          0.9725, -0.9923,  0.0591,  0.9806, -0.7373,  0.9830, -0.0013,  0.0637,\n",
      "          0.9887, -0.9946, -0.9119, -0.6857,  0.3650,  0.0621, -0.9661,  0.0115,\n",
      "          0.9902, -0.4771, -0.9997,  0.9412, -0.9993, -0.1202,  0.9763, -0.0377,\n",
      "          0.9999, -0.8283,  0.1905,  0.1534, -0.9998, -0.9995,  0.0540, -0.1655,\n",
      "         -0.9095,  0.9996, -0.3539,  0.8774, -0.9999,  0.3085,  0.9981,  0.2593,\n",
      "          0.8318, -0.9150, -0.9679, -0.9740, -0.7623,  0.0018,  0.8898, -0.9830,\n",
      "         -0.6928, -0.9300,  1.0000, -0.9982, -0.9065, -0.9884,  0.7844,  0.9037,\n",
      "          0.4408,  0.1464, -0.9218,  0.9302, -0.9442,  0.9973, -0.9939, -0.9963,\n",
      "          0.9998,  0.5109, -0.9965,  0.2434, -0.4521,  0.3922,  0.1305,  0.8537,\n",
      "         -0.8603, -0.2880, -0.9964,  0.8409, -0.9161, -0.9866, -0.6739, -0.3227,\n",
      "         -0.9776,  0.9935,  0.9731,  0.9999, -0.9998,  0.9386, -0.0222,  0.9991,\n",
      "          0.0548, -0.7093,  0.9169,  0.9997, -0.7628,  0.8665, -0.1445,  0.0659,\n",
      "          0.4782, -0.4443,  0.9984, -0.9318,  0.0735, -0.9737, -0.9999,  0.9999,\n",
      "         -0.0253,  0.9916,  0.3062,  0.8545, -0.9106,  0.9857, -0.9849, -0.9302,\n",
      "         -1.0000,  0.1720, -0.9895, -0.9877, -0.1270,  0.9847, -0.9996, -0.9917,\n",
      "         -0.4165, -1.0000,  0.9550, -0.9948, -0.8871, -0.9896,  0.9988, -0.2867,\n",
      "         -0.8413,  0.9756, -0.9648,  0.9592,  0.9145, -0.4687,  0.2182,  0.1638,\n",
      "         -0.8462, -0.9960, -0.9356, -0.9699,  0.9437, -0.9875, -0.8543,  0.9966,\n",
      "          0.9835, -0.9992, -0.9944,  0.9973,  0.2491,  0.9927, -0.5164, -0.9998,\n",
      "         -0.9999,  0.0790,  0.2047,  0.9947, -0.3889,  0.9576,  0.8522, -0.5662,\n",
      "          0.6047, -0.8014, -0.2958, -0.6084, -0.2891,  1.0000, -0.9179,  0.9894]],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "display(HTML(\"<b>Last hidden state:</b>\"))\n",
    "print(res.last_hidden_state)\n",
    "display(HTML(\"<br><b>Pooler output:</b>\"))\n",
    "print(res.pooler_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2900567e-a5b2-4308-ba24-43ffd86b3efe",
   "metadata": {},
   "source": [
    "## Apply to dataset\n",
    "\n",
    "This section shows how the BERT model can be applied to a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38fbf27-20a6-4fcd-ac5e-37fb64368055",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3374942-a756-4e09-a7a2-3f6ddafcfc95",
   "metadata": {},
   "source": [
    "We will be working with the `datasets` library, which is usually used in conjunction with the `transformers` library. So in the following cell we have extracted the `imdb` library, which contains reviews for the movies. We use a small subset of the dataframe to reduce the amount of calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7c47668-c68e-45b4-8e1a-22fc24f4881e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"The Film must have been shot in a day,there are scenes where you can see the camera reflections and its red pointer,even the scenery's green light that blends with the actors!!!The plot and the lines are really awful without even the slightest inspiration(At least as a thriller genre movie).Everything that got to do with Poe in the movie,has a shallow and childish approach.The film is full of clise and no thrilling.If you want to watch a funny b-movie for a relaxing evening with friends then go for it you will enjoy it (As I Did) but there's no way to take this film seriously!\",\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "\n",
    "np.random.seed(100)\n",
    "idx = np.random.randint(len(dataset), size=200)\n",
    "dataset = dataset.select(idx)\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8c692e-7031-418c-aa88-2bfdc306e8d5",
   "metadata": {},
   "source": [
    "Applying tokenization to the dataset. Finally for each element of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6aee675-bacf-4392-8dce-718d616d58c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(example):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        example['text'],\n",
    "        add_special_tokens=True, \n",
    "        return_token_type_ids=False, \n",
    "        truncation=True\n",
    "    )\n",
    "dataset = dataset.map(\n",
    "    tokenization, batched=True\n",
    ")\n",
    "dataset.set_format(\n",
    "    type=\"torch\", \n",
    "    columns=[\"input_ids\", \"attention_mask\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd84007-8792-42b7-ac82-b2b64cad8b8f",
   "metadata": {},
   "source": [
    "After that we have extra keys for each object `input_ids` and `attention_mask` - wich is required for bert model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa92ac52-bf13-4a0f-8412-48644af38c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Keys:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'attention_mask']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br><b>Tokens:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  1109,  2352,  1538,  1138,  1151,  2046,  1107,   170,  1285,\n",
      "          117,  1175,  1132,  4429,  1187,  1128,  1169,  1267,  1103,  4504,\n",
      "        26906,  1105,  1157,  1894,  1553,  1200,   117,  1256,  1103, 19335,\n",
      "          112,   188,  2448,  1609,  1115, 13390,  1116,  1114,  1103,  5681,\n",
      "          106,   106,   106,  1109,  4928,  1105,  1103,  2442,  1132,  1541,\n",
      "         9684,  1443,  1256,  1103, 16960,  7670,   113,  1335,  1655,  1112,\n",
      "          170, 11826,  6453,  2523,   114,   119,  5268,  1115,  1400,  1106,\n",
      "         1202,  1114, 21377,  1107,  1103,  2523,   117,  1144,   170,  8327,\n",
      "         1105,  2027,  2944,  3136,   119,  1109,  1273,  1110,  1554,  1104,\n",
      "          172,  6137,  1162,  1105,  1185, 21401,  1158,   119,  1409,  1128,\n",
      "         1328,  1106,  2824,   170,  6276,   171,   118,  2523,  1111,   170,\n",
      "        22187,  3440,  1114,  2053,  1173,  1301,  1111,  1122,  1128,  1209,\n",
      "         5548,  1122,   113,  1249,   146,  2966,   114,  1133,  1175,   112,\n",
      "          188,  1185,  1236,  1106,  1321,  1142,  1273,  5536,   106,   102])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br><b>Mask:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "display(HTML(\"<b>Keys:</b>\"))\n",
    "print(list(dataset[0].keys()))\n",
    "display(HTML(\"<br><b>Tokens:</b>\"))\n",
    "print(dataset[0]['input_ids'])\n",
    "display(HTML(\"<br><b>Mask:</b>\"))\n",
    "print(dataset[0]['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238b6a4a-b93d-4135-9e15-24f6ce311592",
   "metadata": {},
   "source": [
    "### Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009946cf-c455-42ab-817e-bb8dec8b1e69",
   "metadata": {},
   "source": [
    "Preparing the classical `torch.loader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0e550ad-9765-44a7-95d4-6c3329d23ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "loader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=32, \n",
    "    collate_fn=data_collator, \n",
    "    pin_memory=True,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490c11d7-cca3-4287-a939-1da9d039c04e",
   "metadata": {},
   "source": [
    "Now let's look at what the first element of `loader` will be - it's dict, with all the necessary stuff for fitting and forwarding through bert model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1fff79c-5d65-4fc2-8566-bb36d0ee1070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = next(iter(loader))\n",
    "item.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4ee675-8afa-4b75-b8ba-9341a7a200d6",
   "metadata": {},
   "source": [
    "Finally you can pass `item` to the `model` to get prediction, but you need high-performance hardware to run this: \n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "model(**item)[\"pooler_output\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b0326-a7c2-46a9-a7fd-2f9216fe761a",
   "metadata": {},
   "source": [
    "Finally, we need to model all the batches. In the following cell we have just extracted embeddings for the whole `imdb` dataset.\n",
    "\n",
    "**Note**:\n",
    "\n",
    "- This cell should be started on powerful hardware;\n",
    "- The function nested in `torch.inference_mode` is crucial, without it the model takes too much memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8badcac-b096-4132-ae5f-524c423081eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [01:37<00:00, 13.89s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def get_embeddings(model, loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    total_embeddings = []\n",
    "    labels = []\n",
    "    \n",
    "    for batch in tqdm(loader):\n",
    "\n",
    "        batch = {\n",
    "            key: batch[key].to(device) \n",
    "            for key in ['attention_mask', 'input_ids']\n",
    "        }\n",
    "        \n",
    "        embeddings = model(**batch)['last_hidden_state'][:, 0, :]\n",
    "        total_embeddings.append(embeddings)\n",
    "\n",
    "    return torch.cat(total_embeddings, dim=0)\n",
    "\n",
    "embeddings = get_embeddings(model, loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7795c7d0-440e-4a11-ba87-4f1765e561e1",
   "metadata": {},
   "source": [
    "So for each line from the dataset we got an embedding from the model. It's representation and shape is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0c726a9-8da0-4d88-84c0-3a3e611ccb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Embeddings:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6028,  0.1125, -0.2223,  ..., -0.1698,  0.1655,  0.0645],\n",
       "        [ 0.6183,  0.0239, -0.2428,  ..., -0.1532,  0.1792,  0.1111],\n",
       "        [ 0.3977,  0.1045, -0.1627,  ..., -0.0737,  0.2369,  0.1243],\n",
       "        ...,\n",
       "        [ 0.5241,  0.1857, -0.4136,  ..., -0.2503,  0.0959,  0.2350],\n",
       "        [ 0.6121,  0.0789, -0.1658,  ..., -0.1467,  0.4310,  0.1626],\n",
       "        [ 0.6343,  0.0062, -0.1479,  ..., -0.2116,  0.1337,  0.1664]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br><b>Shape:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 768])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(\"<b>Embeddings:</b>\"))\n",
    "display(embeddings)\n",
    "display(HTML(\"<br><b>Shape:</b>\"))\n",
    "display(embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
