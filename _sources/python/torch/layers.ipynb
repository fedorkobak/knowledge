{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67beb034-0408-4f3e-ae5f-b9d134adba64",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    "Layer in torch is some transformation with some inputs and some outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fcbd104-4e79-406c-99b9-d6e2979b2d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from math import prod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85e4a60-103f-4cf2-a6e6-32ac1dc99c10",
   "metadata": {},
   "source": [
    "To perform a layer transformation on the tensor `X`, simply use the syntax `layer(X)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6bd924-b58f-459c-a344-793248e069c9",
   "metadata": {},
   "source": [
    "## Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a22a9c3-620a-4e09-82be-0363292487de",
   "metadata": {},
   "source": [
    "The `torch.nn.Linear` layer performs the following operation:\n",
    "\n",
    "$$X_{n \\times l} \\cdot \\omega_{l \\times k} + b_k$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $l$ - number of inputs\n",
    "- $k$ - number of outputs\n",
    "- $n$ - number of input samples\n",
    "- $X_{n \\times l}$ - input tensor\n",
    "- $\\omega_{l \\times k}$ - weight matrix of the layer\n",
    "- $b_k$ - bias vector of the layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf498d04",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell applies the tensor to some data and manually performs the same transformation. The results should be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90385899-ed31-4bd7-8601-d5ef8f966acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer transformation\n",
      "[-0.6235317587852478, -1.1335619688034058, -0.70122230052948]\n",
      "X@w+b\n",
      "[-0.6235317587852478, -1.1335619688034058, -0.70122230052948]\n"
     ]
    }
   ],
   "source": [
    "in_features = 5\n",
    "out_features = 3\n",
    "\n",
    "linear = nn.Linear(\n",
    "    in_features = in_features, \n",
    "    out_features = out_features\n",
    ")\n",
    "\n",
    "X = torch.rand(in_features)\n",
    "\n",
    "print(\"Layer transformation\")\n",
    "print(linear(X).tolist())\n",
    "print(\"X@w+b\")\n",
    "print((linear.weight@X + linear.bias).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2efedf",
   "metadata": {},
   "source": [
    "### Define values\n",
    "\n",
    "To define custom values for tensors you have to use access `weight` and `bias` fater layer creation. They are belongs to the `type(linear_layer.weight)` data type. So you have to use method `copy_` under `torch.no_grad` context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085fc399",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Here’s an example of how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91af648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = nn.Linear(in_features=3, out_features=4)\n",
    "\n",
    "default_weights = torch.ones_like(linear_layer.weight)\n",
    "default_biases = torch.zeros_like(linear_layer.bias)\n",
    "\n",
    "with torch.no_grad():\n",
    "    linear_layer.weight.copy_(default_weights)\n",
    "    linear_layer.bias.copy_(default_biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5013a97b",
   "metadata": {},
   "source": [
    "After completing the process, we have the `weight` tensor initialized with ones and the `bias` tensor initialized with zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d5ceadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(linear_layer.weight)\n",
    "print(linear_layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d80a2c",
   "metadata": {},
   "source": [
    "### dtype\n",
    "\n",
    "By default, tensors used in `torch.nn.Linear` have a `float32` data type. This can lead to issues when processing tensors with different data types. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1660663",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell defines a tensor with a `float16` data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a604411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.3904e+04,  3.6992e+00,  3.5763e-07],\n",
       "        [ 0.0000e+00, -5.1200e+02,         nan],\n",
       "        [-5.1200e+02,  4.3789e+00,  2.0000e+00]], dtype=torch.float16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_size = 3\n",
    "\n",
    "tensor = torch.empty(\n",
    "    size=(tensor_size, tensor_size), \n",
    "    dtype=torch.float16\n",
    ")\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e652a465",
   "metadata": {},
   "source": [
    "The following cell defines a tensor with a `float32` data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2f28cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(tensor_size, tensor_size)\n",
    "for p in layer.parameters():\n",
    "    print(p.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1c8738",
   "metadata": {},
   "source": [
    "Trying to apply the `layer` to the tensor will result in an error stating that the data types are incompatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71a1ae9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Half and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Half and Float"
     ]
    }
   ],
   "source": [
    "layer(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f70496",
   "metadata": {},
   "source": [
    "The following cell demonstrates how to change the data type of tensors used in `nn.Linear`. After modifying the data types, you can successfully apply the `layer` to the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6f9ae10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6652.0000, 24528.0000, -9664.0000],\n",
       "        [       nan,        nan,        nan],\n",
       "        [   76.9375,  -286.0000,   113.6875]], dtype=torch.float16,\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for p in layer.parameters():\n",
    "    p.data = p.data.to(torch.float16)\n",
    "\n",
    "layer(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932cfaf1",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "A dropout layer randomly sets some components of the input tensor to zero with a given probability $p$. During training, the remaining non-zero components are scaled by a factor of $ \\frac{1}{1-p}$ to prevent signal attenuation. Formally, if we start with a tensor $x_i$, where $i \\in \\mathbb{N}^k$ represents the indices of the $k$-dimensional tensor, the output after applying dropout is given by:\n",
    "\n",
    "$$\n",
    "x'_i = x_i \\cdot p_i \\cdot \\frac{1}{1-p},\n",
    "$$\n",
    "\n",
    "where $p_i$ is sampled from a Bernoulli distribution with parameter $p$, i.e., $p_i \\sim \\text{Bernoulli}(p)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085d31ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This example demonstrates the transformation of a tensor after passing through a dropout layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "49ae0a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]], dtype=torch.float64)\n",
      "\n",
      "Dropout result:\n",
      "tensor([[ 1.4286,  2.8571,  0.0000],\n",
      "        [ 5.7143,  0.0000,  8.5714],\n",
      "        [10.0000,  0.0000, 12.8571]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(111)\n",
    "\n",
    "dropout_layer = nn.Dropout(p=0.3)\n",
    "tensor = (torch.arange(3 * 3, dtype=float) + 1).reshape((3, 3))\n",
    "\n",
    "print(\"Original tensor:\")\n",
    "print(tensor)\n",
    "print()\n",
    "print(\"Dropout result:\")\n",
    "print(dropout_layer(tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d1a2c2",
   "metadata": {},
   "source": [
    "## Batch normalisation\n",
    "\n",
    "This is a transformation that normalizes data along the last dimension, which is assumed to separate different objects in the sample. It uses standartization method of scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dc4376",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider a two-dimensional example where each object is represented by a row vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d5f30ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2821,  0.4937,  2.2798,  3.1782],\n",
       "        [ 0.8915,  0.6284,  0.9562,  2.9096],\n",
       "        [ 0.3376,  0.5322, -0.5936,  2.1784],\n",
       "        [-0.2011, -0.0289,  2.5192,  3.6398],\n",
       "        [ 0.0640,  0.2830,  1.4539,  3.7226]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objecs_numer = 5; features_number = 4\n",
    "X = (\n",
    "    torch.normal(0, 1, [objecs_numer, features_number]) +\\\n",
    "    torch.arange(features_number)\n",
    ")\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6ca6b1",
   "metadata": {},
   "source": [
    "Let's examine standardization implemented manually – simply substituting the mean and dividing by the standard deviation along each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99b80e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6310,  0.4277,  0.7702,  0.0838],\n",
       "        [ 0.2199,  0.9421, -0.2954, -0.3453],\n",
       "        [-0.3422,  0.5747, -1.5431, -1.5133],\n",
       "        [-0.8889, -1.5676,  0.9630,  0.8212],\n",
       "        [-0.6199, -0.3769,  0.1053,  0.9536]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X - X.mean(axis=0))/(X.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9438ceb9",
   "metadata": {},
   "source": [
    "The following cell demonstrates the application of the most straightforward normalization method, `nn.BatchNorm1d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c880071f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8235,  0.4781,  0.8612,  0.0937],\n",
       "        [ 0.2459,  1.0532, -0.3303, -0.3860],\n",
       "        [-0.3826,  0.6425, -1.7252, -1.6919],\n",
       "        [-0.9938, -1.7524,  1.0766,  0.9181],\n",
       "        [-0.6930, -0.4213,  0.1177,  1.0661]],\n",
       "       grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn = torch.nn.BatchNorm1d(num_features=features_number)\n",
    "bn(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4c3c08",
   "metadata": {},
   "source": [
    "We obtained results very close to manual normalization, but not perfectly identical. This discrepancy arises from the numerous mechanisms involved in layer fitting, where computation aligns with classical calculation only under specific circumstances. The following cell replicates such conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "48884670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6310,  0.4277,  0.7702,  0.0838],\n",
       "        [ 0.2199,  0.9421, -0.2954, -0.3453],\n",
       "        [-0.3422,  0.5747, -1.5431, -1.5133],\n",
       "        [-0.8889, -1.5676,  0.9630,  0.8212],\n",
       "        [-0.6199, -0.3769,  0.1053,  0.9536]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn = torch.nn.BatchNorm1d(num_features=features_number, eps=0, affine=False, momentum=1)\n",
    "bn(X)\n",
    "bn.eval()\n",
    "bn(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434917a5",
   "metadata": {},
   "source": [
    "Therefore, we achieved identical numerical results to the manual implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72975a47",
   "metadata": {},
   "source": [
    "## Convolutional\n",
    "\n",
    "Convolutional layers are implemented in Torch using the classes `torch.nn.Conv<D>d`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c416d8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Consider the example of an `nn.Conv1d` layer. Suppose we want to perform convolutions with a two-dimensional kernel on a one-channel sequence, producing a single-channel output. The following cell defines and prints the parameters of the `Conv1d` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ce0f2791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[1., 2.]]], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.], requires_grad=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_conv = nn.Conv1d(\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    kernel_size=2\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    show_conv.weight = nn.Parameter(\n",
    "        torch.arange(\n",
    "            prod(show_conv.weight.shape), dtype=torch.float\n",
    "        ).reshape_as(show_conv.weight) + 1\n",
    "    )\n",
    "    show_conv.bias = nn.Parameter(torch.tensor([0.]))\n",
    "\n",
    "print(\"Weight\")\n",
    "display(show_conv.weight)\n",
    "print(\"Bias\")\n",
    "display(show_conv.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af6544",
   "metadata": {},
   "source": [
    "Here is an example of data that can be processed using the layer declared above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "33df3cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.,  4.]],\n",
       "\n",
       "        [[ 5.,  6.,  7.,  8.,  9.]],\n",
       "\n",
       "        [[10., 11., 12., 13., 14.]],\n",
       "\n",
       "        [[15., 16., 17., 18., 19.]],\n",
       "\n",
       "        [[20., 21., 22., 23., 24.]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_count = 5\n",
    "channels_count = 1\n",
    "sequesnce_lenth = 5\n",
    "\n",
    "data = (\n",
    "    torch.arange(\n",
    "        samples_count * channels_count * sequesnce_lenth,\n",
    "        dtype=torch.float\n",
    "    )\n",
    "    .reshape([samples_count, channels_count, sequesnce_lenth])\n",
    ")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca605391",
   "metadata": {},
   "source": [
    "Here are 5 samples from a series of 5 elements, each with one input channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4e482d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.,  5.,  8., 11.]],\n",
       "\n",
       "        [[17., 20., 23., 26.]],\n",
       "\n",
       "        [[32., 35., 38., 41.]],\n",
       "\n",
       "        [[47., 50., 53., 56.]],\n",
       "\n",
       "        [[62., 65., 68., 71.]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_conv(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e196610",
   "metadata": {},
   "source": [
    "Check if the computation for some element matches our expectation:\n",
    "\n",
    "$$x'_{2,3} = x_{2,3}w_{1} + x_{2,4}w_{2} + b = 7 \\times 1 + 8 \\times 2 + 0 = 23$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $x_{ij}$: $j$-th element of the sequence of the $i$-th sample.\n",
    "- $x'_{ij}$: $j$-th element of the output of the $i$-th sample.\n",
    "- $w_i$: $i$-th weight of the layer under consideration.\n",
    "- $b$: bias of the layer under consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c32b77",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "\n",
    "Pooling layers aggregate different subsets of an array according to a specified function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce60f00",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following cell demonstrates the application of `torch.nn.MaxPooling` on a vector. Pooling is primarily designed for convolutional networks, so it is applied along the channels, which is the outermost dimension of the input. Therefore, an extra dimension is added to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c09c9222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      "tensor([[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]], dtype=torch.float16)\n",
      "Output\n",
      "tensor([[2., 5., 8.]], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "input = torch.arange(10, dtype=torch.float16)[None, :]\n",
    "print(\"Input\")\n",
    "print(input)\n",
    "\n",
    "output = torch.nn.MaxPool1d(kernel_size=3)(input)\n",
    "print(\"Output\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265e0009",
   "metadata": {},
   "source": [
    "As a result, the values were computed as follows:\n",
    "\n",
    "- $w_1' = \\max(w_1, w_2, w_3) = 2$.\n",
    "- $w_2' = \\max(w_3, w_4, w_5) = 5$.\n",
    "- $w_3' = \\max(w_6, w_7, w_8) = 8$.\n",
    "\n",
    "Where:\n",
    "\n",
    "- $w'_i$ is the $i$-th element of the output.\n",
    "- $w_i$ is the $i$-th element of the input.\n",
    "\n",
    "**Note** that the last element was skipped because it couldn't form a complete kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18effc33",
   "metadata": {},
   "source": [
    "## Flatten/unflaten\n",
    "\n",
    "The `torch.nn.Flatten` layer merges some dimensions into a single dimension. Conversely, `torch.nn.Unflatten` reshapes a specified axis with the specified dimensionality. Check more in the [specific page](layers/flatten_unflatten.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b8a5cd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The following example creates array that we'll use as example. You can consider it as 3 three dimentional samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11120953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 3, 3])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.arange(81).reshape([3,3,3,3])\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9be53c",
   "metadata": {},
   "source": [
    "Now lets apply the default `torch.nn.Flatten` to the array from the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01919a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 27])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.nn.Flatten()(input)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23a506c",
   "metadata": {},
   "source": [
    "It seems intuitive to get the same array but with one-dimensional observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cda26c",
   "metadata": {},
   "source": [
    "Now we can revert everything with `torch.nn.Unflatten` — we specify the second dimensionality to be unflattened and transform it back into 3-dimensional tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc94d247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 3, 3])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.nn.Unflatten(dim=1, unflattened_size=(3,3,3))(x)\n",
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
